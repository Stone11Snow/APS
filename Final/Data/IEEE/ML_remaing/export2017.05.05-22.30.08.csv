"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7780197,7780462,7780758,7779917,7778702,7780502,7781103,7780507,7780949,7780473,7780429,7780447,7777885,7780509,7777794,7780432,7780416,7780910,7769202,7780389,7780885,7780431,7780764,7780418,7780615,7779433,7780654,7780588,7780723,7777912,7780823,7780648,7780280,7780584,7780378,7780660,7774462,7771780,7762055,7774759,7752890,7580567,7733086,7772257,7589111,7776343,7777403,7774847,7776482,7777224,7776314,7774674,7776495,7362158,7764764,7765358,7764405,7765565,7738441,7765094,7760071,7758034,7756772,7759936,7760517,7761146,7759587,7761334,7759215,7760571,7762921,7760576,7757700,7760581,7761567,7761105,7758450,7761543,7759677,7760098,7759685,7758049,7758052,7636984,7756666,7752363,7751655,7752381,7754874,7752249,7756144,7750987,7753615,7753399,7752225,7756295,7753690,7752312,7752409,7755734",2017/05/05 22:30:08
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Convolutional neural network-based real-time ROV detection using forward-looking sonar image","J. Kim; S. C. Yu","Dept. of Creative IT Engineering, Pohang University of Science and Technology, South-Korea","2016 IEEE/OES Autonomous Underwater Vehicles (AUV)","20161212","2016","","","396","400","Agent system is strategy to enhance the underwater manipulation. The conventional manipulation is generally robot arm-based configuration which has singular points. On the other hand, the agent system is an armless manipulation that the agent vehicle works as the end-effector. If the location of the agent can be measured, the end effector is able to be place to any position. To implement this system, the method of an agent vehicle localization is proposed. The method uses the sonar images of moving agent obtained by forward-looking sonar. To detect the location of the agent in the sonar images, the convolutional neural network is applied. We applied the state-of-art object-detection algorithm to the agent vehicle system. The fast object-detection algorithm based on neural network can fulfil the real-time detection and show the remarkable validity. It means the underwater robot can begin navigation under its feed-back. Through field experiment, we confirm the proposed method can detect and track the agent in the successive sonar images.","","Electronic:978-1-5090-2442-1; POD:978-1-5090-2443-8; USB:978-1-5090-5716-0","10.1109/AUV.2016.7778702","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778702","agent vehicle;armless manipulation;convolutional neural network;forward-looking sonar;object detection;sonar image processing","Machine learning algorithms;Neural networks;Real-time systems;Sonar detection;Training;Vehicles","autonomous underwater vehicles;end effectors;feedback;neural nets;object detection;object tracking;sonar imaging","agent detection;agent location detection;agent location measurement;agent tracking;agent vehicle localization;agent vehicle system;armless manipulation;convolutional neural network-based real-time ROV detection;end-effector;feedback;forward-looking sonar image;object-detection algorithm;underwater manipulation enhancement;underwater robot","","","","","","","6-9 Nov. 2016","","IEEE","IEEE Conference Publications"
"Exploiting tree structures for classifying programs by functionalities","Viet Anh Phan; Ngoc Phuong Chau; Minh Le Nguyen","Japan Advanced Institute of Science and Technology, Japan","2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","20161201","2016","","","85","90","Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. The aim of this paper is to apply several natural language processing techniques, which deal with software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. To speed up computational time, we propose a pruning tree technique to eliminate redundant branches of ASTs. In addition, the k-Nearest Neighbor (kNN) algorithm was adopted to compare with other methods whereby the distance between programs is measured by using the tree edit distance (TED) and the Levenshtein distance. These algorithms are evaluated based on the performance of solving 104-label program classification problem. The experiments show that due to the use of appropriate data structures although kNN is a simple machine learning algorithm, the classifiers achieve the promising results.","","Electronic:978-1-4673-8929-7; POD:978-1-4673-8930-3","10.1109/KSE.2016.7758034","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758034","","Convolutional codes;Feature extraction;Machine learning algorithms;Software;Software engineering;Software metrics;Syntactics","computational linguistics;learning (artificial intelligence);natural language processing;pattern classification;software engineering;source code (software);tree data structures","AST;Levenshtein distance;TED;data structures;k-nearest neighbor algorithm;machine learning;natural language processing;program abstract syntax trees;program classification;pruning tree;software engineering;source code;tree edit distance","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Underdetermined Convolutive Source Separation Using GEM-MU With Variational Approximated Optimum Model Order NMF2D","A. Al-Tmeme; W. L. Woo; S. S. Dlay; B. Gao","School of Electrical and Electronic Engineering, on leave from the Al Khwarizmi College of Engineering, Newcastle University, University of Baghdad, Newcastle upon Tyne, U.K.Iraq","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20161128","2017","25","1","35","49","An unsupervised machine learning algorithm based on nonnegative matrix factor Two-dimensional deconvolution (NMF2D) with approximated optimum model order is proposed. The proposed algorithm adapted under the hybrid framework that combines the generalized EM algorithm with multiplicative update. As the number of parameters in the NMF2D grows exponentially the number of frequency basis increases linearly, the issues of model-order fitness, initialization, and parameters estimation become ever more critical. This paper proposes a variational Bayesian method to optimize the number of components in the NMF2D by using the Gamma-Exponential process as the observation-latent model. In addition, it is shown that the proposed Gamma-Exponential process can be used to initialize the NMF2D parameters. Finally, the paper investigates the issue and advantages of using different window length. Experimental results for the synthetic convolutive mixtures and live recordings verify the competence of the proposed algorithm.","2329-9290;23299290","","10.1109/TASLP.2016.2620600","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7636984","Audio source separation;generalized expectation-maximization algorithm;nonnegative matrix factorization;optimum model order selection;variational Bayesian","Adaptation models;Approximation algorithms;IEEE transactions;Machine learning algorithms;Source separation;Speech;Time-frequency analysis","Bayes methods;approximation theory;convolution;deconvolution;expectation-maximisation algorithm;matrix decomposition;parameter estimation;source separation;unsupervised learning;variational techniques","GEM-MU algorithm;NMF2D parameter;gamma-exponential process;generalized EM algorithm;generalized expectation-maximization algorithm;model-order fitness;nonnegative matrix factor two-dimensional deconvolution;observation-latent model;parameter estimation;synthetic convolutive mixture;underdetermined convolutive source separation;unsupervised machine learning algorithm;variational Bayesian method;variational approximated optimum model order","","","","","","20161024","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks","K. Lin; J. Lu; C. S. Chen; J. Zhou","Inst. of Inf. Sci., Taipei, Taiwan","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","1183","1192","In this paper, we propose a new unsupervised deep learning approach called DeepBit to learn compact binary descriptor for efficient visual object matching. Unlike most existing binary descriptors which were designed with random projections or linear hash functions, we develop a deep neural network to learn binary descriptors in an unsupervised manner. We enforce three criterions on binary codes which are learned at the top layer of our network: 1) minimal loss quantization, 2) evenly distributed codes and 3) uncorrelated bits. Then, we learn the parameters of the networks with a back-propagation technique. Experimental results on three different visual analysis tasks including image matching, image retrieval, and object recognition clearly demonstrate the effectiveness of the proposed approach.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.133","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780502","","Binary codes;Machine learning;Neural networks;Optimization;Quantization (signal);Training data;Visualization","backpropagation;file organisation;image matching;image retrieval;neural nets;object recognition","DeepBit;backpropagation;compact binary descriptors;image matching;image retrieval;linear hash functions;object recognition;unsupervised deep learning;unsupervised deep neural networks;visual object matching","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Healthcare Big Data Voice Pathology Assessment Framework","M. S. Hossain; G. Muhammad","Department of Software Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Access","20161202","2016","4","","7806","7815","The fast-growing healthcare big data plays an important role in healthcare service providing. Healthcare big data comprise data from different structured, semi-structured, and unstructured sources. These data sources vary in terms of heterogeneity, volume, variety, velocity, and value that traditional frameworks, algorithms, tools, and techniques are not fully capable of handling. Therefore, a framework is required that facilitates collection, extraction, storage, classification, processing, and modeling of this vast heterogeneous volume of data. This paper proposes a healthcare big data framework using voice pathology assessment (VPA) as a case study. In the proposed VPA system, two robust features, MPEG-7 low-level audio and the interlaced derivative pattern, are used for processing the voice or speech signals. The machine learning algorithms in the form of a support vector machine, an extreme learning machine, and a Gaussian mixture model are used as the classifier. In the experiments, the proposed VPA system shows its efficiency in terms of accuracy and time requirement.","2169-3536;21693536","","10.1109/ACCESS.2016.2626316","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738441","Healthcare big data;classification;feature extraction;voice pathology","Big data;Biomedical monitoring;Classification;Data mining;Feature extraction;Machine learning algorithms;Medical services;Pathology","Big Data;Gaussian processes;audio signal processing;feature extraction;health care;learning (artificial intelligence);mixture models;pattern classification;speech processing;support vector machines","Gaussian mixture model;MPEG-7 low-level audio;VPA system;extreme learning machine;healthcare Big Data voice pathology assessment framework;interlaced derivative pattern;machine learning algorithm;support vector machine","","","","","","20161108","2016","","IEEE","IEEE Journals & Magazines"
"Deep learning for financial sentiment analysis on finance news providers","M. Y. Day; C. C. Lee","Department of Information Management, Tamkang University, Taiwan","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","1127","1134","Investors have always been interested in stock price forecasting. Since the development of electronic media, hundreds pieces of financial news are released on different media every day. Numerous studies have attempted to examine whether the stock price forecasting through text mining technology and machine learning could lead to abnormal returns. However, few of them involved the discussion on whether using different media could affect forecasting results. Financial sentiment analysis is an important research area of financial technology (FinTech). This research focuses on investigating the influence of using different financial resources to investment and how to improve the accuracy of forecasting through deep learning. The experimental result shows various financial resources have significantly different effects to investors and their investments, while the accuracy of news categorization could be improved through deep learning.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752381","Deep Learning;Finance News Providers;Financial Sentiment Analysis;Financial Technology (FinTech);Stock Prediction","Forecasting;Investment;Machine learning;Media;Share prices;Text mining","data mining;financial management;learning (artificial intelligence);sentiment analysis","FinTech;deep learning;electronic media;finance news providers;financial sentiment analysis;financial technology;machine learning;stock price forecasting;text mining technology","","","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"Steps in deployment and development of Convolutional Neural Network based applications","Ş. M. Măduţa; C. D. Căleanu","Faculty of Electronics and Telecommunications, POLITEHNICA University, Timi&#x015F;oara, Rom&#x00E2;nia","2016 12th IEEE International Symposium on Electronics and Telecommunications (ISETC)","20161212","2016","","","247","251","Designated among 10 breakthrough technologies by MIT Technology Review [1], Deep Learning (DL) outperform current approaches in many situations, e.g. image or speech processing. One of the most important deep architecture is represented by the Convolutional Neural Network (CNN). The purpose of this paper is to provide practical recommendations in the deployment and development of the CNN based applications. They refer to the hardware as well as software available solutions and go beyond by providing guidance in choosing the appropriate hyper-parameters (structure, training algorithm, learning rate, regularization techniques, etc.). The experimental results are reported using the CIFAR-10 dataset.","","Electronic:978-1-5090-3748-3; POD:978-1-5090-3749-0","10.1109/ISETC.2016.7781103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7781103","Convolutional Neural Networks;Deep learning;hyper-parameters","Computer architecture;Graphics processing units;Hardware;Machine learning;Neural networks;Training","feedforward neural nets;learning (artificial intelligence)","CIFAR-10 dataset;CNN based applications;DL;MIT Technology Review;convolutional neural network based application deployment;convolutional neural network based application development;deep architecture;deep learning","","","","","","","27-28 Oct. 2016","","IEEE","IEEE Conference Publications"
"Recurrent Attention Models for Depth-Based Person Identification","A. Haque; A. Alahi; L. Fei-Fei","Comput. Sci. Dept., Stanford Univ., Stanford, CA, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","1229","1238","We present an attention-based model that reasons on human body shape and motion dynamics to identify individuals in the absence of RGB information, hence in the dark. Our approach leverages unique 4D spatio-temporal signatures to address the identification problem across days. Formulated as a reinforcement learning task, our model is based on a combination of convolutional and recurrent neural networks with the goal of identifying small, discriminative regions indicative of human identity. We demonstrate that our model produces state-of-the-art results on several published datasets given only depth images. We further study the robustness of our model towards viewpoint, appearance, and volumetric changes. Finally, we share insights gleaned from interpretable 2D, 3D, and 4D visualizations of our model's spatio-temporal attention.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.138","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780507","","Computational modeling;Machine learning;Random access memory;Shape;Tensile stress;Three-dimensional displays;Training","feedforward neural nets;learning (artificial intelligence);motion estimation;recurrent neural nets;spatiotemporal phenomena","2D visualizations;3D visualizations;4D spatiotemporal signatures;4D visualizations;attention-based model;convolutional neural networks;depth-based person identification;human body shape;human identity;model spatiotemporal attention;motion dynamics;recurrent attention models;recurrent neural networks;reinforcement learning task","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Partially Shared Deep Neural Network in sound source separation and identification using a UAV-embedded microphone array","T. Morito; O. Sugiyama; R. Kojima; K. Nakadai","Graduate School of Information Science and Engineering, Tokyo Institute of Technology, 2-12-1, O-okayama, Meguro-ku, 152-8552, JAPAN","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20161201","2016","","","1299","1304","This paper addresses sound source separation and identification for noise-contaminated acoustic signals recorded with a microphone array embedded in an Unmanned Aerial Vehicle (UAV), aiming at people's voice detection quickly and widely in a disaster situation. The key approach to achieve this is Deep Neural Network (DNN), but it is well known that training a DNN needs a huge dataset to improve its performance. In a practical application, building such a dataset is not often realistic owing to the cost of manual data annotation. Therefore, we propose a Partially-Shared Deep Neural Network (PS-DNN) which can learn multiple tasks at the same time with a small amount of annotated data. Preliminary results show that the PS-DNN outperforms conventional DNN-based approaches which require fully-annotated data in training in terms of identification accuracy. In addition, it maintains performance even when noise-suppressed signals are used for sound source separation training, and partially annotated data is used for sound source identification training.","","Electronic:978-1-5090-3762-9; POD:978-1-5090-3763-6; USB:978-1-5090-3761-2","10.1109/IROS.2016.7759215","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759215","deep learning;robot audition;sound source identification;sound source separation","Acoustics;Machine learning;Microphone arrays;Neural networks;Source separation;Training","autonomous aerial vehicles;microphone arrays;neural nets;source separation","PS-DNN;UAV-embedded microphone array;noise-contaminated acoustic signals;shared deep neural network;sound source identification training;sound source separation;unmanned aerial vehicle","","","","","","","9-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Big data and clustering algorithms","V. W. Ajin; L. D. Kumar","Department of Computer Science, SCT College of Engineering, Trivandrum, India","2016 International Conference on Research Advances in Integrated Navigation Systems (RAINS)","20161205","2016","","","1","5","Data mining is the method which is useful for extracting useful information and data is extorted, but the classical data mining approaches cannot be directly used for big data due to their absolute complexity. The data that is been formed by numerous scientific applications and incorporated environment has grown rapidly not only in size but also in variety in recent era. The data collected is of very large amount and there is difficulty in collecting and assessing big data. Clustering algorithms have developed as a powerful meta learning tool which can precisely analyze the volume of data produced by modern applications. The main goal of clustering is to categorize data into clusters such that objects are grouped in the same cluster when they are “similar” according to similarities, traits and behavior. The most commonly used algorithm in clustering are partitioning, hierarchical, grid based, density based, and model based algorithms. A review of clustering and its different techniques in data mining is done considering the criteria's for big data. Where most commonly used and effective algorithms like K-Means, FCM, BIRCH, CLIQUE algorithms are studied and compared on big data perspective.","","Electronic:978-1-5090-1111-7; POD:978-1-5090-1112-4","10.1109/RAINS.2016.7764405","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7764405","BIRCH;BigData;CLIQUE;Clustering;FCM;K-Means","Algorithm design and analysis;Approximation algorithms;Big data;Clustering algorithms;Data mining;Machine learning algorithms;Partitioning algorithms","Big Data;data analysis;data mining;learning (artificial intelligence);natural sciences computing;pattern clustering","Big Data;FCM;birch algorithm;clique algorithms;clustering algorithms;data analysis;data mining;density based algorithm;grid based algorithm;hierarchical algorithm;information extraction;k-means algorithm;meta learning tool;model based algorithms;partitioning algorithm;scientific applications","","","","","","","6-7 May 2016","","IEEE","IEEE Conference Publications"
"Study of transductive learning and unsupervised feature construction methods for biological sequence classification","A. Stanescu; K. Tangirala; D. Caragea","Computing and Information Sciences, Kansas State University, Manhattan, KS, U.S.A.","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","999","1006","Next Generation Sequencing (NGS) technologies have led to fast and inexpensive production of large amounts of biological sequence data, including nucleotide sequences and derived protein sequences. These fast-increasing volumes of data pose challenges to computational methods for annotation. Machine learning approaches, primarily supervised algorithms, have been widely used to assist with classification tasks in bioinformatics. However, supervised algorithms rely on large amounts of labeled data in order to produce quality predictors. Oftentimes, labeled data is difficult and expensive to acquire in sufficiently large quantities. When only limited amounts of labeled data but considerably larger amounts of unlabeled data are available for a specific annotation problem, semi-supervised learning approaches represent a cost-effective alternative. In this work, we focus on a special case of semi-supervised learning, namely transductive learning, in which the algorithm has access during the training phase to the instances that need to be labeled. Transduction is particularly suitable for biological sequence classification, where the goal is generally to label a given set of unlabeled instances. However, a challenge that needs to be addressed in this context consists of identification of compact sets of informative features. Given the lack of labeled data, standard supervised feature selection methods may result in unreliable features. Therefore, we study recently proposed unsupervised feature construction approaches together with transductive learning. Experimental results on two classification problems, namely cassette exon identification and protein localization, show that the unsupervised features result in better performance than the supervised features.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752363","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752363","","Bioinformatics;DNA;Machine learning algorithms;Prediction algorithms;Proteins;Support vector machines","bioinformatics;learning (artificial intelligence);pattern classification;proteins","NGS technologies;bioinformatics;biological sequence classification;biological sequence data;cassette exon identification;machine learning;next generation sequencing technologies;nucleotide sequences;protein localization;protein sequences;semisupervised learning;standard supervised feature selection methods;supervised algorithms;transductive learning;unsupervised feature construction methods","","","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"Learning Deep Representation for Imbalanced Classification","C. Huang; Y. Li; C. C. Loy; X. Tang","Dept. of Inf. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","5375","5384","Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional neural network (CNN) typically follow classic strategies such as class re-sampling or cost-sensitive training. In this paper, we conduct extensive and systematic experiments to validate the effectiveness of these classic schemes for representation learning on class-imbalanced data. We further demonstrate that more discriminative deep representation can be learned by enforcing a deep network to maintain both intercluster and inter-class margins. This tighter constraint effectively reduces the class imbalance inherent in the local data neighborhood. We show that the margins can be easily deployed in standard deep learning framework through quintuplet instance sampling and the associated triple-header hinge loss. The representation learned by our approach, when combined with a simple k-nearest neighbor (kNN) algorithm, shows significant improvements over existing methods on both high-and low-level vision classification tasks that exhibit imbalanced class distribution.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.580","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780949","","Face;Fasteners;Feature extraction;Image edge detection;Machine learning;Neural networks;Training","convolution;image classification;image representation;learning (artificial intelligence);neural nets;sampling methods","CNN;contemporary classification;deep convolutional neural network;deep learning representation;discriminative deep representation;highly-skewed class distribution;imbalanced classification;k-nearest neighbor;kNN;quintuplet instance sampling;vision classification tasks;vision domain","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"A very short term wind power prediction approach based on Multilayer Restricted Boltzmann Machine","Xiaosheng Peng; Lei Xiong; Jinyu Wen; Yuan Xu; Wenhan Fan; Shuanglei Feng; Bo Wang","School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China","2016 IEEE PES Asia-Pacific Power and Energy Engineering Conference (APPEEC)","20161212","2016","","","2409","2413","The wind power prediction (WPP) is challenging as a large amount of data with complex nonlinear relationship should be fitted by the prediction method. To improve the accuracy, WPP based on the Multilayer Restricted Boltzmann Machine (MRBM), which is a deep learning neural network with strong feature interpretation ability, is presented in the paper. To explore the influencing factors of prediction accuracy, the number of hidden layers and the number of nodes in each layer of MRBM are studied. Furthermore, the classic Back Propagation Neural Network (BPNN) based WPP, as a reference, is compared with the MRBM method. The results show that the accuracy of MRBM based WPP is higher than that of BPNN based WPP. The Root Mean Square Error (RMSE) of the MRBM based prediction is 4.5% lower than that of BPNN in some period, and the error distribution of MRBM based WPP is with better concentration ability than that of BPNN based WPP.","","Electronic:978-1-5090-5418-3; POD:978-1-5090-5419-0","10.1109/APPEEC.2016.7779917","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779917","Deep Learning;Error Distribution;Restricted Boltzmann Machine;Wind Power Prediction","Feature extraction;Machine learning;Neurons;Testing;Training;Unsupervised learning;Wind power generation","Boltzmann machines;backpropagation;load forecasting;mean square error methods;power engineering computing;wind power","BPNN based WPP;MRBM based WPP;RMSE;WPP;classic backpropagation neural network;deep learning neural network;error distribution;feature interpretation ability;hidden layer;multilayer restricted Boltzmann machine;prediction method;root mean square error;very short term wind power prediction approach","","","","","","","25-28 Oct. 2016","","IEEE","IEEE Conference Publications"
"ASP Vision: Optically Computing the First Layer of Convolutional Neural Networks Using Angle Sensitive Pixels","H. G. Chen; S. Jayasuriya; J. Yang; J. Stephen; S. Sivaramakrishnan; A. Veeraraghavan; A. Molnar","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","903","912","Deep learning using convolutional neural networks (CNNs) is quickly becoming the state-of-the-art for challenging computer vision applications. However, deep learning's power consumption and bandwidth requirements currently limit its application in embedded and mobile systems with tight energy budgets. In this paper, we explore the energy savings of optically computing the first layer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs), custom CMOS diffractive image sensors which act similar to Gabor filter banks in the V1 layer of the human visual cortex. ASPs replace both image sensing and the first layer of a conventional CNN by directly performing optical edge filtering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our experimental results (both on synthetic data and a hardware prototype) for a variety of vision tasks such as digit recognition, object recognition, and face identification demonstrate 97% reduction in image sensor power consumption and 90% reduction in data bandwidth from sensor to CPU, while achieving similar performance compared to traditional deep learning pipelines.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780473","","Bandwidth;Biomedical optical imaging;Image sensors;Machine learning;Optical device fabrication;Optical imaging;Optical sensors","computer vision;convolution;image filtering;learning (artificial intelligence);neural nets","ASP vision;CNN;angle sensitive pixel;computer vision;convolutional neural network;deep learning;optical edge filtering","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Auto-tuning Spark big data workloads on POWER8: Prediction-based dynamic SMT threading","Z. Jia; C. Xue; G. Chen; J. Zhan; L. Zhang; Y. Lin; P. Hofstee","Institute of Computing Technology, Chinese Academy of Sciences, China","2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)","20161201","2016","","","387","400","Much research work devotes to tuning big data analytics in modern data centers, since even a small percentage of performance improvement immediately translates to huge cost savings because of the large scale. Simultaneous multithreading (SMT) receives great interest from data center communities, as it has the potential to boost performance of big data analytics by increasing the processor resources utilization. For example, the emerging processor architectures like POWER8 support up to 8-way multithreading. However, as different big data workloads have disparate architectural characteristics, how to identify the most efficient SMT configuration to achieve the best performance is challenging in terms of both complex application behaviors and processor architectures. In this paper, we specifically focus on auto-tuning SMT configuration for Spark-based big data workloads on POWER8. However, our methodology could be generalized and extended to other programming software stacks and other architectures. We propose a prediction-based dynamic SMT threading (PBDST) framework to adjust the thread count in SMT cores on POWER8 processors by using versatile machine learning algorithms. Its innovation lies in adopting online SMT configuration predictions derived from microarchitecture level profiling, to regulate the thread counts that could achieve nearly optimal performance. Moreover it is implemented at Spark software stack layer and transparent to user applications. After evaluating a large set of machine learning algorithms, we choose the most efficient ones to perform online predictions. The experimental results demonstrate that our approach can achieve up to 56.3% performance improvement and an average performance gain of 16.2% in comparison with the default configuration-the maximum SMT configuration-SMT8 on our system.","","Electronic:978-1-4503-4121-9; POD:978-1-5090-5308-7","10.1145/2967938.2967957","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756772","Dynamic SMT tuning;POWER8;Spark big data","Big data;Hardware;Instruction sets;Machine learning algorithms;Sparks;Training","Big Data;computer centres;data analysis;learning (artificial intelligence);multi-threading","8-way multithreading;PBDST framework;POWER8 processors;Spark big data workloads autotuning;Spark software stack layer;big data analytics;data centers;microarchitecture level profiling;online SMT configuration predictions;prediction-based dynamic SMT threading;processor architectures;processor resources utilization;simultaneous multithreading;versatile machine learning algorithms","","","","","","","11-15 Sept. 2016","","IEEE","IEEE Conference Publications"
"Zero-shot Image Classification Based on Deep Feature Extraction","X. Wang; C. Chen; Y. Cheng; Z. J. Wang","professor in the School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou 221116, China. (email: chengyuhu@163.com)","IEEE Transactions on Cognitive and Developmental Systems","","2016","PP","99","1","1","The attribute-based zero-shot learning methods generally use low-level features of images to train attribute classifiers, and the corresponding classification accuracy heavily depends on specific low-level features. Because deep networks can automatically extract features from original unlabeled images and the extracted features can better represent the nature of original images, we proposed a zero-shot image classification method based on deep feature extraction. In the image preprocessing step, in order to reduce the computational complexity and the correlations between pixels, image patches extraction and ZCA whitening are performed. The compressed feature representations of unlabeled image patches are learnt through a stacked sparse autoencoder and a feature mapping matrix can be obtained. Further, we use the feature mapping matrix as a convolution kernel to convolve with image patches. Since the convolution operation results in the feature vector with huge dimensionality, the convolution features will be pooled to reduce the number of network parameters and to reduce the spatial resolution of the network to prevent over-fitting. Finally, the exacted image features are used to train the conventional indirect attribute prediction model to predict image attributes and classify images under the zero-shot setting. Experimental results on the Shoes, OSR and a-Yahoo datasets show that, compared with several popular zero-shot learning methods, the proposed method can yield more accurate attribute prediction and better zero-shot image classification.","2379-8920;23798920","","10.1109/TCDS.2016.2632178","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755734","attribute;convolutional neural network;feature;stacked sparse autoencoder;zero-shot learning","Feature extraction;Image classification;Machine learning;Neural networks;Predictive models;Semantics;Training","","","","","","","","20161123","","","IEEE","IEEE Early Access Articles"
"Polarimetric SAR Image Classification Using Deep Convolutional Neural Networks","Y. Zhou; H. Wang; F. Xu; Y. Q. Jin","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, shanghai, China","IEEE Geoscience and Remote Sensing Letters","20161208","2016","13","12","1935","1939","Deep convolutional neural networks have achieved great success in computer vision and many other areas. They automatically extract translational-invariant spatial features and integrate with neural network-based classifier. This letter investigates the suitability and potential of deep convolutional neural network in supervised classification of polarimetric synthetic aperture radar (POLSAR) images. The multilooked POLSAR data in the format of coherency or covariance matrix is first converted into a normalized 6-D real feature vector. The six-channel real image is then fed into a four-layer convolutional neural network tailored for POLSAR classification. With two cascaded convolutional layers, the designed deep neural network can automatically learn hierarchical polarimetric spatial features from the data. Two experiments are presented using the AIRSAR data of San Francisco, CA, and Flevoland, The Netherlands. Classification result of the San Francisco case shows that slant built-up areas, which are conventionally mixed with vegetated area in polarimetric feature space, can now be successfully distinguished after taking into account spatial features. Quantitative analysis with respect to ground truth information available for the Flevoland test site shows that the proposed method achieves an accuracy of 92.46% in classifying the considered 15 classes. Such results are comparable with the state of the art.","1545-598X;1545598X","","10.1109/LGRS.2016.2618840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762055","Deep convolutional neural network;polarimetric synthetic aperture radar (POLSAR);supervised classification","Convolution;Data mining;Feature extraction;Machine learning;Neural networks;Synthetic aperture radar;Training","geophysical image processing;image classification;neural nets;radar polarimetry;remote sensing by radar;synthetic aperture radar","AIRSAR data;California;Flevoland;Netherlands;POLSAR images;San Francisco;USA;computer vision;covariance matrix;deep convolutional neural networks;neural network based classifier;normalized 6D real feature vector;polarimetric SAR image classification;polarimetric feature space;polarimetric synthetic aperture radar images;translational invariant spatial features;vegetated area","","","","","","20161129","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"Farmland Scene Classification Based on Convolutional Neural Network","Z. Deli; C. Bingqi; Y. Yunong","Coll. of Eng., China Agric. Univ., Beijing, China","2016 International Conference on Cyberworlds (CW)","20161124","2016","","","159","162","This paper proposed a farmland scene classification method based on CNN (Convolutional neural network). The farmland image datasets are divided into 4 types, namely, Crops_field, House_field, Not_farming_field and Woods_field. There are 100 pictures in each type, 80 images in each type are used as training sets, and the remaining 20 images are processed as test sets. Design a CNN with 2 convolution layers and 2 sub sample layers.In the training process, input images are restricted to 64*64, and the convolutional kernel is 5*5. Use the opensource toolkit of deep learning namely Tensorflow as the realization platform. After 700 times trainings, we validated the effects on the dataset, The corresponding correct rates of the four scenes are 79%, 82%, 76% and 75%. The result show that this method can achieve satisfactory effect.","","Electronic:978-1-5090-2303-5; POD:978-1-5090-2304-2","10.1109/CW.2016.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756144","CNN;deep learning;deep neural network;feature extraction","Convolution;Feature extraction;Kernel;Machine learning;Neural networks;Neurons;Training","convolution;crops;farming;image classification;learning (artificial intelligence);neural nets","CNN;convolutional kernel;convolutional neural network;crops field;deep learning;farmland image datasets;farmland scene classification;house field;image processing;opensource toolkit;tensorflow;woods field","","","","","","","28-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Investigating the possibility of using a single electrode brain-computer interface device for human machine interaction by means of cluster analysis","A. H. Ali; R. S. H. Al-Musawi","Department of Electronic and Communications Engineering, University of Kufa, PO. Box 21, Najaf, Iraq 54001","2016 Al-Sadeq International Conference on Multidisciplinary in IT and Communication Science and Applications (AIC-MITCSA)","20161201","2016","","","1","6","The use of a consumer-grade Brain-Computer Interface (BCI) has seen significant interests among researchers and hobbyists like communities. It has been suggested as a viable mean to control robots, improve learning experience and even to classify thought patterns. This paper investigates the possibility of using the NeuroSky Mindwave headset, a very cheap and popular single electrode BCI, for such endeavors by means of unsupervised machine learning algorithms. Firstly, the raw Electroencephalography (EEG) signals from 10 different subjects were acquired while they performed various mental activities. The mental activities ranged from listening to relaxing music to doing mathematical calculations. Secondly, the EEG signals were filtered to obtain the Gamma, Beta, Alpha, Theta and Delta brainwaves. Finally, k-means, fuzzy c-means and Self-Organizing Maps (SOMs) clustering algorithms have been applied to group the brainwaves according to their similarities. The performance of the cluster algorithms was benchmarked using distance metric maps, cluster silhouettes, Calinski-Harabasz index and Davies-Bouldin index. K-means clustering algorithm has showed some power of separating different mental activities into groups. The minimum Mean Silhouette Value has been found to be 0.475 when the number of clusters is 3 and the highest CH-index registered has been 65.7. These results show an interesting possibility for using the MindWave headset in applications where the number of mental activities to be harvested may not be greater than 2 or 3 at most.","","Electronic:978-1-5090-3247-1; POD:978-1-5090-3248-8","10.1109/AIC-MITCSA.2016.7759936","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759936","Calinski-Harabasz index;Davies-Bouldin index;EEG signal;FCM;MindWave;SOMs;cluster analysis;k-means","Algorithm design and analysis;Clustering algorithms;Digital filters;Electroencephalography;Headphones;Machine learning algorithms;Neurons","biomedical electrodes;brain-computer interfaces;electroencephalography;fuzzy set theory;headphones;human computer interaction;medical signal processing;pattern clustering;self-organising feature maps;unsupervised learning","Calinski-Harabasz index;Davies-Bouldin index;EEG signals;NeuroSky Mindwave headset;SOM clustering algorithms;alpha brainwaves;beta brainwaves;cluster analysis;cluster silhouettes;delta brainwaves;distance metric maps;electroencephalography signal;fuzzy c-means;gamma brainwaves;human machine interaction;k-means clustering algorithm;mental activities;minimum mean silhouette value;self-organizing maps;single electrode BCI;single electrode brain-computer interface device;theta brainwaves;unsupervised machine learning algorithms","","","","","","","9-10 May 2016","","IEEE","IEEE Conference Publications"
"Going deeper than deep learning for massive data analytics under physical constraints","B. D. Rouhani; A. Mirhoseini; F. Koushanfar","UC San Diego","2016 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","20161124","2016","","","1","3","Deep Neural Networks (DNNs) are a set of powerful yet computationally complex learning mechanisms that are projected to dominate various artificial intelligence and massive data analytic domains. Physical viability, such as timing, memory, or energy efficiency, are standing challenges in realizing the true potential of DNNs. We propose DeLight, a set of novel methodologies which aim to bring physical constraints as design parameters in the training and execution of DNN architectures. We use physical profiling to bound the network size in accordance to the pertinent platform's characteristics. An automated customization methodology is proposed to adaptively conform the DNN configurations to meet the characterization of the underlying hardware while minimally affecting the inference accuracy. The key to our approach is a new content- and resource-aware transformation of data to a lower-dimensional embedding by which learning the correlation between data samples requires significantly smaller number of neurons. We leverage the performance gain achieved as a result of the data transformation to enable the training of multiple DNN architectures that can be aggregated to further boost the inference accuracy. An accompanying API is also developed, which can be used for rapid prototyping of an arbitrary DNN application customized to the platform. Proof-of concept evaluations for deployment of different imaging, audio, and smart-sensing applications demonstrate up to 100-fold performance improvement compared to the state-of-the-art DNN solutions.","","Electronic:978-1-4503-4483-8; POD:978-1-5090-3590-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7750987","","Computational modeling;Data models;Machine learning;Neural networks;Neurons;Runtime;Training","Big Data;application program interfaces;computational complexity;data analysis;learning (artificial intelligence);neural nets;resource allocation","API;DNN architectures;DNN configurations;DeLight;artificial intelligence;automated customization methodology;computationally complex learning mechanisms;content-aware data transformation;deep learning;deep neural networks;energy efficiency;lower-dimensional embedding;massive data analytics;physical constraints;physical viability;platform characteristics;rapid prototyping;resource-aware data transformation;smart-sensing applications","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Composition-Preserving Deep Photo Aesthetics Assessment","L. Mai; H. Jin; F. Liu","Portland State Univ., Portland, OR, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","497","506","Photo aesthetics assessment is challenging. Deep convolutional neural network (ConvNet) methods have recently shown promising results for aesthetics assessment. The performance of these deep ConvNet methods, however, is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, scaling, or padding, which often damages image composition, reduces image resolution, or causes image distortion, thus compromising the aesthetics of the original images. In this paper, we present a composition-preserving deep Con-vNet method that directly learns aesthetics features from the original input images without any image transformations. Specifically, our method adds an adaptive spatial pooling layer upon the regular convolution and pooling layers to directly handle input images with original sizes and aspect ratios. To allow for multi-scale feature extraction, we develop the Multi-Net Adaptive Spatial Pooling ConvNet architecture which consists of multiple sub-networks with different adaptive spatial pooling sizes and leverage a scene-based aggregation layer to effectively combine the predictions from multiple sub-networks. Our experiments on the large-scale aesthetics assessment benchmark (AVA [29]) demonstrate that our method can significantly improve the state-of-the-art results in photo aesthetics assessment.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780429","","Adaptive systems;Computer vision;Feature extraction;Machine learning;Neural networks;Training;Visualization","feature extraction;learning (artificial intelligence);neural net architecture;visual databases","AVA large-scale aesthetics assessment benchmark;adaptive spatial pooling sizes;aesthetic feature learning;aspect ratios;composition-preserving deep-ConvNet method;composition-preserving deep-photo aesthetics assessment;convolution layers;deep convolutional neural network;image composition;image cropping;image distortion;image padding;image resolution;image scaling;input images;multinet adaptive spatial pooling ConvNet architecture;multiscale feature extraction;scene-based aggregation layer","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Deep Saliency with Encoded Low Level Distance Map and High Level Features","G. Lee; Y. W. Tai; J. Kim","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","660","668","Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regions in a scene. These advances have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection. In this paper, we demonstrate that hand-crafted features can provide complementary information to enhance performance of saliency detection that utilizes only high level features. Our method utilizes both high level and low level features for saliency detection under a unified deep learning framework. The high level features are extracted using the VGG-net, and the low level features are compared with other parts of an image to form a low level distance map. The low level distance map is then encoded using a convolutional neural network(CNN) with multiple 1 1 convolutional and ReLU layers. We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region. Our experiments show that our method can further improve the performance of state-of-the-art deep learning-based saliency detection methods.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.78","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780447","","Benchmark testing;Computer architecture;Feature extraction;Image color analysis;Image segmentation;Machine learning;Microprocessors","feature extraction;image classification;neural nets","CNN;ReLU layers;VGG-net;convolutional neural network;feature extraction;fully connected neural network classifier;hand-crafted features;high level features;low level distance map;low level features;saliency detection;unified deep learning framework","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Network classification using adjacency matrix embeddings and deep learning","K. Wu; P. Watters; M. Magdon-Ismail","Department of Computer Science, Rensselaer Polytechnic Institute, Troy, New York 12180","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","299","306","We study a natural problem: Given a small piece of a large parent network, is it possible to identify the parent network? We approach this problem from two perspectives. First, using several “sophisticated” or “classical” network features that have been developed over decades of social network study. These features measure aggregate properties of the network and have been found to take on distinctive values for different types of network, at the large scale. By using these classical features within a standard machine learning framework, we show that one can identify large parent networks from small (even 8-node) subgraphs. Second, we present a novel adjacency matrix embedding technique which converts the small piece of the network into an image and, within a deep learning framework, we are able to obtain prediction accuracies upward of 80%, which is comparable to or slightly better than the performance from classical features. Our approach provides a new tool for topology-based prediction which may be of interest in other network settings. Our approach is plug and play, and can be used by non-domain experts. It is an appealing alternative to the often arduous task of creating domain specific features using domain expertise.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752249","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752249","","Aggregates;Computer science;Electronic mail;Kernel;Machine learning;Social network services;Standards","graph theory;learning (artificial intelligence);matrix algebra;network theory (graphs);pattern classification","8-node subgraphs;adjacency matrix embeddings;deep learning;domain expertise;domain specific features;machine learning framework;network classification;network features;network settings;nondomain experts;parent network;social network;topology-based prediction","","","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"A Method for Special Vehicle Recognition Based on Deep-Transfer Model","Y. Chen; C. Yang; S. Yang","Sch. of Electr. Eng. & Autom., Harbin Inst. of Technol. Harbin, Harbin, China","2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)","20161208","2016","","","167","170","As an application of image recognition, special vehicle recognition is very important in military field. This paper proposes a deep-transfer model (DTM) to overcome the problems in existing recognition methods. The DTM combines deep-learning and transfer-learning to solve the difficulty in training deep model with insufficient simples, improving the performance of the recognition algorithm. At last, the special vehicle dataset is built to evaluate the proposed DTM method. The results demonstrate that the DTM method outperforms the existing method in special vehicle recognition application.","","CD:978-1-5090-1194-0; Electronic:978-1-5090-1195-7; POD:978-1-5090-1196-4","10.1109/IMCCC.2016.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774759","deep learning;image classification;special vehicle recognition;transfer learning","Adaptation models;Feature extraction;Image recognition;Image reconstruction;Machine learning;Training;Vehicles","image classification;learning (artificial intelligence);military vehicles","DTM method;deep-learning;deep-transfer model;image recognition;military field;performance improvement;special vehicle recognition;transfer-learning;vehicle dataset","","","","","","","21-23 July 2016","","IEEE","IEEE Conference Publications"
"Boosted LMS-based piecewise linear adaptive filters","D. Kari; I. Marivani; I. Delibalta; S. S. Kozat","Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","1593","1597","We introduce the boosting notion extensively used in different machine learning applications to adaptive signal processing literature and implement several different adaptive filtering algorithms. In this framework, we have several adaptive constituent filters that run in parallel. For each newly received input vector and observation pair, each filter adapts itself based on the performance of the other adaptive filters in the mixture on this current data pair. These relative updates provide the boosting effect such that the filters in the mixture learn a different attribute of the data providing diversity. The outputs of these constituent filters are then combined using adaptive mixture approaches. We provide the computational complexity bounds for the boosted adaptive filters. The introduced methods demonstrate improvement in the performances of conventional adaptive filtering algorithms due to the boosting effect.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760517","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760517","","Adaptive filters;Boosting;Europe;Machine learning algorithms;Signal processing algorithms","adaptive filters;adaptive signal processing;computational complexity;learning (artificial intelligence);least mean squares methods;piecewise linear techniques","adaptive mixture approach;adaptive signal processing;boosted LMS-based piecewise linear adaptive filter;computational complexity;machine learning application","","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"Resource-constrained implementation and optimization of a deep neural network for vehicle classification","R. Xie; H. Huttunen; S. Lin; S. S. Bhattacharyya; J. Takala","Department of Pervasive Computing, Tampere University of Technology, Finland","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","1862","1866","Deep learning has attracted great research interest in recent years in many signal processing application areas. However, investigation of deep learning implementations in highly resource-constrained contexts has been relatively unexplored due to the large computational requirements involved. In this paper, we investigate the implementation of a deep learning application for vehicle classification on multicore platforms with limited numbers of available processor cores. We apply model-based design methods based on signal processing oriented dataflow models of computation, and using the resulting dataflow representations, we apply various design optimizations to derive efficient implementations on three different multicore platforms. Using model-based design techniques throughout the design process, we demonstrate the ability to flexibly experiment with optimizing design transformations, and alternative multicore target platforms to achieve efficient implementations that are tailored to the resource constraints of these platforms.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760571","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760571","Dataflow;deep learning;model-based design;multicore platforms;signal processing systems","Computational modeling;Convolution;Kernel;Machine learning;Multicore processing;Vehicles","data flow computing;image classification;learning (artificial intelligence);multiprocessing systems;optimisation","alternative multicore target platforms;computational requirements;deep learning application;deep neural network;model-based design methods;processor cores;resource-constrained implementation;resource-constrained optimization;resulting dataflow representations;signal processing oriented dataflow models;vehicle classification","","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"System self-awareness towards deep learning and discovering high-value information","Y. Zhao; C. C. Zhou","Information Sciences Department, Naval Postgraduate School, Monterey, CA 93943, USA","2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","20161212","2016","","","1","8","In this paper, we show a System Self-Awareness concept and theory that can be used to discover authoritative and popular information as well as emerging and anomalous information when traditional connections among information nodes (e.g., hyperlinks or citations) are not available. The different categories of information can be all high-value depending on the application requirements. A System Self-Awareness is a data-driven framework, modeled and measured using a recursive distributed infrastructure named Collaborative Learning Agent and a Deep Learning method named Lexical Link Analysis. The combination of the three allows Deep Reinforcement Learning and Swarm Intelligence to be extended and enhanced in a completely new perspective.","","Electronic:978-1-5090-1496-5; POD:978-1-5090-1497-2","10.1109/UEMCON.2016.7777885","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777885","Big Data;Collaborative Learning Agent;Deep Learning;Lexical Link Analysis;System Self-Awareness;distributed systems;high-value information discovery;modularity;recursion;swarm intelligence","Analytical models;Collaborative work;Context;Current measurement;Machine learning;Semantics;Social network services","learning (artificial intelligence);multi-agent systems;swarm intelligence;text analysis","authoritative information;collaborative learning agent;data-driven framework;deep reinforcement learning;information nodes;lexical link analysis;recursive distributed infrastructure;swarm intelligence;system self-awareness","","","","","","","20-22 Oct. 2016","","IEEE","IEEE Conference Publications"
"Facial Age Estimation with Age Difference","Z. Hu; Y. Wen; J. Wang; M. Wang; R. Hong; S. Yan","School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798 (Email: huzhen.ice@gmail.com)","IEEE Transactions on Image Processing","","2016","PP","99","1","1","Age estimation based on the human face remains a significant problem in computer vision and pattern recognition. In order to estimate an accurate age or age group of a facial image, most of the existing algorithms require a huge face data set attached with age labels. This imposes a constraint on the utilization of the immensely unlabeled or weakly labeled training data, e.g. the huge amount of human photos in the social networks. These images may provide no age label, but it is easily to derive the age difference for an image pair of the same person. To improve the age estimation accuracy, we propose a novel learning scheme to take advantage of these weakly labeled data via the deep Convolutional Neural Networks (CNNs). For each image pair, Kullback-Leibler divergence is employed to embed the age difference information. The entropy loss and the cross entropy loss are adaptively applied on each image to make the distribution exhibit a single peak value. The combination of these losses is designed to drive the neural network to understand the age gradually from only the age difference information. We also contribute a dataset including more than one hundred thousand face images attached with their taken dates. Each image is both labeled with the timestamp and people identity. Experimental results on two aging face databases show the advantages of the proposed age difference learning system and the state-of-the-art performance is gained.","1057-7149;10577149","","10.1109/TIP.2016.2633868","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762921","Age estimation;K-L divergence distance;age difference;convolutional neural networks","Aging;Entropy;Estimation;Face;Feature extraction;Machine learning;Neural networks","","","","","","","","20161201","","","IEEE","IEEE Early Access Articles"
"MiRTDL: A Deep Learning Approach for miRNA Target Prediction","S. Cheng; M. Guo; C. Wang; X. Liu; Y. Liu; X. Wu","School of Computer Science and Technology, Harbin Institute of Technology, 92 West Dazhi Street, Nan Gang District, Harbin, China","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20161207","2016","13","6","1161","1169","MicroRNAs (miRNAs) regulate genes that are associated with various diseases. To better understand miRNAs, the miRNA regulatory mechanism needs to be investigated and the real targets identified. Here, we present miRTDL, a new miRNA target prediction algorithm based on convolutional neural network (CNN). The CNN automatically extracts essential information from the input data rather than completely relying on the input dataset generated artificially when the precise miRNA target mechanisms are poorly known. In this work, the constraint relaxing method is first used to construct a balanced training dataset to avoid inaccurate predictions caused by the existing unbalanced dataset. The miRTDL is then applied to 1,606 experimentally validated miRNA target pairs. Finally, the results show that our miRTDL outperforms the existing target prediction algorithms and achieves significantly higher sensitivity, specificity and accuracy of 88.43, 96.44, and 89.98 percent, respectively. We also investigate the miRNA target mechanism, and the results show that the complementation features are more important than the others.","1545-5963;15455963","","10.1109/TCBB.2015.2510002","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362158","Constraint relaxation;convolutional neural network;miRNA;target prediction","Diseases;Machine learning;Matched filters;Neural networks;Prediction algorithms;RNA;Support vector machines","RNA;bioinformatics;biological techniques;diseases;genetics;learning (artificial intelligence);molecular biophysics;neural nets","CNN;MiRTDL;balanced training dataset;convolutional neural network;deep learning approach;diseases;essential information;genes;miRNA regulatory mechanism;miRNA target pairs;miRNA target prediction algorithm;miRTDL;microRNA;real targets","","1","","","","20151222","November 1 2016","","IEEE","IEEE Journals & Magazines"
"Deep-STEP: A Deep Learning Approach for Spatiotemporal Prediction of Remote Sensing Data","M. Das; S. K. Ghosh","Department of Computer Science and Engineering, IIT Kharagpur, Kharagpur, India","IEEE Geoscience and Remote Sensing Letters","20161208","2016","13","12","1984","1988","With the advent of advanced remote sensing technologies in past few decades, acquiring higher resolution satellite images has become easier and cheaper in recent days. However, on the other hand, it has offered a big challenge to the remote sensing community in smart image interpretation from such huge volume of data. Deep learning, which offers efficient algorithms for extracting multiple levels of feature abstractions, may be suitable to serve the purpose. This letter presents a deep learning approach (Deep-STEP) for spatiotemporal prediction of satellite remote sensing data. The proposed learning architecture is derived from a deep stacking network, consisting of a stack of multilayer perceptron, each of which models the spatial feature of the associated region at a particular time instant. The proposed method has been demonstrated on normalized difference vegetation index (NDVI) data sets, derived from satellite remote sensing imagery, containing several thousands to millions of pixels/records. The experimental results (related to NDVI prediction) reveal that the proposed architecture exhibits fairly satisfactory performance with promising learning capabilities.","1545-598X;1545598X","","10.1109/LGRS.2016.2619984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752890","Deep learning;deep stacking network (DSN);satellite remote sensing imagery;spatiotemporal prediction","Computational modeling;Machine learning;Remote sensing;Satellites;Spatiotemporal phenomena;Stacking;Training","remote sensing;vegetation","Deep-STEP;NDVI dataset;deep learning approach;deep stacking network;normalized difference vegetation index;remote sensing technology;satellite images;satellite remote sensing data;satellite remote sensing imagery","","","","","","20161122","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification","T. Xiao; H. Li; W. Ouyang; X. Wang","Dept. of Electron. Eng., Chinese Univ. of Hong Kong, Hong Kong, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","1249","1258","Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform stateof-the-art methods on multiple datasets by large margins.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780509","","Biological neural networks;Feature extraction;Machine learning;Neurons;Pipelines;Robustness;Training","image representation;learning (artificial intelligence);neural nets","CNN training;convolutional neural networks;data variations;deep feature representations learning;domain guided dropout;person reidentification","","2","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"An overview on text coherence methods","M. Abdolahi; M. Zahedi","Kharazmi International Campus Shahrood University of Technology, Shahrood. Iran","2016 Eighth International Conference on Information and Knowledge Technology (IKT)","20161212","2016","","","1","5","The increasing availability of texts generated in many aria and online information has necessitated intensive research in the area of automatic text coherence identification within the Natural Language Processing (NLP) community. Over the past two decades, the problem has been addressed from many different perspectives, in varying domains and using various paradigms such as text summarization, text simplification, text generation and machine translation. This survey intends to investigate some of the most relevant approaches both in the areas of semantic and syntactic text coherence recognition methods, giving special emphasis to empirical methods and syntactic techniques. Special attention is devoted to categorize and classify of proposed methods, as future research on coherence and cohesion of texts is strongly dependent on progress in this area.","","Electronic:978-1-5090-4335-4; POD:978-1-5090-4336-1","10.1109/IKT.2016.7777794","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777794","Text coherence;coheherence translation;coherence summarization;text cohesion;topic relation","Coherence;Computational modeling;Feature extraction;Machine learning algorithms;Natural language processing;Pragmatics;Syntactics","language translation;natural language processing;text analysis","NLP community;automatic text coherence identification;machine translation;natural language processing community;online information;semantic text coherence recognition;syntactic text coherence recognition;text generation;text simplification;text summarization","","","","","","","7-8 Sept. 2016","","IEEE","IEEE Conference Publications"
"Spatially Binned ROC: A Comprehensive Saliency Metric","C. Wloka; J. Tstotsos","Electr. Eng. & Comput. Sci. Dept., York Univ., Toronto, ON, Canada","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","525","534","A recent trend in saliency algorithm development is large-scale benchmarking and algorithm ranking with ground truth provided by datasets of human fixations. In order to accommodate the strong bias humans have toward central fixations, it is common to replace traditional ROC metrics with a shuffled ROC metric which uses randomly sampled fixations from other images in the database as the negative set. However, the shuffled ROC introduces a number of problematic elements, including a fundamental assumption that it is possible to separate visual salience and image spatial arrangement. We argue that it is more informative to directly measure the effect of spatial bias on algorithm performance rather than try to correct for it. To capture and quantify these known sources of bias, we propose a novel metric for measuring saliency algorithm performance: the spatially binned ROC (spROC). This metric provides direct in-sight into the spatial biases of a saliency algorithm without sacrificing the intuitive raw performance evaluation of traditional ROC measurements. By quantitatively measuring the bias in saliency algorithms, researchers will be better equipped to select and optimize the most appropriate algorithm for a given task. We use a baseline measure of inherent algorithm bias to show that Adaptive Whitening Saliency (AWS) [14], Attention by Information Maximization (AIM) [8], and Dynamic Visual Attention (DVA) [20] provide the least spatially biased results, suiting them for tasks in which there is no information about the underlying spatial bias of the stimuli, whereas algorithms such as Graph Based Visual Saliency (GBVS) [18] and Context-Aware Saliency (CAS) [15] have a significant inherent central bias.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780432","","Algorithm design and analysis;Benchmark testing;Machine learning algorithms;Measurement;Prediction algorithms;Predictive models;Visualization","image processing","AWS;CAS;DVA;GBVS;ROC measurements;adaptive whitening saliency;algorithm ranking;benchmarking;central bias;central fixations;context-aware saliency;dynamic visual attention;graph based visual saliency;ground truth;image spatial arrangement;intuitive raw performance evaluation;saliency algorithm development;saliency algorithm performance;saliency algorithms;saliency metric;shuffled ROC metrics;spROC;spatially binned ROC;visual salience","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Identify Coincidental Correct Test Cases Based on Fuzzy Classification","Z. Li; M. Li; Y. Liu; J. Geng","Coll. of Inf. Sci. & Technol., Beijing Univ. of Chem. Technol., Beijing, China","2016 International Conference on Software Analysis, Testing and Evolution (SATE)","20161212","2016","","","72","77","In software testing, Coincidental Correct (CC) test case, which implement the faulty statement but with a correct output, has been investigated with a negative effects on coverage-based fault localization. Coincidental correct test case identification and manipulation had been studied and many identification methods are proposed, in which clustering based method is widely used. In this paper, a machine learning based fuzzy classification technique is proposed. We first present an approach to identify truly CC test cases for single fault version programs. Then KNN algorithm is adopted to classify the remaining passed test cases and three types of modified fuzzy suspiciousness metrics are presented based on three proposed CC test cases manipulation strategies. Empirical studies are conducted on 102 faulty versions of six programs, and the results indicate that the proposed approach makes the recall and false positive of CC test cases are 82% and 5% in average. In addition, the proposed fuzzy CC test cases manipulation strategies can improve the effectiveness of fault localization.","","Electronic:978-1-5090-4517-4; POD:978-1-5090-4518-1","10.1109/SATE.2016.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780197","Coincidental Correct;Coverage Based Fault Localization;Fuzzy Classification","Classification algorithms;Clustering algorithms;Fault diagnosis;Machine learning algorithms;Measurement;Support vector machines;Testing","fuzzy set theory;learning (artificial intelligence);pattern classification;program testing","CC test cases manipulation strategies;KNN algorithm;clustering based method;coincidental correct test cases;coverage-based fault localization;machine learning based fuzzy classification technique;single fault version programs;software testing","","","","","","","3-4 Nov. 2016","","IEEE","IEEE Conference Publications"
"Deep Interactive Object Selection","N. Xu; B. Price; S. Cohen; J. Yang; T. Huang","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","373","381","Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780416","","Euclidean distance;Image segmentation;Machine learning;Manganese;Optimization;Semantics;Training","generalisation (artificial intelligence);image colour analysis;image sampling;image segmentation;learning (artificial intelligence);probability;random processes;visual databases","Euclidean distance maps;FCN-8s model;PASCAL segmentation dataset;RGB image channels;background distribution estimation;boundary segments;deep-fully-convolutional network finetuning;deep-interactive object selection;deep-learning-based algorithm;foreground distribution estimation;generalization ability;graph cut optimization;object classes;output probability maps;random sampling strategies;seen objects;unseen objects;user click pattern modelling;user interaction reduction;user-provided negative clicks;user-provided positive clicks","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Learning Deep Structure-Preserving Image-Text Embeddings","L. Wang; Y. Li; S. Lazebnik","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","5005","5013","This paper proposes a method for learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities. The network is trained using a large-margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature. Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval. Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase localization on the Flickr30K Entities dataset.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.541","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780910","","Bidirectional control;Computer vision;Image color analysis;Kernel;Machine learning;Training;Visualization","document image processing;learning (artificial intelligence);neural nets","Flickr30K entities dataset;Flickr30K image-sentence datasets;MSCOCO image-sentence datasets;cross-view ranking constraints;deep structure-preserving image-text embeddings;image-to-text retrieval;linear projections;metric learning;nonlinearities;phrase localization;text-to-image retrieval;two-branch neural network;within-view neighborhood structure preservation constraints","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Maximal frequent sequences for document classification","H. N. T. Tuyet; T. Hanh","Faculty of Information Technology, Posts and Telecommunications Institute of Technology, Ho Chi Minh city, Vietnam","2016 International Conference on Advanced Technologies for Communications (ATC)","20161205","2016","","","152","157","Document Classification has attracted several attentions from researchers due to the increase of digital form documents and the need of these documents' organization. One of the most popular approaches to deal with this problem is based on machine learning techniques [1]. However, the result of classification much depends on the linguistic preprocess and the document representation. The dependence is more obvious to languages whose blanks are used to separate not only words but also syllables that constitute words such as Vietnamese, Chinese language. In this paper, we propose a language-independent classifier relied on a flexible feature called Maximal Frequent Sequences (MFSs) [2]. In addition, we design and implement a novel algorithm to find MFSs. Our algorithm follows the MFS definition of H. Ahonen-Myka [2] and ignores the expensive pruning phrase. The experiments shows that our classifying approach achieves the average 85.16% and 89.27% F-measure on 7 classes of the common dataset Reuters-21578 and 5 classes of Vietnamese documents, respectively.","","CD:978-1-4673-8767-5; Electronic:978-1-5090-2711-8; POD:978-1-5090-3760-5","10.1109/ATC.2016.7764764","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7764764","Document Classification;Maximal Frequent Sequences;Support Vector Machines","Algorithm design and analysis;Classification algorithms;Feature extraction;Machine learning algorithms;Pragmatics;Support vector machines;Text categorization","document handling;learning (artificial intelligence);natural language processing;pattern classification;support vector machines","Chinese language;MFS;Vietnamese language;document classification;language-independent classifier;machine learning techniques;maximal frequent sequences;support vector machines","","","","","","","12-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Guest Editorial Screen Content Video Coding and Applications","W. H. Peng; J. Xu; R. A. Cohen; J. Ostermann","Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan","IEEE Journal on Emerging and Selected Topics in Circuits and Systems","20161212","2016","6","4","389","392","This special issue aims to present recent technical advances in screen content video coding and applications. Screen content video has recently evolved from a niche to the mainstream due to the rapid advances in mobile and cloud technologies. Real-time, low-latency transport of screen visuals between devices in the form of screen content video is becoming prevalent in many applications, e.g., wireless displays, screen mirroring, display interfaces, screen/desktop virtualization and cloud-based mobile virtual reality. Today’s commonly-used video coding methods, however, have been developed primarily with camera-captured content in mind. These new applications create an urgent need for efficient coding of screen content video, especially as the support of 4k or even 8k resolution begins to achieve mass market appeal.","2156-3357;21563357","","10.1109/JETCAS.2016.2631958","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769202","","High efficiency video coding;ISO Standards;ITU Standards;Image segmentation;Machine learning;Special issues and sections;Streaming media;Transform coding;Video coding","","","","","","","","20161205","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"A SVM based routing scheme in VANETs","L. Zhao; Y. Li; C. Meng; C. Gong; X. Tang","Shenyang Aerospace University, 37 Daoyi South Avenue, Shenyang, China","2016 16th International Symposium on Communications and Information Technologies (ISCIT)","20161124","2016","","","380","383","In recent years, VANET as the key communication networking technology has been attracted by academia and industries with remarkable development. However, there is still many shortcomings especially in communication efficiency where routing algorithm is one of them. In the design phase of routing algorithms, only very few features of vehicle nodes are considered manually to improve the classic algorithms e.g. GPSR. This paper studies the features of vehicle nodes and drivers. In addition, we introduce machine learning algorithm in particular Support Vector Machine (SVM) to process the vehicle data and generate routing metric to enhance the effect of these features. This paper studies the methods of analyzing and processing probe car data and also discusses the possibility of applying machine learning algorithms in the generation of VANET routing algorithm. The simulation results show better reliability and communication efficiency is achieved.","","Electronic:978-1-5090-4099-5; POD:978-1-5090-4100-8","10.1109/ISCIT.2016.7751655","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7751655","Routing algorithm;SVM;VANETs","Algorithm design and analysis;Decision support systems;Machine learning algorithms;Routing;Support vector machines;Vehicles;Vehicular ad hoc networks","learning (artificial intelligence);support vector machines;telecommunication network routing;traffic engineering computing;vehicular ad hoc networks","SVM-based routing scheme;VANET;communication efficiency;communication reliability;machine learning algorithm;support vector machine;vehicle drivers;vehicle nodes","","","","","","","26-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Deep action classification via matrix completion","S. Bomma; N. M. Robertson","Institute of Sensors, Signals and Systems, Heriot-Watt University, United Kingdom","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","1886","1890","Matrix completion is the task of predicting unknown or missing entries in a data matrix. The estimation of the missing entries is based on the assumption that the underlying matrix is a low rank one. Deep learning has evolved as an efficient tool for feature extraction in many large-scale image based applications. Exploiting the techniques from both domains, we propose a novel solution to the problem of simultaneous classification of actions from multiple test videos with deep features using matrix completion methods. Learned features from a convolutional neural network and corresponding labels from data are concatenated to form a big matrix with unknown or missing entries in the place of test data labels. Convex rank minimization algorithms are used to complete this matrix. The proposed method achieves stable performance even in situations with more than 50% of features and labels missing.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760576","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760576","","Convolution;Feature extraction;Machine learning;Minimization;Neural networks;Training;Videos","convex programming;feature extraction;image classification;learning (artificial intelligence);matrix algebra;minimisation;neural nets;video signal processing","convex rank minimization algorithms;convolutional neural network;data matrix;deep action classification;deep learning;feature extraction;large-scale image based applications;matrix completion;missing entries;simultaneous action classification;test data labels;test videos;unknown entries","","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"Hyperspectral Image Classification Based on Nonlinear Spectral–Spatial Network","B. Pan; Z. Shi; N. Zhang; S. Xie","Image Processing Center, State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics and the Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","20161208","2016","13","12","1782","1786","Recently, for the task of hyperspectral image classification, deep-learning-based methods have revealed promising performance. However, the complex network structure and the time-consuming training process have restricted their applications. In this letter, we construct a much simpler network, i.e., the nonlinear spectral-spatial network (NSSNet), for hyperspectral image classification. NSSNet is developed from the basic structure of a principal component analysis network. Nonlinear information is included in NSSNet, to generate a more discriminative feature expression. Moreover, spectral and spatial features are combined to further improve the classification accuracy. Experimental results indicate that our method achieves better performance than state-of-the-art deep-learning-based methods.","1545-598X;1545598X","","10.1109/LGRS.2016.2608963","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580567","Deep learning;hyperspectral image classification;nonlinear spectral–spatial network (NSSNet)","Feature extraction;Hyperspectral imaging;Imaging;Kernel;Machine learning;Principal component analysis","hyperspectral imaging;image classification;learning (artificial intelligence);remote sensing","PCA network;classification accuracy;complex network structure;deep-learning-based method;hyperspectral image classification;nonlinear spectral-spatial network;principal component analysis;simpler network;time-consuming training process","","","","","","20160930","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"Application of deep learning to computer vision: A comprehensive study","S. M. S. Islam; S. Rahman; M. M. Rahman; E. K. Dey; M. Shoyaib","Institute of Information Technology, University of Dhaka","2016 5th International Conference on Informatics, Electronics and Vision (ICIEV)","20161201","2016","","","592","597","Deep learning is a new era of machine learning research, where many layers of information processing stages are exploited for unsupervised feature learning. Using multiple levels of representation and abstraction, it helps a machine to understand about data (e.g., images, sound and text) more accurately. Many deep learning models have been proposed for solving the problem of different applications. Therefore, a comprehensive knowledge of these models is demanded to select the appropriate one for a specific application areas in signal or data processing. This paper reviews several deep learning models proposed for different application area in the field of computer vision, and makes a comprehensive evaluation of two well-known models namely AlexNet and VGG_S in nine different benchmark datasets. The experimental results show that these two models perform better than the existing state-of-the-art deep learning models in one dataset.","","Electronic:978-1-5090-1269-5; POD:978-1-5090-1270-1","10.1109/ICIEV.2016.7760071","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760071","AlexNet;CNN;Comprehensive study;Deep learning;VGG_S","Computational modeling;Computer architecture;Computer vision;Electronic mail;Feature extraction;Machine learning;Training","computer vision;learning (artificial intelligence)","AlexNet;VGG-S;computer vision;data processing;deep learning application;information processing stages;machine learning research;signal processing;unsupervised feature learning","","","","","","","13-14 May 2016","","IEEE","IEEE Conference Publications"
"Deep Learning With Attribute Profiles for Hyperspectral Image Classification","E. Aptoula; M. C. Ozdemir; B. Yanikoglu","Gebze Technical University, Gebze, Turkey","IEEE Geoscience and Remote Sensing Letters","20161208","2016","13","12","1970","1974","Effective spatial-spectral pixel description is of crucial significance for the classification of hyperspectral remote sensing images. Attribute profiles are considered as one of the most prominent approaches in this regard, since they can capture efficiently arbitrary geometric and spectral properties. Lately though, the advent of deep learning in its various forms has also led to remarkable classification performances by operating directly on hyperspectral input. In this letter, we explore the collaboration potential of these two powerful feature extraction approaches. Specifically, we propose a new strategy for hyperspectral image classification, where attribute filtered images are stacked and provided as input to convolutional neural networks. Our experiments with two real hyperspectral remote sensing data sets show that the proposed strategy leads to a performance improvement, as opposed to using each of the involved approaches individually.","1545-598X;1545598X","","10.1109/LGRS.2016.2619354","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733086","Attribute profiles (APs);deep learning;hyperspectral images;mathematical morphology;pixel classification","Feature extraction;Gray-scale;Hyperspectral imaging;Machine learning;Neural networks","feature extraction;hyperspectral imaging;image classification;neural nets;remote sensing","arbitrary geometric properties;convolutional neural network;deep learning;feature extraction;hyperspectral remote sensing dataset;hyperspectral remote sensing image classification;spectral properties","","","","","","20161103","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"The Generation and Evolution of Adaptation Rules in Requirements Driven Self-Adaptive Systems","T. Zhao","Key Lab. of High-Confidence Software Technol., China","2016 IEEE 24th International Requirements Engineering Conference (RE)","20161205","2016","","","456","461","One of the challenges in self-adaptive software systems is to make adaptation plans in response to possible changes. A good plan mechanism shall have the capability of: 1) selecting the most appropriate adaptation actions in response to changes both in the environment and requirements, 2) making adaptation decisions efficiently to react timely to arising situations at run-time. In existing approaches for plan process, rule-based adaptation provides an efficient offline planning method. However, it can react neither to changeable requirements nor to unexpected environment changes. On the contrary, goal-based and utility-based approaches provide online planning mechanisms, which can well handle a highly uncertain environment with dynamically changing requirements and environment. However, online adaptation decision making is often computationally expensive and may encounter less-efficiency problems. The aim of our research is to improve the planning processin requirements driven self-adaptive systems, i.e., enabling the self-adaptive system to efficiently make adaptation plans to cope with the dynamic environment and changeable requirements. To achieve such advantages, we propose a solution to enhance the traditional rule-based adaptation with a rule generation and a rule evolution process, so that the proposed approach can maintain the advantages of efficient planning process while being enhanced with the capability of dealing with runtime uncertainty.","","Electronic:978-1-5090-4121-3; POD:978-1-5090-4122-0","10.1109/RE.2016.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765565","adaptation plan;case-based reasoning;reinforcement learning;requirement driven self-adaptation","Algorithm design and analysis;Learning (artificial intelligence);Machine learning algorithms;Planning;Software algorithms;Software systems","decision making;formal specification;knowledge based systems","adaptation actions;adaptation rule evolution;adaptation rule generation;dynamic environment;goal-based approaches;offline planning method;online adaptation decision making;online planning mechanisms;plan process;planning processing requirements;requirements driven self-adaptive software systems;rule-based adaptation;runtime uncertainty;utility-based approaches","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"Device-free Wireless Localization and Activity Recognition: A Deep Learning Approach","J. Wang; X. Zhang; Q. Gao; H. Yue; H. Wang","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian 116023, China.(email:wangjie@gdlut.edu.cn)","IEEE Transactions on Vehicular Technology","","2016","PP","99","1","1","Device-free wireless localization and activity recognition (DFLAR) is a new technique which could estimate the location and activity of a target by analyzing its shadowing effect on surrounding wireless links. This technique neither requires the target to be equipped with any device nor involves privacy concerns, which makes it an attractive and promising technique for many emerging smart applications. The key question of DFLAR is how to characterize the influence of the target on wireless signals. Existing work generally utilizes statistical features extracted from wireless signals, such as mean and variance in time-domain, and energy as well as entropy in frequencydomain, to characterize the influence of the target. However, a feature suitable for distinguishing some activities or gestures may perform poorly when it is used to recognize other activities or gestures. Therefore, one has to manually design handcraft features for a specific application. Inspired by its excellent performance in extracting universal and discriminative features, in this paper, we propose a deep learning approach for realizing DFLAR. Specifically, we design a sparse autoencoder network to automatically learn discriminative features from the wireless signals, and merge the learned features into a softmax regression based machine learning framework to realize location, activity, and gesture recognition simultaneously. Extensive experiments performed in a clutter indoor laboratory and an apartment with 8 wireless nodes demonstrate that the DFLAR system using the learned features could achieve 0.85 or higher accuracy, which is better than the systems utilizing traditional handcraft features.","0018-9545;00189545","","10.1109/TVT.2016.2635161","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765094","Device-free localization;activity recognition;deep learning;wireless networks","Feature extraction;Machine learning;Performance evaluation;Time-domain analysis;Wireless communication;Wireless sensor networks","","","","","","","","20161202","","","IEEE","IEEE Early Access Articles"
"Multi-view SAS image classification using deep learning","D. P. Williams; S. Dugelay","NATO STO CMRE, La Spezia, Italy","OCEANS 2016 MTS/IEEE Monterey","20161201","2016","","","1","9","A new approach is proposed for multi-view classification when sonar data is in the form of imagery and each object has been viewed an arbitrary number of times. An image-fusion technique is employed in conjunction with a deep learning algorithm (based on Boltzmann machines) so that the sonar data from multiple views can be combined and exploited at the (earliest) image level. The method utilizes single-view imagery and, whenever available, multi-view fused imagery, in the same unified classification framework. The promise of the proposed approach is demonstrated in the context of an object classification task with real synthetic aperture sonar (SAS) imagery collected at sea.","","Electronic:978-1-5090-1537-5; POD:978-1-5090-1527-6","10.1109/OCEANS.2016.7761334","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761334","","Clutter;Feature extraction;Machine learning;Rocks;Standards;Synthetic aperture sonar","image classification;learning (artificial intelligence);sonar","Boltzmann machines;SAS imagery;deep learning algorithm;image-fusion technique;multiple views;multiview SAS image classification;multiview fused imagery;object classification task;single-view imagery;sonar data;synthetic aperture sonar","","","","","","","19-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"Multi-dimensional and customizable open-source labware for promoting big data analytical skills in STEM education","Ying Xie; Kai Qian; Jing He","Department of Computer Science, Kennesaw State University, Georgia, USA","2016 IEEE Frontiers in Education Conference (FIE)","20161201","2016","","","1","5","In order to remove resource barriers and smooth the learning curve for education on big data analytics in STEM disciplines, we develop an portable open source labware that is called STEM-BD for promoting education on big data analytics. STEM-BD integrates the following four critical components, big data platform, big data sets, data analytics algorithms and hands-on lab exercises in a multi-dimensional and customizable way. In this paper, we provide a detailed description of the design goal of STEM-BD, its prototype, preliminary evaluation results, and future development.","","Electronic:978-1-5090-1790-4; POD:978-1-5090-1791-1","10.1109/FIE.2016.7757700","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7757700","STEM education;big data;component;labware","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Education;Machine learning algorithms;Sparks","Big Data;STEM;computer aided instruction;computer science education;laboratories;public domain software","Big Data analytical skills;Big Data sets;STEM education;STEM-BD;data analytics algorithms;hands-on lab exercises;multidimensional-customizable-portable open-source labware","","","","","","","12-15 Oct. 2016","","IEEE","IEEE Conference Publications"
"Optimal-MDPREF: Novel approach for distributive association rules mining","M. Mohammed; B. Youssef; G. Taoufiq","Laboratory Informatics, Imaging and Modeling of Complex Systems - IIMCS Laboratory, Dept. of Mathematics and Computer Sciences, FSTS, University of Hassan 1, Settat, Morocco","2016 11th International Conference on Intelligent Systems: Theories and Applications (SITA)","20161208","2016","","","1","6","Association Rules is applied on databases - small or big - as a technique to discover useful knowledge with regard to predefined conditions, it is in the core of Data mining and is also used in several fields of business such as insurance, medicine, education.... The algorithms which are used to mine Association Rules, usually generate a huge number of rules. this fact makes them beyond the user's ability to interpret. Prior to this is that the generation process may not in most cases be fast enough to save time and effort. The objective of this contribution empowered by MDPref algorithm is to lessen the number of rules and to make the generation process relatively faster via scaling-distributive approach to make its efficiency better and the runtime shorter.","","Electronic:978-1-5090-5781-8; POD:978-1-5090-5782-5","10.1109/SITA.2016.7772257","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7772257","Association Rules mining;Bagging;Data partitioning;MDP<inf>REF</inf> Algorithm;Optimization","Algorithm design and analysis;Classification algorithms;Data mining;Databases;Machine learning algorithms;Partitioning algorithms;Runtime","Big Data;data mining","data mining;distributive association rules mining;knowledge discovery;optimal-MDP<sub>REF</sub>;scaling-distributive approach","","","","","","","19-20 Oct. 2016","","IEEE","IEEE Conference Publications"
"Learning Dense Correspondence via 3D-Guided Cycle Consistency","T. Zhou; P. Krähenbühl; M. Aubry; Q. Huang; A. A. Efros","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","117","126","Discriminative deep learning approaches have shown impressive results for problems where human-labeled ground truth is plentiful, but what about tasks where labels are difficult or impossible to obtain? This paper tackles one such problem: establishing dense visual correspondence across different object instances. For this task, although we do not know what the ground-truth is, we know it should be consistent across instances of that category. We exploit this consistency as a supervisory signal to train a convolutional neural network to predict cross-instance correspondences between pairs of images depicting objects of the same category. For each pair of training images we find an appropriate 3D CAD model and render two synthetic views to link in with the pair, establishing a correspondence flow 4-cycle. We use ground-truth synthetic-to-synthetic correspondences, provided by the rendering engine, to train a ConvNet to predict synthetic-to-real, real-to-real and real-to-synthetic correspondences that are cycle-consistent with the ground-truth. At test time, no CAD models are required. We demonstrate that our end-to-end trained ConvNet supervised by cycle-consistency outperforms state-of-the-art pairwise matching methods in correspondence-related tasks.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780389","","Computer vision;Machine learning;Shape;Solid modeling;Three-dimensional displays;Training;Visualization","CAD;computer vision;convolution;learning (artificial intelligence);object detection;rendering (computer graphics)","3D CAD model;3D-guided cycle consistency;ConvNet training;computer vision;convolutional neural network training;deep learning;dense visual correspondence;human-labeled ground truth;object instance;supervisory signal;view rendering","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition","Z. Deng; A. Vahdat; H. Hu; G. Mori","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","4772","4781","Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.516","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780885","","Computational modeling;Graphical models;Machine learning;Recurrent neural networks","image motion analysis;image recognition;inference mechanisms;recurrent neural nets","deep neural networks;graphical models;group activity recognition;recurrent neural network;sequential inference;structure inference machines","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Automatic Change Detection in Synthetic Aperture Radar Images Based on PCANet","F. Gao; J. Dong; B. Li; Q. Xu","Department of Computer Science and Technology, Ocean University of China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","20161208","2016","13","12","1792","1796","This letter presents a novel change detection method for multitemporal synthetic aperture radar images based on PCANet. This method exploits representative neighborhood features from each pixel using PCA filters as convolutional filters. Thus, the proposed method is more robust to the speckle noise and can generate change maps with less noise spots. Given two multitemporal images, Gabor wavelets and fuzzy c-means are utilized to select interested pixels that have high probability of being changed or unchanged. Then, new image patches centered at interested pixels are generated and a PCANet model is trained using these patches. Finally, pixels in the multitemporal images are classified by the trained PCANet model. The PCANet classification result and the preclassification result are combined to form the final change map. The experimental results obtained on three real SAR image data sets confirm the effectiveness of the proposed method.","1545-598X;1545598X","","10.1109/LGRS.2016.2611001","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589111","Change detection;Gabor wavelets;PCANet;synthetic aperture radar (SAR) images","Clustering algorithms;Machine learning;Partitioning algorithms;Principal component analysis;Speckle;Synthetic aperture radar;Training","fuzzy systems;image classification;image filtering;remote sensing by radar;synthetic aperture radar","Gabor wavelet;PCA filter;PCANet model;SAR image data;automatic change detection;fuzzy c-means;image classification;image patch;multitemporal SAR image;multitemporal image;representative neighborhood feature;speckle noise;synthetic aperture radar","","","","","","20161012","Dec. 2016","","IEEE","IEEE Journals & Magazines"
"Speaker-aware long short-term memory multi-task learning for speech recognition","G. Pironkov; S. Dupont; T. Dutoit","TCTS Lab, University of Mons, Belgium","2016 24th European Signal Processing Conference (EUSIPCO)","20161201","2016","","","1911","1915","In order to address the commonly met issue of overfitting in speech recognition, this article investigates Multi-Task Learning, when the auxiliary task focuses on speaker classification. Overfitting occurs when the amount of training data is limited, leading to an over-sensible acoustic model. Multi-Task Learning is a method, among many other regularization methods, which decreases the overfitting impact by forcing the acoustic model to train jointly for multiple different, but related, tasks. In this paper, we consider speaker classification as an auxiliary task in order to improve the generalization abilities of the acoustic model, by training the model to recognize the speaker, or find the closest one inside the training set. We investigate this Multi-Task Learning setup on the TIMIT database, while the acoustic modeling is performed using a Recurrent Neural Network with Long Short-Term Memory cells.","","Electronic:978-0-9928-6265-7; POD:978-1-5090-1891-8","10.1109/EUSIPCO.2016.7760581","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760581","","Acoustics;Feature extraction;Hidden Markov models;Machine learning;Speech;Speech recognition;Training","learning (artificial intelligence);recurrent neural nets;signal classification;speech recognition","TIMIT database;acoustic model;over-sensible acoustic model;recurrent neural network;speaker classification;speaker-aware long short-term memory multitask learning;speech recognition","","","","","","","Aug. 29 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"Bilingual automatic text summarization using unsupervised deep learning","S. P. Singh; A. Kumar; A. Mangal; S. Singhal","AAI, Center for development of Advanced Computing, Pune, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","20161124","2016","","","1195","1200","In the world of digitization, the growth of big data is raising at large scale with usage of high performance computing. The huge data in English and Hindi is available on internet and social media which need to be extracted or summarized in user required form. In this paper we are presenting Bilingual (Hindi and English) unsupervised automatic text summarization using deep learning. which is an important research area with in Natural Language Processing, Machine Learning and data mining, to improve result accuracy, we are using restricted Boltzmann machine to generate a shorter version of original document without losing its important information. In this algorithm we are exploring the features to improve the relevance of sentences in the dataset.","","CD:978-1-4673-9936-4; DVD:978-1-4673-9937-1; Electronic:978-1-4673-9939-5; POD:978-1-4673-9940-1; USB:978-1-4673-9938-8","10.1109/ICEEOT.2016.7754874","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7754874","Automatic Summarization;Bilingual;Deep Learning RBM;dataset;unsupervised","Feature extraction;Machine learning;Natural languages;Optimization;Speech;Tagging","Big Data;Boltzmann machines;Internet;data mining;learning (artificial intelligence);social networking (online);text analysis","Big Data;Boltzmann machine;English data;Hindi data;Internet;bilingual unsupervised automatic text summarization;data extraction;data mining;digitization;machine learning;natural language processing;social media;unsupervised deep learning","","","","","","","3-5 March 2016","","IEEE","IEEE Conference Publications"
"Image retrieval and classification on deep convolutional SparkNet","H. Li; Peng Su; Zhizhen Chi; Jingjing Wang","Department of Electronic Engineering, The Chinese University of Hong Kong, New Territories, Hong Kong","2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","20161124","2016","","","1","6","Image retrieval and classification are hot topics in computer vision and have attracted great attention nowadays with the emergence of large-scale data. We propose a new scheme to use both deep learning models and large-scale computing platform and jointly learn powerful feature representations in image classification and retrieval. We achieve a superior performance on the ImageNet dataset, where the framework is easy to be embedded for daily user experience. First we conduct the classification task using deep convolutional neural networks with several novel techniques, including batch normalization and multi-crop testing to obtain a better performance. Then we transfer the network's knowledge to image retrieval task by comparing the feature codebook of the query image with those feature database extracted from the deep model. Such a search pipeline is implemented in a MapReduce framework on the Spark platform, which is suitable for large-scale and real-time data processing. At last, the system outputs to users some textual information of the predicted object searching from Internet as well as similar images from the retrieval stage, making our work a real application.","","Electronic:978-1-5090-2708-8; POD:978-1-5090-2709-5","10.1109/ICSPCC.2016.7753615","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753615","","Convolutional codes;Feature extraction;Image retrieval;Machine learning;Sparks;Training","Internet;computer vision;data handling;embedded systems;image classification;image retrieval;learning (artificial intelligence);neural nets;parallel processing","ImageNet dataset;Internet;MapReduce framework;Spark platform;computer vision;deep convolutional SparkNet;deep convolutional neural networks;feature database extraction;image classification;image retrieval;large-scale computing platform;large-scale data processing;multicrop testing;query image;real-time data processing","","","","","","","5-8 Aug. 2016","","IEEE","IEEE Conference Publications"
"Deep convolutional neural networks for dense non-uniform motion deblurring","J. Cronje","Council for Scientific and Industrial Research, Pretoria, South Africa","2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)","20161201","2015","","","1","5","The work in this paper address the problem of removing non-uniform motion blur from a single image. The motion vector for an image patch is estimated by using a convolutional neural network (CNN). All the predicted motion vectors are combined to form a dense non-uniform motion estimation map. Furthermore, a second CNN is trained to perform deblurring given a blurry image patch and the estimated motion vector. Combining the two trained networks result in a deep learning approach that can enhance degraded images. The results show that this approach can accurately determine non-uniform motion blur and restore blurred images.","","Electronic:978-1-5090-0357-0; POD:978-1-5090-0358-7","10.1109/IVCNZ.2015.7761567","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761567","","Deconvolution;Estimation;Image restoration;Kernel;Machine learning;Neural networks;Training","image restoration;learning (artificial intelligence);motion estimation;neural nets;vectors","CNN;blurred image restoration;blurry image patch;deep convolutional neural networks;deep learning;dense nonuniform motion deblurring;dense nonuniform motion estimation map;motion vector","","","","","","","23-24 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond","N. D. B. Bruce; C. Catton; S. Janjic","Univ. of Manitoba, Winnipeg, MB, Canada","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","516","524","In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learning model based on fully convolutional networks (FCNs) is presented, which shows very favorable performance across a wide variety of benchmarks relative to existing proposals. We also demonstrate that the manner in which training data is selected, and ground truth treated is critical to resulting model behaviour. Recent efforts have explored the relationship between human gaze and salient objects, and we also examine this point further in the context of FCNs. Close examination of the proposed and alternative models serves as a vehicle for identifying problems important to developing more comprehensive models going forward.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780431","","Benchmark testing;Data models;Machine learning;Object segmentation;Predictive models;Training;Visualization","gaze tracking;image segmentation;learning (artificial intelligence);neural nets","FCN;deep learning model;feature contrast;fully convolutional networks;human gaze prediction;salient object segmentation;visual saliency modeling","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Saliency Guided Dictionary Learning for Weakly-Supervised Image Parsing","B. Lai; X. Gong","Coll. of Inf. Sci. & Electron. Eng., Zhejiang Univ., Hangzhou, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","3630","3639","In this paper, we propose a novel method to perform weakly-supervised image parsing based on the dictionary learning framework. To deal with the challenges caused by the label ambiguities, we design a saliency guided weight assignment scheme to boost the discriminative dictionary learning. More specifically, with a collection of tagged images, the proposed method first conducts saliency detection and automatically infers the confidence for each semantic class to be foreground or background. These clues are then incorporated to learn the dictionaries, the weights, as well as the sparse representation coefficients in the meanwhile. Once obtained the coefficients of a superpixel, we use a sparse representation classifier to determine its semantic label. The approach is validated on the MSRC21, PASCAL VOC07, and VOC12 datasets. Experimental results demonstrate the encouraging performance of our approach in comparison with some state-of-the-arts.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.395","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780764","","Cows;Dictionaries;Image segmentation;Machine learning;Predictive models;Semantics;Training","image classification;image representation","saliency detection;saliency guided dictionary learning;sparse representation classifier;weakly-supervised image parsing","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Deep learning of structured environments for robot search","J. A. Caley; N. R. J. Lawrance; G. A. Hollinger","Robotics Program, School of Mechanical, Industrial and Manufacturing Engineering, Oregon State University, United States","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20161201","2016","","","3987","3992","Robots often operate in built environments containing underlying structure that can be exploited to help predict future observations. In this work, we present a deep learning based approach to predict exit locations of buildings. This technique exploits the inherent structure of buildings to create a model. A convolutional neural network is trained using a database of building blueprints and used to guide a search within a building. This technique is compared to standard frontier exploration and a traditional image processing approach of extracting features through histogram of gradients (HOG) and training a support vector machine (SVM). After validation through simulation, we show that the proposed deep learning technique reduces the amount of building exploration required to find the goal by 36%.","","Electronic:978-1-5090-3762-9; POD:978-1-5090-3763-6; USB:978-1-5090-3761-2","10.1109/IROS.2016.7759587","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759587","","Biological neural networks;Buildings;Feature extraction;Machine learning;Robots;Training","buildings (structures);control engineering computing;convolution;learning (artificial intelligence);mobile robots;neural nets;support vector machines","building blueprints database;building exploration;buildings exit location prediction;convolutional neural network;deep learning based approach;histogram of gradients;robot search;support vector machine","","","","","","","9-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"In the Shadows, Shape Priors Shine: Using Occlusion to Improve Multi-region Segmentation","Y. Kihara; M. Soloviev; T. Chen","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","392","401","We present a new algorithm for multi-region segmentation of 2D images with objects that may partially occlude each other. Our algorithm is based on the observation that human performance on this task is based both on prior knowledge about plausible shapes and taking into account the presence of occluding objects whose shape is already known - once an occluded region is identified, the shape prior can be used to guess the shape of the missing part. We capture the former aspect using a deep learning model of shape, for the latter, we simultaneously minimize the energy of all regions and consider only unoccluded pixels for data agreement. Existing algorithms incorporating object shape priors consider every object separately in turn and can't distinguish genuine deviation from the expected shape from parts missing due to occlusion. We show that our method significantly improves on the performance of a representative algorithm, as evaluated on both preprocessed natural and synthetic images. Furthermore, on the synthetic images, we recover the ground truth segmentation with good accuracy.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780418","","Data models;Image segmentation;Level set;Linear programming;Machine learning;Observers;Shape","image segmentation;learning (artificial intelligence);natural scenes;shape recognition","2D images;deep learning model;multiregion segmentation;natural images;object shape priors;occlusion;synthetic images","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Deep Decision Network for Multi-class Image Classification","V. N. Murthy; V. Singh; T. Chen; R. Manmatha; D. Comaniciu","Sch. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2240","2248","In this paper, we present a novel Deep Decision Network (DDN) that provides an alternative approach towards building an efficient deep learning network. During the learning phase, starting from the root network node, DDN automatically builds a network that splits the data into disjoint clusters of classes which would be handled by the subsequent expert networks. This results in a tree-like structured network driven by the data. The proposed method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. DDN also has the ability to make early decisions thus making it suitable for timesensitive applications. We validate DDN on two publicly available benchmark datasets: CIFAR-10 and CIFAR-100 and it yields state-of-the-art classification performance on both the datasets. The proposed algorithm has no limitations to be applied to any generic classification problems.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.246","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780615","","Airplanes;Covariance matrices;Decision trees;Machine learning;Symmetric matrices;Training;Vegetation","image classification;learning (artificial intelligence);trees (mathematics)","CIFAR-100;DDN;deep decision network;deep learning network;multiclass image classification;tree-like structured network","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Playful creation of digital stories with eShadow","A. Moraiti; N. Moumoutzis; M. Christoulakis; A. Pitsiladis; G. Stylianakis; Y. Sifakis; I. Maragoudakis; S. Christodoulakis","Laboratory of Distributed Multimedia Information Systems & Applications, School of Electronic & Computer Engineering, Technical University of Crete, Chania, Greece","2016 11th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)","20161124","2016","","","139","144","eShadow promotes a new way of dramatized and personalized digital storytelling inspired by the rich tradition of shadow theatre. It enables game-like interventions combining several digital tools for the production of digital stories covering all five phases of filmmaking: scenario development, pre-production, production, post-production and distribution. eShadow can be used (a) to create digital shadow puppets, and (b) to set up, perform and record the scenes of the digital story (production phase). This way, digital story creation is wrapped around interesting game-like decisions, playful improvisations and creative learning. eShadow has been extensively used to support cross-curricular learning mainly in Greek schools. It is currently extended to support marionette-like interactions to promote its use in countries with relevant storytelling cultures.","","Electronic:978-1-5090-5246-2; POD:978-1-5090-5247-9; Paper:978-1-5090-5245-5","10.1109/SMAP.2016.7753399","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753399","cross-curricular learning;digital storytelling;playful learning;shadow theatre","Collaboration;Context;Creativity;Machine learning;Performance evaluation;Production;Usability","computer aided instruction;computer games","creative learning;digital shadow puppets;digital stories;digital tools;dramatized digital storytelling;eShadow;filmmaking;game-like cross-curricular learning;game-like decisions;marionette-like interactions;personalized digital storytelling;traditional Greek shadow theatre","","","","","","","20-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"A novel approach to Sandhi splitting at character level for Kannada language","M. R. Shree; S. Lakshmi; Shambhavi B. R","Department of CSE, Amruta Institute of Engineering and Management Sciences, Bidadi, India","2016 International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)","20161212","2016","","","17","20","Natural Language Processing (NLP) is a field of computational linguistics related to interactions between computers and human languages. Parsing of input text of any human language is a part of NLP. This parsing technique requires processing of text on a word by word basis. To process any individual word especially in Sanskrit and all Dravidian languages, Sandhi splitting is a major task. Sandhi is also called Morphophonemics concerned with changes that occur when two words or separate morphemes come together to form a new word. Exact splitting point is essential for text processing tasks such as POS tagging and in turn parsing. We have adopted a novel approach to internal Sandhi splitting technique on Kannada language. Each Kannada word is split into morphemes according to valid morph patterns. After the division of each word into lexical morphemes, we have manually tagged each split word into root-begins, root-continuous and suffix. This work has been done on Kannada language. We have trained the system with a list of 1000 tagged words using a CRF (Conditional Random Fields) tool and nearly 400 raw split words (untagged words) are given as input to the CRF tool. The system generates a list of tagged split words for the given input according to the trained data. The system output has been compared with the manually tagged data. We have verified the data using 5 fold test, which takes five different combinations of trained data (1000 words) and test data (400 words). The average Precision, Recall and F-Measure of Tagging accuracy of CRF model for Kannada corpus in 5 fold test are nearly equal to 98.08, 92.91 and 95.43. This method can be successfully implemented in all other Dravidian languages for the Sandhi splitting.","","Electronic:978-1-5090-1022-6; POD:978-1-5090-1023-3; Paper:978-1-5090-1020-2; USB:978-1-5090-1021-9","10.1109/CSITSS.2016.7779433","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779433","Conditional Random Fields;Natural Language Processing;Sandhi Splitter","Machine learning algorithms;Manuals;Testing","grammars;natural language processing;text analysis;word processing","CRF model;CRF tool;Dravidian languages;F-Measure;Kannada corpus;Kannada language;NLP;POS tagging;Sanskrit;character level Sandhi splitting;computational linguistics;computer languages;conditional random field tool;human languages;input text parsing;internal Sandhi splitting technique;morphophonemics;natural language processing;precision;recall;root-begins;root-continuous;suffix;tagging accuracy;word by word text processing","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Automatic annotation of coral reefs using deep learning","A. Mahmood; M. Bennamoun; S. An; F. Sohel; F. Boussaid; R. Hovey; G. Kendrick; R. B. Fisher","The University of Western Australia, Australia","OCEANS 2016 MTS/IEEE Monterey","20161201","2016","","","1","5","Healthy coral reefs play a vital role in maintaining biodiversity in tropical marine ecosystems. Deep sea exploration and imaging have provided us with a great opportunity to look into the vast and complex marine ecosystems. Data acquisition from the coral reefs has facilitated the scientific investigation of these intricate ecosystems. Millions of digital images of the sea floor have been collected with the help of Remotely Operated Vehicles (ROVs) and Autonomous Underwater Vehicles (AUVs). Automated technology to monitor the health of the oceans allows for transformational ecological outcomes by standardizing methods for detecting and identifying species. Manual annotation is a tediously repetitive and a time consuming task for marine experts. It takes 10-30 minutes for a marine expert to meticulously annotate a single image. This paper aims to automate the analysis of large available AUV imagery by developing advanced deep learning tools for rapid and large-scale automatic annotation of marine coral species. Such an automated technology would greatly benefit marine ecological studies in terms of cost, speed, accuracy and thus in better quantifying the level of environmental change marine ecosystems can tolerate. We propose a deep learning based classification method for coral reefs. We also report the application of the proposed technique towards the automatic annotation of unlabelled mosaics of the coral reef in the Abrolhos Islands, Western Australia. Our proposed method automatically quantifies the coral coverage in this region and detects a decreasing trend in coral population which is in line with conclusions by marine ecologists.","","Electronic:978-1-5090-1537-5; POD:978-1-5090-1527-6","10.1109/OCEANS.2016.7761105","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761105","classification;corals;deep learning;marine ecosystems;marine images","Australia;Ecosystems;Feature extraction;Machine learning;Sociology;Statistics;Training","autonomous underwater vehicles;data acquisition;image classification;learning (artificial intelligence);marine engineering;robot vision","AUV imagery;Abrolhos Islands;ROV;Western Australia;advanced deep learning tools;autonomous underwater vehicles;biodiversity;coral reefs;data acquisition;deep learning based classification method;deep sea exploration;deep sea imaging;digital images;large-scale automatic annotation;marine coral species;remotely operated vehicles;tropical marine ecosystems","","","","","","","19-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"MDL-CW: A Multimodal Deep Learning Framework with CrossWeights","S. Rastegar; M. S. Baghshah; H. R. Rabiee; S. M. Shojaee","Dept. of Comput. Eng., Sharif Univ. of Technol. Tehran, Tehran, Iran","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2601","2609","Deep learning has received much attention as of the most powerful approaches for multimodal representation learning in recent years. An ideal model for multimodal data can reason about missing modalities using the available ones, and usually provides more information when multiple modalities are being considered. All the previous deep models contain separate modality-specific networks and find a shared representation on top of those networks. Therefore, they only consider high level interactions between modalities to find a joint representation for them. In this paper, we propose a multimodal deep learning framework (MDLCW) that exploits the cross weights between representation of modalities, and try to gradually learn interactions of the modalities in a deep network manner (from low to high level interactions). Moreover, we theoretically show that considering these interactions provide more intra-modality information, and introduce a multi-stage pre-training method that is based on the properties of multi-modal data. In the proposed framework, as opposed to the existing deep methods for multi-modal data, we try to reconstruct the representation of each modality at a given level, with representation of other modalities in the previous layer. Extensive experimental results show that the proposed model outperforms state-of-the-art information retrieval methods for both image and text queries on the PASCAL-sentence and SUN-Attribute databases.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.285","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780654","","Cats;Data models;Dogs;Feature extraction;Machine learning;Random variables;Training","image retrieval;learning (artificial intelligence);text analysis;visual databases","MDL-CW;MDLCW;PASCAL-sentence;SUN-attribute databases;cross weights;high level interactions;image queries;intramodality information;modality-specific networks;multimodal data;multimodal deep learning framework;multimodal representation learning;multistage pretraining method;text queries","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Analysis of exhaled human breath via terahertz molecular spectroscopy","I. R. Medvedev; R. Schueler; J. Thomas; O. Kenneth; H. J. Nam; N. Sharma; Q. Zhong; D. J. Lary; P. Raskin","Wright State University, Dayton, OH, 45435 USA","2016 41st International Conference on Infrared, Millimeter, and Terahertz waves (IRMMW-THz)","20161201","2016","","","1","2","We report on our progress in utilizing THz breath sensing in several bio-medical diagnostic applications. Our work bears promise in applying this technology to non-invasive analysis of blood glucose based on chemical composition of breath, as well as assessment of asthma related airway inflammation. Our most recent testing of CMOS based THz breath sensor, in the evolution of this technology towards compact and affordable implementations, is discussed.","","Electronic:978-1-4673-8485-8; POD:978-1-4673-8486-5","10.1109/IRMMW-THz.2016.7758450","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758450","","Blood;Chemicals;Ethanol;Machine learning algorithms;Prediction algorithms;Sensors;Sugar","CMOS integrated circuits;pneumodynamics;terahertz spectroscopy;terahertz wave detectors","CMOS;THz breath sensing;asthma related airway inflammation;bio-medical diagnostic;blood glucose;breath sensor;chemical composition;exhaled human breath;terahertz molecular spectroscopy","","","","","","","25-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"A Key Volume Mining Deep Framework for Action Recognition","W. Zhu; J. Hu; G. Sun; X. Cao; Y. Qiao","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","1991","1999","Recently, deep learning approaches have demonstrated remarkable progresses for action recognition in videos. Most existing deep frameworks equally treat every volume i.e. spatial-temporal video clip, and directly assign a video label to all volumes sampled from it. However, within a video, discriminative actions may occur sparsely in a few key volumes, and most other volumes are irrelevant to the labeled action category. Training with a large proportion of irrelevant volumes will hurt performance. To address this issue, we propose a key volume mining deep framework to identify key volumes and conduct classification simultaneously. Specifically, our framework is trained is optimized in an alternative way integrated to the forward and backward stages of Stochastic Gradient Descent (SGD). In the forward pass, our network mines key volumes for each action class. In the backward pass, it updates network parameters with the help of these mined key volumes. In addition, we propose ""Stochastic out"" to model key volumes from multi-modalities, and an effective yet simple ""unsupervised key volume proposal"" method for high quality volume sampling. Our experiments show that action recognition performance can be significantly improved by mining key volumes, and we achieve state-of-the-art performance on HMDB51 and UCF101 (93.1%).","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.219","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780588","","Computer vision;Machine learning;Neural networks;Proposals;Stochastic processes;Three-dimensional displays;Training","data mining;gradient methods;image recognition;learning (artificial intelligence);stochastic processes;video signal processing","SGD;action recognition;deep learning;high quality volume sampling;key volume mining deep framework;spatial-temporal video clip;stochastic gradient descent;stochastic out method;unsupervised key volume proposal method;video label","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Predicting Seizures from Electroencephalography Recordings: A Knowledge Transfer Strategy","J. Liang; R. Lu; C. Zhang; F. Wang","Dept. of Autom., Tsinghua Univ., Beijing, China","2016 IEEE International Conference on Healthcare Informatics (ICHI)","20161208","2016","","","184","191","Epilepsy, a brain disorder afflicts nearly 1% of the world's population, is characterized by the occurrence of spontaneous seizures. For most epilepsy patients, the drugs are either not effective or produce severe side-effects. Seizure forecasting systems have the potential to help patients with epilepsy lead more normal lives. Recently multi-center clinical studies showed evidence of premonitory symptoms in 6.2% of 500 patients with epilepsy, and some interviews of epilepsy patients also found that a certain amount of patients felt ""auras"". All these are promising signs suggesting that seizure might be predictable. In this paper, we will study the application of deep learning techniques for seizure prediction with EEG signals. Deep learning methods have been shown to be very effective on exploring the latent structures from continuous signals and they have achieved state-of-the-art performance on speech analysis. One potential requirement for deep learning algorithms to work is a huge training set, which could be difficult for a specific medical problem. Therefore we specifically investigated a transfer learning strategy: we performed the major seizure prediction task on the data from American Epilepsy Society Seizure Prediction Challenge1, and we adopted another 6 publicly available EEG datasets2, which are not directly related to seizure prediction, as auxiliary information to pre-train the deep neural network for getting a good initial point. Our results show that with those auxiliary information, the prediction performance can be boosted. This observation is validated with different predictive models, which opens another gate for effective integration and utilization of medical data resources.","","Electronic:978-1-5090-6117-4; POD:978-1-5090-6118-1","10.1109/ICHI.2016.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776343","","Brain models;Electroencephalography;Epilepsy;Feature extraction;Machine learning;Training","brain;electroencephalography;knowledge management;learning (artificial intelligence);medical disorders;medical signal processing;neural nets","American Epilepsy Society Seizure Prediction Challenge;EEG datasets;EEG signals;brain disorder;deep learning techniques;deep neural network pretraining;electroencephalography recordings;epilepsy patients;knowledge transfer strategy;medical data resource utilization;seizure prediction;speech analysis;spontaneous seizures;transfer learning strategy","","","","","","","4-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Code Selection Mechanism Using Deep Learning","H. Cui; S. Hirasawa; H. Takizawa; H. Kobayashi","Grad. Sch. of Inf. Sci., Tohoku Univ., Sendai, Japan","2016 IEEE 10th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSOC)","20161208","2016","","","385","392","Sparse Matrix-Vector multiplication (SpMV) is a computational kernel widely used in many applications. There are many different implementations using different processors and algorithms for SpMV. The performances of different SpMV implementations are quite different, and it is basically difficult to choose the implementation that has the best performance for a given sparse matrix and a given platform without performance profiling. This work presents a prototype implementation of an effective machine learning system for SpMV code selection best suited for a given matrix. Instead of using predefined features of a matrix for performance prediction, a feature image and a deep learning network are used to map each sparse matrix to the implementation that has the best performance in advance of the execution. The performance gain by the mechanism is evaluated by using a machine learning method for predicting the best SpMV implementation. According to our evaluation, the proposed mechanism can select an optimal or suboptimal implementation in most cases, though the prediction is not perfect. These results demonstrate the feasibility that the proposed machine learning approach can capture underlying features of an input sparse matrix useful for SpMV code selection.","","Electronic:978-1-5090-3531-1; POD:978-1-5090-3532-8","10.1109/MCSoC.2016.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774462","SpMV;autotuning;code selection;deep learning","Computational modeling;Computers;Image recognition;Kernel;Machine learning;Program processors;Sparse matrices","learning (artificial intelligence);mathematics computing;matrix multiplication;sparse matrices;vectors","SpMV code selection;computational kernel;deep learning;machine learning system;sparse matrix-vector multiplication","","","","","","","21-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"Regularized logistic regression algorithm learns progressive sweep algorithm","M. Esmaeilpour; F. Allahabadi; Z. Esmaeilpour","Electrical and Computer Engineering Department, Faculty of Engineering, Kharazmi University, Tehran, Iran","2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)","20161201","2015","","","1","4","In this paper a high-level parametric formulation for surface reconstruction is introduced that avoids any explicit low-level parametric formulations. First we generate surfaces for a wide category of sample point clouds, including many convex, concave and flat structures, using progressive sweep algorithm, then we train regularized logistic regression algorithm based on these generated surfaces for the given sample point clouds. Finally, the achieved high-level parameters, after the termination of training procedure, will be used for reconstructing surfaces for any given test point clouds. Simulations show that, compared to the low-level parametric formulations, its high-level counterpart is less accurate for surface reconstruction purposes since this type of formulation is just implicitly imitating the manner of low-level parametric equations and is deprived of the vast advantages of forming velocity fields which plays a crucial role in low-level parametric formulations. Thus at the current time, it can be reasonable to see the mean squared error between the desired surface for a given point cloud, and the reconstructed surface by high-level parametric formulation is not satisfactorily low, in general yet. It should be added that, high-level parametric formulation is usually very fast in act and can open a new era in the field of computer vision and graphics.","","Electronic:978-1-5090-0357-0; POD:978-1-5090-0358-7","10.1109/IVCNZ.2015.7761543","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761543","","Image reconstruction;Logistics;Machine learning algorithms;Shape;Surface reconstruction;Three-dimensional displays;Training","computer graphics;computer vision;image reconstruction;mean square error methods;regression analysis;surface reconstruction","computer graphics;computer vision;concave structures;convex structures;explicit low-level parametric formulations;flat structures;high-level parametric formulation;low-level parametric equations;mean squared error;point clouds;progressive sweep algorithm;regularized logistic regression algorithm;surface reconstruction","","","","","","","23-24 Nov. 2015","","IEEE","IEEE Conference Publications"
"WarpNet: Weakly Supervised Matching for Single-View Reconstruction","A. Kanazawa; D. W. Jacobs; M. Chandraker","Univ. of Maryland, College Park, MD, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","3253","3261","We present an approach to matching images of objects in fine-grained datasets without using part annotations, with an application to the challenging problem of weakly supervised single-view reconstruction. This is in contrast to prior works that require part annotations, since matching objects across class and pose variations is challenging with appearance features alone. We overcome this challenge through a novel deep learning architecture, WarpNet, that aligns an object in one image with a different object in another. We exploit the structure of the fine-grained dataset to create artificial data for training this network in an unsupervised-discriminative learning approach. The output of the network acts as a spatial prior that allows generalization at test time to match real images across variations in appearance, viewpoint and articulation. On the CUB-200-2011 dataset of bird categories, we improve the AP over an appearance-only network by 13.6%. We further demonstrate that our WarpNet matches, together with the structure of fine-grained datasets, allow single-view reconstructions with quality comparable to using annotated point correspondences.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.354","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780723","","Birds;Computer architecture;Feature extraction;Image reconstruction;Machine learning;Shape;Training","image matching;image reconstruction;unsupervised learning","CUB-200-2011 dataset;WarpNet;fine-grained datasets;image matching;single-view reconstruction;unsupervised-discriminative learning;weakly supervised matching","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Classification in dynamic streaming networks","Y. Yao; L. B. Holder","School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","138","145","Traditional network classification techniques will become computationally intractable when applied on a network which is presented in a streaming fashion with continuous updates. In this paper, we examine the problem of classification in dynamic streaming networks, or graphs. Two scenarios have been considered: the graph transaction scenario and the one large graph scenario. We propose a unified framework consisting of three components: a subgraph extraction method, an online version of an existing graph kernel, and two kernel-based incremental learners. We demonstrate the advantages of our framework via empirical evaluations on several real-world network datasets.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752225","","Computational modeling;Data mining;Entropy;Kernel;Machine learning algorithms;Social network services;Support vector machines","graph theory;learning (artificial intelligence);pattern classification","dynamic streaming networks;graph kernel;graph transaction scenario;kernel-based incremental learners;network classification;subgraph extraction method","","","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"A novel Learning Object framework for Confidence Based Learning","R. Chatterjee; J. K. Mandal","Department of Computer Science and Engineering, NITTTR, Block-FC, Sector - III, Salt Lake City, Kolkata, India","2016 International Conference on Information Science and Communications Technologies (ICISCT)","20161208","2016","","","1","6","Confidence Based Learning (CBL) is a novel method in Teaching-Learning system where the process measures both the knowledge and confidence of a particular learner. In this paper a new framework has been proposed for Learning Objects (LO) suited for CBL. The proposal also takes care of the requirement for implementation including Learning Object Metadata (LOM). The novel framework proposed in this paper is used for prescribing learning content. The results show the progress in the performance of a typical learner through multiple iterations.","","CD:978-1-5090-3545-8; Electronic:978-1-5090-3546-5; POD:978-1-5090-3547-2","10.1109/ICISCT.2016.7777403","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777403","Confidence based Learning (CBL);Learning Object Metadata(LOM);Learning Objects;e-learning","Animation;Machine learning;Mathematical model;Media;Metadata;Videos;XML","meta data;teaching","CBL;LOM;confidence based learning;learning content;learning object framework;learning object metadata;teaching-learning system;typical learner performance","","","","","","","2-4 Nov. 2016","","IEEE","IEEE Conference Publications"
"Online process phase detection using multimodal deep learning","X. Li; Y. Zhang; M. Li; S. Chen; F. R. Austin; I. Marsic; R. S. Burd","Rutgers University, Piscataway, NJ, USA","2016 IEEE 7th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","20161212","2016","","","1","7","We present a multimodal deep-learning structure that automatically predicts phases of the trauma resuscitation process in real-time. The system first pre-processes the audio and video streams captured by a Kinect's built-in microphone array and depth sensor. A multimodal deep learning structure then extracts video and audio features, which are later combined through a “slow fusion” model. The final decision is then made from the combined features through a modified softmax classification layer. The model was trained on 20 trauma resuscitation cases (>13 hours), and was tested on 5 other cases. Our results showed over 80% online detection accuracy with 0.7 F-Score, outperforming previous systems.","","Electronic:978-1-5090-1496-5; POD:978-1-5090-1497-2","10.1109/UEMCON.2016.7777912","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777912","Activity recognition;Kinect;deep learning;multimodal sensing;process phase recognition;trauma resuscitation","Biomedical imaging;Feature extraction;Hidden Markov models;Image segmentation;Machine learning;Phase detection;Surgery","feature extraction;health care;learning (artificial intelligence)","audio feature extraction;modified softmax classification layer;multimodal deep-learning structure;online process phase detection;slow fusion model;trauma resuscitation process;video feature extraction","","","","","","","20-22 Oct. 2016","","IEEE","IEEE Conference Publications"
"SLAM with objects using a nonparametric pose graph","B. Mu; S. Y. Liu; L. Paull; J. Leonard; J. P. How","Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, United States","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20161201","2016","","","4602","4609","Mapping and self-localization in unknown environments are fundamental capabilities in many robotic applications. These tasks typically involve the identification of objects as unique features or landmarks, which requires the objects both to be detected and then assigned a unique identifier that can be maintained when viewed from different perspectives and in different images. The data association and simultaneous localization and mapping (SLAM) problems are, individually, well-studied in the literature. But these two problems are inherently tightly coupled, and that has not been well-addressed. Without accurate SLAM, possible data associations are combinatorial and become intractable easily. Without accurate data association, the error of SLAM algorithms diverge easily. This paper proposes a novel nonparametric pose graph that models data association and SLAM in a single framework. An algorithm is further introduced to alternate between inferring data association and performing SLAM. Experimental results show that our approach has the new capability of associating object detections and localizing objects at the same time, leading to significantly better performance on both the data association and SLAM problems than achieved by considering only one and ignoring imperfections in the other.","","Electronic:978-1-5090-3762-9; POD:978-1-5090-3763-6; USB:978-1-5090-3761-2","10.1109/IROS.2016.7759677","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759677","","Machine learning;Object detection;Proposals;Robustness;Simultaneous localization and mapping;Three-dimensional displays","SLAM (robots);graph theory;object detection;pose estimation;robot vision;sensor fusion","SLAM;data association;nonparametric pose graph;object detection;robotic application;simultaneous localization and mapping","","","","","","","9-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Novel AdaBoost Framework With Robust Threshold and Structural Optimization","P. B. Zhang; Z. X. Yang","Department of Electromechanical Engineering, Faculty of Science and Technology, University of Macau, Macau 999078, China.","IEEE Transactions on Cybernetics","","2016","PP","99","1","13","The AdaBoost algorithm is a popular ensemble method that combines several weak learners to boost generalization performance. However, conventional AdaBoost.RT algorithms suffer from the limitation that the threshold value must be manually specified rather than chosen through a self-adaptive mechanism, which cannot guarantee a result in an optimal model for general cases. In this paper, we present a generic AdaBoost framework with robust threshold mechanism and structural optimization on regression problems. The error statistics of each weak learner on one given problem dataset is utilized to automate the choice of the optimal cut-off threshold value. In addition, a special single-layer neural network is employed to provide a second opportunity to further adjust the structure and strength the adaption capability of the AdaBoost regression model. Moreover, to consolidate the theoretical foundation of AdaBoost algorithms, we are the first to conduct a rigorous and comprehensive theoretical analysis on the proposed approach. We prove that the general bound on the empirical error with a fraction of training examples is always within a limited soft margin, which indicates that our novel algorithm can avoid over-fitting. We further analyze the bounds on the generalization error directly under probably approximately correct learning. The extensive experimental verifications on the UCI benchmarks have demonstrated that the performance of the proposed method is superior to other state-of-the-art ensemble and single learning algorithms. Furthermore, a real-world indoor positioning application has also revealed that the proposed method has higher positioning accuracy and faster speed.","2168-2267;21682267","","10.1109/TCYB.2016.2623900","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756295","AdaBoost algorithm;bounds of empirical error and generalization error;ensemble method;indoor positioning system;robust threshold;special single-layer neural network;structural optimization","Algorithm design and analysis;Approximation algorithms;Machine learning algorithms;Neural networks;Optimization;Robustness;Training","","","","","","","","20161124","","","IEEE","IEEE Early Access Articles"
"Imbalanced network traffic classification based on ensemble feature selection","Yaojun Ding","Department of Information Engineering, Gansu Institute of Political Science and Law, Lanzhou, China","2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","20161124","2016","","","1","4","In order to improve the classification efficiency of large scale imbalanced network traffic, a classification method based on ensemble feature selection is proposed. The method firstly based on the characteristics of SU algorithm on different data sets to generate the feature subset. According to the data set of support degree and the threshold to produce integrated feature subset, based on the accuracy and recall rate, ROC area three criteria in the decision tree model compared the different feature selection methods of class effect. Experimental results show that the ensemble feature selection method in imbalanced network traffic classification performance is better than the general SU algorithm.","","Electronic:978-1-5090-2708-8; POD:978-1-5090-2709-5","10.1109/ICSPCC.2016.7753690","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7753690","feature selection;symmetrical uncertainty;traffic classification","Algorithm design and analysis;Classification algorithms;IP networks;Machine learning algorithms;Protocols;Telecommunication traffic;Uncertainty","decision trees;feature selection;pattern classification;set theory;telecommunication computing;telecommunication traffic","ROC area;SU algorithm;decision tree model;ensemble feature selection;integrated feature subset;large scale imbalanced network traffic classification;recall rate","","","","","","","5-8 Aug. 2016","","IEEE","IEEE Conference Publications"
"2-D localization of passive UHF RFID tags using location fingerprinting","S. Nosović; A. Ascher; J. Lechner; B. Bruegge","Lehrstuhl f&#x00FC;r Angewandte Softwaretechnik, Technische Universit&#x00E4;t M&#x00FC;nchen, Germany","2016 8th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)","20161205","2016","","","207","212","Localization of UHF RFID tags in an industrial environments is difficult due to signal reflections and multipaths caused by steel and metal objects. Existing solutions have shown decent accuracy for small distances but fail to maintain the accuracy as the distance between the antenna and the tag increases. In this paper, we describe a novel UHF RFID localization approach based on location fingerprinting. The approach uses machine learning to transform localization into a classification problem. Location fingerprints are generated using outputs Bartlett beamformer and MUSIC algorithms that estimate the incoming angle of a signal. We evaluated our approach in an industrial environment, and the results show that we achieve a high classification accuracy and maintain it with the increase of the distance between the tag and the antenna.","","Electronic:978-1-4673-8818-4; POD:978-1-4673-8819-1; USB:978-1-4673-8817-7","10.1109/ICUMT.2016.7765358","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765358","","Antenna measurements;Antennas;Machine learning algorithms;Multiple signal classification;Passive RFID tags","UHF antennas;array signal processing;direction-of-arrival estimation;learning (artificial intelligence);radiofrequency identification;signal classification","2D localization;Bartlett beamformer;MUSIC algorithms;UHF RFID localization;UHF antenna;angle of arrival estimation;classification accuracy;industrial environments;location fingerprinting;machine learning;passive UHF RFID tags;radiofrequency identification;signal reflections","","","","","","","18-20 Oct. 2016","","IEEE","IEEE Conference Publications"
"Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting","A. Jourabloo; X. Liu","Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","4188","4196","Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction. Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results. In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM. We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors. The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.454","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780823","","Databases;Face;Machine learning;Shape;Solid modeling;Three-dimensional displays;Two dimensional displays","cameras;computer vision;face recognition;feature extraction;learning (artificial intelligence);matrix algebra;neural nets;pose estimation;visual databases","3D shape parameters;3DMM fitting problem;AFLW database;AFW database;CNN learning;CNN-based dense 3D model fitting;CNN-based regressors;camera projection matrix;cascaded CNN regressor method;computer vision;dense-3D shape;large-pose face alignment;pose-invariant appearance features","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Exploring public sentiments for livable places based on a crowd-calibrated sentiment analysis mechanism","L. You; B. Tunçer","Architecture and Sustainable Design, Singapore University of Technology and Design, Singapore 487372","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","693","700","With the explosion of social networks, people more often share their opinions on-line, which provides a great opportunity to detect the public sentiment of a place in an automatic and timely way comparing to the conventional approaches, e.g., surveys, workshops and interviews. Even through the application of social sentiment analysis is widely discussed in many domains, e.g., politics, e-commerce, economy, and health and environment, to the best of our knowledge, no research has ever studied the effects of public sentiments of social networks in the domain of place design. In order to fill this vacancy, a sentiment analysis service, called geo-sentiment analysis service, is required, whose cores are 1) a social sentiment analysis engine, and 2) an intuitive and interactive visualization service. Thus, this paper firstly proposes CGSA: a Crowd-calibrated Geo-Sentiment Analysis mechanism, which can 1) start the sentiment analysis process based on the design of CTS (Compound Training Samples), and SSF (Social Sentiment Features), 2) perform three analyses, namely sentiment, clustering and time series analysis on geotagged social network messages, and 3) collect crowd-labelled data based on a crowdsourced calibration service to gradually improve the classification accuracy. As proved by two detailed analyses, SSF has the best accuracy in training sentiment classifiers, and the performance of the calibrated classifier increases gradually and significantly from 74.71% to 80.05% in three calibration cycles. Moreover, as a part of a big project “Liveable Places”, “Sentiment in places” service with two visualization modes, namely 2D sentiment dashboard and 3D sentiment map, is implemented to support local authorities, urban designers and city planners better understand the effects of public sentiments regarding place (re)design in the testbed area: Jurong East, Singapore.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752312","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752312","","Calibration;Machine learning algorithms;Sentiment analysis;Tagging;Training;Twitter","pattern classification;public administration;sentiment analysis","2D sentiment dashboard;3D sentiment map;CGSA;CTS;SSF;calibration cycles;city planners;classification accuracy;compound training samples;crowd-calibrated geo-sentiment analysis mechanism;crowd-calibrated sentiment analysis mechanism;crowd-labelled data;crowdsourced calibration service;geo-sentiment analysis service;geotagged social network messages;interactive visualization service;livable places;public sentiments;sentiment analysis process;sentiment classifiers;social networks;social sentiment analysis engine;social sentiment features;time series analysis;urban designers","","1","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"Deep learning for benthic fauna identification","A. Marburg; K. Bigham","Applied Physics Laboratory, University of Washington, Seattle, 98105, United States of America","OCEANS 2016 MTS/IEEE Monterey","20161201","2016","","","1","5","This paper describes the application of convolutional neural networks (CNNs) to the identification and classification of ten classes of benthic macrofauna in high-resolution photomosaics captured on the Pacific continental shelf by an ROV. Each photomosaic was previously hand-annotated with the location and classification of each animal, providing a training set for the machine learning algorithms. These annotations are used to extract image patches around each contact, resulting in approximately 5000 image samples, which are supplemented with randomly selected image patches representing the background. The resulting corpus of data is used to train a series of convolutional neural networks in the Nvidia DIGITS and Google Tensorflow environments. Due to the relatively sparse nature of the training data set, a number of data augmentation approaches are used to increase the diversity of training data. The performance of the resulting algorithm is evaluated in three problem scenarios: first, classification of fauna in an image patch known to contain a target; second, classification of a given image patch as either background or non-background; and third, a single-pass combination of the two problems. The presented networks prove highly accurate at background/non-background segmentation with ~96% accuracy. Fauna identification is less reliable at ~89% accuracy, and unified segmentation and identification proves to be the most challenging at ~88% accuracy.","","Electronic:978-1-5090-1537-5; POD:978-1-5090-1527-6","10.1109/OCEANS.2016.7761146","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7761146","","Biological system modeling;Google;Machine learning;Marine animals;Neural networks;Training","biology;convolution;feature extraction;identification;image classification;learning (artificial intelligence);neural nets","CNN;benthic fauna identification;convolutional neural network;deep learning;fauna classification;image patch extraction;machine learning algorithm","","","","","","","19-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"Deep learning based parts of speech tagger for Bengali","M. F. Kabir; K. Abdullah-Al-Mamun; M. N. Huda","Department of Computer Science and Engineering, United International University, Dhaka, Bangladesh","2016 5th International Conference on Informatics, Electronics and Vision (ICIEV)","20161201","2016","","","26","29","This paper describes the Part of Speech (POS) tagger for Bengali Language. Here, POS tagging is the process of assigning the part of speech tag or other lexical class marker to each and every word in a sentence. In many Natural Language Processing (NLP) applications, POS tagging is considered as the one of the basic necessary tools. Identifying the ambiguities in language lexical items is the challenging objective in the process of developing an efficient and accurate POS Tagger. Different methods of automating the process have been developed and employed for Bengali. In this paper, we report about our work on building POS tagger for Bengali using the Deep Learning. Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words. It is observed from the experiments based on Linguistic Data Consortium (LDC) catalog number LDC2010T16 and ISBN 1-58563-561-8 corpus that 93.33% accuracy is obtained for Bengali POS tagger using the Deep Learning.","","Electronic:978-1-5090-1269-5; POD:978-1-5090-1270-1","10.1109/ICIEV.2016.7760098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760098","Deep Belief Network;Deep Learning;Linear Activation Function;Part of Speech Tagging","Dictionaries;Hidden Markov models;Machine learning;Natural language processing;Pragmatics;Speech;Tagging","learning (artificial intelligence);natural language processing;text analysis;word processing","Bengali language;LDC;NLP;POS tagging;deep learning;linguistic data consortium;natural language processing;parts of speech tagging","","","","","","","13-14 May 2016","","IEEE","IEEE Conference Publications"
"Comparative Deep Learning of Hybrid Representations for Image Recommendations","C. Lei; D. Liu; W. Li; Z. J. Zha; H. Li","CAS Key Lab. of Technol. in Geo-Spatial Inf. Process. & Applic. Syst., Univ. of Sci. & Technol. of China, Hefei, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2545","2553","In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed hybrid and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two sub-networks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other state-of-the-art image recommendation solutions.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.279","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780648","","Convolution;Image representation;Machine learning;Semantics;Training;Training data;Visualization","image representation;learning (artificial intelligence);recommender systems","comparative deep learning method;dual-net deep network;hybrid representations;image recommendations;image representations;latent space;network complexity","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Fusion and binarization of CNN features for robust topological localization across seasons","R. Arroyo; P. F. Alcantarilla; L. M. Bergasa; E. Romera","Department of Electronics, University of Alcal&#x00E1; (UAH), Alcal&#x00E1; de Henares, 28871, Madrid, Spain","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20161201","2016","","","4656","4663","The extreme variability in the appearance of a place across the four seasons of the year is one of the most challenging problems in life-long visual topological localization for mobile robotic systems and intelligent vehicles. Traditional solutions to this problem are based on the description of images using hand-crafted features, which have been shown to offer moderate invariance against seasonal changes. In this paper, we present a new proposal focused on automatically learned descriptors, which are processed by means of a technique recently popularized in the computer vision community: Convolutional Neural Networks (CNNs). The novelty of our approach relies on fusing the image information from multiple convolutional layers at several levels and granularities. In addition, we compress the redundant data of CNN features into a tractable number of bits for efficient and robust place recognition. The final descriptor is reduced by applying simple compression and binarization techniques for fast matching using the Hamming distance. An exhaustive experimental evaluation confirms the improved performance of our proposal (CNN-VTL) with respect to state-of-the-art methods over varied long-term datasets recorded across seasons.","","Electronic:978-1-5090-3762-9; POD:978-1-5090-3763-6; USB:978-1-5090-3761-2","10.1109/IROS.2016.7759685","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759685","","Computer architecture;Computer vision;Machine learning;Neural networks;Proposals;Robustness;Visualization","convolution;data compression;feature extraction;image coding;image fusion;image matching;mobile robots;neurocontrollers;robot vision","CNN features;Hamming distance;automatically learned descriptors;binarization techniques;computer vision;convolutional neural networks;data compression;fast matching;hand-crafted features;image description;image fusion;intelligent vehicles;life-long visual topological localization;mobile robotic systems;place recognition;robust topological localization;seasonal changes","","1","","","","","9-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Research on Text Categorization of KNN Based on K-Means for Class Imbalanced Problem","W. Yu; X. Linying","Sch. Of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China","2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)","20161208","2016","","","579","583","With the rapid development of Web and the rapid expansion of text information, how to effectively organize and manage these information is a great challenge for the current information science. Text automatic classification technology can effectively organize a large number of texts and help people to improve the efficiency of information retrieval. It has become one of the most important research directions in the field of information processing. There are many mature methods of text classification, where K-Nearest Neighbor algorithm has good accuracy, it is suitable for multiple classification problems and has been widely used in the field of document classification. However, when dealing with the training set with class imbalanced problem, the classification results tend to be biased towards majority class, so that the accuracy of the classifier is greatly reduced. In order to solve this problem, two strategies that construction of samples based on clustering and weighted KNN based on sample density are proposed in this paper to improve the traditional KNN algorithm. Four datasets which have different class imbalanced rates are extracted from the entire corpus, and we use classic KNN, NWKNN and Kmeans-KNN algorithm to perform cross validation on each dataset. The results show that compared with the traditional KNN algorithm and NWKNN algorithm, the proposed method can effectively improve the classification accuracy and G-mean value, and has better stability under the class imbalanced problem.","","CD:978-1-5090-1194-0; Electronic:978-1-5090-1195-7; POD:978-1-5090-1196-4","10.1109/IMCCC.2016.61","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774847","Class imbalanced problem;K-Nearest Neighbor;K-means;Text categorization","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Machine learning algorithms;Mathematical model;Text categorization;Training","learning (artificial intelligence);pattern classification;text analysis","KNN;class imbalanced problem;information management;information organization;information processing;information retrieval;k-means;k-nearest neighbor algorithm;text categorization;text information;weighted KNN","","","","","","","21-23 July 2016","","IEEE","IEEE Conference Publications"
"CORP: Cooperative Opportunistic Resource Provisioning for Short-Lived Jobs in Cloud Systems","J. Liu; H. Shen; L. Chen","Dept. of Electr. & Comput. Eng., Clemson Univ., Clemson, SC, USA","2016 IEEE International Conference on Cluster Computing (CLUSTER)","20161208","2016","","","90","99","In cloud systems, achieving high resource utilization and low Service Level Objective (SLO) violation rate are important to the cloud provider for high profit. For this purpose, recently, some methods have been proposed to predict allocated but unused resources and reallocate them to long-running service jobs. However, the accuracy of their prediction method relies on the existence of patterns in jobs' resource utilization. Therefore, these methods cannot be used for short-lived jobs, which usually do not have certain patterns but exhibit frequent fluctuations in resource requirements. Also, these methods may result in resource fragmentation and lead to low resource utilization because they neglect job resource intensity in multi-resource allocation and may allocate much more resources to jobs. To handle this problem, we propose a Cooperative Opportunistic Resource Provisioning scheme (CORP) for short-lived jobs. CORP uses the deep learning method to predict the amount of temporarily-unused resource of each short-lived job. It also predicts the fluctuations of the amount of unused resource using Hidden Markov Model, and adjusts the predicted amount for the peak and valley of unused resource, and dynamically allocates the corrected amount of resource to jobs. Further, CORP uses a job packing strategy by leveraging complementary jobs' requirements on different resource types and allocates such jobs to the same VM to fully utilize unused resources, which increases resource utilization. Extensive experimental results based on a real cluster and Amazon EC2 show that CORP achieves high resource utilization and low SLO violation rate compared to previous resource provisioning schemes.","","Electronic:978-1-5090-3653-0; POD:978-1-5090-3654-7","10.1109/CLUSTER.2016.65","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776482","Cloud;Prediction;Quality of service;Resource allocation;Resource utilization","Cloud computing;Hidden Markov models;Machine learning;Micromechanical devices;Prediction methods;Resource management;Training data","cloud computing;resource allocation","CORP;SLO violation rate;VM;cloud provider;cloud systems;complementary jobs requirements;cooperative opportunistic resource provisioning scheme;hidden Markov model;high resource utilization;job packing strategy;job resource intensity;multiresource allocation;resource fragmentation;resource provisioning schemes;service level objective violation;short-lived jobs;temporarily-unused resource","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"Vietnamese transition-based dependency parsing with supertag features","K. V. Nguyen; Ngan Luu-Thuy Nguyen","Department of Information Science and Engineering, University of Information Technology, Vietnam National University - Ho Chi Minh City, Vietnam","2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","20161201","2016","","","175","180","In recent years, dependency parsing is a fascinating research topic and has a lot of applications in natural language processing. In this paper, we present an effective approach to improve dependency parsing by utilizing supertag features. We performed experiments with the transition-based dependency parsing approach because it can take advantage of rich features. Empirical evaluation on Vietnamese Dependency Treebank showed that, we achieved an improvement of 18.92% in labeled attachment score with gold supertags and an improvement of 3.57% with automatic supertags.","","Electronic:978-1-4673-8929-7; POD:978-1-4673-8930-3","10.1109/KSE.2016.7758049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758049","Dependency parsing;supertags;transition-based parsing system","Electronic mail;Gold;Information technology;Machine learning algorithms;Natural language processing;Syntactics;Urban areas","grammars;natural language interfaces","Vietnamese Dependency Treebank;Vietnamese transition-based dependency parsing;automatic supertags;dependency parsing improvement;gold supertags;natural language processing;supertag features","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Deep learning approach for Network Intrusion Detection in Software Defined Networking","T. A. Tang; L. Mhamdi; D. McLernon; S. A. R. Zaidi; M. Ghogho","School of Electronic and Electrical Engineering, The University of Leeds, UK","2016 International Conference on Wireless Networks and Mobile Communications (WINCOM)","20161208","2016","","","258","263","Software Defined Networking (SDN) has recently emerged to become one of the promising solutions for the future Internet. With the logical centralization of controllers and a global network overview, SDN brings us a chance to strengthen our network security. However, SDN also brings us a dangerous increase in potential threats. In this paper, we apply a deep learning approach for flow-based anomaly detection in an SDN environment. We build a Deep Neural Network (DNN) model for an intrusion detection system and train the model with the NSL-KDD Dataset. In this work, we just use six basic features (that can be easily obtained in an SDN environment) taken from the forty-one features of NSL-KDD Dataset. Through experiments, we confirm that the deep learning approach shows strong potential to be used for flow-based anomaly detection in SDN environments.","","Electronic:978-1-5090-3837-4; POD:978-1-5090-3938-8","10.1109/WINCOM.2016.7777224","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777224","SDN;deep learning;intrusion detection;network security;software defined networking","Computer crime;Feature extraction;Intrusion detection;Machine learning;Monitoring;Software","computer network security;neural nets;software defined networking","DNN model;NSL-KDD Dataset;SDN;deep learning approach;deep neural network model;flow-based anomaly detection;future Internet;global network overview;intrusion detection system;network security;potential threats;software defined networking","","","","","","","26-29 Oct. 2016","","IEEE","IEEE Conference Publications"
"Beyond algorithms: Evolving intelligence","R. Eberhart","Professor Emeritus, Purdue School of Engineering and Technology, IUPUI, Indianapolis, Indiana USA","2016 Swarm/Human Blended Intelligence Workshop (SHBI)","20161212","2016","","","1","4","This paper discusses possible approaches to evolving intelligence in which blended intelligence and extended analog computing play roles. Deep learning and universal learning are briefly summarized. A definition of blended intelligence is proposed, followed by an introduction to extended analog computing. Implementing extended analog computing to achieve intelligence without algorithms is discussed. A revised definition of blended intelligence that includes algorithmless computing is proposed.","","Electronic:978-1-5090-3502-1; POD:978-1-5090-3503-8","10.1109/SHBI.2016.7780280","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780280","algorithms;blended intelligence;extended analog computing;intelligence;swarm intelligence","Analog computers;Computational modeling;Machine learning;Neural networks;Particle swarm optimization;Pins;Program processors","analogue computers;learning (artificial intelligence)","algorithmless computing;blended intelligence;deep learning;evolving intelligence;extended analog computing;universal learning","","","","","","","21-23 Oct. 2016","","IEEE","IEEE Conference Publications"
"Learning Semantic Representations for Rating Vietnamese Comments","D. H. Pham; A. C. Le; T. K. C. Le","University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam","2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","20161201","2016","","","193","198","Opinion mining and sentiment analysis has recently become a hot topic in the field of natural language processing and text mining. This paper addresses the problem of overall rating for comments in Vietnamese language. The traditional approach of using bag-of-words for feature representation would cause a very high dimensional feature space and doesn't reflect relationship between words. To capture more linguistic information, this paper provides a new neural network model containing three layers: (1) word embedding; (2) comment representation (i.e. comment feature vector); and (3) comment rating prediction. In which, the word embedding layer is designed to learn word embeddings which can capture semantic and syntactic relations between words, the second layer uses a semantic composition model for comment representation, and the third layer is designed as a perceptron and it stands for predicting overall rating of a comment. In experiment, we use a Vietnamese data set which contains comments on the domain of mobile phone products. Experimental results show that our proposed model outperforms traditional neural network models with comment representations based on bag of word model or word vector averaging.","","Electronic:978-1-4673-8929-7; POD:978-1-4673-8930-3","10.1109/KSE.2016.7758052","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758052","Comment rating prediction;Deep learning;Neural network;Semantic composition model;Sentiment analysis","Computational modeling;Machine learning;Neural networks;Predictive models;Semantics;Sentiment analysis","data mining;learning (artificial intelligence);neural nets;sentiment analysis","Vietnamese comments;Vietnamese data set;Vietnamese language;comment feature vector;comment rating prediction;comment representations;feature representation;learning;mobile phone products;natural language processing;neural network models;opinion mining;semantic composition model;semantic representations;sentiment analysis;syntactic relations;text mining;word embedding layer;word embeddings;word vector averaging","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Sketch Me That Shoe","Q. Yu; F. Liu; Y. Z. Song; T. Xiang; T. M. Hospedales; C. C. Loy","Queen Mary, Univ. of London, London, UK","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","799","807","We investigate the problem of fine-grained sketch-based image retrieval (SBIR), where free-hand human sketches are used as queries to perform instance-level retrieval of images. This is an extremely challenging task because (i) visual comparisons not only need to be fine-grained but also executed cross-domain, (ii) free-hand (finger) sketches are highly abstract, making fine-grained matching harder, and most importantly (iii) annotated cross-domain sketch-photo datasets required for training are scarce, challenging many state-of-the-art machine learning techniques. In this paper, for the first time, we address all these challenges, providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application. We introduce a new database of 1,432 sketchphoto pairs from two categories with 32,000 fine-grained triplet ranking annotations. We then develop a deep tripletranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data. Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for finegrained cross-domain ranking tasks.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.93","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780462","","Data models;Footwear;Image retrieval;Machine learning;Training;Training data;Visualization","image matching;image retrieval;learning (artificial intelligence)","annotated cross-domain sketch-photo datasets;data augmentation;deep network training;deep tripletranking model;fine-grained cross-domain ranking tasks;fine-grained matching;fine-grained sketch-based image retrieval;fine-grained triplet ranking annotations;fine-grained visual comparisons;free-hand human sketches;instance-level SBIR;instance-level image retrieval;machine learning techniques;shoe sketching","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Temporal Multimodal Learning in Audiovisual Speech Recognition","D. Hu; X. Li; X. Lu","Sch. of Comput. Sci. & Center for Opt. IMagery Anal. & Learning, Northwestern Polytech. Univ., Xi'an, China","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","3574","3582","In view of the advantages of deep networks in producing useful representation, the generated features of different modality data (such as image, audio) can be jointly learned using Multimodal Restricted Boltzmann Machines (MRB-M). Recently, audiovisual speech recognition based the M-RBM has attracted much attention, and the MRBM shows its effectiveness in learning the joint representation across audiovisual modalities. However, the built networks have weakness in modeling the multimodal sequence which is the natural property of speech signal. In this paper, we will introduce a novel temporal multimodal deep learning architecture, named as Recurrent Temporal Multimodal RB-M (RTMRBM), that models multimodal sequences by transforming the sequence of connected MRBMs into a probabilistic series model. Compared with existing multimodal networks, it's simple and efficient in learning temporal joint representation. We evaluate our model on audiovisual speech datasets, two public (AVLetters and AVLetters2) and one self-build. The experimental results demonstrate that our approach can obviously improve the accuracy of recognition compared with standard MRBM and the temporal model based on conditional RBM. In addition, RTMRBM still outperforms non-temporal multimodal deep networks in the presence of the weakness of long-term dependencies.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.389","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780758","","Hidden Markov models;Lips;Machine learning;Mouth;Speech;Speech recognition;Visualization","Boltzmann machines;learning (artificial intelligence);probability;recurrent neural nets;speech recognition","MRB-M;RTMRBM;audiovisual modalities;audiovisual speech datasets;audiovisual speech recognition;deep networks;multimodal restricted Boltzmann machines;multimodal sequence;probabilistic series model;recurrent temporal multimodal RB-M;speech signal;temporal multimodal deep learning architecture","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"A novel feature fitting simulation algorithm for estimating electric vehicle demand","S. M. Shahrukh; K. Bhattacharya","Dept. of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada, N2L 3G1","2016 IEEE Electrical Power and Energy Conference (EPEC)","20161208","2016","","","1","7","Electric vehicles will drive the future, therefore forecasting and simulating `transportation electrification' demand over the coming years has become important for utilities. Ever since electricity was commercialized there has been a need for demand forecasting and simulation because electricity provider's ability to produce energy far exceeds their ability to store energy. This is an industry worth billions of dollars and therefore even a marginal improvement in the way it's predicted can have a great impact. Demand forecasting is critical for unit commitment and broadly effects the wholesale electricity market price. With the addition of transportation electrification this process has become even more challenging. Traditional ways are hard to model and computationally intensive, nowadays this type of problem is of great interest in the field of machine learning as well, because of the availability of large datasets from utilities. However datasets for transportation electrification still remain a huge challenge therefore more work needs to be done in forecasting electric vehicular loads. This paper tackles this new problem and presents a new method called Feature Fitting Simulation Algorithm to estimate electric vehicle charging demand profiles. The simulation was performed on MATLAB and Excel using various tools and functions to ensure the algorithms run on optimum efficiency. The novel feature of the algorithm is its hybrid structure of considering both historical and simulation data for temporal predication, secondly it introduces two key variables scaling and sensitivity to better control the time series output. FFSA is vetted against machine learning algorithms and the results indicate a better performance achieved by FFSA.","","Electronic:978-1-5090-1919-9; POD:978-1-5090-1920-5","10.1109/EPEC.2016.7771780","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7771780","Demand forecasting;Electric vehicles","Computational modeling;Data models;Load modeling;Machine learning algorithms;Predictive models;Sensitivity;Vehicles","electric vehicles;feature selection;learning (artificial intelligence);load forecasting;power engineering computing;secondary cells","demand forecasting;electric vehicle charging demand profiles;electric vehicle demand;feature fitting simulation algorithm;transportation electrification demand;unit commitment","","","","","","","12-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"VLAD3: Encoding Dynamics of Deep Features for Action Recognition","Y. Li; W. Li; V. Mahadevan; N. Vasconcelos","","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","1951","1960","Previous approaches to action recognition with deep features tend to process video frames only within a small temporal region, and do not model long-range dynamic information explicitly. However, such information is important for the accurate recognition of actions, especially for the discrimination of complex activities that share sub-actions, and when dealing with untrimmed videos. Here, we propose a representation, VLAD for Deep Dynamics (VLAD<sup>3</sup>), that accounts for different levels of video dynamics. It captures short-term dynamics with deep convolutional neural network features, relying on linear dynamic systems (LDS) to model medium-range dynamics. To account for long-range inhomogeneous dynamics, a VLAD descriptor is derived for the LDS and pooled over the whole video, to arrive at the final VLAD<sup>3</sup> representation. An extensive evaluation was performed on Olympic Sports, UCF101 and THUMOS15, where the use of the VLAD<sup>3</sup> representation leads to state-of-the-art results.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.215","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780584","","Computational modeling;Dynamics;Feature extraction;Hidden Markov models;Machine learning;Nonhomogeneous media;Trajectory","feature extraction;image representation;object recognition;video signal processing","LDS;Olympic Sports dataset;THUMOS15 dataset;UCF101 dataset;VLAD for deep dynamics approach;VLAD<sup>3</sup> approach;action recognition;deep features;linear dynamic systems;video dynamics","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Industry Track Abstracts","","","2016 IEEE International Conference on Healthcare Informatics (ICHI)","20161208","2016","","","xxxiv","xxxviii","Provides an abstract for each of the presentations and a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.","","Electronic:978-1-5090-6117-4; POD:978-1-5090-6118-1","10.1109/ICHI.2016.109","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776314","","Data mining;Economics;Industries;Informatics;Machine learning algorithms;Medical services","","","","","","","","","4-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Is it truly a 5-star movie? Restoring the movie's truthful rating","W. Huang; Y. Yu","Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai, China","2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20161124","2016","","","1337","1338","Authenticity is the key for online review sites. Due to the significant development of review sites, the reviews are now highly important to users, producers and other stakeholders. Driven by interest, some imposters begin to post fake reviews to promote or discredit target products. The fake reviews not only mislead the users but also damage the service provider's credit. Current works mostly aim at classifying whether a specific review is fake or not, using context-based or user-based approaches. However, the aggregated rating of the product is viewer's most concern. Therefore, we propose a novel task to restore the truthful rating and further tackle it by statistical and deep learning techniques. We also assemble and publish a movie-review dataset for this task.","","Electronic:978-1-5090-2846-7; POD:978-1-5090-2847-4; USB:978-1-5090-2845-0","10.1109/ASONAM.2016.7752409","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752409","","Linear regression;Machine learning;Motion pictures;Roads;Stakeholders;Testing;Training","Web sites;entertainment;learning (artificial intelligence);reviews","authenticity;deep learning techniques;fake reviews;movie truthful rating;movie-review dataset;online review sites","","","","","","","18-21 Aug. 2016","","IEEE","IEEE Conference Publications"
"Generation and Comprehension of Unambiguous Object Descriptions","J. Mao; J. Huang; A. Toshev; O. Camburu; A. Yuille; K. Murphy","Univ. of California, Los Angeles, Los Angeles, CA, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","11","20","We propose a method that can generate an unambiguous description (known as a referring expression) of a specific object or region in an image, and which can also comprehend or interpret such an expression to infer which object is being described. We show that our method outperforms previous methods that generate descriptions of objects without taking into account other potentially ambiguous objects in the scene. Our model is inspired by recent successes of deep learning methods for image captioning, but while image captioning is difficult to evaluate, our task allows for easy objective evaluation. We also present a new large-scale dataset for referring expressions, based on MSCOCO. We have released the dataset and a toolbox for visualization and evaluation, see https://github.com/ mjhucla/Google_Refexp_toolbox.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780378","","Automobiles;Context;Google;Machine learning;Recurrent neural networks;Training;Visualization","learning (artificial intelligence);object detection","MSCOCO;dataset evaluation;dataset visualization;deep learning;image captioning;unambiguous object descriptions","","1","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"DeepCham: Collaborative Edge-Mediated Adaptive Deep Learning for Mobile Object Recognition","D. Li; T. Salonidis; N. V. Desai; M. C. Chuah","Lehigh Univ., Bethlehem, PA, USA","2016 IEEE/ACM Symposium on Edge Computing (SEC)","20161208","2016","","","64","76","Deep learning techniques achieve state-of-the-art performance on many computer vision related tasks, e.g. large-scale object recognition. In this paper we show that recognition accuracy degrades when used in daily mobile scenarios due to context variations caused by different locations, time of a day, etc. To solve this problem, we present DeepCham - the first adaptive mobile object recognition framework that allows deep learning techniques to be used successfully in mobile environments. Specifically, DeepCham is mediated by an edge master server which coordinates with participating mobile users to collaboratively train a domain-aware adaptation model which can yield much better object recognition accuracy when used together with a domain-constrained deep model. DeepCham generates high-quality domain-aware training instances for adaptation from in-situ mobile photos using two major steps: (i) a distributed algorithm which identifies qualifying images stored in each mobile device for training, (ii) a user labeling process for recognizable objects identified from qualifying images using suggestions automatically generated by a generic deep model. Using a newly collected dataset with smartphone images collected from different locations, time of a day, and device types, we show that DeepCham improves the object recognition accuracy by 150% when compared to that achieved merely using a generic deep model. In addition, we investigated how major design factors affect the performance of DeepCham. Finally, we demonstrate the feasibility of DeepCham using an implemented prototype.","","Electronic:978-1-5090-3322-5; POD:978-1-5090-3323-2","10.1109/SEC.2016.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774674","Adaptive Deep Learning;Crowd-sourcing;Edge Computing;Mobile Object Recognition","Adaptation models;Machine learning;Mobile communication;Mobile handsets;Object recognition;Training;Visualization","computer vision;learning (artificial intelligence);mobile computing;object recognition;smart phones","DeepCham;collaborative edge-mediated adaptive deep learning;computer vision related tasks;design factors;domain-aware adaptation model;domain-constrained deep model;edge master server;generic deep model;large-scale object recognition;mobile object recognition framework;mobile scenarios;participating mobile users;smartphone images","","","","","","","27-28 Oct. 2016","","IEEE","IEEE Conference Publications"
"Multiview Convolutional Neural Networks for Multidocument Extractive Summarization","Y. Zhang; M. J. Er; R. Zhao; M. Pratama","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 636798.","IEEE Transactions on Cybernetics","","2016","PP","99","1","13","Multidocument summarization has gained popularity in many real world applications because vital information can be extracted within a short time. Extractive summarization aims to generate a summary of a document or a set of documents by ranking sentences and the ranking results rely heavily on the quality of sentence features. However, almost all previous algorithms require hand-crafted features for sentence representation. In this paper, we leverage on word embedding to represent sentences so as to avoid the intensive labor in feature engineering. An enhanced convolutional neural networks (CNNs) termed multiview CNNs is successfully developed to obtain the features of sentences and rank sentences jointly. Multiview learning is incorporated into the model to greatly enhance the learning capability of original CNN. We evaluate the generic summarization performance of our proposed method on five Document Understanding Conference datasets. The proposed system outperforms the state-of-the-art approaches and the improvement is statistically significant shown by paired $t$-test.","2168-2267;21682267","","10.1109/TCYB.2016.2628402","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756666","Convolutional neural networks (CNNs);deep learning;multidocument summarization (MDS);multiview learning;word embedding","Computational modeling;Computer vision;Data mining;Feature extraction;Machine learning;Neural networks;Semantics","","","","","","","","20161128","","","IEEE","IEEE Early Access Articles"
"Actions ~ Transformations","X. Wang; A. Farhadi; A. Gupta","Carnegie Mellon Univ., Pittsburgh, PA, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20161212","2016","","","2658","2667","What defines an action like ""kicking ball""? We argue that the true meaning of an action lies in the change or transformation an action brings to the environment. In this paper, we propose a novel representation for actions by modeling an action as a transformation which changes the state of the environment before the action happens (precondition) to the state after the action (effect). Motivated by recent advancements of video representation using deep learning, we design a Siamese network which models the action as a transformation on a high-level feature space. We show that our model gives improvements on standard action recognition datasets including UCF101 and HMDB51. More importantly, our approach is able to generalize beyond learned action categories and shows significant performance improvement on cross-category generalization on our new ACT dataset.","","Electronic:978-1-4673-8851-1; POD:978-1-4673-8852-8","10.1109/CVPR.2016.291","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780660","","Feature extraction;Hidden Markov models;Machine learning;Standards;Testing;Training;Visualization","image recognition;image representation;learning (artificial intelligence);video signal processing","action recognition;action representation;action transformation;cross-category generalization;deep learning;high-level feature space;siamese network;video representation","","","","","","","27-30 June 2016","","IEEE","IEEE Conference Publications"
"Conflict Prediction-Based Transaction Execution for Transactional Memory in Multi-core In-memory Databases","M. Yoon; M. H. Kang; Y. W. Jang; J. W. Chang","Dept. of Comput. Eng., Chonbuk Nat. Univ., Jeonju, South Korea","2016 IEEE International Conference on Cluster Computing (CLUSTER)","20161208","2016","","","148","149","This paper proposed a novel hybrid transactional memory(HyTM) that exploits the benefits of both Haswell's RTM(restricted transactional memory) and software transactional memory(STM). Unlike the existing HyTMs, the proposed HyTM can predict and resolve conflicts between transaction running concurrently by using a prediction matrix and transaction metadata. Also the proposed HyTM can provide the optimal HTM configuration for a given workload by computing the optimal retry threshold based on deep learning algorithms.","","Electronic:978-1-5090-3653-0; POD:978-1-5090-3654-7","10.1109/CLUSTER.2016.79","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776495","HTM;STM;concurrency control;in-memory database;multi-core;transactional memory","Databases;Hardware;Machine learning;Memory management;Software;Software algorithms;Synchronization","concurrency (computers);learning (artificial intelligence);meta data;multiprocessing systems;storage management;transaction processing","Haswell's RTM;HyTM;STM;concurrent transaction;conflict prediction-based transaction execution;deep learning algorithms;hybrid transactional memory;multicore in-memory databases;optimal retry threshold;prediction matrix;software transactional memory;transaction metadata","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
