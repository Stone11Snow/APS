"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6062901,5999639,6064602,6063470,6061271,6061017,6061269,6061612,6061192,6062173,6061369,6062192,6061380,6062231,6048475,6051663,6047845,5728809,6047839,6046255,6040498,6036809,6038707,6032547,6040716,6040496,6040497,6040710,6032650,6036587,6032624,6032588,6032623,6040723,6036820,6032606,6032259,6030580,6027517,6025461,6024595,6026739,6021738,6021646,6019779,5871268,5709966,6018688,5719568,6016983,6012800,6016120,6012066,6012146,6012203,6007516,6007449,6006865,6004944,6003077,6004191,6002015,5664772,5992419,5992634,5992241,5734802,5980857,5978440,5978504,5978485,5974290,5976499,5970159,5970160,5970170,5970173,5970171,5742981,5961913,5960074,5960352,5960063,5958118,5958085,5711645,5728926,5709988,5952848,5949674,5946367,5947617,5946148,5949420,5946764,5928667,5946840,5432956,5945579,5945638",2017/05/04 20:41:12
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"NEOS: A pure named entity oriented search engine","X. Li; G. Dai; S. Lai; H. Dai","College of Automation, Northwestern Polytechnical University, Xian, China","2011 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","20111027","2011","","","1","4","The new generation of search engine is in urgent need to be aware of entities in addition to search text. In this paper, we first introduce the concept of entity standardization, and then model the entity search problem in a pure entity oriented way. Finally, we implement a prototype of a pure named entity oriented search engine(NEOS), which handles various query types in semantical meaning level. The evaluation of NEOS achieves rational results.","","Electronic:978-1-4577-0894-7; POD:978-1-4577-0893-0","10.1109/ICSPCC.2011.6061612","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061612","entity search;information retrieval;search engine","Electronic mail;Feature extraction;Prototypes;Search engines;Search problems;Standardization;Transforms","information retrieval;search engines","NEOS;entity search problem;entity standardization;pure named entity oriented search engine;semantical meaning;text searching","","0","","12","","","14-16 Sept. 2011","","IEEE","IEEE Conference Publications"
"Modeling musicological information as trigrams in a system for simultaneous chord and local key extraction","J. Pauwels; J. P. Martens; M. Leman","Digital Speech and Signal Processing group (ELIS-DSSP), Ghent University, Belgium","2011 IEEE International Workshop on Machine Learning for Signal Processing","20111031","2011","","","1","6","In this paper, we discuss the introduction of a trigram musicological model in a simultaneous chord and local key extraction system. By enlarging the context of the musicological model, we hoped to achieve a higher accuracy that could justify the associated higher complexity and computational load of the search for the optimal solution. Experiments on multiple data sets have demonstrated that the trigram model has indeed a larger predictive power (a lower perplexity). This raised predictive power resulted in an improvement in the key extraction capabilities, but no improvement in chord extraction when compared to a system with a bigram musicological model.","1551-2541;15512541","Electronic:978-1-4577-1623-2; POD:978-1-4577-1621-8; USB:978-1-4577-1622-5","10.1109/MLSP.2011.6064602","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6064602","Chord extraction;key extraction;music information retrieval;music signal processing","Acoustics;Computational modeling;Context;Context modeling;Data models;Harmonic analysis;Psychoacoustic models","audio signal processing;information retrieval;music","bigram musicological model;computational load;key extraction capability;local key extraction system;multiple data sets;musicological information modeling;optimal solution;predictive power;simultaneous chord extraction system;trigram musicological model","","1","","18","","","18-21 Sept. 2011","","IEEE","IEEE Conference Publications"
"Group participation in the search process?","S. L. Toze; L. McCay Peet; E. G. Toms","iLab, Faculty of Management, Dalhousie University, Halifax, NS Canada","2011 International Conference on Collaboration Technologies and Systems (CTS)","20110711","2011","","","69","76","How do people search for information in group settings? Many assumptions are made about collaborative search from surveys and laboratory studies. Yet we have little evidence that explains how groups manage the process as our understanding is based on individual search. Using video and logfiles of groups working on complex projects, we extracted 21 episodes in which groups identified a need, searched for information, and found that information or stopped. Within each episode we identified each activity in the search process and classified each by level of participation. Results showed that Individuals did most of the Finding, while the Group/Pairs were involved in identifying gaps (Needing) and extracting information from found objects (Using).","","Electronic:978-1-61284-639-2; POD:978-1-61284-638-5","10.1109/CTS.2011.5928667","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928667","Collaboration;Groupwork;Information Search and Retrieval;Workflow","Collaboration;Data analysis;Data mining;Laboratories;Portable computers;Search problems","information retrieval","collaborative search;group participation;information search process","","1","","33","","","23-27 May 2011","","IEEE","IEEE Conference Publications"
"Harvesting Wikipedia Knowledge to Identify Topics in Ongoing Natural Language Dialogs","A. Breuing; U. Waltinger; I. Wachsmuth","Fac. of Technol., Artificial Intell. Group, Bielefeld, Germany","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","445","450","This paper introduces a model harvesting the crowd-sourced encyclopedic knowledge provided by Wikipedia to improve the conversational abilities of an artificial agent. More precisely, we present a model for automatic topic identification in ongoing natural language dialogs. On the basis of a graph-based representation of the Wikipedia category system, our model implements six tasks essential for detecting the topical overlap of coherent dialog contributions. Thereby the identification process operates online to handle dialog streams of constantly changing topical threads in real-time. The realization of the model and its application to our conversational agent aims to improve human-agent conversations by transferring human-like topic awareness to the artificial interlocutor.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.158","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040710","Human-Agent Interaction;Information Retrieval;Topic Identification;Wikipedia","Electronic publishing;Encyclopedias;History;Humans;Internet;Natural languages","Web sites;artificial intelligence;category theory;encyclopaedias;interactive systems;natural language processing","Wikipedia category system;Wikipedia knowledge harvesting;artificial agent;artificial interlocutor;automatic topic identification process;coherent dialog contribution;encyclopedic knowledge;graph-based representation;model harvesting;ongoing natural language dialog stream","","1","","22","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Data mining technique for expertise search in a special interest group knowledge portal","W. M. Zulhafizsyam Wan Ahmad; S. Sulaiman; U. K. Yusof","School of Computer Sciences, Universiti Sains Malaysia, 11800 USM, Penang, Malaysia","2011 3rd Conference on Data Mining and Optimization (DMO)","20110804","2011","","","20","25","The Internet contributes to the development of electronic community (e-community) portals. Such portals become an indispensable platform for members especially for a Special Interest Groups (SIG) to share knowledge and expertise in their respective fields. Finding expertise over the e-community portal will help interested people and researchers to identify other experts, working in the same area. However, it is quite a cumbersome task to search such expertise in the portal. In order to find an expert, expertise data mining could be a solution to ease the search of experts. Performing effective data mining technique will help to analyze and measure expertise level accurately in a SIG portal. This paper proposes a method called Expertise Data Mining (EDM) that comprises a few techniques for expertise search in a SIG portal. It expects to improve the finding of experts among the members of a SIG e-community.","2155-6938;21556938","Electronic:978-1-61284-212-7; POD:978-1-61284-211-0","10.1109/DMO.2011.5976499","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976499","Data mining;Web mining;expertise search;information retrieval;knowledge discovery","Current measurement;Data mining;Databases;Google;HTML;Knowledge engineering;Portals","data mining;portals","SIG;e-community;electronic community portal;expertise data mining;group knowledge portal;special interest group","","0","","31","","","28-29 June 2011","","IEEE","IEEE Conference Publications"
"A layered approach for enabling context-sensitive content","M. Dinsoreanu; B. Neacsa; C. Cupse; R. Potolea","Computer Science Department, Technical University of Cluj-Napoca, Romania","2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing","20111020","2011","","","19","26","Modern society is characterized by abundance of data, yet lack of (relevant) information. A major challenge consists in selecting valuable information according to specific criteria. Moreover, ranking it and defining relevance according to the context is decisive. We propose a framework to retrieve content according to context. Our approach relies on a three layers model, each contributing to a better context matching. The implementation has been tuned, and the experiments performed on the Context Match problem.","","Electronic:978-1-4577-1481-8; POD:978-1-4577-1479-5","10.1109/ICCP.2011.6047839","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047839","advertising;classification;context match;data mining;implementation;information retrieval","Advertising;Context;HTML;History;Semantics;Taxonomy;Web pages","content management;content-based retrieval","content retrieval;context match problem;context sensitive content;three layer model","","0","","19","","","25-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Probabilistic Template-Based Chord Recognition","L. Oudre; C. Fevotte; Y. Grenier","TELECOM ParisTech, Paris, France","IEEE Transactions on Audio, Speech, and Language Processing","20110818","2011","19","8","2249","2259","This paper describes a probabilistic approach to template-based chord recognition in music signals. The algorithm only takes chromagram data and a user-defined dictionary of chord templates as input data. No training or musical information such as key, rhythm, or chord transition models is required. The chord occurrences are treated as probabilistic events, whose probabilities are learned from the song using an expectation-maximization (EM) algorithm. The adaptative estimation of these probabilities (together with an ad-hoc postprocessing filtering) has the desirable effect of smoothing out spurious chords that would occur in our previous baseline work. Our algorithm is compared to various methods that entered the Music Information Retrieval Evaluation eXchange (MIREX) in 2008 and 2009, using a diverse set of evaluation metrics, some of which are new. The systems are tested on two evaluation corpuses; the first one is composed of the Beatles catalog (180 pop-rock songs) and the other one is constituted of 20 songs from various artists and music genres. Results show that our method outperforms state-of-the-art chord recognition systems.","1558-7916;15587916","","10.1109/TASL.2010.2098870","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5664772","Chord recognition;music information retrieval (MIR);music signal processing;music signal representation","Dictionaries;Harmonic analysis;Hidden Markov models;Multiple signal classification;Probabilistic logic;Training;Vocabulary","expectation-maximisation algorithm;information retrieval;music;probability;signal representation","Beatles catalog;ad-hoc postprocessing filtering;adaptative estimation;chord recognition;chord templates;chord transition;chromagram data;expectation-maximization;music information retrieval evaluation exchange;music signals;probabilistic events;probabilistic template;user-defined dictionary","","2","","36","","20101210","Nov. 2011","","IEEE","IEEE Journals & Magazines"
"Application of an ant colony algorithm for text indexing","N. Lachetar; H. Bahi","Computer Science department, University 20 aout 1955 Skikda, Skikda, Algeria","2011 International Conference on Multimedia Computing and Systems","20110711","2011","","","1","6","Every day, the mass of information available to us increases. This information would be irrelevant if our ability to efficiently access did not increase as well. For maximum benefit, we need tools that allow us to search, sort, index, store, and analyze the available data. We also need tools helping us to find in a reasonable time the desired information by performing certain tasks for us. One of the promising areas is the automatic text categorization. Imagine ourselves in the presence of a considerable number of texts, which are more easily accessible if they are organized into categories according to their theme. Of course, one could ask a human to read the texts and classify them manually. This task is hard if done for hundreds, even thousands of texts. So, it seems necessary to have an automated application, which would consist on indexing text databases. In this article, we present our experiments in automated text categorization, where we suggest the use of an ant colony algorithm. A Naive Bayes algorithm is used as a baseline in our tests.","Pending","Electronic:978-1-61284-732-0; POD:978-1-61284-730-6","10.1109/ICMCS.2011.5945579","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945579","Ant Colony Algorithm;Information Retrieval;Naive Bayes Algorithm;Text categorization","Algorithm design and analysis;Classification algorithms;Encoding;Probability;Semantics;Text categorization;Training","Bayes methods;indexing;text analysis","Naive Bayes algorithm;ant colony algorithm;automated application;automatic text categorization;text databases;text indexing","","0","","8","","","7-9 April 2011","","IEEE","IEEE Conference Publications"
"Third international workshop on search-driven development: users, infrastructure, tools, and evaluation (SUITE 2011)","S. Bajracharya; A. Kuhn; Y. Ye","Black Duck Software, USA, USA, USA","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","1228","1229","SUITE is a workshop that focuses on exploring the notion of search as a fundamental activity during software development. The first two editions of SUITE were held at ICSE 2009/2010 [1, 2], and they have focused on the building of a research community that brings researchers and practioners who are interested in the research areas that SUITE addresses. While this thrid workshop continues the effort of community building, it puts more focus on addressing directly some of the urgent issues identified by previous two workshops, encouraging researchers to contribute to and take advantage of common datasets that we have started assembling for SUITE research.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1986059","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032650","information needs;search driven development;software information retrieval","Buildings;Conferences;Human computer interaction;Programming;Search engines;Software;Software engineering","","","","1","","2","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"Multiple query-dependent RankSVM aggregation for document retrieval","Y. Wang; M. Lu; X. Pang; M. Xie; Y. Huang","College of Information Technology Science, Nankai University, Tianjin, China","2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","20110711","2011","","","1","6","This paper is concerned with supervised rank aggregation, which aims to improve the ranking performance by combining the outputs from multiple rankers. However, there are two main shortcomings in previous rank aggregation approaches. Firstly, the learned weights for base rankers do not distinguish the differences among queries. This is suboptimal since queries vary significantly in terms of ranking. Besides, most current aggregation functions are unsupervised. A supervised aggregation function could further improve the ranking performance. In this paper, the significant difference existing among queries is taken into consideration, and a supervised rank aggregation approach is proposed. As a case study, we employ RankSVM model to aggregate the base rankers, referred to as Q.D.RSVM, and prove that Q.D.RSVM can set up query-dependent weights for different base rankers. Experimental results based on benchmark datasets show our approach outperforms conventional ranking approaches.","","Electronic:978-1-4244-9927-4; POD:978-1-4244-9926-7","10.1109/CIDM.2011.5949420","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949420","Information Retrieval;Learning to Rank;Query-dependent;Rank Aggregation;RankSVM","Aggregates;Equations;Feature extraction;Information retrieval;Mathematical model;Optimization;Training","information retrieval;support vector machines","Q.D.RSVM;document retrieval;multiple query-dependent RankSVM aggregation;query-dependent weights;supervised rank aggregation","","0","","21","","","11-15 April 2011","","IEEE","IEEE Conference Publications"
"JDF: detecting duplicate bug reports in Jazz","Y. Song; X. Wang; T. Xie; L. Zhang; H. Mei","North Carolina State University, Raleigh, NC","2010 ACM/IEEE 32nd International Conference on Software Engineering","20111027","2010","2","","315","316","Both developers and users submit bug reports to a bug repository. These reports can help reveal defects and improve software quality. As the number of bug reports in a bug repository increases, the number of the potential duplicate bug reports increases. Detecting duplicate bug reports helps reduce development efforts in fixing defects. However, it is challenging to manually detect all potential duplicates because of the large number of existing bug reports. This paper presents JDF (representing Jazz Duplicate Finder), a tool that helps users to find potential duplicates of bug reports on Jazz, which is a team collaboration platform for software development and process management. JDF finds potential duplicates for a given bug report using natural language and execution information.","0270-5257;02705257","Electronic:978-1-60558-719-6; POD:978-1-60558-719-6","10.1145/1810295.1810368","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062192","bug report;execution information;information retrieval","Calculators;Computer architecture;Educational institutions;Measurement;Natural languages;Servers;Software","groupware;natural language processing;software development management;software quality;team working","JDF;bug reports;bug repository;execution information;natural language;process management;software development;software quality;team collaboration","","1","","7","","","2-8 May 2010","","IEEE","IEEE Conference Publications"
"Exploring Melodic Motif to Support an Affect-Based Music Compositional Intelligence","R. Legaspi; A. Ueda; R. Cabredo; T. Nishikawa; K. Fukui; K. Moriyama; S. Kurihara; M. Numao","Inst. of Sci. & Ind. Res., Osaka Univ., Ibaraki, Japan","2011 Third International Conference on Knowledge and Systems Engineering","20111031","2011","","","219","225","Although the design of our constructive adaptive user interface (CAUI) for an affect-based music compositional artificial intelligence has been modified on several fronts since the time it was introduced, what has become a persisting limitation of our research is the extent by which it should efficiently cover music theory effectively. This paper reports our initial investigation on the possible significant contribution of melodic motif in creating compositions that are more fluent and cohesive. From an initial collection of 10 melodic motifs from different musical pieces, we provided heuristic-based renditions to these melodic motifs, four for each one, and obtained a total of 50 melodic motifs. We asked 10 subjects to provide self-annotations of the affective flavor of these motifs. We then represented these motifs as first-order logic predicates and employed inductive logic programming for the CAUI to learn relations of user affect perceptions and music features. To obtain new compositions, we first used a genetic algorithm with a fitness function that is based on the induced relations for the CAUI to generate chordal tone variants. We then used probabilistic modifications for the CAUI to alter these chordal tones to become non-harmonic tones. The CAUI composed 60 new user-specific affect-based musical pieces for each subject. Our results indicate that the compositions differ significantly for only one pair of affect type when the subject evaluations of the CAUI compositions were compared using paired t-test. However, when we compared the subject evaluations of the quality of the melodies and of the musical pieces from when melodic motif variants were not considered, the improvement is significant with t-values of 5.86 and 6.33, respectively, for a significance level of 0.01.","","Electronic:978-0-7695-4567-7; POD:978-1-4577-1848-9","10.1109/KSE.2011.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063470","emotion recognition;human-computer interaction;music information retrieval","Biological cells;Convergence;Genetic algorithms;Humans;Instruments;Machine learning;Training","emotion recognition;genetic algorithms;inductive logic programming;music;probabilistic logic","CAUI;affect based music compositional artificial intelligence;chordal tone variants;constructive adaptive user interface;first order logic predicate;fitness function;genetic algorithm;heuristic based rendition;inductive logic programming;melodic motif;nonharmonic tones;paired t-test;user affect perception;user specific affect based musical piece","","0","","17","","","14-17 Oct. 2011","","IEEE","IEEE Conference Publications"
"Challenges in Open Source Intelligence","C. Best","OSVision Ltd., London, UK","2011 European Intelligence and Security Informatics Conference","20111027","2011","","","58","62","A host of tools and techniques are now available for data mining on the Internet. The explosion in social media usage and people reporting brings a new range of problems related to trust and credibility. Traditional media monitoring systems have now reached such sophistication that real time situation monitoring is possible. The challenge though is deciding what reports to believe, how to index them and how to process the data. Vested interests allow groups to exploit both social media and traditional media reports for propaganda purposes. The importance of collecting reports from all sides in a conflict and of balancing claims and counter-claims becomes more important as ease of publishing increases. Today the challenge is no longer accessing open source information but in the tagging, indexing, archiving and analysis of the information. This requires the development of general-purpose and domain specific knowledge bases. Intelligence tools are needed which allow an analyst to rapidly access relevant data covering an evolving situation, ranking sources covering both facts and opinions.","","Electronic:978-0-7695-4406-9; POD:978-1-4577-1464-1","10.1109/EISIC.2011.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061192","Information Extraction;Information Retrieval;OSINT;Terrorism Forecasting","Data mining;Databases;Internet;Knowledge based systems;Media;Monitoring;Terrorism","Internet;data mining;information retrieval;public domain software","Internet;archiving;data covering;data mining;indexing;intelligence tool;media monitoring system;open source information;open source intelligence;real time situation monitoring;social media usage;tagging","","2","","19","","","12-14 Sept. 2011","","IEEE","IEEE Conference Publications"
"Nonlinear audio recurrence analysis with application to genre classification","J. Serrà; C. A. de los Santos; R. G. Andrzejak","Universitat Pompeu Fabra, Dept. of Information and Communication Technologies, Roc Boronat 138, 08018 Barcelona, Spain","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20110711","2011","","","169","172","In this paper we apply nonlinear signal analysis to a music information retrieval task. More concretely, we apply the concept of recurrence plots and recurrence histograms to extract information from music audio frames. We evaluate the effectiveness of this approach with a typical genre classification framework and compare it against a baseline obtained from standard spectrum-based descriptors. The accuracy reached by the histogram-based descriptors alone does not surpass the one achieved by the spectral-based descriptors. However, we show that the combination of both descriptor sources results in consistent improvements up to 5 absolute percent points. This high lights the potential of nonlinear signal analysis for quantitative music description. In particular, it suggests that the information resulting from this approach is complementary to the information obtained through tile commonly used spectral representation.","1520-6149;15206149","Electronic:978-1-4577-0539-7; POD:978-1-4577-0538-0; USB:978-1-4577-0537-3","10.1109/ICASSP.2011.5946367","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946367","Audio Recurrence;Descriptor Extraction;Music Information Retrieval;Nonlinear Time Series Analysis","Accuracy;Data mining;Feature extraction;Histograms;Multiple signal classification;Signal analysis;Time frequency analysis","audio signal processing;information retrieval;music;signal classification;statistical analysis","genre classification;histogram-based descriptors;music information retrieval task;nonlinear audio recurrence analysis;nonlinear signal analysis;recurrence histograms concept;recurrence plots concept;spectrum-based descriptors","","2","","14","","","22-27 May 2011","","IEEE","IEEE Conference Publications"
"Detection of transactions boundaries in a video's sequence using prediction and validation approach","L. Guezouli; H. Essafi","University of Batna, Computer Science Department, Batna - Algeria","2011 International Conference on Multimedia Computing and Systems","20110711","2011","","","1","4","This paper presents a new method of video sequence segmentation. The principal aim of our approach is the detection of cuts suites in a video sequence (boundaries of transactions). The method is based on two thresholds which allow the localization of transactions suites in a video sequence.","Pending","Electronic:978-1-61284-732-0; POD:978-1-61284-730-6","10.1109/ICMCS.2011.5945638","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945638","Information Retrieval;Video processing;Video segmentation","Equations;Histograms;Image segmentation;Mathematical model;Optical imaging;Pixel;Video sequences","image segmentation;image sequences;video signal processing","prediction approach;transactions boundaries detection;validation approach;video sequence segmentation","","0","","15","","","7-9 April 2011","","IEEE","IEEE Conference Publications"
"Multiple Pitch Estimation Using Non-Homogeneous Poisson Processes","P. H. Peeling; S. J. Godsill","Dept. of Eng., Univ. of Cambridge, Cambridge, UK","IEEE Journal of Selected Topics in Signal Processing","20110915","2011","5","6","1133","1143","Novel statistical models are proposed and developed in this paper for automated multiple-pitch estimation problems. Point estimates of the parameters of partial frequencies of a musical note are modeled as realizations from a non-homogeneous Poisson process defined on the frequency axis. When several notes are combined, the processes for the individual notes combine to give a new Poisson process whose likelihood is easy to compute. This model avoids the data-association step of linking the harmonics of each note with the corresponding partials and is ideal for efficient Bayesian inference of unknown multiple fundamental frequencies in a signal.","1932-4553;19324553","","10.1109/JSTSP.2011.2158804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871268","Bayesian methods;frequency estimation;matching pursuit algorithms;music information retrieval;spectral analysis","Bayesian methods;Clutter;Data models;Discrete Fourier transforms;Estimation;Frequency estimation;Harmonic analysis","frequency estimation;music;signal processing;stochastic processes","efficient Bayesian inference;multiple fundamental frequencies;multiple pitch estimation;musical note;non-homogeneous Poisson processes;partial frequencies;statistical models","","7","","33","","20110609","Oct. 2011","","IEEE","IEEE Journals & Magazines"
"Utilizing SSTAG: A Novel Tag Recommendation Mechanism to Web Page Search","G. He; T. Zhang; B. Lee; J. S. Kim","Sch. of Comput. Sci., Univ. of Seoul, Seoul, South Korea","2011 IEEE 11th International Conference on Computer and Information Technology","20111010","2011","","","455","460","As a social bookmark tool, Folksonomy gives high freedom to users and allows users to share and notate resources. However, many tags applied arbitrarily by users can not really reflect the contents of web pages and lead to ineffectiveness in information retrieval. Moreover, there are still some important tasks about how to eliminate ambiguity more easily and recommend more interested web pages to users. To resolve the above problems, we propose a novel mechanism named SSTAG, and it can recommend a set of Super-tags to users for their choices based on keywords input. As various topics related to the keywords, the Super-tags are selected from different clusters of web pages related to the keywords. A user chooses a Super-tag, which means the user may have chosen an interested topic, and then some more detailed tags in the topic are recommended as Sub-tags. The relationship between Super-tag and Sub-tag is just like navigation and positioning. Likewise, the user can choose one Sub-tag and submit it with the Super-tag. By means of the user's choice, this system can capture users' preference and recommend a series of related web pages. We employ a real world dataset to examine the mechanism, and the experimental results show that this mechanism can eliminate ambiguity efficiently and recommend a set of appropriate tags.","","Electronic:978-0-7695-4388-8; POD:978-1-4577-0383-6","10.1109/CIT.2011.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036809","Information Retrieval;Tag Recommendation;Web Page Search","Clustering algorithms;Educational institutions;Information filters;Semantics;Web pages","Internet;information retrieval;recommender systems","Folksonomy;SSTAG;Sub-tag;Super-tags;Web page search;information retrieval;social bookmark tool;tag recommendation mechanism;user preference","","0","","14","","","Aug. 31 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Extracting the multilingual terminology from a web-based encyclopedia","S. Fatiha","Department of Computer Science, Universit&#x00E9; du Quebec &#x00E0; Montr&#x00E9;al, 201, av. Pr&#x00E9;sident-Kennedy, local PK-4150 H2X 3Y7, Canada","2011 FIFTH INTERNATIONAL CONFERENCE ON RESEARCH CHALLENGES IN INFORMATION SCIENCE","20110829","2011","","","1","5","Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and to exploit the idea of using multilingual web-based encyclopedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and data. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statistics-based and linguistic-based approaches were applied on different pairs of languages including Japanese, French and English. These evaluations showed a real open improvement and a good quality of the extracted term candidates for building or enriching multilingual ontologies, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.","2151-1349;21511349","Electronic:978-1-4244-8671-7; POD:978-1-4244-8670-0","10.1109/RCIS.2011.6006865","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6006865","Cross-Language Information Retrieval;comparable corpora;linguistics-based information;terminology;translation","Dictionaries;Electronic publishing;Encyclopedias;Internet;Ontologies","Internet;dictionaries;encyclopaedias;ontologies (artificial intelligence);query processing;statistical analysis","English language;French language;Japanese language;Web-based encyclopedia;Wikipedia link information;bilingual terminology extraction;cross-language information retrieval system;dictionaries;multilingual linguistic resources;multilingual ontologies;multilingual terminology extraction;parallel corpora;source query;statistics-based approaches","","1","","14","","","19-21 May 2011","","IEEE","IEEE Conference Publications"
"Cataloga: A Software for Semantic-Based Terminological Data Mining","A. Elia; M. Monteleone; A. Postiglione","Dipt. di Sci. Politiche, Sociali e della Comun., Univ. degli Studi di Salerno, Fisciano, Italy","2011 First International Conference on Data Compression, Communications and Processing","20111027","2011","","","153","156","This paper is focused on Catalog a, a software package based on Lexicon-Grammar theoretical and practical analytical framework and embedding a ling ware module built on compressed terminological electronic dictionaries. We will here show how Catalog a can be used to achieve efficient data mining and information retrieval by means of lexical ontology associated to terminology-based automatic textual analysis. Also, we will show how accurate data compression is necessary to build efficient textual analysis software. Therefore, we will here discuss the creation and functioning of a software for semantic-based terminological data mining, in which a crucial role is played by Italian simple and compound-word electronic dictionaries. Lexicon-Grammar is one of the most profitable and consistent methods for natural language formalization and automatic textual analysis it was set up by French linguist Maurice Gross during the '60s, and subsequently developed for and applied to Italian by Annibale Elia, Emilio D'Agostino and Maurizio Martin Elli. Basically, Lexicon-Grammar establishes morph syntactic and statistical sets of analytic rules to read and parse large textual corpora. The analytical procedure here described will prove itself appropriate for any type of digitalized text, and will represent a relevant support for the building and implementing of Semantic Web (SW) interactive platforms.","","Electronic:978-0-7695-4528-8; POD:978-1-4577-1458-0","10.1109/CCP.2011.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061017","Automatic Textual Analysis;Cataloga software;Information Retrieval;Lexicon-Grammar;Semantic-Based Terminological Data Mining","Compounds;Data mining;Dictionaries;Geology;Pragmatics;Semantics;Software","cataloguing;data compression;data mining;dictionaries;grammars;information retrieval;interactive systems;ontologies (artificial intelligence);semantic Web;text analysis","Cataloga software package;Italian compound-word electronic dictionary;Italian simple-word electronic dictionary;automatic text analysis;compressed terminological electronic dictionary;data compression;information retrieval;lexical ontology;lexicon-grammar practical analytical framework;lexicon-grammar theoretical analytical framework;lingware module;morphosyntactic analytic rule sets;natural language formalization;semantic Web interactive platform;semantic-based terminological data mining;statistical analytic rule sets;terminology-based automatic text analysis;text analysis software;textual corpora","","0","","19","","","21-24 June 2011","","IEEE","IEEE Conference Publications"
"DRing: A Layered Scheme for Range Queries over DHTs","N. Hidalgo; E. Rosas; L. Arantes; O. Marin; P. Sens; X. Bonnaire","REGAL, Univ. Pierre et Marie Curie, Paris, France","2011 IEEE 11th International Conference on Computer and Information Technology","20111010","2011","","","29","34","Traditional DHT structures provide very poor support for range queries, since uniform hashing destroys data locality. Several schemes have been proposed to overcome this issue, but they fail to combine load balancing, low message overhead, and low latency in search operations. In this article we present DRing, an efficient layered solution that directly supports range queries over a ring-like DHT structure. We improve load balancing by using only the nodes that store data, and by updating neighbour information through an optimistic approach. DRing produces low overhead and low latency in environments where queries significantly outnumber data insertion operations. We analyze DRing through simulation and show that our solution does not rely on data distribution.","","Electronic:978-0-7695-4388-8; POD:978-1-4577-0383-6","10.1109/CIT.2011.100","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036587","DHT;Information Retrieval;Peer-to-Peer;Range Queries","Data models;Indexing;Load management;Maintenance engineering;Peer to peer computing;Routing;Structural rings","file organisation;peer-to-peer computing;query processing;resource allocation","DRing;data distribution;data insertion operations;data searches;large-scale networks;layered scheme;load balancing;low latency search operations;optimistic approach;ring-like DHT structure","","3","","11","","","Aug. 31 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"The Three Rs of Cyberphysical Spaces","V. Menon; B. Jayaraman; V. Govindaraju","Amrita Vishwa Vidyapeetham, India","Computer","20110912","2011","44","9","73","79","The ability to identify people and answer questions about their where abouts in a cyberphysical space is critical to many applications. Integrating recognition with spatiotemporal reasoning enhances the overall performance of information retrieval.","0018-9162;00189162","","10.1109/MC.2011.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5719568","Biometric recognition;Cyberphysical spaces;Information retrieval;Performance metrics;Spatiotemporal reasoning;State transition systems","Biometrics;Cognition;Cyberspace;Hidden Markov models;Information retrieval;Monitoring;Performance evaluation","inference mechanisms;question answering (information retrieval);spatiotemporal phenomena","cyberphysical space;information retrieval;people identification;spatiotemporal reasoning","","5","","9","","20110224","Sept. 2011","","IEEE","IEEE Journals & Magazines"
"FAST: Friends Augmented Search Techniques - System Design & Data-Management Issues","C. von der Weth; A. Datta","DERI, Nat. Univ. of Ireland, Galway, Ireland","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","356","363","Improving web search solely based on algorithmic refinements has reached a plateau. The emerging generation of searching techniques tries to harness the ``wisdom of crowds'', using inputs from users in the spirit of Web 2.0. In this paper, we introduce a framework facilitating friends augmented search techniques (FAST). To that end, we present a browser add-on as front end for collaborative browsing and searching, supporting synchronous and asynchronous collaboration between users. We then describe the back end, a distributed key-value store for efficient information retrieval in the presence of an evolving knowledge base. The mechanisms we explore in supporting efficient query processing for FAST are applicable for many other recent Web 2.0 applications that rely on similar key-value stores. The specific collaborative search tool we present is expected to be an useful utility in its own right and spur further research on friends augmented search techniques, while the data-management techniques we developed are of general interest and applicability.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.239","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038707","browser add-on;collaboration;distributed information retrieval;key-value store;social search","Bandwidth;Browsers;Collaboration;Indexes;Query processing;Search engines;Web pages","Internet;data handling;groupware;online front-ends;query processing","FAST;Web 2.0;Web search;algorithmic refinements;asynchronous collaboration;browser add-on;collaborative browsing;collaborative search tool;collaborative searching;data-management issue;data-management techniques;distributed key-value store;friends augmented search techniques;information retrieval;knowledge base;query processing;synchronous collaboration;system design issue;wisdom of crowds","","1","","19","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Requirements tracing: discovering related documents through artificial pheromones and term proximity","H. Sultanov","University of Kentucky, Lexington, KY, USA","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","1173","1175","Requirements traceability is an important undertaking as part of ensuring the quality of software in the early stages of the Software Development Life Cycle. This paper demonstrates the applicability of swarm intelligence to the requirements tracing problem using pheromone communication and a focus on the common text around linking terms or words in order to find related textual documents. Through the actions and contributions of each individual member of the swarm, the swarm as a whole exposes relationships between documents in a collective manner. Two techniques have been examined, simple swarm and pheromone swarm. The techniques have been validated using two real-world datasets from two problem domains.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1986033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032624","information retrieval;requirements traceability;software engineering;swarms","Educational institutions;Joining processes;Presses;Software;Thesauri;Vocabulary","document handling;software quality","artificial pheromones;document discovery;pheromone communication;pheromone swarm;requirements traceability;requirements tracing problem;simple swarm;software development life cycle;software quality;swarm intelligence;term proximity;textual documents","","0","","13","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"Classifying the Arabic web — A pilot study","M. Abdeen; A. Elsehemy; T. Nazmy; M. C. E. Yagoub","Dept. of Comput. Sci., Ain Shams Univ., Cairo, Egypt","2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)","20110929","2011","","","000865","000868","The world-wide-web has become the favorite destination of information seekers across the globe. With its massive amount of information that includes billions of web pages, information for just about any topic is a click-of-finger away. Analyzing the massive content of the web has many important aspects such as information discovering, efficient search engines and social and political patterns. Web mining techniques such as text classification and categorization are being used to provide an ""under-the-microscope"" picture of the web. The Arabic web represents an important portion of the web. With Arabic as the 5th most spoken language in the world and with the increasing number of Arabic Internet users at exponential rates, it is becoming important to analyze the Arabic web content and study its trends. This paper presents a close look at the content of the Arabic web. It presents the percentiles of the contents of the web in five categories, namely, politics, culture, sports, economics and religion. We used two different text classification algorithms and compared their results. We have also compared between the two text classification techniques in terms of precision and recall. The classifiers shown that the economics and politics are the highest percentiles (65% combined) while the culture and religion categories scored the lowest percentiles (about 10% combined).","0840-7789;08407789","Electronic:978-1-4244-9789-8; POD:978-1-4244-9788-1; USB:978-1-4244-9787-4","10.1109/CCECE.2011.6030580","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030580","Arabic Web;Data mining;Information retrieval;Text classification;Web Mining","","Internet;Web sites;content management;cultural aspects;data mining;information retrieval;text analysis","Arabic Internet users;Arabic Web content analysis;Web mining techniques;Web pages;World Wide Web;culture;economics;information discovering;politics;religion;search engines;sports;text categorization;text classification","","1","","15","","","8-11 May 2011","","IEEE","IEEE Conference Publications"
"Integration system of network information resources based on multi-agent collaboration","J. y. Wang; Z. Zhu","Foshan Univ., Foshan, China","2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","20110915","2011","3","","2044","2049","In order to provide high quality of network information service for enterprise, the integration system of network information resources is designed based on multi-agent collaboration. The system may be logically divided into three function modules: User Module, Processing Module and Search Module. This paper presents the essential function of each constituent part, collection workflow, evaluation workflow, Web information retrieval workflow, and image information retrieval workflow of network information resources. The key technologies are also discussed in detail, including User Interest Model matching algorithm for user personalized service, and content based image retrieval algorithm. The system is provided with intelligence, flexibility, and robustness.","","Electronic:978-1-61284-181-6; POD:978-1-61284-180-9","10.1109/FSKD.2011.6019779","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6019779","Web information retrieval;content based image retrieval;integration system framework;multi-agent collaboration;network information resources","Collaboration;Databases;Information processing;Information services;Search engines;Web pages","content-based retrieval;image retrieval;information networks;multi-agent systems;search problems","Web information retrieval workflow;content based image retrieval algorithm;image information retrieval workflow;multiagent collaboration;network information resource integration system;processing module;search module;user interest model matching algorithm;user module","","2","","6","","","26-28 July 2011","","IEEE","IEEE Conference Publications"
"Portfolio: a search engine for finding functions and their usages","C. McMillan; M. Grechanik; D. Poshyvanyk; Q. Xie; C. Fu","College of William &#x0026; Mary, Williamsburg, VA, USA","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","1043","1045","In this demonstration, we present a code search system called Portfolio that retrieves and visualizes relevant functions and their usages. We will show how chains of relevant functions and their usages can be visualized to users in response to their queries.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1985991","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032588","information retrieval;source code search engines","Context;Data mining;Java;Navigation;Portfolios;Search engines;Software","functions;query processing;search engines;source coding","code search system;portfolio;queries;relevant functions retrieval;relevant functions visualization;search engine","","6","","22","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"An Approach to Enable Cloud Service Providers to Arrange IaaS, PaaS, and Saas Using External Virtualization Infrastructures","A. Celesti; F. Tusa; M. Villari; A. Puliafito","Dept. of Math., Univ. of Messina, Messina, Italy","2011 IEEE World Congress on Services","20110908","2011","","","607","611","Nowadays, the cloud computing ecosystem is more and more distributed and heterogeneous. Cloud service providers begin to build their services using cloud-based services offered by other service providers. This raises several issues due to integration between services and provider themselves. In this paper, we propose a practice addressing such a concern in a ""Vertical Supply Chain"" scenario of distributed clouds.","2378-3818;23783818","Electronic:978-0-7695-4461-8; POD:978-1-4577-0879-4","10.1109/SERVICES.2011.92","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012800","Cloud Computing;Cloud Management;Information Retrieval;Vertical Supply Chain","Cloud computing;Servers;Streaming media;Supply chains;Time factors;Transcoding","cloud computing;virtualisation","cloud computing ecosystem;cloud service providers;distributed clouds;external virtualization infrastructures;vertical supply chain","","6","","7","","","4-9 July 2011","","IEEE","IEEE Conference Publications"
"Efficient relational keyword search system","P. T. T. Khine; H. P. P. Win; K. N. N. Tun","University of Computer Studies Yangon, Myanmar","2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing","20111020","2011","","","65","70","Keyword-based search is popularized in Information Retrieval (IR) communities and Internet search engines on the Web. Nowadays, social networking, micro-blogging, and other data-driven websites store large amounts of information in relational databases. But searching in databases users need to know a database schema and a query language. The usability of Database is severely limited unless easier ways to access databases are developed. While traditional database management systems offer powerful query languages, they do not allow keyword-based search. Keyword search techniques on the Web cannot directly be applied to databases because the data on the Internet and database are in different forms. Therefore, we propose a search system for relational data with our proposed methods that can support keyword-based search over relational database. The main contributions of this system compose of the direct indexing of records in database, the schema based queries for relevant answers, and ranking of the relevant answer sets. As long as the database table records can be extended, this system can be easily extendable for further searching records from tables.","","Electronic:978-1-4577-1481-8; POD:978-1-4577-1479-5","10.1109/ICCP.2011.6047845","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6047845","Information Retrieval;Keyword Search;Relational Database","Indexing;Keyword search;Matched filters;Relational databases","Internet;indexing;query languages;query processing;relational databases;search engines","Internet search engines;answer set ranking;database schema;database table records;indexing;information retrieval;query language;relational database;relational keyword search system","","0","","18","","","25-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"SPARK2: Top-k Keyword Query in Relational Databases","Y. Luo; W. Wang; X. Lin; X. Zhou; J. Wang; K. Li","Laboratory Le2i of CNRS, Dijon","IEEE Transactions on Knowledge and Data Engineering","20111020","2011","23","12","1763","1780","With the increasing amount of text data stored in relational databases, there is a demand for RDBMS to support keyword queries over text data. As a search result is often assembled from multiple relational tables, traditional IR-style ranking and query evaluation methods cannot be applied directly. In this paper, we study the effectiveness and the efficiency issues of answering top-k keyword query in relational database systems. We propose a new ranking formula by adapting existing IR techniques based on a natural notion of virtual document. We also propose several efficient query processing methods for the new ranking method. We have conducted extensive experiments on large-scale real databases using two popular RDBMSs. The experimental results demonstrate significant improvement to the alternative approaches in terms of retrieval effectiveness and efficiency.","1041-4347;10414347","","10.1109/TKDE.2011.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728809","Top-k;information retrieval.;keyword search;relational database","Electronic mail;Information retrieval;Keyword search;Query processing;Relational databases;Semantics","query processing;question answering (information retrieval);relational databases;text analysis","IR-style ranking;RDBMS;SPARK2;effectiveness issues;efficiency issues;large-scale real database;multiple relational table;query evaluation method;query processing method;relational database;text data storage;top-k keyword query answering;virtual document","","12","","65","","20110310","Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Using open-source tools for web crawling and indexing of open content","A. Ricardo; C. Serrão","ISCTE-IUL School of Technology and Architecture, ISCTE-IUL Lisbon University Institute, ISTA, ADETTIIUL, Lisboa, Portugal","International Conference on Information Society (i-Society 2011)","20110808","2011","","","429","434","The Internet has made possible the access to thousands of freely available music tracks under the Creative Commons or Public Domain licenses. This number keeps growing on a yearly basis. In practical terms, it is extremely difficult to browse this huge music collection, because it is widely dispersed throughout multiple websites. The work presented on this paper addresses the problem of indexing this large collection of free music. This is a very relevant problem because currently there are no integrated database or index holding information about this music material. Indexing this content will allow, for instance, the development of music recommendation systems that will also work with noncommercial content. In this paper the authors present a system proposal that has been developed to tackle the available free music indexing problem and how this system can be integrated with other systems (such as music recommendation systems) to allow the end users to enjoy free and open music content.","","Electronic:978-0-9564263-8-3; POD:978-1-61284-148-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978485","collaborative filtering;music information retrieval;music recommendation systems;recommendation methods","Argon;Indexing;Information filters;Internet","Internet;Web sites;indexing;music;public domain software;recommender systems","Internet;Web crawling;Websites;creative commons;index holding information;music recommendation systems;music tracks;open content indexing;open source tools;public domain licenses","","0","","19","","","27-29 June 2011","","IEEE","IEEE Conference Publications"
"A Computational Framework for Search, Discovery, and Trending of Patient Health in Radiology Reports","R. M. Patton; C. C. Rojas; B. G. Beckerman; T. E. Potok","Comput. Sci. & Eng. Div., Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2011 IEEE First International Conference on Healthcare Informatics, Imaging and Systems Biology","20111027","2011","","","104","111","The healthcare industry as a whole lags far behind other industries in terms of knowledge discovery capabilities. There are many piece-wise approaches to analysis of patient records. Unfortunately, there are few approaches that enable a completely automated approach that supports not just search, but also discovery and prediction of patient health. The work presented here describes a computational framework that provides near complete automation of the discovery and trending of patient characteristics. This approach has been successfully applied to the domain of mammography, but could be applied to other domains of radiology with minimal effort.","","Electronic:978-0-7695-4407-6; POD:978-1-4577-0325-6","10.1109/HISB.2011.4","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061380","genetic algorithm;information retrieval;radiology;wavelets","Biopsy;Computational modeling;Genetic algorithms;Indexing;Radiology","data handling;data mining;diagnostic radiography;health care;mammography;medical computing","computational framework;mammography;patient health data search;patient health data trending;patient health knowledge discovery;patient health prediction;radiology reports","","0","","38","","","26-29 July 2011","","IEEE","IEEE Conference Publications"
"Combining monaural source separation with Long Short-Term Memory for increased robustness in vocalist gender recognition","F. Weninger; J. L. Durrieu; F. Eyben; G. Richard; B. Schuller","Institute for Human-Machine Communication, Technische Universit&#x00E4;t M&#x00FC;nchen, D-80290, Germany","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20110711","2011","","","2196","2199","We present a novel and unique combination of algorithms to detect the gender of the leading vocalist in recorded popular music. Building on our previous successful approach that enhanced the harmonic parts by means of Non-Negative Matrix Factorization (NMF) for increased accuracy, we integrate on the one hand a new source separation algorithm specifically tailored to extracting the leading voice from monaural recordings. On the other hand, we introduce Bidirectional Long Short-Term Memory Recurrent Neural Networks (BLSTM-RNNs) as context-sensitive classifiers for this scenario, which have lately led to great success in Music Information Retrieval tasks. Through a combination of leading voice separation and BLSTM networks, as opposed to a baseline approach using Hidden Naive Bayes on the original recordings, the accuracy of simultaneous detection of vocal presence and vocalist gender on beat level is improved by up to 10% absolute. Furthermore, using this technique we achieve 91.6% accuracy in determining the gender of the predominant vocalist on song level, which is 4% absolute above our previous best result.","1520-6149;15206149","Electronic:978-1-4577-0539-7; POD:978-1-4577-0538-0; USB:978-1-4577-0537-3","10.1109/ICASSP.2011.5946764","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946764","Long Short-Term Memory;Music Information Retrieval;Non-Negative Matrix Factorization","Accuracy;Context;Harmonic analysis;Robustness;Source separation;Support vector machines;Training","matrix decomposition;recurrent neural nets;speech recognition","BLSTM-RNN;hidden naive Bayes;long short-term memory;monaural source separation;music information retrieval tasks;nonnegative matrix factorization;short-term memory recurrent neural networks;vocalist gender recognition","","1","","11","","","22-27 May 2011","","IEEE","IEEE Conference Publications"
"Applying Multidimensional Association Rule Mining to Feedback-Based Recommendation Systems","Y. F. Huang; S. D. Lin","Dept. of Comput. Sci. & Inf. Eng., Nat. Yunlin Univ. of Sci. & Technol., Touliu, Taiwan","2011 International Conference on Advances in Social Networks Analysis and Mining","20110818","2011","","","412","417","The main characteristic of collaborative filtering is to provide personalized recommendations to a customer based on the customer profile, without considering content information about domain items. In this paper, we investigated to use a relevance feedback mechanism in the collaborative recommendation system. First, we used the Self-organizing Map (SOM) method to avoid suffering from the scalability and sparsity problem in the collaborative filtering. In addition, we adopted the Statistical Attribute Distance (SAD) method which uses the similarity in statistics of customers' ratings to calculate customer correlations, instead of using the statistics of customers that rate for similar items. Then, the multi-tier granule mining algorithm was used to find association rules. Finally, with the relevance feedback mechanism and the association rules, the recommendations could be refined to provide customers more relevance information.","","Electronic:978-0-7695-4375-8; POD:978-1-61284-758-0","10.1109/ASONAM.2011.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992634","association rule mining;collaborative filtering;information retrieval;recommended systems;relevance feedback","Association rules;Collaboration;Correlation;Databases;Filtering;Motion pictures","customer profiles;data mining;groupware;information filtering;recommender systems;relevance feedback;self-organising feature maps;statistical analysis","SOM method;collaborative filtering;collaborative recommendation system;customer correlation;customer profile;feedback-based recommendation system;multidimensional association rule mining;multitier granule mining algorithm;personalized recommendations;relevance feedback;self-organizing map;statistical attribute distance method","","0","","14","","","25-27 July 2011","","IEEE","IEEE Conference Publications"
"FLAT3: feature location and textual tracing tool","T. Savage; M. Revelle; D. Poshyvanyk","College of William and Mary, Williamsburg, VA","2010 ACM/IEEE 32nd International Conference on Software Engineering","20111027","2010","2","","255","258","Feature location is the process of finding the source code that implements a functional requirement of a software system. It plays an important role in software maintenance activities, but when it is performed manually, it can be challenging and time-consuming, especially for large, long-lived systems. This paper describes a tool called FLAT<sup>3</sup> that integrates textual and dynamic feature location techniques along with feature annotation capabilities and a useful visualization technique, providing a complete suite of tools that allows developers to quickly and easily locate the code that implements a feature and then save these annotations for future use.","0270-5257;02705257","Electronic:978-1-60558-719-6; POD:978-1-60558-719-6","10.1145/1810295.1810345","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062173","concept location;dynamic analysis;information retrieval;program comprehension;software evolution and maintenance","Computer science;Information retrieval;Libraries;Software engineering;Software maintenance;Visualization","formal specification;software maintenance;software tools","FLAT3 tool;dynamic feature location;feature annotation capability;functional requirement;software maintenance;software system;source code;textual tracing tool;visualization technique","","10","","20","","","2-8 May 2010","","IEEE","IEEE Conference Publications"
"Mobile Querying of Online Semantic Web Data for Context-Aware Applications","W. Van Woensel; S. Casteleyn; E. Paret; O. De Troyer","Vrije Universiteit Brussel","IEEE Internet Computing","20111031","2011","15","6","32","39","Mobile devices are increasingly multifunctional and personal, providing mobile applications with the necessary user information to achieve personalization. At the same time, detection technologies let such devices find nearby physical entities and thus map the user's environment. By exploiting existing online Semantic Web sources about these detected entities, mobile applications can further improve personalization. SCOUT is a mobile application framework that supports linking physical entities to online semantic data sources. It provides applications with an integrated, query-able view on these sources and the user's environment. The authors developed a tailored data management approach to efficiently access these distributed online semantic sources.","1089-7801;10897801","","10.1109/MIC.2011.108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999639","information filtering;information search and retrieval;information storage and retrieval;location-dependent and sensitive;metadata;mobile applications","Indexing;Information retrieval;Mobile communication;Query processing;Resource description framework;Semantic Web","mobile computing;query processing;semantic Web","SCOUT;context-aware applications;detected entity;detection technology;distributed online semantic sources;linking physical entity;mobile application framework;mobile devices;mobile querying;online semantic Web data;online semantic Web sources;online semantic data sources;personalization;query-able view;tailored data management approach;user information","","4","","3","","20110825","Nov.-Dec. 2011","","IEEE","IEEE Journals & Magazines"
"Towards a Benchmark and Automatic Calibration for IR-Based Concept Location","S. D. Ohlemacher; A. Marcus","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","246","249","There has been a great deal of research into the use of Information Retrieval (IR)-based techniques to support concept location in source code. Much of this research has been focused on determining how to use various IR techniques to support concept location. Very little attention has been given to the effect of different configurations of corpus building and indexing on query results. In this paper, we propose a tool designed to support large-scale studies of IR techniques in varying configurations of parameters with the intention of automatically calibrating these parameters. We also discuss preliminary efforts to create the benchmark data such studies require.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970173","benchmark;concept location;data model;information retrieval;parameter calibration;software change reenactment","Conferences","indexing;management of change;query processing;software engineering","automatic calibration;concept location;corpus building;information retrieval;query result indexing;software change process;source code","","1","","11","","","22-24 June 2011","","IEEE","IEEE Conference Publications"
"An Analytic Process Schema for Collaborative Multi-Touch Applications","A. Lingnau; A. Harrer","Dept. of Comput. & Inf. Sci., Univ. of Strathclyde, Glasgow, UK","2011 IEEE 11th International Conference on Advanced Learning Technologies","20110818","2011","","","115","117","Analysis of computer supported collaboration is per-se a non-trivial task, particularly when it is completely dependent on dedicated log files and video capture without active support by the user. We present a scalable approach for analyzing childrens' interaction in a collaborative multi-touch setting using a tabletop interface.","2161-3761;21613761","Electronic:978-0-7695-4346-8; POD:978-1-61284-209-7","10.1109/ICALT.2011.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992241","children;collaboration;computer;information retrieval;interaction analysis;multi-touch;tabletop","Collaboration;Collaborative work;Computers;Context;Encoding;Object recognition;Video recording","groupware;human computer interaction;microcomputers;touch sensitive screens","analytic process schema;children interaction analysis;collaborative multitouch application;computer supported collaboration analysis;tabletop interface","","0","","9","","","6-8 July 2011","","IEEE","IEEE Conference Publications"
"A New Language Model Combining Single and Compound Terms","A. Hammache; R. Ahmed-Ouamer; M. Boughanem","Lab. LARI, Univ. Mouloud Mammeri, Tizi-Ouzou, Algeria","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","67","70","Most traditional information retrieval systems are based on single terms indexing. However, it is admitted that semantic content of a document (or a query) cannot be accurately captured by a simple set of independent keywords. Although, several works have incorporated phrases or other syntactic information in IR, such attempts have shown slight benefit, at best. Particularly in language modeling approaches this is achieved through the use of the big ram or n-gram models. However, in these models all big rams/n-grams are considered and weighted uniformly. In this paper we introduce a new approach to weight and consider only certain types of N-grams ""compound terms"". Experimental results on three test collections showed an improvement.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040498","compound term indexing;information retrieval;language model","Compounds;Computational modeling;Data models;Hidden Markov models;Indexing;Information retrieval;Smoothing methods","document handling;indexing;information retrieval systems;natural language processing","bigram models;compound terms;document semantic content;information retrieval systems;language modeling approach;n-gram models;single terms indexing","","0","","21","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Handling verbose queries for spoken document retrieval","S. H. Lin; E. E. Jan; B. Chen","National Taiwan Normal University, Taiwan","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20110711","2011","","","5552","5555","Query-by-example information retrieval provides users a flexible but efficient way to accurately describe their information needs. The query exemplars are usually long and in the form of either a partial or even a full document. However, they may contain extraneous terms that would have potential negative impacts on the retrieval performance. In order to alleviate those negative impacts, we propose a novel term-based query reduction mechanism so as to improve the informativeness of verbose query exemplars. We also explore the notion of term discrimination power to select a salient subset of query terms automatically. Experiments on the TDT Chinese collection show that the proposed approach is indeed effective and promising.","1520-6149;15206149","Electronic:978-1-4577-0539-7; POD:978-1-4577-0538-0; USB:978-1-4577-0537-3","10.1109/ICASSP.2011.5947617","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5947617","Query-by-example;information retrieval;term-based query reduction;verbose query","Entropy;Hidden Markov models;Information retrieval;Markov processes;Semantics;Supervised learning;Training","document handling;natural language processing;query processing;speech recognition","TDT Chinese collection;query by example information retrieval;spoken document retrieval;term based query reduction mechanism;verbose queries handling;verbose query exemplars","","1","","13","","","22-27 May 2011","","IEEE","IEEE Conference Publications"
"Improving the key extraction performance of a simultaneous local key and chord estimation system","J. Pauwels; J. P. Martens; M. Lemanz","Digital Speech and Signal Processing group (ELIS-DSSP), Ghent University, Belgium","2011 IEEE International Conference on Multimedia and Expo","20110905","2011","","","1","6","In this paper, significant improvements of a previously developed key and chord extraction system are proposed. The major improvement is the introduction of a separate acoustic model, designed to verify local key hypotheses. The conducted experimental evaluation shows that the presented system improves the state of the art in local key estimation. Our experimental study further demonstrates that the chord estimation performance is already quite robust, whereas the key estimation performance still happens to be sensitive to a number of factors. In particular, we present figures that illustrate the significant impact of the embedded musicological model and the duration of the processed excerpt on the key estimation accuracy.","1945-7871;19457871","Electronic:978-1-61284-350-6; POD:978-1-61284-348-3; USB:978-1-61284-349-0","10.1109/ICME.2011.6012146","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012146","Key extraction;chord extraction;music information retrieval;music signal processing","Accuracy;Acoustics;Computational modeling;Data mining;Data models;Estimation;Hidden Markov models","acoustic signal processing;music","chord estimation system;key extraction performance;local key estimation system;musicological model;separate acoustic model","","1","","20","","","11-15 July 2011","","IEEE","IEEE Conference Publications"
"Automatic music classification for Dangdut and campursari using Naïve Bayes","C. M. Viny; F. Kurniawan; Tony","Lab. of Knowledge Data Eng., Tarumanagara Univ., Jakarta, Indonesia","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","20110919","2011","","","1","6","Music classification can be performed by classifying music according to its genre, style, mood, and others. Various methods have been implemented to automatically classify music. Naïve Bayes learning algorithm is one of the most efficient and effective classification algorithm. Dangdut and campursari music are the music often heard by Indonesian. But the classification of dangdut and campursari music is still rarely performed. In this study, we perform automatic music classification for dangdut and campursari music. We use Naïve Bayes to classify music and the data was discretized based on Minimum Description Length Principle (MDLP). We used jSymbolic to extract feature from MIDI files. Currently, we use 45 features that are included in the category of instruments and pitch. This experiment produced the accuracy of 85.14%.","2155-6822;21556822","Electronic:978-1-4577-0752-0; POD:978-1-4577-0753-7","10.1109/ICEEI.2011.6021738","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021738","Campursari;Dangdut;Minimum Description Length Principle;Music Information Retrieval;Naïve Bayes","Accuracy;Educational institutions;Feature extraction;Instruments;Testing;Training;Training data","learning (artificial intelligence);music;pattern classification","Campursari music;Dangdut music;Indonesia;MIDI file feature extraction;jSymbolic;minimum description length principle;music classification;naive Bayes learning algorithm","","0","","13","","","17-19 July 2011","","IEEE","IEEE Conference Publications"
"Ontology and search engine for cloud computing system","J. Kang; K. M. Sim","Multiagent and Cloud Computing Systems Lab., Department of Information and Communication, Gwangju Institute of Science and Technology (GIST), Korea","Proceedings 2011 International Conference on System Science and Engineering","20110725","2011","","","276","281","Cloud computing is becoming a well-known buzzword nowadays. Many companies, such as Amazon, Google, Microsoft and so on, accelerate their paces in developing Cloud computing systems and enhancing their services to cater for a wide variety of users. However, there is no study that focuses on search engine for Cloud computing system. Hence, this paper presents search engine for Cloud computing system (Cloudle). To semantically define the relationship among Cloud services, Cloud ontologies (CO-1 and CO-2) are designed. CO-2 contains a set of Cloud concepts, individuals of those concepts, and the relationship among those individuals while CO-1 contains only concepts. It is used for determining the similarity among Cloud services with three kinds of reasoning methods (1) concept similarity reasoning, (2) object property similarity reasoning, and (3) datatype property similarity reasoning. The architecture of Cloudle is also briefly presented. Finally, empirical results show that the result of using Cloudle with CO-2 has significantly better performance in finding the appropriate Cloud service than the result using Cloudle with CO-1, the result without the Cloud ontology, and the result without Cloudle.","2325-0909;23250909","Electronic:978-1-61284-471-8; POD:978-1-61284-351-3; USB:978-1-61284-472-5","10.1109/ICSSE.2011.5961913","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961913","Cloud Computing;Cloud Ontology;Information retrieval;Search Engines","Cloud computing;Cognition;Computer architecture;Information filters;OWL;Ontologies;Search engines","Web services;cloud computing;inference mechanisms;ontologies (artificial intelligence);search engines","CO-1;CO-2;Cloudle;cloud computing system;cloud ontologies;cloud services;concept similarity reasoning;datatype property similarity reasoning;object property similarity reasoning;search engine","","9","","8","","","8-10 June 2011","","IEEE","IEEE Conference Publications"
"The amblr: A mobile spatial audio music browser","R. Stewart; M. Sandler","Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary, University of London, UK","2011 IEEE International Conference on Multimedia and Expo","20110905","2011","","","1","6","Music collections are often visualized in a two-dimensional space to show relationships between songs. Some user inter faces interacting with these two-dimensional maps of songs use spatial auditory display to allow easier access to the au dio content. A common auditory display is to have multiple songs playing simultaneously from differing spatial locations around the user. However, when using this style of interface the sonification of the collection needs to be limited to a lo cal subset of the collection, usually three to six songs. This paper presents the amblr, a spatial audio music browser that allows a user to auralize the collection. It combines effective design from previous work with new approaches to create a novel interface. This allows for a more intuitive navigation of a virtual space populated by a large collection songs without relying on textual metadata.","1945-7871;19457871","Electronic:978-1-61284-350-6; POD:978-1-61284-348-3; USB:978-1-61284-349-0","10.1109/ICME.2011.6012203","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012203","auditory display;binaural;music information retrieval","Auditory displays;Graphical user interfaces;Mobile handsets;Presses;Pressing;Servers;Visualization","graphical user interfaces;mobile handsets;music;online front-ends","AMBLR;audio content;collection auralization;mobile spatial audio music browser;music collections;spatial auditory display;user interfaces;virtual space","","0","","13","","","11-15 July 2011","","IEEE","IEEE Conference Publications"
"Finding relevant functions in millions of lines of code","C. McMillan","College of William &#x0026; Mary, Williamsburg, VA, USA","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","1170","1172","Source code search engines locate and display fragments of code relevant to user queries. These fragments are often isolated and detached from one another. Programmers need to see how source code interacts in order to understand the concepts implemented in that code, however. In this paper, we present Portfolio, a source code search engine that retrieves and visualizes relevant functions as chains of function invocations. We evaluated Portfolio against Google Code Search and Koders in a case study with 49 professional programmers. Portfolio outperforms both of these engines in terms of relevance and visualization of the returned results.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1986032","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032623","information retrieval;source code search engines","Engines;Google;Portfolios;Search engines;Software;Storage area networks;Visualization","query processing;search engines;source coding","Google code search;code fragments;invocation;professional programmer;source code search engine;user queries","","1","","13","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"Automatic Classification of Change Requests for Improved IT Service Quality","C. Kadar; D. Wiesmann; J. Iria; D. Husemann; M. Lucic","Zurich Res. Lab., IBM, Ruschlikon, Switzerland","2011 Annual SRII Global Conference","20110718","2011","","","430","439","Faulty changes to the IT infrastructure can lead to critical system and application outages, and therefore cause serious economical losses. In this paper, we describe a change planning support tool that aims at assisting the change requesters in leveraging aggregated information associated with the change, like past failure reasons or best implementation practices. The thus gained knowledge can be used in the subsequent planning and implementation steps of the change. Optimal matching of change requests with the aggregated information is achieved through the classification of the change request into about 200 fine-grained activities. We propose to automatically classify the incoming change requests using various information retrieval and machine learning techniques. The cost of building the classifiers is reduced by employing active learning techniques or by leveraging labeled features. Historical tickets from two customers were used to empirically assess and compare the accuracy of the different classification approaches (Lucene index, multinomial logistic regression, and generalized expectation criteria).","2166-0778;21660778","Electronic:978-0-7695-4371-0; POD:978-1-61284-415-2","10.1109/SRII.2011.95","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958118","automation;change management;generalized expectation criteria;information retrieval;logistic regression;service quality;text classification","Accuracy;Computational modeling;Data models;Indexes;Information retrieval;Machine learning;Training","DP industry;classification;information retrieval;learning (artificial intelligence);management of change;planning (artificial intelligence)","IT infrastructure;Lucene index;active learning techniques;application outages;change planning support tool;change request optimal matching;change requests automatic classification;economical losses;faulty changes;generalized expectation criteria;improved IT service quality;information retrieval technique;machine learning technique;multinomial logistic regression;subsequent planning","","5","4","23","","","March 29 2011-April 2 2011","","IEEE","IEEE Conference Publications"
"Content-aware auto-soundtracks for personal photo music slideshows","P. Dunker; P. Popp; R. Cook","Media Technology Lab (MTL), Gracenote Inc, Emeryville, CA, USA","2011 IEEE International Conference on Multimedia and Expo","20110905","2011","","","1","5","We present a novel slideshow generation concept based on content-aware photo-music mapping. Current technologies for automated personal photo slideshow generation primarily focus on photo presentations and visual effects. These solutions utilize either a manually chosen single song or preset slideshow songs. Our technology focuses on an automatic soundtrack generation process that attempts to comprehend what the photos depict and choose music accordingly. The process comprises analyzing a photo album, composing an auto-soundtrack by dynamically mapping song segments to photo events, and automatically choosing content-aware transitions and visual effects appropriate for the presentation.","1945-7871;19457871","Electronic:978-1-61284-350-6; POD:978-1-61284-348-3; USB:978-1-61284-349-0","10.1109/ICME.2011.6012066","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6012066","Personal photo slideshows;emotion recognition;music information retrieval;photo information retrieval","Argon","content-based retrieval;emotion recognition;image retrieval;music;personal computing","automated personal photo slideshow generation;content-aware auto-soundtracks;content-aware photo-music mapping;manually chosen single song;music information retrieval;personal photo music slideshows;photo album;photo information retrieval;photo presentation;preset slideshow song;visual effects","","2","","14","","","11-15 July 2011","","IEEE","IEEE Conference Publications"
"Knowledge Extraction and Reuse within ""Smart"" Service Centers","C. Wang; R. Akella; S. Ramachandran; D. Hinnant","Technol. & Inf. Manage., Univ. of California Santa Cruz, Santa Cruz, CA, USA","2011 Annual SRII Global Conference","20110718","2011","","","163","176","In this paper, we describe the initial version of a text analytics system under development and use at Cisco, where the objective is to ""optimize"" the productivity and effectiveness of the service center. More broadly, we discuss the practical needs in industry for developing powerful ""Smart"" Service Centers and the gaps in research to meet these needs. Ideally, service engineers in service centers should be utilized to handle issues which have not been solved previously and machines should be used to solve problems already solved, or at least help the service engineers obtain pertinent information from related and solved service cases when responding to a new request. Such a role for a machine would be a core element of the ""Smart Services"" offering. Hence, design of a highly efficient human-machine combination to derive insights from text and respond to a user request, is critical and fundamental, this enables service agents to capture relevant information quickly and accurately, and to develop the foundation for upper layer applications. Despite extensive earlier literature, the optimization for service process that involves very long, unstructured documents referencing a number of technology and product related terms with implicit inter-relationships has not been fully investigated. Our approach enables firms such as Cisco to achieve efficient service delivery by automating knowledge extraction to support ""Self Service"" by end users. The Cisco text analytics system termed Service Request Analyzer and Recommender (SRAR) addresses gaps in the Support Services function, by optimizing the use of human resources and software analytics in the service delivery process. The Analyzer is able to handle complex service requests (SRs) and to present categorized and pertinent information to service agents, based on which the Recommender, an upper layer application, is built to retrieve similar solved SRs, when presented with a new request. Our contributions in the context of- - text analysis and system design are three-fold. First, we identify the elements of the diagnostic process underlying the creation of SRs, and design a hierarchical classifier to decompose the complex SRs into those elements. Such decomposition provides specific information from the functional perspectives about ""What was the problem?"" ""Why did it occur?"" and ""How was it solved?"" which assists service agents in acquiring the knowledge they need more effectively and rapidly. Second, we build an SR Recommender on top of SR Analyzer to extend the system functionality for improved knowledge reuse, to measure SR similarity for more accurate recommendation of SRs. Third, we validate our SRAR in an initial pilot study in the service center for Cisco network diagnostics and support, and demonstrate the effectiveness and extensibility of our system. Our system appears applicable to the service centers across multiple domains, including networks, aerospace, semiconductors, automotive, health care, and financial services, and potentially adapted and expanded to all the other business functions of an enterprise. We conclude by indicating open research problems and new research directions, to expand the set of problems that need to be addressed in developing a Smart Support Services capability, and the solutions required to achieve them. These include the capture, retrieval, and reuse of more refined, structured and granulated knowledge, as well as the use of forum threads and semi-automated, dynamic categorization, together with considerations of the optimal use of humans and machine learning based software. Other aspects we discuss include recommendation systems based on temporal pattern clustering and incentives for experts to permit their expertise to be captured for machine (re-)use.","2166-0778;21660778","Electronic:978-0-7695-4371-0; POD:978-1-61284-415-2","10.1109/SRII.2011.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958085","diagnostic business process;information retrieval;knowledge extraction;knowledge reuse;service centers;smart services;text mining","Business;Context;Data mining;Humans;Knowledge engineering;Semantics;Strontium","data mining;information retrieval;knowledge acquisition;learning (artificial intelligence);recommender systems;text analysis","Cisco;SRAR;complex service requests;human-machine combination;information retrieval;knowledge extraction;knowledge reuse;machine learning based software;optimization;recommendation systems;service delivery process;service request analyzer and recommender;smart service centers;support services function","","0","","21","","","March 29 2011-April 2 2011","","IEEE","IEEE Conference Publications"
"Fast historic document retrieval by extracting document image summary","Chwan-Yi Shiah; Yun-Sheng Yen","Department of Applied Informatics, Fo Guang University, YiLan, Taiwan 26247","2011 International Conference on Multimedia Technology","20110825","2011","","","3062","3065","Historic documents such as Chinese calligraphy and old newspapers usually were handwritten or printed in poor quality so that an automatic optical character recognition procedure for scanned document images is difficult to apply. Thus efficient pattern matching techniques are required in order to do content-based information retrieval based on user's queries. In this paper, a fast pattern clustering and image matching procedure is proposed to do image/pattern search in a historic document image based on user's query images. The image summary extracted from the document image is constructed so that a set of distinct image clusters are formed. A couple of distance measures that calculate distance between image patterns are also proposed to evaluate their cluster similarities. By precise pattern matching and hierarchical image clustering, our experimental results show that an online query image can produce accurate and faster results than traditional approaches for a broad range of historic document images.","","Electronic:978-1-61284-774-0; POD:978-1-61284-771-9","10.1109/ICMT.2011.6003077","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003077","historic document image;image clustering;information retrieval;pattern matching","Clustering algorithms;Complexity theory;Histograms;Image segmentation;Information retrieval;Pattern matching;Shape","content-based retrieval;feature extraction;image matching;optical character recognition;pattern clustering;query processing","Chinese calligraphy;automatic optical character recognition procedure;content-based information retrieval;distinct image clusters;document image summary extraction;fast historic document retrieval;hierarchical image clustering;image matching procedure;image-pattern search;old newspapers;pattern clustering;pattern matching techniques;scanned document images;user query images","","0","","14","","","26-28 July 2011","","IEEE","IEEE Conference Publications"
"Scalable Automatic Concept Mining from Execution Traces","S. Medini","SOCCER Lab., Ecole Polytech. de Montreal, Montreal, QC, Canada","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","238","241","Concept identification is the task of locating and identifying concepts (e.g., domain concepts) into code region or, more generally, into artifact chunks. Concept identification is fundamental to program comprehension, software maintenance, and evolution. Different static, dynamic, and hybrid approaches for concept identification exist in the literature. Both static and dynamic techniques have advantages and limitations. In fact, they can be considered to complement each other. Indeed, recent works focused on hybrid techniques to improve the performance in time as well as accuracy (i.e., precision and recall) of the concept location process. Furthermore, sometimes only a single execution trace is available, however, to the best of our knowledge, only few works attempt to automatically identify concepts in a single execution trace. We propose an approach built upon a dynamic-programming algorithm to split an execution trace into segments likely representing concepts. The approach improves performance and scalability with respect to currently available techniques. We also plan to use techniques derived from Latent Dirichlet Allocation (LDA)to automatically assign meanings to segments.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970171","Concept identification;Dynamic analysis;Information retrieval;Latent Dirichlet Allocation","Data mining;Heuristic algorithms;Information retrieval;Resource management;Scalability;Software maintenance","data mining;dynamic programming;program diagnostics;software maintenance","artifact chunks;concept identification;concept location process;dynamic programming algorithm;execution traces;latent dirichlet allocation;program comprehension;scalable automatic concept mining;software evolution;software maintenance","","1","","22","","","22-24 June 2011","","IEEE","IEEE Conference Publications"
"On ensembles of biclusters generated by NichePSO","L. Menezes; A. L. V. Coelho","Graduate Program in Applied Informatics, University of Fortaleza (UNIFOR), Fortaleza-CE, Brazil","2011 IEEE Congress of Evolutionary Computation (CEC)","20110714","2011","","","601","607","Ensemble methods combine multiple models into a single framework for coping better with Machine Learning tasks. Recently, the well-known Bagging approach was adapted to solve biclustering problems, where the objective is to find large sub-groups of samples and attributes of the data matrix with the samples showing high correlation over the attributes. In this paper, aiming at the generation of more diverse and high-quality biclusters to be fused through an ensemble perspective, we have adopted a well-known multimodal Particle Swarm Optimization algorithm, namely NichePSO. In particular, the study brings a preliminary comparative assessment of the biclustering results delivered by NichePSO operating alone and by two ensemble settings (one of which is Bagging) operating on the biclusters produced by NichePSO. The assessment was done based on bioinformatics and collaborative filtering datasets, and the results achieved so far reveal the usefulness of ensembling the repertory of biclusters produced by NichePSO.","1089-778X;1089778X","Electronic:978-1-4244-7835-4; POD:978-1-4244-7834-7","10.1109/CEC.2011.5949674","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5949674","Bagging;Biclustering;Bioinformatics;Ensembles;Information Retrieval;Particle Swarm Optimization","Bagging;Bioinformatics;Coherence;Context;Indexes;Particle swarm optimization;Probability","bioinformatics;information filtering;learning (artificial intelligence);particle swarm optimisation","NichePSO;bagging approach;biclustering problems;biclusters ensemble;bioinformatics;collaborative filtering datasets;data matrix;ensemble methods;machine learning tasks;preliminary comparative assessment;well-known multimodal particle swarm optimization algorithm","","0","","24","","","5-8 June 2011","","IEEE","IEEE Conference Publications"
"Efficient Information Source Monitoring Method Considering User Utility in Large-Scale Networks","S. Sugawara; K. Honda; Y. Ishibashi","Dept. of Sci. & Eng. Simulation, Nagoya Inst. of Technol., Nagoya, Japan","2011 IEEE/IPSJ International Symposium on Applications and the Internet","20110825","2011","","","388","393","This paper proposes an efficient information monitoring method in large-scale network, which adjusts its observation intervals according to the information sources' update intervals considering user utility decrease and monitoring cost. The usefulness of the proposal is shown by using computer simulations while the observation cost is neither extremely emphasized nor neglected.","","Electronic:978-0-7695-4423-6; POD:978-1-4577-0531-1","10.1109/SAINT.2011.74","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004191","Web monitoring;information retrieval;large-scale network;simulation;user utility","Computational modeling;Computer simulation;Gaussian distribution;Information retrieval;Monitoring;Timing","information networks;information retrieval;monitoring","computer simulations;information monitoring method;information retrieval;information source monitoring method;large-scale networks;monitoring cost;observation intervals;user utility decrease","","0","","10","","","18-21 July 2011","","IEEE","IEEE Conference Publications"
"Noise control in document classification based on fuzzy formal concept analysis","S. T. Li; F. C. Tsai","Institute of Information Management, National Cheng Kung University, Tainan, Taiwan","2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)","20110901","2011","","","2583","2588","Document classification is critical due to explosive increasing of text in modern world. However, most of existing document classification algorithms are easily affected by noise data. Therefore, in document classification tasks, the ability of noise control is as important as the ability to classify exactly. In this paper, we propose a novel classification framework based on fuzzy formal concept analysis to moderate the impact from noise. In addition, the well-organized concepts also provide inherent relations, which support knowledge codification and distribution effectively. Experimental results using Reuters 21578 dataset demonstrates significant noise control benefit and superior classification accuracy.","1098-7584;10987584","Electronic:978-1-4244-7317-5; POD:978-1-4244-7315-1; USB:978-1-4244-7316-8","10.1109/FUZZY.2011.6007449","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007449","fuzzy formal concept analysis;information retrieval;noise control;text classification","Accuracy;Animals;Classification algorithms;Context;Lattices;Noise;Text categorization","document handling;fuzzy systems;interference suppression","document classification algorithm;document classification task;fuzzy formal concept analysis;noise control benefit;superior classification accuracy;support knowledge codification","","2","","27","","","27-30 June 2011","","IEEE","IEEE Conference Publications"
"Can Better Identifier Splitting Techniques Help Feature Location?","B. Dit; L. Guerrouj; D. Poshyvanyk; G. Antoniol","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","11","20","The paper presents an exploratory study of two feature location techniques utilizing three strategies for splitting identifiers: Camel Case, Samurai and manual splitting of identifiers. The main research question that we ask in this study is if we had a perfect technique for splitting identifiers, would it still help improve accuracy of feature location techniques applied in different scenarios and settings? In order to answer this research question we investigate two feature location techniques, one based on Information Retrieval and the other one based on the combination of Information Retrieval and dynamic analysis, for locating bugs and features using various configurations of preprocessing strategies on two open-source systems, Rhino and jEdit. The results of an extensive empirical evaluation reveal that feature location techniques using Information Retrieval can benefit from better preprocessing algorithms in some cases, and that their improvement in effectiveness while using manual splitting over state-of-the-art approaches is statistically significant in those cases. However, the results for feature location technique using the combination of Information Retrieval and dynamic analysis do not show any improvement while using manual splitting, indicating that any preprocessing technique will suffice if execution data is available. Overall, our findings outline potential benefits of putting additional research efforts into defining more sophisticated source code preprocessing techniques as they can still be useful in situations where execution information cannot be easily collected.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970159","dynamic analysis;feature location;identifier splitting algorithms;information retrieval","Accuracy;Algorithm design and analysis;Dictionaries;Gold;Large scale integration;Manuals;Software","information retrieval","CamelCase;Rhino;Samurai;feature location technique;identifier splitting technique;information retrieval;jEdit;open source system;preprocessing strategies","","22","1","35","","","22-24 June 2011","","IEEE","IEEE Conference Publications"
"Measuring Structural Similarity in Music","J. P. Bello","Department of Music and Performing Arts Professions, New York University, New York, NY, USA","IEEE Transactions on Audio, Speech, and Language Processing","20110718","2011","19","7","2013","2025","This paper presents a novel method for measuring the structural similarity between music recordings. It uses recurrence plot analysis to characterize patterns of repetition in the feature sequence, and the normalized compression distance, a practical approximation of the joint Kolmogorov complexity, to measure the pairwise similarity between the plots. By measuring the distance between intermediate representations of signal structure, the proposed method departs from common approaches to music structure analysis which assume a block-based model of music, and thus concentrate on segmenting and clustering sections. The approach ensures that global structure is consistently and robustly characterized in the presence of tempo, instrumentation, and key changes, while the used metric provides a simple to compute, versatile and robust alternative to common approaches in music similarity research. Finally, experimental results demonstrate success at characterizing similarity, while contributing an optimal parameterization of the proposed approach.","1558-7916;15587916","","10.1109/TASL.2011.2108287","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711645","Audio signal processing;computer audition;music information retrieval (MIR);music structure analysis;sound similarity","Delay;Discrete cosine transforms;Feature extraction;Mel frequency cepstral coefficient;Music;Robustness;Time series analysis","audio recording;audio signal processing;data compression;information retrieval;music;pattern clustering","MIR;clustering section;distance measurement;feature sequence;joint Kolmogorov complexity;music audio recording;music block-based model;music information retrieval;music structure analysis;normalized compression distance;optimal parameterization;pairwise similarity measurement;recurrence plot analysis;repetition pattern characterization;segmenting section;signal structure intermediate representation;structural similarity measurement","","14","","58","","20110210","Sept. 2011","","IEEE","IEEE Journals & Magazines"
"Selecting Answers to Questions from Web Documents by a Robust Validation Process","A. Grappy; B. Grau; M. H. Falco; A. L. Ligozat; I. Robba; A. Vilnat","LIMSI, Univ. Paris-Sud, Orsay, France","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","55","62","Question answering (QA) systems aim at finding answers to question posed in natural language using a collection of documents. When the collection is extracted from the Web, the structure and style of the texts are quite different from those of newspaper articles. We developed a QA system based on an answer validation process able to handle Web specificity. A large number of candidate answers are extracted from short passages in order to be validated according to question and passages characteristics. The validation module is based on a machine learning approach. It takes into account criteria characterizing both passage and answer relevance at surface, lexical, syntactic and semantic levels to deal with different types of texts. We present and compare results obtained for factual questions posed on a Web and on a newspaper collection. We show that our system outperforms a baseline by up to 48% in MRR.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.210","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040496","Web document analysis;answer validation;fine-grained information retrieval;question-answering system","Data mining;Feature extraction;HTML;Reliability;Semantics;Springs;Syntactics","Internet;feature extraction;learning (artificial intelligence);natural language processing;program verification;question answering (information retrieval);text analysis","QA system;Web document;answer selection;document collection;machine learning;natural language;newspaper article;question answering system;robust validation process;semantic level","","0","","31","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Augmenting virtual-reality environments with social-signal based music content","I. Karydis; I. Deliyannis; A. Floros","Department of Informatics, Ionian University, Corfu, Greece","2011 17th International Conference on Digital Signal Processing (DSP)","20110829","2011","","","1","6","Virtual environments and computer games incorporate music in order to enrich the audiovisual experience and further immerse users. Selecting musical content during design-time can have a controversial result based on the preferences of the users involved, while limiting the interactivity of the environment, affecting thus the effectiveness of immersion. In this work, we introduce a framework for the selection and incorporation of user preferable musical data into interactive virtual environments and games. The framework designates guidelines for both design and run-time annotation of scenes. Consequently, personal music preferences collected through local repositories or social networks can be processed, analysed, categorised and prepared for direct incorporation into virtual environments. This permits automated audio selection based on scene characteristics and scene characters' interaction, enriching or replacing the default designer choices. Proof-of-concept is given via development of a web-service that provides a video game with a dynamic interactive audio content based on predefined video game scene annotation and user musical preferences recorded in social network services.","1546-1874;15461874","Electronic:978-1-4577-0274-7; POD:978-1-4577-0273-0","10.1109/ICDSP.2011.6004944","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004944","music information retrieval;social-networking;social-signal processing;user immersion","Computers;Feature extraction;Games;Humans;Signal processing;Social network services;Virtual reality","Web services;computer games;music;social networking (online);virtual reality","Web service;automated audio selection;computer games;dynamic interactive audio content;interactive virtual environments;local repositories;predefined video game scene annotation;run-time scene annotation;scene character interaction;social networks;social signal based music content;user preferable musical data;virtual reality environments","","1","","32","","","6-8 July 2011","","IEEE","IEEE Conference Publications"
"Term proximity in document retrieval systems","B. Ali; B. Abdelkrim; S. Mebarek","University Hadj Lakhdar - BATNA, Department of Computer Science, 1, rue Boukhlouf Mohamed El Hadi 05000 Batna, Algeria","2011 IEEE International Conference on Computer Science and Automation Engineering","20110714","2011","4","","267","271","The Automated information retrieval has appeared with the huge production of documents stored digitally, it allows you to find documents relevant to a user query formulated by keywords. The use of the neighborhood words of query can improve the relevance of research. The proposed method of representation of the documents has the advantage of integrating the information in its model of the neighborhood of terms. We evaluated our approach in terms of research relevance also the time of indexing and research. The obtained results show a remarkable improvement in the relevance due to the use of the neighborhood of the terms, and this hasn't influence on the indexing and research time that stay so quick.","","Electronic:978-1-4244-8728-8; POD:978-1-4244-8727-1","10.1109/CSAE.2011.5952848","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5952848","information retrieval;neighborhood of terms;relevance of search;text indexing","Dictionaries;Indexing;Size measurement;Time measurement;Weight measurement","query processing;text analysis","document representation;document retrieval system;query neighborhood words;research relevance;term proximity;user query","","0","","18","","","10-12 June 2011","","IEEE","IEEE Conference Publications"
"A Hybrid Approach to Automatic Web Services Discovery","Y. H. Tsai; S. Y. Hwang; Y. Tang","Dept. of Inf. Manage., Nat. Sun Yat-sen Univ., Kaohsiung, Taiwan","2011 International Joint Conference on Service Sciences","20110721","2011","","","277","281","Service-oriented Architecture (SOA) is gaining its popularity as a new paradigm for developing next generation service systems. One of the most touted features with SOA is its ability to automatically discover Web services that meet the need of users. Existing researches in Web services discovery either use information retrieval techniques or semantic-based methods for locating Web services. In this paper, we proposed a hybrid approach that incorporate both textual and ontology information about Web services. The proposed approach uses multiple criteria decision making technique to determine the weights of different attributes. 103 real-world Web services are used in our experiments, and the experimental results show that our proposed approach generally yields better result than existing methods that employ only one source of information.","","Electronic:978-0-7695-4421-2; POD:978-1-4577-0326-3","10.1109/IJCSS.2011.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960352","SOA;Web service discovery;information retrieval;ontology;semantic Web","Integrated circuits;Ontologies;Quality of service;Semantic Web;Semantics;Service oriented architecture","Web services;decision making;service-oriented architecture","SOA;automatic Web services discovery;multiple criteria decision making;next generation service systems;service-oriented architecture","","4","","23","","","25-27 May 2011","","IEEE","IEEE Conference Publications"
"Proximity-based traceability: An empirical validation using ranked retrieval and set-based measures","W. K. Kong; J. H. Hayes","Department of Computer Science, University of Kentucky","Workshop on Empirical Requirements Engineering (EmpiRE 2011)","20111017","2011","","","45","52","The automatic generation of traceability links attempts to reduce the burden of building requirements traceability matrices (RTMs) that will be vetted by a human analyst before use in verification and validation tasks such as criticality assessment or change impact analysis. Information Retrieval (IR) techniques, notably the Vector Space Model (VSM), have been used with some success to build textual artifact traceability matrices. A limitation of the VSM is that it disregards word or term location and the relationship between words in the textual artifacts being traced. This paper presents a VSM enhancement with consideration for term location, validating it on four datasets using ranked retrieval and set-based measures. These two types of measures provide a more detailed comparison between the two traceability techniques.","2329-6348;23296348","Electronic:978-1-4577-1076-6; POD:978-1-4577-1075-9","10.1109/EmpiRE.2011.6046255","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6046255","Dynamic link generation;Empirical validation;Information retrieval;Measures;Requirements tracing;TF-IDF;Term proximity;Verification and Validation","Communities;Filtering;Humans;Information retrieval;Software systems;Sun","information retrieval;program diagnostics;program verification;task analysis","VSM enhancement;change impact analysis;criticality assessment;human analyst;information retrieval technique;proximity based traceability;ranked retrieval;requirement traceability matrices;set based measure;textual artifact traceability matrices;validation task;vector space model;verification task","","4","","27","","","30-30 Aug. 2011","","IEEE","IEEE Conference Publications"
"Signal Processing for Music Analysis","M. Muller; D. P. W. Ellis; A. Klapuri; G. Richard","Saarland University","IEEE Journal of Selected Topics in Signal Processing","20110915","2011","5","6","1088","1110","Music signal processing may appear to be the junior relation of the large and mature field of speech signal processing, not least because many techniques and representations originally developed for speech have been applied to music, often with good results. However, music signals possess specific acoustic and structural characteristics that distinguish them from spoken language or other nonmusical signals. This paper provides an overview of some signal analysis techniques that specifically address musical dimensions such as melody, harmony, rhythm, and timbre. We will examine how particular characteristics of music signals impact and determine these techniques, and we highlight a number of novel music analysis and retrieval tasks that such processing makes possible. Our goal is to demonstrate that, to be successful, music audio signal processing techniques must be informed by a deep and thorough insight into the nature of music itself.","1932-4553;19324553","","10.1109/JSTSP.2011.2112333","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709966","Beat;digital signal processing;harmony;melody;music analysis;music information retrieval;music signals;pitch;rhythm;source separation;timbre;voice separation","Harmonic analysis;Instruments;Multiple signal classification;Music;Spectrogram;Time frequency analysis","audio signal processing;music;source separation","harmony;melody;music analysis;music audio signal processing techniques;rhythm;timbre","","61","1","217","","20110204","Oct. 2011","","IEEE","IEEE Journals & Magazines"
"Analysis of log files as a security aid","Jorge Pinto Leite","Departamento de Engenharia Inform&#x00E1;tica, Instituto Superior de Engenharia do Porto, Portugal","6th Iberian Conference on Information Systems and Technologies (CISTI 2011)","20110804","2011","","","1","6","Log files are the history books of a computer system. In particular, they tell a good portion of the security related events and menaces that a system has to withstand and, sometimes, fails to resist to. Therefore, log files' analysis can be valuable to system and security administrators if the difficulty of extracting the relevant information of the different kinds of data and formats can be surmounted. And if we consider a huge system in terms of users, services and accesses, the difficulty of the analysis task rises enormously. In infra-structures where there is more than one server and communication link, it is possible for the administrator and security team to configure all systems to record their logs in a central huge repository, but the search for abnormalities is quite impossible without specialized tools. As we believe that the collected information on log files can be valuable, we use information retrieval open source tools to index the log files' fields and search for patterns of suspected behavior, which may indicate a system intrusion. Our development allows queries based on variables introduced by the analyst. The preliminary results obtained when using log files from an academic institution indicates that our approach is effective and can be used as a security aid.","2166-0727;21660727","Electronic:978-989-96247-5-7; POD:978-1-4673-0175-6","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5974290","data mining;information retrieval;log;security","Colon;Documentation;Indexing;Information retrieval;Java;Security","security of data","academic institution;information retrieval open source tools;log file analysis;security aid;security related events;suspected behavior patterns;system intrusion","","0","","9","","","15-18 June 2011","","IEEE","IEEE Conference Publications"
"Fine artists of the world unite: Bridging heterogeneous distributed open data sources of fine art","F. Buchanan; N. Capanni; H. González-Vélez","School of Computing, Robert Gordon University, Aberdeen AB25 1HG, UK","International Conference on Information Society (i-Society 2011)","20110808","2011","","","224","229","The sources of information on the Web relating to Fine Art and in particular to Fine Artists are numerous, heterogeneous, and distributed. Data relating to the biographies of an artist, images of their artworks, location of the art-works, and exhibition reviews invariably reside in distinct and seemingly unrelated sources. This paper presents a solution to the problems of heterogeneity and distribution of Fine Art Data. The techniques of intelligent information retrieval and classification are examined as methods to retrieve and classify web-based information on the subject of Fine Art and Artists. A prototype computer application, implanted in Java coupled with XML, demonstrates a possible solution to the problem. This constructs relevant links between heterogeneous sources of Fine Art data by aggregating, classifying and re-presenting it, in an intelligent and automated manner. Open data from a variety of web resources are utilised, including The Guardian's Open-Platform, The Brooklyn Museum's API, and The Victoria and Albert Museum's API. In re-presenting this information via a Web interface, the project increases the accessibility of Fine Art Data.","","Electronic:978-0-9564263-8-3; POD:978-1-61284-148-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978440","Data Repositories;Information Retrieval;Open Data;Semantic Web;e-Art","Educational institutions","Internet;Java;XML;application program interfaces;art;classification;data handling;human computer interaction;information resources;information retrieval","Brooklyn Museum API;Java;The Guardian Open-Platform;The Victoria and Albert Museum API;Web interface;Web resources;XML;fine art data;heterogeneous distributed open data sources;intelligent information classification;intelligent information retrieval","","0","","23","","","27-29 June 2011","","IEEE","IEEE Conference Publications"
"Exploring How to Support Software Revision in Software Non-intensive Projects Using Existing Techniques","H. Kaiya; K. Hara; K. Kobayashi; A. Osada; K. Kaijiri","Dept. of Comput. Sci., Shinshu Univ., Nagano, Japan","2011 IEEE 35th Annual Computer Software and Applications Conference Workshops","20111003","2011","","","327","334","Most industrial products are developed based on their former products including software. Revising existing software according to new requirements is thus an important issue. However, innovative techniques for software revision cannot be easily introduced to projects where software is not a central part. In this paper, we report how to explore and apply software engineering techniques to such non-ideal projects to encourage technology transfer to industry. We first show our experiences with industrial partners to explore which tasks could be supported in such projects and which techniques could be applied to such tasks. As a result, we found change impact analysis could be technically supported, and traceability techniques using information retrieval seemed to be suitable for it. We second had preliminary experiences of a method using such techniques with data in industry and evaluated them with our industrial partners. Based on the evaluation, we third improved such a method by using following techniques, indexing of technical documents for characterizing requirements changes, machine learning on source codes for validating predicted traceability and static source code analysis for finding indirect impacts. Our industrial partners finally evaluated the improved method, and they confirmed the improved method worked better than ever.","","Electronic:978-07695-4459-5; POD:978-1-4577-0980-7","10.1109/COMPSACW.2011.61","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032259","CASE Tool;Change Impact Analysis;Indexing;Information Retrieval;Machine Learning;Requirements Change;Software Revision;Traceability","Cryptography;Indexing;Industries;Machine learning;Measurement;Software;Unified modeling language","document handling;indexing;information retrieval;learning (artificial intelligence);program diagnostics;software engineering","industrial products;information retrieval;machine learning;predicted traceability validation;requirement changes;software engineering techniques;software nonintensive projects;software revision;source codes;static source code analysis;technical document indexing;technology transfer;traceability techniques","","0","","17","","","18-22 July 2011","","IEEE","IEEE Conference Publications"
"SUITE 2010: 2nd International Workshop on Search-Driven Development - Users, Infrastructure, Tools & Evaluation","S. Bajracharya; A. Kuhn; Y. Ye","University of California Irvine","2010 ACM/IEEE 32nd International Conference on Software Engineering","20111027","2010","2","","427","428","SUITE is a workshop that focuses on exploring the notion of search as a fundamental activity during software development. The first edition of SUITE (SUITE 2009 [4]) was held at ICSE 2009. SUITE 2010, like its predecessor, devotes its attention to various research topics pertaining to the information needs of software developers. In SUITE 2010, we plan to emphasize open issues identified in SUITE 2009. We aim to continue building an active network of people interested in the research area that SUITE addresses.","0270-5257;02705257","Electronic:978-1-60558-719-6; POD:978-1-60558-719-6","10.1145/1810295.1810412","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062231","information needs;search driven development;software information retrieval","","","","","2","","18","","","2-8 May 2010","","IEEE","IEEE Conference Publications"
"A semi-automatic source code documentation method for small software development teams","J. C. Zanoni; M. P. Ramos; C. A. Tacla; G. Y. Sato; E. C. Paraiso","DIA - Artificial Intelligence Division, TECPAR - Paran&#x00E1; Institute of Technology, Curitiba, Brazil","Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","20110721","2011","","","113","119","Software developers often face the task of documenting source code. For many of them, documenting code development is a boring task. However, source code documentation is an important task, especially when dealing with groups of developers. An updated documentation allows group members to have greater visibility on what has been and is being developed, allowing the reuse of source code. This research aims at designing, developing and validating a semi-automatic documentation method for source code from the existing design documentation on a particular project being developed by a small team, as well as updating this documentation from information gathered from the source code under development. It is understood as design documentation, those documents or parts of documents that are linked directly to the code under construction.","","Electronic:978-1-4577-0387-4; POD:978-1-4577-0386-7","10.1109/CSCWD.2011.5960063","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960063","information retrieval;small teams of software development;source code documentation","Capability maturity model;Documentation;Information retrieval;Java;Pattern matching;Programming;Software","document handling;software engineering","code development document;design documentation;semi-automatic source code documentation method;software development teams","","2","","21","","","8-10 June 2011","","IEEE","IEEE Conference Publications"
"A Hybrid System to Find & Fight Phishing Attacks Actively","H. Bo; W. Wei; W. Liming; G. Guanggang; X. Yali; L. Xiaodong; M. Wei","China Internet Network Inf. Center, Chinese Acad. of Sci., Beijing, China","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","506","509","Traditional anti-phishing methods and tools always worked in a passive way to receive users' submission and determine phishing URLs. Usually, they are not fast and efficient enough to find and take down phishing attacks. We analyze phishing reports from Anti-phishing Alliance of China(APAC) and propose a hybrid method to discover phishing attacks in an active way based on DNS query logs and known phishing URLs. We develop and deploy our system to report living phishing URLs automatically to APAC every day. Our system has become a main channel in supplying phishing reports to APAC in China and can be a good complement to traditional anti-phishing methods.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040723","Anti-Phishing;DNS;Information Retrieval;LD Algorithm","Accuracy;Browsers;Internet;Organizations;Registers;Security;Servers","Web sites;computer crime;query processing","APAC;DNS query logs;anti-phishing methods;find&fight phishing attacks;hybrid method;phishing URL;phishing reports analysis","","1","","12","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Speech processing and retrieval in a personal memory aid system for the elderly","A. Sorin; H. Aronowitz; J. Mamou; O. Toledo-Ronen; R. Hoory; M. Kuritzky; Y. Erez; B. Ramabhadran; A. Sethy","JSM Haifa Research Lab, Israel","2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20110711","2011","","","1749","1752","The paper presents a new application of automatic speech processing in the Ambient Assisted Living area, developed in the course of a three year research project. Recording and automatic processing of spoken conversations plays a major role in this solution enabling effective search in a personal audio archive and fast browsing of conversations. Processing of elderly conversational speech recorded by a distant PDA microphone poses a great challenge. The speech processing flow includes transcription, speaker tracking and combined indexing and search of spoken terms and participating speakers identity extracted from the audio. We present the entire application and individual speech processing components as well as evaluation results of the individual components and of the end-to-end spoken information retrieval solution.","1520-6149;15206149","Electronic:978-1-4577-0539-7; POD:978-1-4577-0538-0; USB:978-1-4577-0537-3","10.1109/ICASSP.2011.5946840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946840","Ambient Assisted Living;Speech transcription;speaker diarization;speaker identification;spoken information retrieval","Decision support systems","speech processing","PDA microphone;personal audio;personal memory aid system retrieval;speakers;speech processing flow;spoken conversations","","0","","11","","","22-27 May 2011","","IEEE","IEEE Conference Publications"
"Unifying Low-Level and High-Level Music Similarity Measures","D. Bogdanov; J. Serra; N. Wack; P. Herrera; X. Serra","Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain","IEEE Transactions on Multimedia","20110718","2011","13","4","687","701","Measuring music similarity is essential for multimedia retrieval. For music items, this task can be regarded as obtaining a suitable distance measurement between songs defined on a certain feature space. In this paper, we propose three of such distance measures based on the audio content: first, a low-level measure based on tempo-related description; second, a high-level semantic measure based on the inference of different musical dimensions by support vector machines. These dimensions include genre, culture, moods, instruments, rhythm, and tempo annotations. Third, a hybrid measure which combines the above-mentioned distance measures with two existing low-level measures: a Euclidean distance based on principal component analysis of timbral, temporal, and tonal descriptors, and a timbral distance based on single Gaussian Mel-frequency cepstral coefficient (MFCC) modeling. We evaluate our proposed measures against a number of baseline measures. We do this objectively based on a comprehensive set of music collections, and subjectively based on listeners' ratings. Results show that the proposed methods achieve accuracies comparable to the baseline approaches in the case of the tempo and classifier-based measures. The highest accuracies are obtained by the hybrid distance. Furthermore, the proposed classifier-based approach opens up the possibility to explore distance measures that are based on semantic notions.","1520-9210;15209210","","10.1109/TMM.2011.2125784","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5728926","Distance measurement;information retrieval;knowledge acquisition;multimedia computing;multimedia databases;music","Euclidean distance;Mel frequency cepstral coefficient;Mood;Principal component analysis;Semantics;Support vector machines","cepstral analysis;distance measurement;geometry;information retrieval;multimedia computing;music;principal component analysis;support vector machines","Euclidean distance;Gaussian mel-frequency cepstral coefficient modeling;audio content:;culture;distance measurement;genre;instruments;moods;multimedia retrieval;music similarity measures;musical dimensions;principal component analysis;rhythm;support vector machines;tempo annotations;tempo-related description;temporal;timbral distance;tonal descriptors","","11","","76","","20110310","Aug. 2011","","IEEE","IEEE Journals & Magazines"
"A Global Dictionary Based Approach to Fast Similar Text Search in Document Repository","S. Y. Park; S. Y. Kim; S. H. Kim; H. G. Cho","Dept. of Comput. Sci. & Eng., Pusan Nat. Univ., Busan, South Korea","2011 IEEE 11th International Conference on Computer and Information Technology","20111010","2011","","","526","532","Text plagiarism is growing rapidly with the development of Internet, so many plagiarism detection algorithms have been proposed. However, most algorithms focus on the optimized one-to-one comparison, rather than massive document comparison. The latter algorithms have a limitation in time performance when users conduct an exhaustive search on a huge set of documents. In this paper, we propose an optimized preprocessing model to detect similar text in massive document repositories. This model uses an efficient data structure called GDIC (Global Dictionary) for preprocessing. After filtering stop words, we choose pairs of documents to be inspected using two methods at the same time, both of which use the concept of a common non-stop word to choose pairs of documents to be inspected, each of which uses it in a slightly different way. The first method chooses pairs of documents with a high frequency of common non-stop words in documents in each of these pairs, while the second method chooses pairs with a high proportion of common non-stop words. We experimentally prove the performance of the model. Our experiments with the proposed preprocessing model is drastically reduced searching time to 64~87%, while the sensitivity stands at 77~96%. When we use this model, GDIC generation time accounts for a large proportion of all of the detection time. In future work, we will optimize GDIC creation time to improve the performance of the entire system.","","Electronic:978-0-7695-4388-8; POD:978-1-4577-0383-6","10.1109/CIT.2011.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6036820","dictionary;information retrieval;plagiarism;text similarity","Data structures;Dictionaries;Inspection;Internet;Plagiarism;Sensitivity","Internet;data structures;dictionaries;query formulation;text analysis","GDIC;Internet;data structure;document repository;global dictionary;similar text detection;text plagiarism;text search","","1","","16","","","Aug. 31 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Extraction of geospatial information on the Web for GIS applications","G. Shi; K. Barker","Department of Computer Science, The University of Calgary, 2500 University Dr. NW, Alberta, Canada T1N 1N4","Cognitive Informatics & Cognitive Computing (ICCI*CC ), 2011 10th IEEE International Conference on","20110908","2011","","","41","48","Many Web pages contain textual descriptions about locations such as addresses, phone numbers, landmarks, and names. These location-related descriptions are valuable geospatial information for business applications. Therefore, the Web can be perceived as a large geospatial database that could provide up-to-date data for Geographic Information Systems (GIS). Currently this rich and frequently updated Web geospatial information is underutilized. Most related work has been focused on identifying and extracting location names from Web pages for the purpose of page indexing. Little research has been found on extracting and using various types of Web geospatial information for enterprise GIS applications. This paper tries to fill the gap by (1) proposing new algorithms for retrieving different types of geospatial information on Web pages and resolving location name ambiguity issues; (2) exploring how GIS can leverage extracted Web geospatial data for enterprise applications.","","Electronic:978-1-4577-1697-3; POD:978-1-4577-1695-9","10.1109/COGINF.2011.6016120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016120","GIE;GIS;Information Retrieval and Extraction;Web Data Integration","Accuracy;Cities and towns;Data mining;Educational institutions;Geospatial analysis;Web pages","Web sites;geographic information systems","GIS applications;Web geospatial information extraction;Web pages;geographic information systems;location-related descriptions;page indexing;textual descriptions","","3","","19","","","18-20 Aug. 2011","","IEEE","IEEE Conference Publications"
"Combining Multiple Retrieval Systems Using Combinatorial Fusion Analysis and Rank-Score Characteristic Function","H. Liu; Z. Wu; D. F. Hsu","Sch. of Electron. Eng. & Comput. Sci., Peking Univ., Beijing, China","2011 14th IEEE International Conference on Computational Science and Engineering","20111031","2011","","","371","378","Combining the resulting lists of multiple information retrieval (IR) systems has been known to outperform, in many cases, the best of the individual systems. However, it remains a challenging question to know what combination method to use and in what conditions the combination system can perform better than its individual systems. In this paper, we use an information fusion paradigm: Combinatorial Fusion Analysis (CFA) to study these issues. We take the TREC dataset as our experiment data and use the rank-score characteristic (RSC) function to measure the cognitive diversity between different individual systems. Results from our experiment demonstrate that: 1) combined system can improve performance only if the individual systems have relative good performance and are diverse, 2) there is no guarantee that the combined system performs better when more individual systems are added, and 3) rank combination is better than score combination in majority of the cases when the diversity between two individual systems measured by the RSC function is large enough.","","Electronic:978-0-7695-4477-9; POD:978-1-4577-0974-6","10.1109/CSE.2011.71","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062901","cognitive diversity;combinatorial fusion analysis(CFA);information retrieval (IR);multiple scoring systems (MSS);rank combination;rank-score characteristic (RSC) function;score combination","Algorithm design and analysis;Computers;Correlation;Diversity reception;Educational institutions;Search engines","information retrieval;sensor fusion","RSC function;TREC dataset;cognitive diversity;combinatorial fusion analysis;information fusion paradigm;information retrieval system;rank-score characteristic function","","1","","22","","","24-26 Aug. 2011","","IEEE","IEEE Conference Publications"
"Implementing MapReduce over language and literature data over the UK National Grid Service","M. S. Sarwar; M. Alexander; J. Anderson; J. Green; R. O. Sinnott","National e-Science Centre, University of Glasgow, Glasgow, UK","2011 7th International Conference on Emerging Technologies","20111020","2011","","","1","6","Humanities researchers are producing large volumes and heterogeneous varieties of language and literature data collections in digital format. These collections include dictionaries, thesauri, corpora, images, audio and video resources. The increased availability of these datasets brought about by advances and adaptations of the Internet and increased digitisation of humanities data resources, poses new challenges for humanities researchers. Many of these challenges are related to data access and usage and include security, integrity, interoperability, information retrieval, sharing, licensing and copyright. The JISC-funded project Enhancing Repositories for Language and Literature Research (ENROLLER; https://www.enroller.org.uk) is addressing these issues through development of a targeted e-Research environment. A key component of this effort is in supporting large-scale analysis of diverse language and literature data sets. To this end, this paper presents the application of the MapReduce algorithm, that supports information retrieval and linguistic analysis on those datasets. In particular, we describe how MapReduce is used to provide advanced bulk search capabilities exploiting a range of high performance computing resources including the UK National Grid Service (www.ngs.ac.uk) and ScotGrid (www.scotgrid.ac.uk) to offer a step change in the kinds of research that can be undertaken by this community. We also present performance analysis results based on the application of these systems.","","Electronic:978-1-4577-0768-1; POD:978-1-4577-0769-8","10.1109/ICET.2011.6048475","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6048475","ENROLLER;Grid information retrieval;MapReduce;NGS;Scotgrid;eHumanities","Data mining;Dictionaries;Indexing;Instruction sets;Pragmatics;Thesauri","Internet;copyright;data integrity;dictionaries;grid computing;humanities;information retrieval;natural languages;open systems;scientific information systems;security of data;thesauri","ENROLLER;Internet;JlSC-funded project;MapReduce algorithm;ScotGrid;UK National Grid Service;audio resource;bulk search capability;copyright;data access;data integrity;data security;data usage;dictionary;e-Research environment;enhancing repositories for language and literature research project;high performance computing resource;humanities data resource digitisation;humanities researcher;information retrieval;information sharing;interoperability;language data collection;linguistic analysis;literature data collection;thesaurus;video resource","","2","","11","","","5-6 Sept. 2011","","IEEE","IEEE Conference Publications"
"Formalization of natural language queries","H. Boumechaal; Z. Boufaida","LIRE Laboratory, Mentouri University, Constantine Algeria","2011 International Symposium on Innovations in Intelligent Systems and Applications","20110711","2011","","","495","499","Most existing systems of information retrieval are limited to a search by keywords, based on the syntactic content of documents. The absence of semantics conducts to a very complex formulation of queries which generates the problem of access to relevant information on the web. In this paper, we implement a conversion tool which takes queries expressed in natural language and an ontology as input and returns the appropriate formal queries. The generated queries are then sent to the reasoner for querying the knowledge bases.","","Electronic:978-1-61284-922-5; POD:978-1-61284-919-5","10.1109/INISTA.2011.5946148","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5946148","Information retrieval;nRQL language;ontology;semantic research","Feathers;Natural languages;OWL;Ontologies;Semantics","Internet;natural languages;ontologies (artificial intelligence);query processing","World Wide Web;conversion tool;information retrieval;natural language queries;ontology;syntactic content","","1","","14","","","15-18 June 2011","","IEEE","IEEE Conference Publications"
"Technology vs. Infoxication -- The Challenges of Obtaining Intelligence from the Buzz","A. F. C. Diaz","Analyst - Competitive Intell. Unit (UIC), CaixaBank / la Caixa, Barcelona, Spain","2011 European Intelligence and Security Informatics Conference","20111027","2011","","","391","391","After four years pioneering the domain of Competitive Intelligence applied to Online Reputation in Spain, ""la Caixa"", the country's 3rd financial entity, is now in the perfect position to evaluate and, when necessary, integrate the newest technologies into its Corporate Intelligence System. The emergence of two activities - Online Reputation Management/Monitoring and Competitive Intelligence - and the progress of two essential and interrelated industries - language engineering and information retrieval - will surely guide the immediate evolution of both our unit and the market as a whole. In this paper I will briefly comment on the key technologies and methodological approaches that, according to our own experience and vision, will define the future of an activity and profession aimed at being central for the everyday decision-making of large corporations. The challenges are as variable and demanding as variable and demanding is the information revolution that will ultimately define our age. Social media and the Buzz, multilingual and multimodal information retrieval, opinion mining and sentiment analysis, the Semantic Web will be among the key words of this presentation, conceived both as an open reflex ion from the perspective of an experienced end-user and as a ground for debate within the privileged audience of EISIC.","","Electronic:978-0-7695-4406-9; POD:978-1-4577-1464-1","10.1109/EISIC.2011.74","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061271","Competitive Intelligence;Information Retrieval;Language Engineering;Online Reputation Management;Semantic Web","Artificial intelligence;Competitive intelligence;Industries;Informatics;Information retrieval;Monitoring;Semantic Web","data mining;information retrieval;semantic Web","competitive intelligence;corporate intelligence system;decision making;information revolution;infoxication;multilingual information retrieval;multimodal information retrieval;online reputation management;online reputation monitoring;opinion mining;semantic Web;sentiment analysis;social media","","0","","","","","12-14 Sept. 2011","","IEEE","IEEE Conference Publications"
"Analysis and Exploitation of Musician Social Networks for Recommendation and Discovery","B. Fields; K. Jacobson; C. Rhodes; M. d'Inverno; M. Sandler; M. Casey","Intelligent Sound and Music Systems group, Department of Computing Goldsmiths, University of London, London, United Kingdom","IEEE Transactions on Multimedia","20110718","2011","13","4","674","686","This paper presents an extensive analysis of a sample of a social network of musicians. The network sample is first analyzed using standard complex network techniques to verify that it has similar properties to other web-derived complex networks. Content-based pairwise dissimilarity values between the musical data associated with the network sample are computed, and the relationship between those content-based distances and distances from network theory explored. Following this exploration, hybrid graphs and distance measures are constructed, and used to examine the community structure of the artist network. Finally, results of these investigations are shown to be mostly orthogonal between these distance spaces. These results are considered with a focus recommendation and discovery applications employing these hybrid measures as their basis.","1520-9210;15209210","","10.1109/TMM.2011.2111365","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709988","Content-based retrieval;graph theory;music information retrieval;shortest path problem;social network services: MySpace","Communities;Complex networks;Materials;Music;MySpace","Internet;data handling;graph theory;music;social networking (online)","Web-derived complex networks;content-based distances;content-based pairwise dissimilarity values;distance measures;hybrid graphs;musical data;musician social networks;network theory;standard complex network techniques","","6","","42","","20110204","Aug. 2011","","IEEE","IEEE Journals & Magazines"
"IQ -- A Web Mining Tool","C. Best; D. Horby","OSVision Ltd., London, UK","2011 European Intelligence and Security Informatics Conference","20111027","2011","","","382","384","A stand-alone tool for monitoring selected sites for user defined study topics is described. The ""IQ"" tool has been developed by OS Vision to allow Open Source Intelligence analysts to define their areas of interests in a flexible way. IQ is also a front-end analysis tool that can connect to one of several real-time information retrieval systems. The first system is OSVision's proprietary media monitoring system which processes about 80,0000 articles per day from over 20000 international sources in multiple languages. The second system is a stand-alone monitor - IQRepository which can be configured and operated by the user giving full control over which sites to monitor and with what time interval. The third information sources that can be monitored are social media systems like Twitter and Facebook. The stand-alone combination of IQRepository and IQ can also meet in-house security requirements.","","Electronic:978-0-7695-4406-9; POD:978-1-4577-1464-1","10.1109/EISIC.2011.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061269","Information Retrieval;OSINT;Web Mining","Geology;Indexes;Media;Monitoring;Security;Servers;Twitter","Internet;data mining;monitoring;online front-ends;public domain software;security of data;social networking (online)","Facebook;IQ tool;IQRepository;OS vision;Twitter;Web mining tool;front-end analysis tool;in-house security requirements;information sources;international sources;multiple languages;open source intelligence analysts;proprietary media monitoring system;real-time information retrieval systems;site monitoring;social media systems;stand-alone monitor;stand-alone tool;user defined study topics","","1","","5","","","12-14 Sept. 2011","","IEEE","IEEE Conference Publications"
"Ontoogle: Enhancing retrieval with ontologies and facets","S. Thadeu Ferreira da Silva; S. d. O. Apolonio; A. S. Vivacqua; J. Oliveira; G. B. Xexéo; M. L. M. Campos","PPGI/IM - Graduate Program in Informatics, Institute of Mathematics (IM), Federal University of Rio de Janeiro (UFRJ), Brazil","Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","20110721","2011","","","192","199","Design is a knowledge intensive activity, which depends on information to be undertaken. Previous designs, recent innovations, new techniques and materials must be taken into account when working on a new project, whatever its nature. Therefore, retrieving appropriate information is of paramount importance to the design process. Information retrieval is an established field, which has seen great progress in recent years. However, as the amount of unstructured information available increases, it becomes harder to retrieve the most relevant information using standard techniques and, frequently, users end up with results that don't fulfill their information needs. In this paper, we present an approach to improve retrieval using query expansion through ontologies allied to faceted navigation. We constructed the Ontoogle tool, which uses an ontology describing a domain to expand the query and define result ranking and presentation using Faceted Navigation. This proposal was evaluated in a case study and the results indicate that this approach contributes to increased satisfaction and confidence of users during their search process.","","Electronic:978-1-4577-0387-4; POD:978-1-4577-0386-7","10.1109/CSCWD.2011.5960074","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5960074","facets;information retrieval;ontologies;query expansion","Business;Context;Google;Navigation;Ontologies;Search engines","ontologies (artificial intelligence);query processing","Ontoogle tool;design process;faceted navigation;information retrieval enhancement;knowledge intensive activity;ontologies;query expansion","","0","","19","","","8-10 June 2011","","IEEE","IEEE Conference Publications"
"Improving IR-based Traceability Recovery Using Smoothing Filters","A. D. Lucia; M. D. Penta; R. Oliveto; A. Panichella; S. Panichella","Software Eng. Lab., Univ. of Salerno, Fisciano, Italy","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","21","30","Information Retrieval methods have been largely adopted to identify traceability links based on the textual similarity of software artifacts. However, noise due to word usage in software artifacts might negatively affect the recovery accuracy. We propose the use of smoothing filters to reduce the effect of noise in software artifacts and improve the performances of traceability recovery methods. An empirical evaluation performed on two repositories indicates that the usage of a smoothing filter is able to significantly improve the performances of Vector Space Model and Latent Semantic Indexing. Such a result suggests that other than being used for traceability recovery the proposed filter can be used to improve performances of various other software engineering approaches based on textual analysis.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970160","Empirical Software Engineering;Information Retrieval;Smoothing Filters;Software Traceability","Accuracy;Large scale integration;Noise;Pixel;Smoothing methods;Software;Vocabulary","filtering theory;indexing;information retrieval;program diagnostics;software engineering","IR based traceability recovery;information retrieval methods;latent semantic indexing;smoothing filters;software artifacts;software engineering approaches;vector space model","","13","","31","","","22-24 June 2011","","IEEE","IEEE Conference Publications"
"The Web as a Source of Evidence for Filtering Candidate Answers to Natural Language Questions","L. Bonnefoy; P. Bellot; M. Benoit","CERI, Univ. of Avignon, Avignon, France","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","63","66","Identifying and extracting named entities from web pages has been the subject of many researches. In this paper, we propose and evaluate some new unsupervised language modeling approaches to determine the membership level of a candidate answer, a named entity, to a natural language question to a very fine-grained conceptual class of entity. We propose to address this issue by using the Web or DBPedia hierarchy as sources of evidence. Then, this level of membership can be used to improve the ranking of candidate answers in a question-answering task. Lastly, we present the results we obtained by participating in TREC 2010 Entity track.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040497","Information filtering;Information retrieval;Natural language processing;Question-Answering;TREC Entity","Computational modeling;Engines;Ontologies;Organizations;Smoothing methods;Web pages;Web search","Internet;information filtering;natural language processing;question answering (information retrieval)","DBPedia hierarchy;Web pages;candidate answer filtering;named entity extraction;named entity identification;natural language questions;question-answering task;unsupervised language modeling","","2","","10","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Preference Music Ratings Prediction Using Tokenization and Minimum Classification Error Training","J. Reed; C. H. Lee","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Audio, Speech, and Language Processing","20110818","2011","19","8","2294","2303","In order to address two main limitations of current content-based music recommendation approaches, an ordinal regression algorithm for music recommendation that incorporates dynamic information is presented. Instead of assuming that local spectral features within a song are identically and independently distributed examples of an underlying probability density, music is characterized by a vocabulary of acoustic segment models (ASMs), which are found with an unsupervised process. Further, instead of classifying music based on subjective classes, such as genre, or trying to find a universal notion of similarity, songs are classified based on personal preference ratings. The ordinal regression approach to perform the ratings prediction is based on the discriminative-training algorithm known as minimum classification error (MCE) training. Experimental results indicate that improved temporal modeling leads to superior performance over standard spectral-based music representations. Further, the MCE-based preference ratings algorithm is shown to be superior over two other systems. Analysis demonstrates that the superior performance is due to MCE being a non-conservative algorithm that demonstrates immunity to outliers.","1558-7916;15587916","","10.1109/TASL.2011.2129509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734802","Acoustic segment modeling (ASM);content-based recommendation;discriminative-training;minimum classification error (MCE);music information retrieval (MIR);preference rating","Feature extraction;Hidden Markov models;Music;Speech;Speech recognition;Training","multimedia computing;music;pattern classification;recommender systems;regression analysis;unsupervised learning","MCE based preference ratings algorithm;acoustic segment models;content based music recommendation approach;discriminative training algorithm;minimum classification error training;ordinal regression algorithm;personal preference ratings;preference music ratings prediction;probability density;spectral based music representation;tokenization;universal similarity notion","","2","","45","","20110317","Nov. 2011","","IEEE","IEEE Journals & Magazines"
"Efficient enforcement of dynamic cryptographic access control policies for outsourced data","A. V. D. M. Kayem; P. Martin; S. G. Akl","Dept. of Comput. Sci., Univ. of Cape Town, Cape Town, South Africa","2011 Information Security for South Africa","20110926","2011","","","1","8","Outsourcing of their data to third-party service providers is a cost-effective data management strategy for many organizations. Outsourcing, however, introduces new challenges with respect to ensuring the security and the privacy of the data. In addition to the need for standard access control policies, organizations must now be concerned with the privacy of their data and so hiding the data from the service provider is important. Simply encrypting the data before it is transmitted to the service provider is inefficient and vulnerable to security attacks when the access control policies change. Approaches based on two layers of encryption alleviate the privacy concern but still require re-encryption of the data when policies change. This paper presents a novel and efficient solution that employs two layers of encryption of the data and an encrypted data object containing the second access key. Changes to the access control policies are handled by re-encrypting the object containing the affected key, which is an efficient operation. The paper presents our key management approach, a security analysis of our approach, and an evaluation of the performance of a proof of concept implementation of our approach.","2330-9881;23309881","Electronic:978-1-4577-1483-2; POD:978-1-4799-1696-2","10.1109/ISSA.2011.6027517","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027517","Access Control;Information Retrieval;Information Security and Privacy;Outsourced Data Management","Access control;Bismuth;Encryption;Outsourcing;Standards organizations","access control;cryptography;data encapsulation;data privacy;outsourcing","access control policies;data hiding;data privacy;data re-encryption;data security;dynamic cryptography;key management approach;outsourcing data management;service provider","","2","","29","","","15-17 Aug. 2011","","IEEE","IEEE Conference Publications"
"Iterative context-aware feature location: (NIER track)","X. Peng; Z. Xing; X. Tan; Y. Yu; W. Zhao","Fudan University, Shanghai, China","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","900","903","Locating the program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyze each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input, and they tend to miss the nonlocal interactions among features. In this paper, we propose to address the proceeding two issues in feature location using an iterative context-aware approach. The underlying intuition is that the features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: 1) it takes into account the structural similarity between a feature and a program element to determine their relevance; 2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighboring features and program elements. Our initial evaluation suggests the proposed approach is more robust and can significantly increase the recall of feature location with a slight decrease in precision.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1985939","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032547","feature location;information retrieval;structural similarity","","feature extraction;information retrieval;iterative methods;program diagnostics;software maintenance;ubiquitous computing","information retrieval;iterative context aware feature location;one-time analysis;program element location;software system maintenance;source code;structural similarity","","3","","7","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"Chord Recognition by Fitting Rescaled Chroma Vectors to Chord Templates","L. Oudre; Y. Grenier; C. Fevotte","TELECOM ParisTech, Paris, France","IEEE Transactions on Audio, Speech, and Language Processing","20110728","2011","19","7","2222","2233","In this paper, we propose a simple and fast method for chord recognition in music signals. We extract a chromagram from the signal which transcribes the harmonic content of the piece over time. We introduce a set of chord templates taking into account one or more harmonics of the pitch notes of the chord and calculate a scale parameter to fit the chromagram frames to these chords templates. Several chord types (major, minor, dominant seventh, etc.) are considered. The detected chord over a frame is the one minimizing a measure of fit between the rescaled chroma vector and the chord templates. Several popular distances and divergences from the signal processing or probability fields are considered for our task. Our system is improved by some post-processing filtering that modifies the recognition criteria so as to favor time-persistence. The transcription tool is evaluated on three corpora: the Beatles corpus used for MIREX 08, a 20-audio-song corpus, and a resynthesized MIDI corpus. Our system is also compared to state-of-the-art chord recognition methods. Experimental results show that our method compares favorably to the state-of-the-art and is less computationally demanding than the other evaluated systems. Our systems entered the MIREX 2009 competition and performed very well.","1558-7916;15587916","","10.1109/TASL.2011.2139205","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5742981","Chord recognition;music information retrieval;music signal processing;music signal representation","Data mining;Feature extraction;Harmonic analysis;Hidden Markov models;Multiple signal classification;Probability distribution;Training","audio signal processing;filtering theory;signal representation","Beatles corpus;MIREX 08;chord recognition;chord templates;chromagram;fitting rescaled chroma vectors;harmonic content;music signals;pitch notes;post-processing filtering;signal processing;transcription tool","","5","1","24","","20110407","Sept. 2011","","IEEE","IEEE Journals & Magazines"
"Using semantic search to reduce cognitive load in an electronic health record","A. A. Tawfik; K. M. Kochendorfer; D. Saparova; S. Al Ghenaimi; J. L. Moore","Sch. of Inf. Sci. &amp; Learning Technol., Univ. of Missouri, Columbia, MO, USA","2011 IEEE 13th International Conference on e-Health Networking, Applications and Services","20110922","2011","","","181","184","As electronic health records (EHRs) become more prevalent in health care further research is needed to understand the efficacy within clinical contexts from a human-computer interaction viewpoint. Participants (N=10) were given two authentic scenarios that required users to search for patient information. In the first scenario, participants responded to a patient-specific information need as they normally would. In the second scenario, participants were given a semantic search tool that indexed terms within a patient EHR. Upon completion, participants were then asked questions in a semi-structured interview about current usage of the EHR. Statistically significant results revealed that participants were able to more efficiently navigate through an EHR in terms of time (semantic search M=140 vs. browsing M=239 seconds) and number of clicks (semantic search M=11 vs. browsing M=35). This study suggests that semantic search capabilities may be a good way to reduce cognitive load within clinical settings for similar patient-specific information needs.","","Electronic:978-1-61284-697-2; POD:978-1-61284-695-8; USB:978-1-61284-696-5","10.1109/HEALTH.2011.6026739","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6026739","cognitive informatics;information retrieval;medical information systems;search engines","Accuracy;Informatics;Medical diagnostic imaging;Medical services;Navigation;Particle measurements;Semantics","cognitive systems;health care;human computer interaction;medical information systems","EHR;clinical contexts;cognitive load;electronic health record;health care;human-computer interaction;patient information;reduce cognitive load;semantic search","","5","","19","","","13-15 June 2011","","IEEE","IEEE Conference Publications"
"Design and implementation of intranet search engine system","Y. Hongyin; Q. Xuelei","Coll. of Comput. Sci. &amp; Technol., Taiyuan Univ. of Technol., Taiyuan, China","2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)","20110922","2011","","","302","304","With the development of internet technology, the information in the website increases sharply. In view of the problem of the user's great desire for intranet information retrieval and the inefficiency of the intranet information retrieval service provided by web search engines, in this paper, we study the architecture, key technologies and implementation of the intranet search engine system. We designed and implemented an intranet search engine system based on Lucene. The system contains information gathering module, indexing module, searching module and system interface module, which can index and search many document formats, such as html, word, excel, pdf and so on. Experiments show that the system has a good indexing and retrieval efficiency and performance, which can provide intranet information retrieval service for users effectively.","","Electronic:978-1-61284-722-1; POD:978-1-61284-719-1","10.1109/MEC.2011.6025461","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6025461","Indexing;Lucene;Search Engine;information retrieval","Crawlers;Engines;Indexing;Internet;Search engines","Internet;Web sites;indexing;information retrieval;intranets;search engines","Internet technology;Intranet information retrieval service;Intranet search engine system;Lucene;Website;document formats;indexing module;information gathering module;searching module;system interface module","","0","","10","","","19-22 Aug. 2011","","IEEE","IEEE Conference Publications"
"Text classification using word sequence kernel methods","L. A. Trindade; H. Wang; W. Blackburn; N. Rooney","School of Computing and Mathematics, Faculty of Computing and Engineering, University of Ulster, Northern Ireland, UK","2011 International Conference on Machine Learning and Cybernetics","20110912","2011","4","","1532","1537","This paper presents a comparison study of two sequence kernels for text classification, namely, all common subsequences and sequence kernel. We consider some variations of the two kernels - kernels based on individual features, linear combination of individual kernels and kernels with a factored representation of features - and evaluate them in text classification by employing them as similarity functions in a support vector machine. A sentence is represented as a sequence of words along with their lemma and part-of-speech tags. Experiments show that sequence kernel has a clear advantage over all common subsequences. Since the main difference between the two kernels lies in the fact that the frequency of words (objects) is considered in sequence kernel but not in all common subsequences, we conclude that the frequency of words is an important factor in the successful application of kernels to text classification.","2160-133X;2160133X","Electronic:978-1-4577-0308-9; POD:978-1-4577-0305-8","10.1109/ICMLC.2011.6016983","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6016983","Information retrieval;Kernel methods;Machine learning;Natural language processing;Text classification","Accuracy;Cybernetics;Kernel;Machine learning;Motion pictures;Support vector machines;Text categorization","natural language processing;pattern classification;support vector machines;text analysis","SVM;factored representation;linear combination;part-of-speech tags;support vector machine;text classification;word sequence;word sequence kernel methods","","2","","13","","","10-13 July 2011","","IEEE","IEEE Conference Publications"
"Emerging models and e-infrastructures for teacher education","K. Stefanov; R. Nikolov; P. Boytchev; E. Stefanova; A. Georgiev; I. Koychev; N. Nikolova; A. Grigorov","Fac. of Math &amp; Inf., Sofia Univ. St. Kl. Ohridski, Sofia, Bulgaria","2011 International Conference on Information Technology Based Higher Education and Training","20110915","2011","","","1","8","The paper presents a digital repository of metadata resources for teachers education, as well as a portal for the community of practices, build around the repository. Both the repository and the community are developed in the frame of the European project Share.TEC. Some approaches for endowing digital libraries with adaptability capabilities in order to scaffold and enhance end user experience are examined. The paper provides a general overview of techniques and methods commonly adopted for achieving adaptability. It also discusses how these can be implemented. Finally, it illustrates specific examples and guidelines drawn from the practical experience that the authors are currently gaining in the Share.TEC European project. In this context the adaptability is a key for managing and responding to considerable diversity in user requirements.","","Electronic:978-1-4577-1672-0; POD:978-1-4577-1673-7","10.1109/ITHET.2011.6018688","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6018688","Content management;Continuing education;Digital libraries;Information retrieval;Social network services","Coordinate measuring machines;Education;Europe;Libraries;Ontologies;Portals;User interfaces","computer aided instruction;digital libraries;meta data;user interfaces","Share.TEC European project;adaptability context;digital libraries;digital repository;e-infrastructure;end user experience;meta data resource;teacher education;user requirement","","2","","22","","","4-6 Aug. 2011","","IEEE","IEEE Conference Publications"
"Using Educational Resources to Improve the Efficiency of Web Searches for Additional Learning Material","J. C. Prates; S. S. M. Siqueira","Dept. de Inf. Aplic., UNIRIO, Rio de Janeiro, Brazil","2011 IEEE 11th International Conference on Advanced Learning Technologies","20110818","2011","","","563","567","The Internet is an invaluable source of information that can and should be used to help education. One of the possibilities of Internet in this area is the search for resources that complement the learning process, usually done with the support of the search engines. This search is generally performed using keywords, which implies on contextless results. This work uses information extraction techniques applied to educational resources to expand the queries done by students, adding contextual information in the search and thus recovering more appropriate educational resources. A prototype was developed according to the proposed architecture and a case study conducted in a Brazilian university presented results showing that this proposal can be used in an educational environment to improve the search of educational resources.","2161-3761;21613761","Electronic:978-0-7695-4346-8; POD:978-1-61284-209-7","10.1109/ICALT.2011.171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5992419","Context;Personalization;information retrieval","Context;Correlation;Data mining;Internet;Measurement;Search problems","Internet;computer aided instruction;information resources;information retrieval;search engines","Brazilian university;Internet;Web search;educational environment;educational resources;information extraction techniques;information source;learning material;learning process;search engines","","1","","17","","","6-8 July 2011","","IEEE","IEEE Conference Publications"
"Searching, selecting, and synthesizing source code","C. McMillan","College of William &#x0026; Mary, Williamsburg, VA, USA","2011 33rd International Conference on Software Engineering (ICSE)","20111010","2011","","","1124","1125","As a programmer writes new software, he or she may instinctively sense that certain functionality is generally or widely-enough applicable to have been implemented before. Unfortunately, programmers face major challenges when attempting to reuse this functionality: First, developers must search for source code relevant to the high-level task at hand. Second, they must select specific components from the relevant code to reuse. Third, they synthesize these components into their own software projects. Techniques exist to address specific instances of these three challenges, but these techniques do not support programmers throughout the reuse process. The goal of this research is to create a unified approach to searching, selecting, and synthesizing source code. We believe that this approach will provide programmers with crucial insight on how high-level functionality present in existing software can be reused.","0270-5257;02705257","Electronic:978-1-4503-0445-0; POD:978-1-4503-0445-0","10.1145/1985793.1986013","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032606","information retrieval;source code search engines","Java;Libraries;Portfolios;Programming;Search engines;Software;Software algorithms","search engines;software reusability","high-level functionality;software reusability;source code","","2","","23","","","21-28 May 2011","","IEEE","IEEE Conference Publications"
"On the Effect of the Query in IR-based Concept Location","S. Haiduc; A. Marcus","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","2011 IEEE 19th International Conference on Program Comprehension","20110801","2011","","","234","237","Concept location is an essential task during software maintenance and in particular program comprehension activities. One of the approaches to this task is the based on leveraging the lexical information found in the source code by means of Information Retrieval techniques. All IR-based approaches to concept location are highly dependent on the queries written by the users. An IR approach, even though good on average, might fail when the input query is poor. Currently there is no way to tell when a query leads to poor results for IR-based concept location, unless a considerable effort is put into analyzing the results after the fact. We propose an approach based on recent advances in the field of IR research, which aims at automatically determining the difficulty a query poses to an IR-based concept location technique. We plan to evaluate several models and relate them to IR performance metrics.","1092-8138;10928138","Electronic:978-0-7695-4398-7; POD:978-1-61284-308-7","10.1109/ICPC.2011.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5970170","concept location;information retrieval;program comprehension;query;search;source code","Conferences;Correlation;Estimation;Measurement;Prediction algorithms;Search engines","query processing;software maintenance;software metrics;software performance evaluation","IR performance metrics;IR-based concept location;information retrieval techniques;lexical information;program comprehension activity;software maintenance;source code","","5","","16","","","22-24 June 2011","","IEEE","IEEE Conference Publications"
"An integrated framework for retrieving and analyzing geographic information in web pages","H. Lin; L. Hu; Y. Hu; J. Wu; B. Yu","Key Laboratory of Geographic Information Science, Ministry of Education, East China Normal University, Shanghai 200062, P. R. China","2011 19th International Conference on Geoinformatics","20110811","2011","","","1","6","Most of the information stored in web pages contains geographic context, such as place names, address, and coordinates. Such geographic information often contains great values and is worth retrieving and analyzing. Traditional search engine is limited in its capability of extracting meaningful geographic messages from mass of unstructured and textual source. Even with the retrieved geographic information, specific systems are still needed to show the spatial distributions of such data. This paper presents an integrated framework which can retrieve and analyze geographic information in web pages. This framework integrates the following core functions: geographic information retrieval; geocoding; spatial analysis and statistical analysis. We also demonstrate the effectiveness of this framework by employing it to retrieve and analyze the geographic information from a particular website.","2161-024X;2161024X","Electronic:978-1-61284-848-8; POD:978-1-61284-849-5","10.1109/GeoInformatics.2011.5980857","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5980857","geographic information retrieval;spatial analysis;web pages","Earthquakes;Geographic Information Systems;HTML;Roads;Search engines;Web pages","Web sites;information industry;information retrieval;statistical analysis","Web pages;geocoding;geographic context;geographic information analysis;geographic information retrieval;geographic message extraction;search engine;spatial analysis;spatial distributions;statistical analysis;textual source","","0","","17","","","24-26 June 2011","","IEEE","IEEE Conference Publications"
"Retrieval of Patent Documents from Heterogeneous Sources Using Ontologies and Similarity Analysis","S. Taduri; G. T. Lau; K. H. Law; J. P. Kesan","Eng. Inf. Group, Stanford Univ., CA, USA","2011 IEEE Fifth International Conference on Semantic Computing","20111027","2011","","","538","545","In the past few years, there has been an explosive growth in scientific and legal information related to the patent system. Patents and related documents are siloed into multiple heterogeneous sources. Retrieving relevant information from diverse sources is a non-trivial task and poses many technical challenges. Among the challenges is the issue of terminological inconsistencies that are used in the documents. We tackle the terminological inconsistency issue by exploring domain knowledge through the use of ontology standards. Furthermore, we take advantage of cross-references and structural dependencies between the information sources to enhance terminological comparison. In this paper, we present a similarity analysis methodology which combines knowledge from two distinct sources -- (1) domain ontologies and (2) ontologies which describe the information sources to assist a user in identifying relevant documents across several information sources simultaneously. Specifically, we explore the use of a rule-based system to infer relationships between documents based on pre-defined heuristics. We present our results through a use case in the bio-patent domain with a collection of 1150 patents and 30 court cases.","","Electronic:978-0-7695-4492-2; POD:978-1-4577-1648-5","10.1109/ICSC.2011.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6061369","Court cases;Information Retrieval;Knowledgebase;Ontology;Patent","Context;Databases;Ontologies;Patents;Semantics;Terminology","distributed databases;information resources;information retrieval;knowledge based systems;ontologies (artificial intelligence);patents;pattern matching;scientific information systems","biopatent domain;document identification;heterogeneous source;legal information;ontology standard;patent document retrieval;rule-based system;scientific information;similarity analysis;terminological inconsistency","","2","","38","","","18-21 Sept. 2011","","IEEE","IEEE Conference Publications"
"Visual clustering in web search: An effective approach","H. Badesh; J. Blustein","Faculty of Computer Science, Dalhousie University, Halifax, Canada","International Conference on Information Society (i-Society 2011)","20110808","2011","","","34","38","Presenting search results as a list of hits can be ineffective in assisting users to find relevant documents and discover varied topics among search results. Visualization can assist users to find relevant documents and make the search interface more effective. This paper presents a Data Mountain Search Results Presentation Interface (DMSRPI). The interface is intended to improve the effectiveness in how users search the Web and find relevant information. A user study is yet to be conducted for evaluating the DMSRPI.","","Electronic:978-0-9564263-8-3; POD:978-1-61284-148-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5978504","Web information retrieval;query reformulation;techniques;visual clustering;visualization","Data visualization;Educational institutions;HTML;Navigation;World Wide Web","Internet;data visualisation;document handling;information retrieval;pattern clustering","DMSRPI;Web search;World Wide Web;data mountain search results presentation interface;data visualization;relevant documents;relevant information;search interface;visual clustering","","0","","27","","","27-29 June 2011","","IEEE","IEEE Conference Publications"
"Frequency Based Locality Sensitive Hashing","Kang Ling; Gangshan Wu","State Key Laboratory for Novel Software Technology, Nanjing University, China","2011 International Conference on Multimedia Technology","20110825","2011","","","4929","4932","Nearest Neighbor (NN) search is of major importance to many applications, such as information retrieval, data mining and so on. However, finding the NN in high dimensional space has been proved to be time-consuming. In recent years, Locality Sensitive Hashing (LSH) has been proposed to solve Approximate Nearest Neighbor (ANN) problem. The main drawback of LSH is that it requires quite a lot of memory to achieve good performance, which makes it not that suit for today's application of massive data. We analyze generic LSH scheme as well as the properties of LSH hash functions based on p-stable distributions and propose a new LSH scheme called Frequency Based Locality Sensitive Hashing (FBLSH). FBLSH just uses one function based on p-stable distributions as hash function of a hash table, and it sets a frequency threshold m, only those points which collide with query point more than m times can be candidate ANNs. FBLSH is easy to implement and through experiments, we show that FBLSH can reduce the extra space cost by several orders of magnitude with less (or similar) time cost while achieving better search quality compared with LSH based onp-stable distributions.","","Electronic:978-1-61284-774-0; POD:978-1-61284-771-9","10.1109/ICMT.2011.6002015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6002015","Information Retrieval;LSH;Similarity Search","Algorithm design and analysis;Artificial neural networks;Indexes;Memory management;Nearest neighbor searches;Random access memory;Time frequency analysis","data mining;file organisation;information retrieval;statistical distributions","ANN problem;FBLSH;LSH hash functions;NN search;approximate nearest neighbor problem;data mining;frequency based locality sensitive hashing;frequency threshold;generic LSH scheme;hash table;high dimensional space;information retrieval;nearest neighbor search;p-stable distributions;query point;search quality","","2","","9","","","26-28 July 2011","","IEEE","IEEE Conference Publications"
"Combining the Best of Two Worlds: NLP and IR for Intranet Search","S. Adindla; U. Kruschwitz","Sch. of Comput. Sci. & Electron. Eng., Univ. of Essex, Colchester, UK","2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20111010","2011","1","","483","488","Natural language processing (NLP) is becoming much more robust and applicable in realistic applications. One area in which NLP has still not been fully exploited is information retrieval (IR). In particular we are interested in search over intranets and other local Web sites. We see dialogue-driven search which is based on a largely automated knowledge extraction process as one of the next big steps. Instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager. Here we support this idea with a first task-based evaluation that we conducted on a university intranet. We automatically extracted entities like person names, organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base. We found that users are willing to interact and use those visual interfaces. We also found that users preferred such a system that guides a user through the result set over a baseline approach. The results represent an important first step towards full NLP-driven intranet search.","","Electronic:978-0-7695-4513-4; POD:978-1-4577-1373-6","10.1109/WI-IAT.2011.187","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6040716","dialogue;domain knowledge;information retrieval;natural language processing;visualization","Data mining;Databases;Educational institutions;Knowledge based systems;Search engines;Visualization;Web sites","Web sites;interactive systems;intranets;natural language processing;query processing;user interfaces","IR;Intranet search;NLP;Web sites;dialogue manager;dialogue-driven search;first task-based evaluation;information retrieval;knowledge extraction process;natural language processing;user query;visual graphs;visual interfaces","","0","","28","","","22-27 Aug. 2011","","IEEE","IEEE Conference Publications"
"Fuzzy folksonomy-based index creation for e-Learning content retrieval on cloud computing environments","W. C. Shih; C. T. Yang; S. S. Tseng","Department of Applied Informatics and Multimedia, Asia University, Taichung, 41354, Taiwan","2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)","20110901","2011","","","965","970","Due to the trend of individualization and adaptation of e-Learning, more and more SCORM-compliant teaching materials are developed by institutes and individuals in different sites. Also, cloud computing environments are emerging as powerful infrastructures to support e-Learning applications. Therefore, how to rapidly retrieve SCORM-compliant documents on cloud computing environments has become an important issue. Creating an index from folksonomies has been investigated in previous researches; however, the involved uncertainty has not been addressed. This paper focuses on the fuzzy index creation problem for learning content retrieval. A bottom-up approach to constructing the fuzzy index is proposed. The index creation method has been implemented, and a synthetic learning object repository has been built on a Hadoop cloud platform to evaluate the proposed approach. Experimental results show that this method can increase precision of retrieval.","1098-7584;10987584","Electronic:978-1-4244-7317-5; POD:978-1-4244-7315-1; USB:978-1-4244-7316-8","10.1109/FUZZY.2011.6007516","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6007516","Cloud computing;Folksonomy;Fuzzy Sets;Information retrieval;e-Learning","Cloud computing;Electronic learning;Indexes;Information retrieval;Materials;Ontologies","cloud computing;courseware;fuzzy set theory;indexing;information retrieval;teaching","Hadoop cloud platform;SCORM-compliant retrieval;SCORM-compliant teaching material;cloud computing environments;e-learning content retrieval;fuzzy folksonomy-based index creation;synthetic learning object repository","","3","","27","","","27-30 June 2011","","IEEE","IEEE Conference Publications"
"TraCter: A tool for candidate traceability link clustering","A. Mahmoud; N. Niu","Computer Science and Engineering, Mississippi State University, Mississippi State, MS 39762","2011 IEEE 19th International Requirements Engineering Conference","20111020","2011","","","335","336","Automated tracing tools employ information retrieval (IR) methods to recover traceability links between software artifacts. A large body of research is available on the back-end design of such tools, including artifacts indexing and the underlying IR mechanism. In contrast, less attention has been paid to the front-end presentation of the retrieved results. This paper describes TraCter, a result categorization tool with novel search user interfaces. We discuss the key features of TraCter and its potential improvements over previous work.","1090-705X;1090705X","Electronic:978-1-4577-0924-1; POD:978-1-4577-0921-0","10.1109/RE.2011.6051663","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051663","information retrieval;requirements;result categorization;search user interface;traceability","Algorithm design and analysis;Clustering algorithms;Humans;Indexing;Information retrieval;Software;User interfaces","indexing;information retrieval;program diagnostics;user interfaces","IR mechanism;TraCter;artifact indexing;automated tracing tools;back-end design;candidate traceability link clustering;information retrieval method;result categorization tool;search user interfaces;software artifacts","","6","","8","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Improving the precision ratio using semantic based search","S. Amudaria; S. Sasirekha","Dept. of Inf. Technol., SSN Coll. of Eng., Chennai, India","2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies","20110922","2011","","","465","470","The overall goal of an information retrieval process is to retrieve the information relevant to the given request. The information retrieval techniques commonly used are based on keywords. These techniques use keyword listed to describe the content of information, but one problem with such list is that they do not say anything about the semantic relationships between keywords, nor do they take into account the meaning of words and phrases. To overcome these limitations, from the recent literature it is identified that it is necessary to analyze and determine the semantic features of both the content in document and query. Hence in this paper it is proposed to develop ontology and a comparison is made between the normal search and ontology based semantic search. Various experimental results are carried on, which shows the increase in document retrieval recall and precision rates, thereby demonstrating the effectiveness of the model.","","Electronic:978-1-61284-653-8; POD:978-1-61284-654-5","10.1109/ICSCCN.2011.6024595","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024595","Information retrieval;Ontology;Query extension;Query matching;Semantic extraction","Databases;Matrix decomposition;Ontologies;Search engines;Semantics;Signal processing","natural language processing;ontologies (artificial intelligence);query processing","document retrieval;information retrieval;keywords;ontology based semantic search;phrases;precision ratio;semantic based search;semantic relationships","","0","","15","","","21-22 July 2011","","IEEE","IEEE Conference Publications"
"Weighted ontology for subject search in Learning Content Management System","Y. Anistyasari; R. Sarno","Inf. Dept., Inst. Teknol. Sepuluh Nopember, Surabaya, Indonesia","Proceedings of the 2011 International Conference on Electrical Engineering and Informatics","20110919","2011","","","1","4","Learning Content Management System (LCMS) is a powerful tool for supporting distance learning. One of the LCMS development problems is subject searching. Most users have no idea about subject name of what they are looking for. They only know about subject contents. Nowadays, search engine embedded with LCMS gives result based on string matching keyword based. Precision and recall of this method is low. This research proposes subject name search based on document content using weighted ontology. Ontology is built from extracted term. Each term is given a weight based on the number of its relation. User query is expanded based on its synonym in WordNet. It is also weighted and taken into account of its similarity with course ontology. System retrieves similar or same subjects based on user query. Precision and recall of weighted ontology search result is better than string matching keyword search.","2155-6822;21556822","Electronic:978-1-4577-0752-0; POD:978-1-4577-0753-7","10.1109/ICEEI.2011.6021646","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6021646","Learning Content Management System;system information retrieval;weighted ontology","Computer architecture;Education;Games;Learning systems;Ontologies;Semantics;Software","distance learning;ontologies (artificial intelligence);query processing;search engines;string matching","WordNet;course ontology;distance learning;document content;learning content management system;search engine;string matching keyword search;subject searching;user query;weighted ontology","","2","","18","","","17-19 July 2011","","IEEE","IEEE Conference Publications"
"Interactive Query Expansion With the Use of Clustering-by-Directions Algorithm","A. L. Kaczmarek","Faculty of Electronics, Telecommunications and Informatics, Gdansk University of Technology, Gdansk, Poland","IEEE Transactions on Industrial Electronics","20110711","2011","58","8","3168","3173","This paper concerns clustering-by-directions algorithm. The algorithm introduces a novel approach to interactive query expansion. It is designed to support users of search engines in forming Web search queries. When a user executes a query, the algorithm shows potential directions in which the search can be continued. This paper describes the algorithm, and it presents an enhancement which reduces the computational complexity of the algorithm. Moreover, in this paper, a new type of interface is introduced. It is based on a tag cloud, in which terms are located in a radial arrangement. This paper also presents the new experimental results and the evaluation of the algorithm.","0278-0046;02780046","","10.1109/TIE.2010.2045315","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5432956","Clustering methods;information retrieval;interactive query expansion;search methods","Clustering algorithms;Computational complexity;Humans;Information retrieval;Permission;Search engines;Search methods;Tag clouds;Web pages;Web search","Internet;interactive systems;pattern clustering;query processing;search engines","Web search queries;clustering by direction algorithm;computational complexity;interactive query expansion;search engines;tag cloud","","8","","20","","20100318","Aug. 2011","","IEEE","IEEE Journals & Magazines"
