"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7503003,7502447,7473819,7500204,7502161,7482803,7501348,7499043,7471467,7495568,7495550,7495529,7495740,7496034,7497987,7495713,7495574,7496029,7498000,7495611,7492420,7494106,7478408,7492774,7493276,7470596,7493281,7493485,7365487,7489612,7488411,7489133,7489816,7489214,7488400,7490307,7486509,7486265,7486188,7483152,7483683,7483212,7484193,7448427,7483232,7482127,7482764,7480107,7480119,7480121,7479924,7479230,7479235,7474826,7479304,7479188,7477547,7478523,7474196,7474131,7476223,7475385,7474194,7439866,7307098,7472150,7473509,7470788,7472805,7472796,7472156,7472932,7473512,7470786,7473755,7467728,7463805,7464547,7463854,7167700,7401039,7460537,7439740,7460560,7459413,7459914,7459402,7459358,7458969,7459439,7460526,7459425,7458022,7457061,7457453,7457131,7456932,7455873,7455877,7455862",2017/05/04 23:26:41
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A machine learning approach to predict episodic memory formation","Hanlin Tang; J. Singer; M. Ison; G. Pivazyan; M. Romaine; E. Meller; V. Perron; M. Arlellano; G. Kreiman; M. Romaine; A. Boulin; R. Frias; J. Carroll; S. Dowcett","Children's Hospital, Harvard Medical School, Boston, USA","2016 Annual Conference on Information Science and Systems (CISS)","20160428","2016","","","539","544","Episodic memories constitute the essence of our recollections and are formed by autobiographical experiences and contextual knowledge. Memories are rich and detailed, yet at the same time they can be malleable and inaccurate. The contents that end up being remembered are the result of filtering incoming sensory inputs in the context of previous knowledge. Here we asked whether the quintessentially subjective process of memory construction could be predicted by a supervised machine learning approach based exclusively on content information. We considered audiovisual segments from a movie as a proxy for real-life memory formation and built a quantitative model to explain psychophysics data evaluating recognition memory. The inputs to the model included audiovisual information (e.g. presence of specific characters, objects, voices and sounds), scene information (e.g. location, presence or absence of action) and emotional valence information. The machine-learning model could predict memory formation in single trials both for group averages and individual subjects with an accuracy of up to 80% using solely stimulus content properties. These results provide a quantitative and predictive model that links sensory perception and emotional attributes to memory formation. Furthermore, the results demonstrate that a computational model can make sophisticated inferences about a cognitive process that involves selective filtering and subjective interpretation.","","Electronic:978-1-4673-9457-4; POD:978-1-4673-9458-1","10.1109/CISS.2016.7460560","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460560","behavioral prediction;computational neuroscience;episodic memory;machine-learning","Classification algorithms;Computational modeling;Motion pictures;Prediction algorithms;Predictive models;Support vector machines;Training","learning (artificial intelligence);neurophysiology","autobiographical experiences;contextual knowledge;episodic memory formation;memory construction;psychophysics data;recognition memory;supervised machine learning","","","","10","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"A PSO-based weighting method to enhance machine learning techniques for cooperative spectrum sensing in CR networks","E. Ghazizadeh; B. Nikpour; D. A. Moghadam; H. Nezamabadi-pour","Electrical Eng. Dep., Shahid Bahnoar University","2016 1st Conference on Swarm Intelligence and Evolutionary Computation (CSIEC)","20160602","2016","","","113","118","Cognitive radio (CR) is a recent technology to tackle the problem of radio spectrum scarcity. Successful spectrum sensing is fundamental in performance of CR networks; hence, a PSO-based weighting method is proposed in order to improve the functionality of machine learning techniques which are used with the aim of detecting the activity of secondary users in cooperative cognitive radio (CCR) networks. Regarding classification methods, three supervised classifiers which are supported vector machines (SVM), K-nearest neighbors (K-NN) and naïve Bayes are used for pattern classification. Since our goal is spectrum sensing in CCR networks, the vector of energy levels in radio channel which is considered as a feature vector is fed into the classifier to determine the availability of the channel. The classifier labels each feature vector as two classes: the ""channel available class"" or the ""channel unavailable class"". In our proposed method, first, the three mentioned classifiers go through a training phase. Next, for new feature vectors, a label is assigned to the feature vector by each classifier and the final decision about the availability of the channel is made by a weighted voting method based on the PSO algorithm in an online fashion. The performance of our technique is measured in terms of the classification error. Also, the comparative results show twofold merit over previous methods since it not only reduces the error rate but also decreases the error of the channel available class.","","Electronic:978-1-4673-8737-8; POD:978-1-4673-8738-5","10.1109/CSIEC.2016.7482127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482127","K-NN;Naïve Bayes;PSO algorithm;SVM;cognitive radio;cooperative spectrum sensing","Cascading style sheets;Cognitive radio;Energy states;Error analysis;Sensors;Support vector machines;Training","Bayes methods;cognitive radio;learning (artificial intelligence);particle swarm optimisation;pattern classification;radio networks;radio spectrum management;support vector machines;vectors","CCR networks;K-nearest neighbors classifier;PSO-based weighting method;channel available class;channel unavailable class;cooperative cognitive radio networks;cooperative spectrum sensing;energy level vector;feature vector;machine learning techniques;naïve Bayes classifier;pattern classification;radio spectrum scarcity;supervised classifiers;support vector machines;weighted voting method","","","","20","","","9-11 March 2016","","IEEE","IEEE Conference Publications"
"A review on mobile threats and machine learning based detection approaches","B. Arslan; S. Gunduz; S. Sagiroglu","Department of Computer Engineering, Gazi University, Ankara, Turkey","2016 4th International Symposium on Digital Forensic and Security (ISDFS)","20160519","2016","","","7","13","The research of mobile threats detection using machine learning algorithms have got much attention in recent years due to increase of attacks. In this paper, mobile vulnerabilities were examined based on attack types. In order to prevent or detect these attacks machine learning methods used were analyzed and papers published in between 2009 and 2014 have been evaluated. Most important mobile vulnerabilities implementation format for these threats, detection methods and prevention approaches with the help of machine learning algorithms are presented. The obtained results are compared from their achievements were summarized. The results have shown that selecting and using datasets play an important role on the success of the system. Additionally, supervised learning techniques produce better results while compared with unsupervised ones in intrusion detection.","","Electronic:978-1-4673-9865-7; POD:978-1-4673-9866-4; USB:978-1-4673-9864-0","10.1109/ISDFS.2016.7473509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473509","","Computers;Mobile communication;Mobile handsets;Trojan horses;Viruses (medical)","invasive software;learning (artificial intelligence);mobile computing","attack detection;intrusion detection;machine learning algorithms;machine learning based detection approach;mobile threat detection;mobile vulnerability implementation format;supervised learning techniques","","","","50","","","25-27 April 2016","","IEEE","IEEE Conference Publications"
"Dot-product engine as computing memory to accelerate machine learning algorithms","M. Hu; J. P. Strachan; Zhiyong Li; R. Stanley; Williams","Hewlett Packard Labs, 1501 Page Mill road, Palo Alto, CA USA","2016 17th International Symposium on Quality Electronic Design (ISQED)","20160526","2016","","","374","379","Currently, intense work is underway to develop memristor crossbar arrays for high density, nonvolatile memory applications. However, another capability of memristor crossbars - natural dot-product operation for vectors and matrices - holds even greater potential for next-generation computing, including accelerators, neuromorphic computing, and heterogeneous computing. In this paper, we present a dot-product engine (DPE) based on memristor crossbars optimized for dense matrix computation, which is dominated in most machine learning algorithms. We explored multiple methods to enhance DPE's dot-product computing accuracy. Moreover, instead of training crossbars, we try to directly use existing software-trained weight matrices on DPEs so no heroic effort is needed to innovate learning algorithms for new hardware. Our results show that computations utilizing DPEs can achieve 1000 ~ 10000 times better speed-efficiency product comparing to a state-of-art ASIC [1]. And machine learning algorithm utilizing DPEs can easily achieve software-level accuracy on testing. Both experimental demonstrations and data-calibrated circuit simulations are presented to demonstrate the realistic implementation of a memristor crossbar DPE.","1948-3295;19483295","Electronic:978-1-5090-1213-8; POD:978-1-5090-1214-5","10.1109/ISQED.2016.7479230","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479230","Memristor;computing memory;crossbar;dot-product operation;machine learning","Algorithm design and analysis;Degradation;Machine learning algorithms;Memristors;Programming;Resistance;Wires","learning (artificial intelligence);mathematics computing;matrix multiplication;memristors;vectors","ASIC;DPE;dense matrix computation;dot-product engine;heterogeneous computing;machine learning algorithms;memristor crossbar arrays;memristor crossbars;natural dot-product operation;neuromorphic computing;next-generation computing;nonvolatile memory applications;software-level accuracy;software-trained weight matrices;vectors","","","","16","","","15-16 March 2016","","IEEE","IEEE Conference Publications"
"Student research highlight: Secure and resilient distributed machine learning under adversarial environments","R. Zhang; Q. Zhu","New York University, Brooklyn, NY, USA","IEEE Aerospace and Electronic Systems Magazine","20160617","2016","31","3","34","36","Machine learning algorithms, such as support vector machines (SVMs), neutral networks, and decision trees (DTs) have been widely used in data processing for estimation and detection. They can be used to classify samples based on a model built from training data. However, under the assumption that training and testing samples come from the same natural distribution, an attacker who can generate or modify training data will lead to misclassification or misestimation. For example, a spam filter will fail to recognize input spam messages after training crafted data provided by attackers [1].","0885-8985;08858985","","10.1109/MAES.2016.150202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478408","","Computers;Game theory;Linear programming;Machine learning;Machine learning algorithms;Security;Support vector machines","","","","","","","","","March 2016","","IEEE","IEEE Journals & Magazines"
"Grading of mammalian cumulus oocyte complexes using machine learning for in vitro embryo culture","P. S. Viswanath; T. Weiser; P. Chintala; S. Mandal; R. Dutta","Faculty of Informatics and Mathematics, Technische Universit&#228;t M&#252;nchen, Boltzmannstra&#223;e 3, 85748 Garching b. M&#252;nchen, Germany","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","172","175","Visual observation of Cumulus Oocyte Complexes provides only limited information about its functional competence, whereas the molecular evaluations methods are cumbersome or costly. Image analysis of mammalian oocytes can provide attractive alternative to address this challenge. However, it is complex, given the huge number of oocytes under inspection and the subjective nature of the features inspected for identification. Supervised machine learning methods like random forest with annotations from expert biologists can make the analysis task standardized and reduces inter-subject variability. We present a semiautomatic framework for predicting the class an oocyte belongs to, based on multi-object parametric segmentation on the acquired microscopic image followed by a feature based classification using random forests.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455862","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455862","","Decision trees;Embryo;Feature extraction;Image segmentation;In vitro;Vegetation;Visualization","biomedical optical imaging;cellular biophysics;feature extraction;image classification;image segmentation;learning (artificial intelligence);medical image processing","acquired microscopic image;cumbersome;expert biologists;feature based classification;feature inspection;functional competence;image analysis;in vitro embryo culture;intersubject variability;mammalian cumulus oocyte complexes;molecular evaluation methods;multiobject parametric segmentation;random forest;supervised machine learning methods;task standardized analysis;visual observation","","","","17","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Adaptive sequential optimization with applications to machine learning","C. Wilson; V. V. Veeravalli","Coordinated Science Lab and Electrical and Computer Engineering, University of Illinois at Urbana-Champaign","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","2642","2646","A framework is introduced for solving a sequence of slowly changing optimization problems, including those arising in regression and classification applications, using optimization algorithms such as stochastic gradient descent (SGD). The optimization problems change slowly in the sense that the minimizers change at either a fixed or bounded rate. A method based on estimates of the change in the minimizers and properties of the optimization algorithm is introduced for adaptively selecting the number of samples needed from the distributions underlying each problem in order to ensure that the excess risk, i.e., the expected gap between the loss achieved by the approximate minimizer produced by the optimization algorithm and the exact minimizer, does not exceed a target level. Experiments with synthetic and real data are used to confirm that this approach performs well.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472156","adaptive algorithms;gradient methods;machine learning;stochastic optimization","Algorithm design and analysis;Approximation algorithms;Data models;Hidden Markov models;Machine learning algorithms;Optimization;Stochastic processes","approximation theory;gradient methods;learning (artificial intelligence);minimisation;regression analysis;stochastic programming","SGD;adaptive sequential optimization problem;classification application;exact minimizer;machine learning;regression application;stochastic gradient descent","","","","31","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Monitoring Land-Cover Changes: A Machine-Learning Perspective","A. Karpatne; Z. Jiang; R. R. Vatsavai; S. Shekhar; V. Kumar","Computer Science and Engineering, University of Minnesota, Minneapolis, Minnesota USA","IEEE Geoscience and Remote Sensing Magazine","20160607","2016","4","2","8","21","Monitoring land-cover changes is of prime importance for the effective planning and management of critical, natural and man-made resources. The growing availability of remote sensing data provides ample opportunities for monitoring land-cover changes on a global scale using machine-learning techniques. However, remote sensing data sets exhibit unique domain-specific properties that limit the usefulness of traditional machine-learning methods. This article presents a brief overview of these challenges from the perspective of machine learning and discusses some of the recent advances in machine learning that are relevant for addressing them. These approaches show promise for future research in the detection of land-cover change using machine-learning algorithms.","2473-2397;24732397","","10.1109/MGRS.2016.2528038","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486265","","Data models;Land cover monitoring;Machine learning;Monitoring;Remote sensing;Spatial resolution;Training","land cover;learning (artificial intelligence);remote sensing","land-cover change detection;land-cover change monitoring;machine-learning algorithm;machine-learning perspective;machine-learning technique;remote sensing data","","","","","","","June 2016","","IEEE","IEEE Journals & Magazines"
"Automatic detection of neurons in high-content microscope images using machine learning approaches","G. Mata; M. Radojevic; I. Smal; M. Morales; E. Meijering; J. Rubio","Departamento de Matem&#225;ticas y Computaci&#243;n, Universidad de La Rioja, Spain","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","330","333","The study of neuronal cell morphology and function in relation to neurological disease processes is of high importance for developing suitable drugs and therapies. To accelerate discovery, biological experiments for this purpose are increasingly scaled up using high-content screening, resulting in vast amounts of image data. For the analysis of these data fully automatic methods are needed. The first step in this process is the detection of neuron regions in the high-content images. In this paper we investigate the potential of two machine-learning based detection approaches based on different feature sets and classifiers and we compare their performance to an alternative method based on hysteresis thresholding. The experimental results indicate that with the right feature set and training procedure, machine-learning based methods may yield superior detection performance.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493276","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493276","Neuron screening;feature extraction;high-content analysis;machine learning;object detection","Detectors;Drugs;Feature extraction;Microscopy;Morphology;Neurons;Training","","","","","","20","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for medication adherence monitoring using body-worn sensors","N. Hezarjaribi; R. Fallahzadeh; H. Ghasemzadeh","Embedded & Pervasive Systems Lab (EPSL), School of Electrical Engineering and Computer Science, Washington State University, Pullman, 99164-2752, USA","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20160428","2016","","","842","845","One of the most important challenges in chronic disease self-management is medication non-adherence, which has irrevocable outcomes. Although many technologies have been developed for medication adherence monitoring, the reliability and cost-effectiveness of these approaches are not well understood to date. This paper presents a medication adherence monitoring system by user-activity tracking based on wrist-band wearable sensors. We develop machine learning algorithms that track wrist motions in real-time and identify medication intake activities. We propose a novel data analysis pipeline to reliably detect medication adherence by examining single-wrist motions. Our system achieves an accuracy of 78.3% in adherence detection without need for medication pillboxes and with only one sensor worn on either of the wrists. The accuracy of our algorithm is only 7.9% lower than a system with two sensors that track motions of both wrists.","","Electronic:978-3-9815-3707-9; POD:978-1-4673-9228-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459425","","Decision trees;Feature extraction;Monitoring;Sensors;Tracking;Training;Wrist","body sensor networks;data analysis;diseases;learning (artificial intelligence);medical diagnostic computing;medical signal processing;patient monitoring","body-worn sensors;chronic disease self-management;data analysis pipeline;machine learning approach;medication adherence monitoring system;medication intake activities;single-wrist motions;user-activity tracking;wrist-band wearable sensors","","4","","13","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Fuel consumption prediction of fleet vehicles using Machine Learning: A comparative study","S. Wickramanayake; H. M. N. Dilum Bandara","Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka","2016 Moratuwa Engineering Research Conference (MERCon)","20160530","2016","","","90","95","Ability to model and predict the fuel consumption is vital in enhancing fuel economy of vehicles and preventing fraudulent activities in fleet management. Fuel consumption of a vehicle depends on several internal factors such as distance, load, vehicle characteristics, and driver behavior, as well as external factors such as road conditions, traffic, and weather. However, not all these factors may be measured or available for the fuel consumption analysis. We consider a case where only a subset of the aforementioned factors is available as a multi-variate time series from a long distance, public bus. Hence, the challenge is to model and/or predict the fuel consumption only with the available data, while still indirectly capturing as much as influences from other internal and external factors. Machine Learning (ML) is suitable in such analysis, as the model can be developed by learning the patterns in data. In this paper, we compare the predictive ability of three ML techniques in predicting the fuel consumption of the bus, given all available parameters as a time series. Based on the analysis, it can be concluded that the random forest technique produces a more accurate prediction compared to both the gradient boosting and neural networks.","","Electronic:978-1-5090-0645-8; POD:978-1-5090-0646-5","10.1109/MERCon.2016.7480121","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480121","artificial neural networks;fuel economy;gradient boosting;predictive model;random forest","Boosting;Data models;Fuels;Prediction algorithms;Predictive models;Radio frequency;Vehicles","environmental science computing;fuel economy;learning (artificial intelligence)","ML techniques;bus;fleet management;fleet vehicles;fuel consumption prediction;fuel economy;gradient boosting;machine learning;multivariate time series;neural networks;random forest technique","","","","17","","","5-6 April 2016","","IEEE","IEEE Conference Publications"
"Exploiting the use of machine learning in two different sensor network architectures for indoor localization","E. Carvalho; B. S. Faiçal; G. P. R. Filho; P. A. Vargas; J. Ueyama; G. Pessin","Institute of Exact and Natural Sciences, Federal University of Par&#225;, Bel&#233;m - PA, Brazil","2016 IEEE International Conference on Industrial Technology (ICIT)","20160526","2016","","","652","657","Indoor localization has been an active research area for the last two decades. This emerged in the context of providing a mobile robot the capability to conduct navigation tasks in indoor environments. Although the sensing technologies and techniques proposed for indoor robot localization have proven to be reliable solutions, these cannot be adopted as a solution to people or object localization for indoor environments, particularly, due to their high computational cost and power requirements. In order to mitigate these issues, a low-power consumption sensing technology, based on the strength of WiFi signals, is being studied. Nevertheless, a concern when working with these signals is their vulnerability to interference. This paper exploits the use of machine learning is two different architectures for localization and present how a data filtering technique can alleviate interferences. A step into a fault tolerance approach is also given, presenting that the system can maintain certain reliability even losing some of its parts.","","Electronic:978-1-4673-8075-1; POD:978-1-4673-8076-8","10.1109/ICIT.2016.7474826","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474826","","Buildings;Estimation;Indoor environments;Machine learning algorithms;Robot sensing systems;Support vector machines;Wireless sensor networks","control engineering computing;fault tolerance;filtering theory;indoor navigation;interference (signal);learning (artificial intelligence);mobile robots;path planning;power consumption;wireless LAN;wireless sensor networks","WiFi signals;data filtering technique;fault tolerance;indoor environments;indoor robot localization;interference;low-power consumption sensing technology;machine learning;mobile robot;navigation tasks;sensor network architectures","","","","16","","","14-17 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach to recognize subject based sentiment values of reviews","N. M. De Mel; H. H. Hettiarachchi; W. P. D. Madusanka; G. L. Malaka; A. S. Perera; U. Kohomban","Department of Computer Science & Engineering, University of Moratuwa, Sri Lanka","2016 Moratuwa Engineering Research Conference (MERCon)","20160530","2016","","","6","11","Due to the increase in the number of people participating online on reviewing travel related entities such as hotels, cities and attractions, there is a rich corpus of textual information available online. However, to make a decision on a certain entity, one has to read many such reviews manually, which is inconvenient. To make sense of the reviews, the essential first step is to understand the semantics that lie therein. This paper discusses a system that uses machine learning based classifiers to label the entities found in text into semantic concepts defined in an ontology. A subject classifier with a precision of 0.785 and a sentiment classifier with a correlation coefficient of 0.9423 was developed providing sufficient accuracy for subject categorization and sentiment evaluation in the proposed system.","","Electronic:978-1-5090-0645-8; POD:978-1-5090-0646-5","10.1109/MERCon.2016.7480107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480107","Text classification;feature engineering;machine learning;sentiment analysis","Context;Ontologies;Poles and towers;Sentiment analysis;Support vector machines;Training;Training data","decision making;learning (artificial intelligence);ontologies (artificial intelligence);pattern classification;sentiment analysis","correlation coefficient;decision making;entity labeling;machine learning approach;machine learning based classifiers;ontology;semantic concepts;sentiment classifier;sentiment evaluation;sentiment values;subject categorization;subject classifier;subject recognition;textual information;travel related entity reviewing","","","","20","","","5-6 April 2016","","IEEE","IEEE Conference Publications"
"Classifier Performance in Primary Somatosensory Cortex Towards Implementation of a Reinforcement Learning Based Brain Machine Interface","D. McNiel; M. Bataineh; J. Choi; J. Hessburg; J. Francis","Dept. of Biomed. Eng., Univ. of Houston, Houston, TX, USA","2016 32nd Southern Biomedical Engineering Conference (SBEC)","20160428","2016","","","17","18","Increasingly accurate control of prosthetic limbs has been made possible by a series of advancements in brain machine interface (BMI) control theory. One promising control technique for future BMI applications is reinforcement learning (RL). RL based BMIs require a reinforcing signal to inform the controller whether or not a given movement was intended by the user. This signal has been shown to exist in cortical structures simultaneously used for BMI control. This work evaluates the ability of several common classifiers to detect impending reward delivery within primary somatosensory (S1) cortex during a grip force match to sample task performed by a nonhuman primate. The accuracy of these classifiers was further evaluated over a range of conditions to identify parameters that provide maximum classification accuracy. S1 cortex was found to provide highly accurate classification of the reinforcement signal across many classifiers and a wide variety of data input parameters. The classification accuracy in S1 cortex between rewarding and non-rewarding trials was apparent when the animal was expecting an impending delivery or an impending withholding of reward following trial completion. The high accuracy of classification in S1 cortex can be used to adapt an RL based BMI towards a user's intent. Real-time implementation of these classifiers in an RL based BMI could be used to adapt control of a prosthesis dynamically to match the intent of its user.","","Electronic:978-1-5090-2133-8; POD:978-1-5090-2134-5","10.1109/SBEC.2016.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458969","brain machine interface;classification;machine learning;neuroscience;reward","Biomedical engineering;Brain-computer interfaces;Decoding;Force;Learning (artificial intelligence);Neuroscience;Prosthetics","artificial limbs;brain-computer interfaces;learning (artificial intelligence);medical signal processing;signal classification;somatosensory phenomena","BMI applications;BMI control;RL-based BMI;S1 cortex;brain-machine interface control theory;classification accuracy;cortical structures;data input parameters;grip force;primary somatosensory cortex;prosthetic limbs;real-time implementation;reinforcement learning-based brain-machine interface","","","","9","","","11-13 March 2016","","IEEE","IEEE Conference Publications"
"Joint machine learning and human learning design with sequential active learning and outlier detection for linear regression problems","Xiaohua Li; Jian Zheng","Department of Electrical and Computer Engineering, State University of New York at Binghamton, 13902, USA","2016 Annual Conference on Information Science and Systems (CISS)","20160428","2016","","","407","411","In this paper, we propose a joint machine learning and human learning design approach to make the training data labeling task in linear regression problems more efficient and robust to noise, modeling mismatch, and human labeling errors. Considering a sequential active learning scheme which relies on human learning to enlarge training data set, we integrate it with sparse outlier detection algorithms to mitigate the inevitable human errors during training data labeling. First, we assume sparse human errors and formulate the outlier detection as a sparse optimization problem within the sequential active learning procedure. Then, for non-sparse human errors, with the IRT (item response theory) to model the distribution of human errors, appropriate data are selected to reconstruct a training data set with sparse human errors. Simulations are conducted to verify the desirable performance of the proposed approach.","","Electronic:978-1-4673-9457-4; POD:978-1-4673-9458-1","10.1109/CISS.2016.7460537","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460537","active learning;human learning;item response theory;linear regression;machine learning;training","Data models;Labeling;Linear regression;Machine learning algorithms;Optimization;Robustness;Training data","data handling;learning (artificial intelligence);optimisation;regression analysis","IRT;data labeling task;human labeling errors;human learning design;item response theory;joint machine learning;linear regression problems;nonsparse human errors;outlier detection;sequential active learning procedure;sequential active learning scheme;sparse optimization problem;training data labeling","","","","8","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Internet traffic classification using machine learning approach: Datasets validation issues","H. A. H. Ibrahim; O. R. Aqeel Al Zuobi; M. A. Al-Namari; G. MohamedAli; A. A. A. Abdalla","College of Computer at Al-Gunfudah, Umm Al-Qura University, Saudi Arabia","2016 Conference of Basic Sciences and Engineering Studies (SGCAC)","20160425","2016","","","158","166","Internet traffic classification is an area of current research interest. The failure of port and payload based classification motivates researchers to head towards a machine learning (ML) approach. However, training and testing dataset validation has not been formally addressed. This paper discusses the problem of ML dataset validation and highlights three training issues to be considered in ML classification. The first issue is when training and testing datasets collected from same or different network characteristics. The second issue considers training dataset classes whose real online traffic classes are not presented. The third issue is the geographic place where the network traffic is captured. Real Internet traffic datasets collected from a campus network are used to study the traffic features and classification accuracy for each validation training issue. The experimental results demonstrate that there are differences in some traffic features such as inter-arrival time when training and testing data were collected from different networks. Furthermore, the experiment of the second issue shows that the online classifier achieved the highest accuracy (92.22%) when the ML classifier was trained by dataset classes which have the same ratio of the real online traffic. For the geographic capturing level, the results indicate that there is a difference in the traffic statistical features when the capturing level is different.","","Electronic:978-1-5090-1812-3; POD:978-1-5090-1813-0","10.1109/SGCAC.2016.7458022","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458022","Internet traffic;Internet traffic classification;datasets validation issues;machine learning;online classification","Classification algorithms;Inspection;Internet;Ports (Computers);Testing;Training;World Wide Web","Internet;learning (artificial intelligence);pattern classification","Internet traffic classification;ML approach;dataset validation;machine learning approach;payload based classification;port based classification;traffic statistical features","","","","40","","","20-23 Feb. 2016","","IEEE","IEEE Conference Publications"
"Adaptive Threshold Non-Pareto Elimination: Re-thinking machine learning for system level design space exploration on FPGAs","P. Meng; A. Althoff; Q. Gautier; R. Kastner","Department of Computer Science and Engineering, University of California, San Diego, USA","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20160428","2016","","","918","923","One major bottleneck of the system level OpenCL-to-FPGA design tools is their extremely time consuming synthesis process (including place and route). The design space for a typical OpenCL application contains thousands of possible designs even when considering a small number of design space parameters. It costs months of compute time to synthesize all these possible designs into end-to-end FPGA implementations. Thus, the brute force design space exploration (DSE) is impractical for these design tools. Machine learning is one solution that identifies the valuable Pareto designs by sampling only a small portion of the entire design space. However, most of the existing machine learning frameworks focus on improving the design objective regression accuracy, which is not necessarily suitable for the FPGA DSE task. To address this issue, we propose a novel strategy - Adaptive Threshold Non-Pareto Elimination (ATNE). Instead of focusing on regression accuracy improvement, ATNE focuses on understanding and estimating the inaccuracy. ATNE provides a Pareto identification threshold that adapts to the estimated inaccuracy of the regressor. This adaptive threshold results in a more efficient DSE. For the same prediction quality, ATNE reduces the synthesis complexity by 1.6 - 2.89× (hundreds of synthesis hours) against the other state of the art frameworks for FPGA DSE. In addition, ATNE is capable of identifying the Pareto designs for certain difficult design spaces which the other existing frameworks are incapable of exploring effectively.","","Electronic:978-3-9815-3707-9; POD:978-1-4673-9228-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459439","","Algorithm design and analysis;Complexity theory;Field programmable gate arrays;Hardware;Linear programming;Radio frequency;Training","field programmable gate arrays;learning (artificial intelligence);logic design;regression analysis","ATNE;FPGA DSE task;Pareto identification threshold;adaptive threshold nonPareto elimination;brute force design space exploration;machine learning frameworks;regressor inaccuracy;system level OpenCL-to-FPGA design tools;system level design space exploration","","","","14","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Brain tumor segmentation approaches: Review, analysis and anticipated solutions in machine learning","A. Vidyarthi; N. Mittal","Department of Computer Engineering, Malaviya National Institute of Technology, Jaipur, India","2015 39th National Systems Conference (NSC)","20160613","2015","","","1","6","Brain tumor is one of the most rigorous diseases in the medical science. An effective and efficient analysis is always a key concern for the radiologists in the premature phase of tumor growth. At first sight of the imaging modality like in Magnetic Resonance (MR) imaging, the proper visualization of the tumor cells and its differentiation with its nearby soft tissues is somewhat difficult task. The reason for the above problem is the presence of the low illumination in imaging modalities. One of the solutions of such problem is deal by using machine learning based system diagnosis. In past various segmentation methods had been applied on brain MR imaging system to figure out the proper abnormality region from overall volume of the brain. In this paper a decade survey analysis is presented for all such approaches which are used in machine learning system for tumor segmentation. Further, the paper presents the limitations and advantages of all such approaches in machine learning based diagnosis. At last, the comparative segmentation results are discussed with certain clustering performance measures to analyse the effectiveness of each algorithm.","","Electronic:978-1-4673-6829-2; POD:978-1-4673-6830-8","10.1109/NATSYS.2015.7489133","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489133","Brain Tumor;Clustering Performance Measures;MR Image;Segmentation","Clustering algorithms;Image edge detection;Image segmentation;Imaging;Partitioning algorithms;Prediction algorithms;Tumors","biomedical MRI;brain;diseases;image segmentation;learning (artificial intelligence);medical image processing;tumours","brain MR imaging system;brain tumor segmentation;clustering performance measures;decade survey analysis;diseases;illumination;imaging modality;machine learning based system diagnosis;magnetic resonance imaging;medical science;soft tissue;tumor cells","","","","17","","","14-16 Dec. 2015","","IEEE","IEEE Conference Publications"
"Advanced Machine Learning Approach for Lithium-Ion Battery State Estimation in Electric Vehicles","X. Hu; S. E. Li; Y. Yang","State Key Laboratory of Mechanical Transmissions and the Department of Automotive Engineering, Chongqing University, Chongqing, China","IEEE Transactions on Transportation Electrification","20160615","2016","2","2","140","149","To fulfill reliable battery management in electric vehicles (EVs), an advanced State-of-Charge (SOC) estimator is developed via machine learning methodology. A novel genetic algorithm-based fuzzy C-means (FCM) clustering technique is first used to partition the training data sampled in the driving cycle-based test of a lithium-ion battery. The clustering result is applied to learn the topology and antecedent parameters of the model. Recursive least-squares algorithm is then employed to extract its consequent parameters. To ensure good accuracy and resilience, the backpropagation learning algorithm is finally adopted to simultaneously optimize both the antecedent and consequent parts. Experimental results verify that the proposed estimator exhibits sufficient accuracy and outperforms those built by conventional fuzzy modeling methods.","","","10.1109/TTE.2015.2512237","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365487","Battery Management;Battery management;Electric Vehicle;Energy Storage;Machine Learning;State Estimation;electric vehicle (EV);energy storage;machine learning;state estimation","Algorithm design and analysis;Batteries;Clustering algorithms;Electric vehicles;Energy storage;Machine learning;State estimation","","","","1","","38","","20151224","June 2016","","IEEE","IEEE Journals & Magazines"
"Real time cyber attack analysis on Hadoop ecosystem using machine learning algorithms","M. T. Khorshed; N. A. Sharma; A. V. Dutt; A. B. M. S. Ali; Y. Xiang","School of Information Technology, Deakin University, Australia","2015 2nd Asia-Pacific World Congress on Computer Science and Engineering (APWC on CSE)","20160523","2015","","","1","7","Big Data technologies are exciting cutting-edge technologies that generate, collect, store and analyse tremendous amount of data. Like any other IT revolution, Big Data technologies also have big challenges that are obstructing it to be adopted by wider community or perhaps impeding to extract value from Big Data with pace and accuracy it is promising. In this paper we first offer an alternative view of ""Big Data Cloud"" with the main aim to make this complex technology easy to understand for new researchers and identify gaps efficiently. In our lab experiment, we have successfully implemented cyber-attacks on Apache Hadoop's management interface ""Ambari"". On our thought about ""attackers only need one way in"", we have attacked the Apache Hadoop's management interface, successfully turned down all communication between Ambari and Hadoop's ecosystem and collected performance data from Ambari Virtual Machine (VM) and Big Data Cloud hypervisor. We have also detected these cyber-attacks with 94.0187% accurateness using modern machine learning algorithms. From the existing researchs, no one has ever attempted similar experimentation in detection of cyber-attacks on Hadoop using performance data.","","Electronic:978-1-5090-0713-4; POD:978-1-5090-0714-1","10.1109/APWCCSE.2015.7476223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476223","Ambari;Big Data;Classification;Cloud Computing;Cyber-attack;Hadoop;Internet of Things;Machine Lerning","Big data;Classification algorithms;Cloud computing;Ecosystems;Machine learning algorithms;Ports (Computers);Security","cloud computing;data handling;learning (artificial intelligence);security of data;virtual machines","Ambari virtual machine;Apache Hadoop management interface;Hadoop ecosystem;IT revolution;VM;big data cloud hypervisor;machine learning algorithm;real time cyber attack analysis","","","","31","","","2-4 Dec. 2015","","IEEE","IEEE Conference Publications"
"Gas Source Parameter Estimation Using Machine Learning in WSNs","S. Mahfouz; F. Mourad-Chehade; P. Honeine; J. Farah; H. Snoussi","Laboratoire de mod&#233;lisation et de s&#251;ret&#233; des syst&#232;mes, Institut Charles Delaunay, Universit&#233; de Technologie de Troyes, Troyes, France","IEEE Sensors Journal","20160616","2016","16","14","5795","5804","This paper introduces an original clusterized framework for the detection and estimation of the parameters of multiple gas sources in wireless sensor networks. The proposed method consists of defining a kernel-based detector that can detect gas releases within the network's clusters using concentration measures collected regularly from the network. Then, we define two kernel-based models that accurately estimate the gas release parameters, such as the sources locations and their release rates, using the collected concentrations.","1530-437X;1530437X","","10.1109/JSEN.2016.2569559","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470596","Gas diffusion;gas diffusion;machine learning;one-class classification;ridge regression;source parameter estimation","Area measurement;Detectors;Estimation;Explosions;Pollution measurement;Wireless sensor networks","gas sensors;learning (artificial intelligence);parameter estimation;wireless sensor networks","WSN;kernel-based detector;machine learning;multiple gas source parameter estimation;original clusterized framework;parameter detection;wireless sensor network","","","","18","","20160517","July15, 2016","","IEEE","IEEE Journals & Magazines"
"FASP: A machine learning approach to functional astrocyte phenotyping from time-lapse calcium imaging data","Y. Wang; G. Shi; D. J. Miller; Y. Wang; G. Broussard; Y. Wang; L. Tian; G. Yu","Bradley Department of Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, USA","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","351","354","We propose a machine learning approach to characterize the functional status of astrocytes, the most abundant cells in human brain, based on time-lapse Ca2+ imaging data. The interest in analyzing astrocyte Ca2+ dynamics is evoked by recent discoveries that astrocytes play proactive regulatory roles in neural information processing, and is enabled by recent technical advances in modern microscopy and ultrasensitive genetically encoded Ca2+ indicators. However, current analysis relies on eyeballing the time-lapse imaging data and manually drawing regions of interest, which not only limits the analysis throughput but also at risk to miss important information encoded in the big complex dynamic data. Thus, there is an increased demand to develop sophisticated tools to dissect Ca2+ signaling in astrocytes, which is challenging due to the complex nature of Ca2+ signaling and low signal to noise ratio. We develop Functional AStrocyte Phenotyping (FASP) to automatically detect functionally independent units (FIUs) and extract the corresponding characteristic curves in an integrated way. FASP is data-driven and probabilistically principled, flexibly accounts for complex patterns and accurately controls false discovery rates. We demonstrate the effectiveness of FASP on both synthetic and real data sets.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493281","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493281","astrocyte calcium dynamics;functional phenotyping;machine learning;time-lapse calcium imaging","Algorithm design and analysis;Calcium;Correlation;Imaging;Probabilistic logic;Standards;Testing","","","","","","13","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Horse stress analysis using biomechanical modelling and machine learning approach","H. S. AlZubi; W. Al-Nuaimy; I. S. Young","Department of Electrical Engineering and Electronics, University of Liverpool, Brownlow Hill, Liverpool L69 3GJ, UK","2016 13th International Multi-Conference on Systems, Signals & Devices (SSD)","20160519","2016","","","640","644","Horse transport is a common practice in the equestrian industry, especially with the expansion of this industry around the world. Research has proved that horse transport by road is responsible for high stress levels, which sometimes exceed stress levels caused by exercising during professional horse races. Stress symptoms are reflected in the physiological functions of horses leading to horses suffering from horses fatigue or the injury. The horses stand still in a small box during transport to ensure safety and avoid falls or injuries. The weight is held by the four limbs while the vehicle is moving and vibration forces keep interrupting the balance. This requires the horse to counter these forces in order to keep its balance which demands high energy consumption even for short trips. The horse blood circulation system tries to support the muscles with enough oxygen forcing the heart to beat at high rates. This paper suggests an analytical biomechanical model for the vibration forces to understand how these forces move through horse limbs. This model is proposed to associate vibration forces with high stress levels during transport. Such a direct relationship between vehicle vibration forces and high stress levels will lead to a low cost non-invasive early stress detection system without the need to measure any direct physiological response of the horse. This relationship will also shed light on the importance of optimised vehicle design to reduce vibrations.","","Electronic:978-1-5090-1291-6; POD:978-1-5090-1292-3; USB:978-1-5090-1290-9","10.1109/SSD.2016.7473755","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473755","Biomechanical model;Horse;Horse Transport;Stress","Acceleration;Biological system modeling;Mathematical model;Stress;Vehicles;Vibrations","biomechanics;learning (artificial intelligence);mechanoception;veterinary medicine;vibration control","biomechanical modelling;equestrian industry;horse blood circulation system;horse fatigue;horse stress analysis;horse transport;machine learning approach;professional horse races;stress detection system;stress symptom;vehicle design;vehicle vibration force","","","","11","","","21-24 March 2016","","IEEE","IEEE Conference Publications"
"Review on state of art data mining and machine learning techniques for intelligent Airport systems","C. M. Ariyawansa; A. C. Aponso","Department of Computing, Informatics Institute of Technology, University of Westminster, Colombo, Sri Lanka","2016 2nd International Conference on Information Management (ICIM)","20160526","2016","","","134","138","It is a generally accepted fact that the Airport is the focal point of the country which creates a lasting impression of its people. The challenge faced by airports today is the complexity of players and processes, and the inability of multiple systems to share and analyze data. In order to face this challenge, many airports have implemented isolated solutions. While these solutions may improve specific processes or functions they are not holistic enough. The airport ecosystem must become more `intelligent' to optimize its supply chain, share real-time information, predict certain outcomes and track, manage and locate all of its assets. So the need of the hour is to create a unified, integrated, resourceful and ready to use platform to make intelligent decisions and assist airports to reach its next level. The aim of this paper is to review selected data mining techniques that can be integrated in to such system. Entities such as airlines, airport retails sector and the airport itself is considered for this cause and the data mining techniques that can be applied to these entities to improve the current airport systems such as flight delay prediction, passenger profiling, segmentation, association rule mining are discussed to find better approaches for an intelligent airport system.","","Electronic:978-1-5090-1471-2; POD:978-1-5090-1472-9","10.1109/INFOMAN.2016.7477547","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477547","Data mining;Machine learning;association rule mining;prediction;profiling;segmentation","Airports;Atmospheric modeling;Classification algorithms;Rain;Support vector machines;Vegetation","airports;data mining;decision making;intelligent transportation systems;learning (artificial intelligence)","airport ecosystem;data mining technique;decision making;intelligent airport system;machine learning technique","","","","32","","","7-8 May 2016","","IEEE","IEEE Conference Publications"
"Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering","K. Chen; Q. Huo","University of Science and Technology of China, Hefei, China","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","5880","5884","We present a new approach to scalable training of deep learning machines by incremental block training with intra-block parallel optimization to leverage data parallelism and blockwise model-update filtering to stabilize learning process. By using an implementation on a distributed GPU cluster with an MPI-based HPC machine learning framework to coordinate parallel job scheduling and collective communication, we have trained successfully deep bidirectional long short-term memory (LSTM) recurrent neural networks (RNNs) and fully-connected feed-forward deep neural networks (DNNs) for large vocabulary continuous speech recognition on two benchmark tasks, namely 309-hour Switchboard-I task and 1,860-hour ""Switch-board+Fisher"" task. We achieve almost linear speedup up to 16 GPU cards on LSTM task and 64 GPU cards on DNN task, with either no degradation or improved recognition accuracy in comparison with that of running a traditional mini-batch based stochastic gradient descent training on a single GPU.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472805","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472805","Incremental block training;LVCSR;blockwise model-update filtering;deep learning;parallel optimization","Computational modeling;Data models;Graphics processing units;Machine learning;Optimization;Parallel processing;Training","feedforward neural nets;filtering theory;learning (artificial intelligence);optimisation;scheduling;speech recognition","DNN;LSTM;MPI-based HPC machine learning framework;RNN;blockwise model-update filtering;collective communication;deep bidirectional long short-term memory;distributed GPU cluster;fully-connected feedforward deep neural network;intrablock parallel optimization;leverage data parallelism;minibatch based stochastic gradient descent training;parallel job scheduling;recurrent neural network;scalable incremental block training;switchboard+Fisher task;switchboard-I task;time 1860 hour;time 309 hour;vocabulary continuous speech recognition","","1","","41","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Detection of cyberattacks in a water distribution system using machine learning techniques","P. Nader; P. Honeine; P. Beauseroy","Institut Charles Delaunay (CNRS), Universit&#233; de Technologie de Troyes, France","2016 Sixth International Conference on Digital Information Processing and Communications (ICDIPC)","20160519","2016","","","25","30","Cyberattacks threatening the industrial processes and the critical infrastructures have become more and more complex, sophisticated, and hard to detect. These cyberattacks may cause serious economic losses and may impact the health and safety of employees and citizens. Traditional Intrusion Detection Systems (IDS) cannot detect new types of cyberattacks not existing in their databases. Therefore, IDS need a complementary help to provide a maximum protection to industrial systems against cyberattacks. In this paper, we propose to use machine learning techniques, in particular one-class classification, in order to bring the necessary and complementary help to IDS in detecting cyberattacks and intrusions. One-class classification algorithms have been used in many data mining applications, where the available samples in the training dataset refer to a unique/single class.We propose a simple one-class classification approach based on a new novelty measure, namely the truncated Mahalanobis distance in the feature space. The tests are conducted on a real dataset from the primary water distribution system in France, and the proposed approach is compared with other well-known one-class approaches.","","CD-ROM:978-1-4673-7503-0; Electronic:978-1-4673-7504-7; POD:978-1-4673-7505-4","10.1109/ICDIPC.2016.7470786","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470786","Cyberattack detection;Mahalanobis distance;kernel methods;one-class classification","Classification algorithms;Computer crime;Covariance matrices;Kernel;Quadratic programming;Support vector machines;Training","data mining;learning (artificial intelligence);security of data;water supply","France;IDS;cyberattack detection;data mining application;economic loss;industrial process;intrusion detection system;machine learning technique;one-class classification algorithm;truncated Mahalanobis distance;water distribution system","","","","31","","","21-23 April 2016","","IEEE","IEEE Conference Publications"
"Querying database using a universal natural language interface based on machine learning","H. Bais; M. Machkour; L. Koutti","Team of Engineering of Information Systems, Information Systems and Vision Laboratory, Faculty of Sciences, Ibn Zohr University, Agadir, Morocco","2016 International Conference on Information Technology for Organizations Development (IT4OD)","20160526","2016","","","1","6","Extracting information from a database system becomes a primary obligation. More and more we are forced to recognize the importance of providing easy access to information stored in a database system. However existing tools that allow users to query database using database query languages such as SQL (Structured Query Language) are difficult for non-experts users. Wherefore asking questions to databases in natural language is a very simple method that can provide powerful improvements to the use of data stored in databases. This paper presents the Architecture and the implementation of a generic natural language interface based on machine learning approach for a relational database. The advantage of this interface is that it functions independently of the database domain and automatically improves through experience its knowledge base.","","Electronic:978-1-4673-7689-1; POD:978-1-4673-7690-7","10.1109/IT4OD.2016.7479304","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479304","Extended Context Free Grammar (ECFG);Intermediate XML Logical Query (IXLQ);Machine Learning;Natural Language Processing (NLP);Relational Databases","Database languages;Databases;Knowledge based systems;Natural languages;Semantics;Syntactics;XML","knowledge based systems;learning (artificial intelligence);natural language interfaces;query processing;relational databases","SQL;database query languages;database system;information extraction;knowledge base;machine learning;relational database;structured query language;universal natural language interface","","1","","22","","","March 30 2016-April 1 2016","","IEEE","IEEE Conference Publications"
"Combining Generative and Discriminative Representation Learning for Lung CT Analysis With Convolutional Restricted Boltzmann Machines","G. van Tulder; M. de Bruijne","Biomedical Imaging Group, Erasmus MC, Rotterdam, The Netherlands","IEEE Transactions on Medical Imaging","20160429","2016","35","5","1262","1272","The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.","0278-0062;02780062","","10.1109/TMI.2016.2526687","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401039","Deep learning;X-ray imaging and computed tomography;lung;machine learning;neural network;pattern recognition and classification;representation learning;restricted Boltzmann machine;segmentation","Computed tomography;Feature extraction;Learning systems;Lungs;Neural networks;Standards;Training data","Boltzmann machines;biological tissues;channel bank filters;computerised tomography;feature extraction;image classification;image filtering;image texture;lung;medical image processing;pneumodynamics","airway detection;convolutional restricted Boltzmann machines;discriminative representation learning;feature description;generative learning objective;generative representation learning;lung CT analysis;lung texture classification;lung tissue classification accuracy;standard predefined filter banks;tissue classification system;training data;unlabeled data representations","","4","","47","","20160208","May 2016","","IEEE","IEEE Journals & Magazines"
"Quality of Service timeseries forecasting for Web Services: A machine learning, Genetic Programming-based approach","Y. Syu; Y. Y. Fanjiang; J. Y. Kuo; J. L. Su","Department of Computer Science and Information Engineering, National Taipei University of Technology, Taiwan","2016 Annual Conference on Information Science and Systems (CISS)","20160428","2016","","","343","348","Today, many software systems and applications are consisted of various services on the Web (Cloud). When selecting services or performing a service operation, a critical criterion is Quality of Service (QoS). Because the actual value of some dynamic QoS attributes could vary with time, there must be an approach that can accurately forecast future QoS value. In this paper, we propose to use a machine learning technique, i.e., Genetic Programming (GP), for the problem. When performing QoS forecasting, the proposed approach employs GP to evolve out a predictor, and then uses it to obtain future QoS forecasts. To test and understand the forecasting performance (accuracy) of the proposed approach, in our experiments with a real-world QoS dataset, we compare our approach with other existing QoS forecasting methods, and then prove and discuss its outperformance.","","Electronic:978-1-4673-9457-4; POD:978-1-4673-9458-1","10.1109/CISS.2016.7460526","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460526","Genetic Programming;Machine Learning;QoS Forecasting","Biological cells;Forecasting;Genetic programming;Predictive models;Quality of service;Sociology;Statistics","Web services;genetic algorithms;learning (artificial intelligence);time series","Web services;genetic programming-based approach;machine learning technique;quality of service timeseries forecasting","","","","13","","","16-18 March 2016","","IEEE","IEEE Conference Publications"
"Integrating symbolic and statistical methods for testing intelligent systems: Applications to machine learning and computer vision","A. Ramanathan; L. L. Pullum; F. Hussain; D. Chakrabarty; S. K. Jha","Computational Science and Engineering Division, Oak Ridge National Laboratory, Tennessee, USA","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20160428","2016","","","786","791","Embedded intelligent systems ranging from tiny implantable biomedical devices to large swarms of autonomous unmanned aerial systems are becoming pervasive in our daily lives. While we depend on the flawless functioning of such intelligent systems, and often take their behavioral correctness and safety for granted, it is notoriously difficult to generate test cases that expose subtle errors in the implementations of machine learning algorithms. Hence, the validation of intelligent systems is usually achieved by studying their behavior on representative data sets, using methods such as cross-validation and bootstrapping. In this paper, we present a new testing methodology for studying the correctness of intelligent systems. Our approach uses symbolic decision procedures coupled with statistical hypothesis testing to validate machine learning algorithms. We show how we have employed our technique to successfully identify subtle bugs (such as bit flips) in implementations of the k-means algorithm. Such errors are not readily detected by standard validation methods such as randomized testing. We also use our algorithm to analyze the robustness of a human detection algorithm built using the OpenCV open-source computer vision library. We show that the human detection implementation can fail to detect humans in perturbed video frames even when the perturbations are so small that the corresponding frames look identical to the naked eye.","","Electronic:978-3-9815-3707-9; POD:978-1-4673-9228-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459413","","Algorithm design and analysis;Computer vision;Intelligent systems;Machine learning algorithms;Measurement;Probabilistic logic;Testing","computer vision;embedded systems;learning (artificial intelligence);object detection;pattern clustering;public domain software;software libraries;statistical testing;video signal processing","OpenCV open-source computer vision library;computer vision;embedded intelligent system testing;human detection algorithm;k-means algorithm;machine learning;perturbed video frames;statistical hypothesis testing;symbolic decision procedures","","","","31","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Soft Fall Detection Using Machine Learning in Wearable Devices","D. Genoud; V. Cuendet; J. Torrent","Inst. of Inf. Syst., Univ. of Appl. Sci. Western Switzerland, Sierre, Switzerland","2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)","20160523","2016","","","501","505","Wearable watches provide very useful linear acceleration information that can be use to detect falls. Howeverfalls not from a standing position are difficult to spot amongother normal activities. This paper describes methods, basedon pattern recognition using machine learning, to improve thedetection of ""soft falls"". The values of the linear accelerometersare combined in a robust vector that will be presented as inputto the algorithms. The performance of these different machinelearning algorithms is discussed and then, based on the bestscoring method, the size of the time window fed to the systemis studied. The best experiments lead to results showing morethan 0.9 AUC on a real dataset. In a second part, a prototypeimplementation on an Android platform using the best resultsobtained during the experiments is described.","1550-445X;1550445X","Electronic:978-1-5090-1858-1; POD:978-1-5090-1859-8","10.1109/AINA.2016.124","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474131","Android;Decision Tree Ensemble;Knime;Linear accelerometer;fall detection;machine learning;pattern recognition;smart watch;wearable","Acceleration;Accelerometers;Decision trees;Machine learning algorithms;Robustness;Sensors;Training","Android (operating system);accelerometers;learning (artificial intelligence);pattern recognition;vectors;wearable computers","Android platform;linear acceleration information;linear accelerometers;machine learning;pattern recognition;robust vector;soft fall detection;wearable devices;wearable watches","","","","26","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"A study of Chinese herbal properties based on machine learning","Z. Wang","School of Information Science and Engineering, Center of Intelligent Health Management System, Hangzhou Normal University Hangzhou, China","2015 10th International Conference on Information, Communications and Signal Processing (ICICS)","20160428","2015","","","1","5","A concept of Four Properties (SiQi) of Chinese herbs is the important part of traditional Chinese medicine theory. The Chinese clinical medicine is a process of dialectical theory of governance of Chinese medicine prescriptions based these four properties. The Chinese medicine prescription uses a ""Cold"" and ""Hot"" model to judge the properties of Chinese herbs, and also judge the properties of these composed recipes. It is both important and difficult in the Chinese medical practice to use the model, and therefore, it has become one of the hot issues to be addressed in the research of the modern Chinese clinical medicine. This paper studies the entire prescription as a whole. According to the property of every composing individual herb in the recipe and overall effects and by using several machine-learning methods a new system has demonstrated an optimization for the entire prescriptions. This system is designed and implemented as an online forecasting system to determine the overall properties for a modified traditional well-known prescription. It may help the doctor to decide the dose of each composing herb to automatically generate a well-tuned recipe as the final prescription to fully balance the four properties for his patients.","","Electronic:978-1-4673-7218-3; POD:978-1-4673-7219-0; USB:978-1-4673-7217-6","10.1109/ICICS.2015.7459914","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459914","Chinese medicine prescription;four properties;herbs property;machine learning","Artificial neural networks;Classification algorithms;Error analysis;Kernel;Machine learning algorithms;Predictive models;Support vector machines","learning (artificial intelligence);medical computing;optimisation","Chinese clinical medicine theory;Chinese herbal property;Chinese medicine prescriptions governance;cold model;dialectical theory;hot model;machine learning;online forecasting system;optimization","","","","12","","","2-4 Dec. 2015","","IEEE","IEEE Conference Publications"
"Probabilistic Error Models for machine learning kernels implemented on stochastic nanoscale fabrics","S. Zhang; N. R. Shanbhag","Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, USA","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20160428","2016","","","481","486","Presented in this paper are probabilistic error models for machine learning kernels implemented on low-SNR circuit fabrics where errors arise due to voltage overscaling (VOS), process variations, or defects. Four different variants of the additive error model are proposed that describe the error probability mass function (PMF): additive over Reals Error Model with independent Bernoulli RVs (REM-i), additive over Reals Error Model with joint Bernoulli random variables (RVs) (REM-j), additive over Galois field Error Model with independent Bernoulli RVs (GEM-i), and additive over Galois field Error Model with joint Bernoulli RVs (GEM-j). Analytical expressions for the error PMF is derived. Kernel level model validation is accomplished by comparing the Jensen-Shannon divergence D<sub>JS</sub> between the modeled PMF and the PMFs obtained via HDL simulations in a commercial 45nm CMOS process of MAC units used in a 2<sup>nd</sup> order polynomial support vector machine (SVM) to classify data from the UCI machine learning repository. Results indicate that at the MAC unit level, D<sub>JS</sub> for the GEM-j models are 1-to-2-orders-of-magnitude lower (better) than the REM models for VOS and process variation errors. However, when considering errors due to defects, D<sub>JS</sub> for REM-j is between 1-to-2-orders-of-magnitude lower than the others. Performance prediction of the SVM using these models indicate that when compared with Monte Carlo with HDL generated error statistics, probability of detection pdet estimated using GEM-j is within 3% for VOS error when the error rate p<sub>η</sub> ≤ 80%, and within 5% for process variation error when supply voltage Vdd is between 0.3V and 0.7V. In addition, p<sub>det</sub> using REM-j is within 2% for defect errors when the defect rate (the percentage of circuit nets subject to stuck-at-faults) p<sub>saf</sub> is between 10<sup>-3</sup> and 0.2.","","Electronic:978-3-9815-3707-9; POD:978-1-4673-9228-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459358","","Additives;Computational modeling;Error analysis;Fabrics;Integrated circuit modeling;Kernel;Semiconductor device modeling","error statistics;learning (artificial intelligence);power aware computing;power engineering computing;stochastic processes","GEM-i;GEM-j;PMF;REM-i;REM-j;VOS;additive error model;additive over Galois field Error Model with independent Bernoulli RV;error probability mass function;independent Bernoulli RV;low-SNR circuit fabrics;machine learning kernels;probabilistic error models;reals error model;stochastic nanoscale fabrics;voltage overscaling","","","","22","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Cardiotocography signals with artificial neural network and extreme learning machine","Z. Cömert; A. F. Kocamaz; S. Güngör","Department of Computer Engineering, Bitlis Eren University, Turkey","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","1493","1496","Cardiotocography (CTG) is a monitoring technique that is used routinely during pregnancy and labor to assess fetal well-being. CTG consists of two signals which are fetal heart rate (FHR) and uterine contraction (UC). Twenty-one features representing the characteristic of FHR have been used in this work. The features are obtained from a large dataset consisting of 2126 records in UCI Machine Learning Repository. The prominent features, such as baseline, the number of acceleration and deceleration patterns, and variability recommended by International Federation of Gynecology and Obstetrics (FIGO) have also taken into account during CTG analysis. The features were applied as the input to feedforward neural network (ANN) and Extreme Learning Machine (ELM) to classify FHR patterns in this study. FHR is recently divided into three classes as normal, suspicious and pathological. According to the results of this study, the accuracy of classification of ANN and ELM were obtained as 91.84% and 93.42%, respectively.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7496034","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496034","Cardiotocography;extreme learning machine;feedforward neural network;fetal heart rate classification","Artificial neural networks;Cardiography;Feature extraction;Fetal heart rate;Monitoring;Testing;Training","cardiology;feedforward neural nets;learning (artificial intelligence);medical signal detection;medical signal processing;signal classification","FHR patterns;Federation of Gynecology and Obstetrics;UCI machine learning repository;acceleration patterns;artificial neural network;cardiotocography signals classification;deceleration patterns;extreme learning machine;feedforward neural network;fetal heart rate;monitoring technique;uterine contraction","","","","17","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Modeling Managed Grassland Biomass Estimation by Using Multitemporal Remote Sensing Data—A Machine Learning Approach","I. Ali; F. Cawkwell; E. Dwyer; S. Green","Department of Geodesy and Geoinformation, Remote Sensing Research Group, Vienna University of Technology, Vienna, Austria (e-mail: iffi.math@gmail.com).","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2017","PP","99","1","11","More than 80% of agricultural land in Ireland is grassland, which is a major feed source for the pasture based dairy farming and livestock industry. Many studies have been undertaken globally to estimate grassland biomass by using satellite remote sensing data, but rarely in systems like Ireland's intensively managed, but small-scale pastures, where grass is grazed as well as harvested for winter fodder. Multiple linear regression (MLR), artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS) models were developed to estimate the grassland biomass (kg dry matter/ha/day) of two intensively managed grassland farms in Ireland. For the first test site (Moorepark) 12 years (2001–2012) and for second test site (Grange) 6 years (2001–2005, 2007) of in situ measurements (weekly measured biomass) were used for model development. Five vegetation indices plus two raw spectral bands (RED=red band, NIR=Near Infrared band) derived from an 8-day MODIS product (MOD09Q1) were used as an input for all three models. Model evaluation shows that the ANFIS (<formula><tex>$R_{{rm{Moorepark}}}^2 = ;0.85,;;{rm{RMS}}{{rm{E}}_{{rm{Moorepark}}}} = ;11.07$</tex></formula>; <formula><tex>$R_{{rm{Grange}}}^2 = ;0.76,;;{rm{RMS}}{{rm{E}}_{{rm{Grange}}}} = ;15.35$</tex></formula>) has produced improved estimation of biomass as compared to the ANN and MLR. The proposed methodology will help to better explore the future inflow of remote sensing data from spaceborne sensors for the retrieval of different biophysical parameters, and with the launch of new members of satellite families (ALOS-2, Radarsat-2, Sentinel, TerraSAR-X, TanDEM-X/L) the development of tools to process large volumes of image data will become increasingl- important.","1939-1404;19391404","","10.1109/JSTARS.2016.2561618","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482764","Biomass estimation;machine learning;managed grassland;remote sensing;time series","Agriculture;Biological system modeling;Biomass;Estimation;Monitoring;Remote sensing;Satellites","","","","","","","","20160601","","","IEEE","IEEE Early Access Articles"
"Reducing computational time of closed-loop weather monitoring: A Complex Event Processing and Machine Learning based approach","H. M. C. Chandrathilake; H. T. S. Hewawitharana; R. S. Jayawardana; A. D. D. Viduranga; H. M. N. Dilum Bandara; S. Marru; S. Perera","Dept. of Computer Science & Engineering, University of Moratuwa, Katubeda, Sri Lanka","2016 Moratuwa Engineering Research Conference (MERCon)","20160530","2016","","","78","83","Modern weather forecasting models are developed to maximize the accuracy of forecasts by running computationally intensive algorithms with vast volumes of data. Consequently, algorithms take a long time to execute, and it may adversely affect the timeliness of forecast. One solution to this problem is to run the complex weather forecasting models only on the potentially hazardous events, which are pre-identified by a lightweight data filtering algorithm. We propose a Complex Event Processing (CEP) and Machine Learning (ML) based weather monitoring framework using open source resources that can be extended and customized according to the users' requirements. The CEP engine continuously filters out the input weather data stream to identify potentially hazardous weather events, and then generates a rough boundary enclosing all the data points within the Areas of Interest (AOI). Filtered data points are then fed to the machine learner, where the rough boundary gets more refined by clustering it into a set of AOIs. Each cluster is then concurrently processed by complex weather algorithms of the WRF model. This reduces the computational time by ~75%, as resource heavy weather algorithms are executed using a small subset of data that corresponds to only the areas with potentially hazardous weather.","","Electronic:978-1-5090-0645-8; POD:978-1-5090-0646-5","10.1109/MERCon.2016.7480119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480119","complex event processing;machine learning;weather monitoring","Clustering algorithms;Engines;Filtering algorithms;Indexes;Monitoring;Weather forecasting","environmental monitoring (geophysics);geophysics computing;information filtering;learning (artificial intelligence);pattern clustering;public domain software;weather forecasting","AOI;CEP engine;ML-based weather monitoring framework;WRF model;area-of-interest;closed-loop weather monitoring;complex event processing;complex weather forecasting models;computational time reduction;data clustering;data point filtering;hazardous weather events;input weather data stream;lightweight data filtering algorithm;machine learning-based approach;open source resources;rough boundary","","","","13","","","5-6 April 2016","","IEEE","IEEE Conference Publications"
"Securing pervasive systems against adversarial machine learning","B. Lagesse; C. Burkard; J. Perez","Computing and Software Systems, University of Washington Bothell, Bothell, WA, USA","2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)","20160421","2016","","","1","4","Applications and middleware in pervasive systems frequently rely on machine learning to provide adaptivity and customization that results in a seamless user experience despite operating in a dynamic environment. Machine learning algorithms have been shown to be vulnerable to covert, strategic attacks through the manipulation of training data. Machine learning algorithms in pervasive systems frequently train on data that could be manipulated by a malicious 3rd party. In this paper, we present our ongoing work to develop a security mechanism that is designed to work in the dynamic environments of pervasive computing as opposed to traditional security mechanisms that are designed for static environments. Furthermore, we present our modular testing framework that will be used to rapidly compare our work with other security mechanisms, applications and adversarial models.","","Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7","10.1109/PERCOMW.2016.7457061","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457061","","Machine learning algorithms;Mathematical model;Pervasive computing;Security;Testing;Training;Training data","learning (artificial intelligence);middleware;security of data;ubiquitous computing","adversarial machine learning;covert attack;dynamic environments;middleware;modular testing framework;pervasive computing;pervasive system security;strategic attacks;training data manipulation","","","","13","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Online resource mapping for SDN network hypervisors using machine learning","C. Sieber; A. Basta; A. Blenk; W. Kellerer","Chair of Communication Networks, Department of Electrical and Computer Engineering, Technical University of Munich, Germany","2016 IEEE NetSoft Conference and Workshops (NetSoft)","20160704","2016","","","78","82","The visualization of Software-Defined Networks (SDN) allows multiple tenants to share the same physical infrastructure and to use their own SDN controllers. SDN virtualization is achieved through an SDN network hypervisor that operates between the tenants' controllers and the SDN infrastructure. In order to provide performance guarantees, resource mapping is required for both data plane as well as control plane for each virtual SDN network. In the context of SDN virtualization, the control plane resources include the network hypervisor, which needs to be assigned to guarantee the performance for each tenant. In previous work, the hypervisor resource mapping is based on offline benchmarks that measure the hypervisor resource consumption against the control plane work load, e.g., control plane message rate. These offline benchmarks vary across different hypervisor implementations, e.g., single or multi-threaded, and depend on the capabilities of the deployed hardware platform, e.g., the used CPU. We propose an online approach based on machine learning techniques to determine the mapping of hypervisor resources to the control workload at runtime. This concept is already successfully applied in the context of self-configuring networks. We propose three models to estimate hypervisor resources and compare them for two SDN hypervisor implementations, namely FlowVisor and OpenVirteX. We show through measurements on a real virtualized SDN infrastructure that resource mappings can be learned on runtime with insignificant error margins.","","Electronic:978-1-4673-9486-4; POD:978-1-4673-9487-1","10.1109/NETSOFT.2016.7502447","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502447","","Benchmark testing;Computational modeling;Hardware;Mathematical model;Monitoring;Process control;Virtual machine monitors","learning (artificial intelligence);resource allocation;software defined networking;virtualisation","FlowVisor;OpenVirteX;SDN hypervisor;SDN network hypervisors;hypervisor resources mapping;machine learning techniques;online resource mapping;software-defined networking;virtualized SDN infrastructure","","","","","","","6-10 June 2016","","IEEE","IEEE Conference Publications"
"A co-learning system for humans and machines","T. Ogiso; K. Yamauchi; N. Ishii; Y. Suzuki","Graduate School of Engineering Chubu University Japan","2015 7th International Conference of Soft Computing and Pattern Recognition (SoCPaR)","20160616","2015","","","363","368","Artificial intelligence systems are frequently used to solve various problems in our daily lives. However, these systems require problem-specific big data to facilitate their learning processes. Unfortunately, for unknown environments, there are no previous instances available for learning. To support such learning in unknown environments, we propose a novel hybrid learning system that facilitates collaborative learning between humans and artificial intelligence systems. In this study, we verified that the proposed system accelerated the both human and machine learning by employing a simplified color design task.","","Electronic:978-1-4673-9360-7; POD:978-1-4673-9361-4","10.1109/SOCPAR.2015.7492774","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492774","accelerated learning;collaborative learning;general regression neural network;human skill","Image color analysis;Psychology","Big Data;intelligent robots;learning (artificial intelligence);psychology","artificial intelligence systems;collaborative learning;color design task;human learning;human-machine colearning system;hybrid learning system;learning process;machine learning;problem-specific big data","","","","9","","","13-15 Nov. 2015","","IEEE","IEEE Conference Publications"
"A survey on evolutionary machine learning algorithms for multi-dimensional data classification","Swapna C; R. S. Shaji","Department of Computer Applications, Noorul Islam University, Tamil Nadu, India","2015 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","20160523","2015","","","781","785","The paper presents an analysis of various knowledge discovery estimation methods performed using different methods. Knowledge discovery approach addresses the problem of estimating outlier materials in the presence of abnormal information within the case that very little previous knowledge is offered concerning the nature of the information, the distortion, or the noise. The paper describes a detailed study on various techniques for outlier analysis and the issues associated with individual operations. In some data set an object may be a single point. The distribution of data in such objects is not taken into account, in traditional clustering algorithms. In this paper, divergence approaches are applied for comparing similarity between uncertain objects in continuous and discrete cases. To cluster uncertain objects, integration is done in density-based and partitioning clustering strategies. The proposed paper outlines basic concepts behind several developments, their assumptions and identifiably conditions needed by these approaches along with the algorithm characteristics. The proposed paper illustrates the comparison between approaches and strategies to estimate the novel outlier analysis algorithm for very large database analysis.","","Electronic:978-1-4673-9825-1; POD:978-1-4673-9826-8","10.1109/ICCICCT.2015.7475385","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475385","Abnormal Information;Big Data analysis;Computational approach;Distortion;Outlier","Algorithm design and analysis;Brain modeling;Clustering algorithms;Context;Diseases;Medical diagnostic imaging","data mining;evolutionary computation;learning (artificial intelligence);pattern classification;pattern clustering","clustering algorithms;continuous cases;density-based strategies;discrete cases;evolutionary machine learning algorithms;knowledge discovery estimation methods;multidimensional data classification;outlier analysis algorithm;outlier materials;partitioning clustering strategies","","","","19","","","18-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Integration of machine learning and human learning for training optimization in robust linear regression","X. Li; Y. Chen; K. Zeng","State University of New York at Binghamton, Department of ECE, Binghamton, NY 13902","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","2613","2617","In this paper machine learning and human learning are applied jointly to optimize the training of linear regression. Human learning is exploited to label extra training data so as to resolve problems such as insufficient training and over-fitting. Considering the inevitable human errors in labeling, two machine learning algorithms are developed which optimize the selection of the extra training data and detect human errors during linear regression. The first algorithm assumes sparse human errors and implements a sparse optimization within a sequential active learning procedure. The second algorithm deals with non-sparse human errors. By exploiting the IRT (item response theory) to model the distribution of human errors, it reconstructs the training data set so that the human labeling errors become sparse. Simulations are conducted to show that the two algorithms are effective in resolving the insufficient training and human labeling error problems.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472150","active learning;human learning;item response theory;linear regression;machine learning;training","Labeling;Linear regression;Machine learning algorithms;Optimization;Robustness;Training;Training data","learning (artificial intelligence);optimisation;regression analysis","IRT;human learning;item response theory;machine learning algorithms;robust linear regression;sequential active learning procedure;sparse optimization;training optimization","","1","","9","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Privacy preserving extreme learning machine classification model for distributed systems","F. Ö. Çatak; A. F. Mustaçoğlu; A. E. Topçu","T&#220;B&#304;TAK B&#304;LGEM, Kocaeli, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","313","316","Machine learning based classification methods are widely used to analyze large scale datasets in this age of big data. Extreme learning machine (ELM) classification algorithm is a relatively new method based on generalized single-layer feedforward network structure. Traditional ELM learning algorithm implicitly assumes complete access to whole data set. This is a major privacy concern in most of cases. Sharing of private data (i.e. medical records) is prevented because of security concerns. In this research, we proposed an efficient and secure privacy-preserving learning algorithm for ELM classification over data that is vertically partitioned among several parties. The new learning method preserves the privacy on numerical attributes, builds a classification model without sharing private data without disclosing the data of each party to others.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7495740","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495740","extreme learning machine;homomorphic encryption;privacy preserving data analysis;secure multi-party computation","Big data;Breast cancer;Data privacy;Ionosphere;Privacy;Sonar","data privacy;feedforward;learning (artificial intelligence)","ELM classification;ELM learning algorithm;distributed systems;extreme learning machine classification algorithm;generalized single-layer feedforward network structure;learning method;machine learning based classification methods;numerical attributes;privacy preserving extreme learning machine classification model;privacy-preserving learning algorithm;private data sharing","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Distributed forests for MapReduce-based machine learning","R. Wakayama; R. Murata; A. Kimura; T. Yamashita; Y. Yamauchi; H. Fujiyoshi","Chubu University, Japan","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","20160609","2015","","","276","280","This paper proposes a novel method for training random forests with big data on MapReduce clusters. Random forests are well suited for parallel distributed systems, since they are composed of multiple decision trees and every decision tree can be independently trained by ensemble learning methods. However, naive implementation of random forests on distributed systems easily overfits the training data, yielding poor classification performances. This is because each cluster node can have access to only a small fraction of the training data. The proposed method tackles this problem by introducing the following three steps. (1) ""Shared forests"" are built in advance on the master node and shared with all the cluster nodes. (2) With the help of transfer learning, the shared forests are adapted to the training data placed on each cluster node. (3) The adapted forests on every cluster node are returned to the master node, and irrelevant trees yielding poor classification performances are removed to form the final forests. Experimental results show that our proposed method for MapReduce clusters can quickly learn random forests without any sacrifice of classification performance.","","Electronic:978-1-4799-6100-9; POD:978-1-4799-6101-6; USB:978-1-4799-6099-6","10.1109/ACPR.2015.7486509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486509","","Computer architecture;Decision trees;Distributed databases;Learning systems;Training;Training data;Vegetation","Big Data;decision trees;learning (artificial intelligence);parallel programming","Big Data;MapReduce clusters;MapReduce-based machine learning;cluster nodes;decision trees;distributed forests;ensemble learning methods;master node;parallel distributed systems","","","","9","","","3-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"A machine learning based spectrum-sensing algorithm using sample covariance matrix","Haozhou Xue; Feifei Gao","","2015 10th International Conference on Communications and Networking in China (ChinaCom)","20160623","2015","","","476","480","In this paper, we propose a machine learning based spectrum sensing method using the sample covariance matrix of the received signal vector from multiple antennas. Before sensing, the cognitive radio (CR) will first apply the unsupervised learning algorithm (e.g., K-means Clustering) to discover primary user's (PU) transmission patterns. Then, the supervised learning algorithm (e.g., Support Vector Machine) is used to train CR to distinguish PU's status. These two learning phases are implemented using the feature vector that is formed by two parameters of the sample covariance matrix. One parameter is the ratio between the maximum eigenvalue and the minimum eigenvalue; the other is the ratio between the absolute sum of all matrix elements and absolute sum of the diagonal elements. The proposed method does not need any information about the signal, channel, and the noise power a priori. Simulations clearly demonstrate the effectiveness of the proposed method.","","Electronic:978-1-4799-8795-5; POD:978-1-4799-8796-2","10.1109/CHINACOM.2015.7497987","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497987","cognitive radio;machine learning;multi-antenna;spectrum sensing","Antennas;Clustering algorithms;Covariance matrices;Eigenvalues and eigenfunctions;Machine learning algorithms;Sensors;Support vector machines","","","","","","17","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Reducing Power Consumption in Data Center by Predicting Temperature Distribution and Air Conditioner Efficiency with Machine Learning","Y. Tarutani; K. Hashimoto; G. Hasegawa; Y. Nakamura; T. Tamura; K. Matsudax; M. Matsuoka","Cybermedia Center, Osaka Univ., Toyonaka, Japan","2016 IEEE International Conference on Cloud Engineering (IC2E)","20160602","2016","","","226","227","To reduce the power consumption in data centers, the coordinated control of the air conditioner and the servers is required. It takes tens of minutes for changes of operational parameters of air conditioners including outlet air temperature and volume to be reflected in the temperature distribution in the whole data center. So, the proactive control of the air conditioners is required according to the prediction temperature distribution corresponding to the load on the servers. In this paper, the temperature distribution and the power efficiency of air conditioner were predicted by using a machine-learning technique, and also we propose a method to follow-up proactive control of the air conditioner under the predicted optimum condition. Consequently, by the follow-up proactive control of the air conditioner and the load of servers, power consumption reduction of 30% at maximum was demonstrated.","","Electronic:978-1-5090-1961-8; POD:978-1-5090-1962-5","10.1109/IC2E.2016.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484193","data center;machine learning;power consumption","Atmospheric modeling;Data models;Power demand;Servers;Temperature control;Temperature distribution;Temperature sensors","air conditioning;computer centres;control engineering computing;learning (artificial intelligence);network servers;power consumption;power engineering computing;temperature distribution","air conditioner coordinated control;air conditioner efficiency;data center;machine learning;power consumption reduction;power efficiency;temperature distribution prediction","","","","4","","","4-8 April 2016","","IEEE","IEEE Conference Publications"
"Controlled automatic query expansion based on a new method arisen in machine learning for detection of semantic relationships between terms","N. Ksentini; M. Tmar; F. Gargouri","MIRACL Laboratory, University of Sfax, Tunis, Tunisia","2015 15th International Conference on Intelligent Systems Design and Applications (ISDA)","20160613","2015","","","134","139","With the proliferation of textual data on the web, efficient access to relevant information to meet the user's needs has become an important problem in the information retrieval tasks. This problem is specially due to the short queries submitted usually by users to an information retrieval system to describe their needs. These systems have to complete the user needs with related terms in order to disambiguate the user query and better meet the user's needs. This paper presents a new method to define semantic relationships between terms of the relevant returned documents for a given query in order to improve the description of the user's needs, by expanding automatically the original query with related terms, and to improve the search results. Some experiments have been performed on the CLEF 2014 collection to show the effectiveness of our method.","","Electronic:978-1-4673-8709-5; POD:978-1-4673-8710-1","10.1109/ISDA.2015.7489214","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489214","automatic query expansion;information retrieval;least square method;pseudo relevance feedback;semantic relationships","Indexes;Integrated circuits;Manuals;Nitrogen","learning (artificial intelligence);query processing","controlled automatic query expansion;information retrieval;machine learning;relevant returned document;semantic relationship detection;user query","","","","16","","","14-16 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Python Framework for Exhaustive Machine Learning Algorithms and Features Evaluations","F. Dubosson; S. Bromuri; M. Schumacher","","2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)","20160523","2016","","","987","993","Machine learning domain has grown quickly the last few years, in particular in the mobile eHealth domain. In the context of the DINAMO project, we aimed to detect hypoglycemia on Type 1 diabetes patients by using their ECG, recorded with a sport-like chest belt. In order to know if the data contain enough information for this classification task, we needed to apply and evaluate machine learning algorithms on several kinds of features. We have built a Python toolbox for this reason. It is built on top of the scikit-learn toolbox and it allows evaluating a defined set of machine learning algorithms on a defined set of features extractors, taking care of applying good machine learning techniques such as cross-validation or parameters grid-search. The resulting framework can be used as a first analysis toolbox to investigate the potential of the data. It can also be used to fine-tune parameters of machine learning algorithms or parameters of features extractors. In this paper we explain the motivation of such a framework, we present its structure and we show a case study presenting negative results that we could quickly spot using our toolbox.","1550-445X;1550445X","Electronic:978-1-5090-1858-1; POD:978-1-5090-1859-8","10.1109/AINA.2016.160","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474196","evaluation;features;framework;grid search;machine learning;python","Biomedical monitoring;Data mining;Diabetes;Electrocardiography;Feature extraction;Machine learning algorithms","diseases;electrocardiography;feature extraction;health care;learning (artificial intelligence);medical signal processing;mobile computing;signal classification","DINAMO project;ECG;Python framework;Python toolbox;classification task;cross-validation;exhaustive machine learning algorithms;features evaluations;features extractors;hypoglycemia;mobile ehealth;parameters grid-search;scikit-learn toolbox;type 1 diabetes patients","","2","","15","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning classification of complex vasculature structures from in-vivo bone marrow 3D data","R. A. Khorshed; C. L. Celso","Department of Life Sciences, Imperial College London, SW7 2AZ, UK","2016 IEEE 13th International Symposium on Biomedical Imaging (ISBI)","20160616","2016","","","1217","1220","Blood vessels inside the bone marrow (BM) play a vital role in the maintenance of hematopoietic stem cell (HSCs). Investigating the interaction of HSCs relative to vasculature has become the main headline for many recent studies. Advances in microscopy and image analysis using mouse models have allowed detection, identification and automated quantification of HSCs alongside their vascular niche. This resulted in new hypotheses concerning the activation state of HSCs adjacent to different blood vessel types (for example sinusoids vs. arterioles). Identifying the different types of BM vasculature has become critically important, however it still requires the use of complex immunostainings ex vivo or transgenic reporter mouse lines in vivo. To eliminate these requirements and increase the throughput of studies focusing on the HSC niche, we present a machine learning classification approach based on the Decision Tree Classifier to classify different regions of bone marrow vasculature into four distinct classes based on their discriminative features.","","Electronic:978-1-4799-2349-6; POD:978-1-4799-2351-9","10.1109/ISBI.2016.7493485","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493485","3D images;Decision Tree Classifier;bone marrow;classification;vasculature","Bifurcation;Biomedical imaging;Blood vessels;Bones;Decision trees;Three-dimensional displays;Training","","","","","","9","","","13-16 April 2016","","IEEE","IEEE Conference Publications"
"Machine learning for BCI: towards analysing cognition","K. R. Muller","Machine Learning Group,TU Berlin, Germany","2016 4th International Winter Conference on Brain-Computer Interface (BCI)","20160421","2016","","","1","2","This article discusses machine learning and BCI with a focus on analysing cognition, a topic that has been extensively covered by the author and co-workers in numerous papers and conference papers. Due to the review character of the presentation, a high overlap with the above-mentioned contributions is unavoidable. When analysing cognition, it is often useful to combine information from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012). In BCI recently multimodal fusion concepts have received great attention under the label hybrid BCI (Pfurtscheller et al., 2010, Müller-Putz et al. 2015, Dähne et al. 2015, Fazli et al. 2015) or as data analysis technique for extracting (non-) linear relations between data (see e.g. Biessmann et al., 2010, Biessmann et al., 2011, Fazli et al., 2009, 2011, 2012, Dähne et al., 2013, 2014a,b, 2015, Winkler et al. 2015). They are rooted in the modern machine learning and signal processing techniques that are now available for analysing EEG, for decoding mental states etc. (see Müller et al. 2008, Bünau et al. 2009, Tomioka and Müller, 2010, Blankertz et al., 2008, 2011, Lemm et al., 2011, Porbadnigk et al. 2015 for recent reviews and contributions to Machine Learning for BCI, see Samek et al. 2014 for a review on robust methods). Note that fusing information has also been a very common practice in the sciences and engineering (Waltz and Llinas, 1990). The talk will discuss a number of recent contributions from the BBCI group that have helped to broaden the spectrum of applicability for Brain Computer Interfaces and mental state monitoring in particular and for analysis of neuroimaging data in general. I will introduce a novel reliable method for estimating the Hurst exponent, a quantity that has recently become popular for describing network properties and is being used for diagnostic purposes (cf. Blythe et al. 2014). It is applied to estimate and analyse cognitive pr- perties in neurophysiological data from BCI experiments (Samek et al. 2016). Furthermore if time permits I will discuss a recent attractive application of BCI in the context of video coding (Scholler et al. 2012 and Acqualagna et al 2015).","","Electronic:978-1-4673-7842-0; POD:978-1-4673-7843-7","10.1109/IWW-BCI.2016.7457453","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457453","","Brain-computer interfaces;Cognition;Data analysis;Electroencephalography;Neuroimaging;Neuroscience;Robustness","brain-computer interfaces;data analysis;learning (artificial intelligence);medical signal processing","BCI;brain computer interfaces;cognition analysis;conference abstracts;data analysis technique;decoding mental states;machine learning;mental state monitoring;multimodal fusion concepts;neuroimaging data;signal processing techniques;video coding","","","","","","","22-24 Feb. 2016","","IEEE","IEEE Conference Publications"
"Prediction and diagnosis of diabetes mellitus — A machine learning approach","V. V. Vijayan; C. Anjali","Department of Computer Science Engineering, Mar Baselios college of Engineering and Technology, Trivandrum, India","2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS)","20160613","2015","","","122","127","Diabetes is a disease caused due of the expanded level of sugar fixation in the blood. Various computerized information systems were outlined utilizing diverse classifiers for anticipating and diagnosing diabetes. Selecting legitimate classifiers clearly expands the exactness and proficiency of the system. Here a decision support system is proposed that uses AdaBoost algorithm with Decision Stump as base classifier for classification. Additionally Support Vector Machine, Naive Bayes and Decision Tree are also implemented as base classifiers for AdaBoost algorithm for accuracy verification. The accuracy obtained for AdaBoost algorithm with decision stump as base classifier is 80.72% which is greater compared to that of Support Vector Machine, Naive Bayes and Decision Tree.","","CD-ROM:978-1-4673-6669-4; Electronic:978-1-4673-6670-0; POD:978-1-4673-6671-7","10.1109/RAICS.2015.7488400","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488400","AdaBoost Algorithm;Decision Stump;Decision Tree;Ensembling;Hybridization;Medical Data Mining;Naive Bayes;Pattern Recognition;Preprocessing;Support Vector Machine","Classification algorithms;Decision trees;Diabetes;Prediction algorithms;Sugar;Support vector machines;Training","Bayes methods;data mining;decision trees;diseases;learning (artificial intelligence);medical information systems;pattern classification;support vector machines","AdaBoost algorithm;base classifier;decision stump;decision support system;decision tree classifier;diabetes mellitus diagnosis;diabetes mellitus prediction;legitimate classifiers;machine learning approach;naive Bayes classifier;sugar fixation level;support vector machine classifier","","","","22","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"Smart city planning by estimating energy efficiency of buildings by extreme learning machine","Ö. F. Ertugrul; Y. Kaya","Dept. of Electrical and Electronics Engineering, Batman University, Turkey","2016 4th International Istanbul Smart Grid Congress and Fair (ICSG)","20160620","2016","","","1","5","Estimation of energy efficiency is one of the major issues in smart city planning. Although, there are some papers about estimation of energy efficiency of the buildings, there is still a requirement of an effective method that can be used in all climatic zones. Therefore, extreme learning method (ELM), which is a training method for single hidden layer neural network, was employed in the dataset that contains the properties of buildings such as shape, area and height and cooling and heating loads were calculated. Achieved results by ELM were compared with the results in the literature and the results obtained by some popular machine learning methods such as artificial neural network, linear regression, and etc. Obtained results by ELM found acceptable.","","Electronic:978-1-5090-0866-7; POD:978-1-5090-0867-4; USB:978-1-5090-0865-0","10.1109/SGCF.2016.7492420","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7492420","Energy Efficiency of Building;Extreme Learning Machine;Smart City","Buildings;Cooling;Energy efficiency;Heating;Learning systems;Mathematical model;Training","","","","","","23","","","20-21 April 2016","","IEEE","IEEE Conference Publications"
"Machine learning resistant strong PUF: Possible or a pipe dream?","A. Vijayakumar; V. C. Patil; C. B. Prado; S. Kundu","Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, USA","2016 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)","20160623","2016","","","19","24","Physically unclonable functions (PUFs) are emerging as hardware primitives for key-generation and light-weight authentication. Strong PUFs represent a variant of PUFs which respond to a user challenge with a response determined by its unique manufacturing process variations. Unfortunately many of the Strong PUFs have been shown to be vulnerable to model building attacks when an attacker has access to challenge and response pairs. In mounting a model building attack, typically machine learning is used to build a software model to forge the PUF. Researchers have long been interested in designing Strong PUFs that are resistant to model building attacks. However, with innovations in application of machine learning, nearly all Strong PUFs presented in the literature have been broken. In this paper, first we present results from a set of experiments designed to show that if certain randomness properties can be met, cascaded structure based Strong PUFs can indeed be made machine learning (ML) attack resistant against known ML attacks. Next we conduct machine learning experiments on an abstract PUF model using Support Vector Machines, Logistic Regression, Bagging, Boosting and Evolutionary techniques to establish criteria for machine learning resistant Strong PUF design. This paper does not suggest how to harvest the process variation, which remains within the purview of a circuit designer; rather it suggests what properties of the building blocks to aim for towards building a machine learning resistant Strong PUF - thus paving the path for a systematic design approach.","","Electronic:978-1-4673-8826-9; POD:978-1-4673-8827-6","10.1109/HST.2016.7495550","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495550","Machine Learning;Physically Unclonable Function;Security","Bagging;Boosting;Integrated circuit modeling;Logistics;Machine learning algorithms;Resistance;Support vector machines","cryptography;evolutionary computation;learning (artificial intelligence);message authentication;regression analysis;support vector machines","ML attack resistant;bagging;boosting;evolutionary techniques;key-generation authentication;light-weight authentication;logistic regression;machine learning;model building attack;physically unclonable functions;randomness properties;software model;strong PUF;support vector machines","","1","","21","","","3-5 May 2016","","IEEE","IEEE Conference Publications"
"Joint modulation format/bit-rate classification and signal-to-noise ratio estimation in multipath fading channels using deep machine learning","F. N. Khan; C. Lu; A. P. T. Lau","The Hong Kong Polytechnic University, Hong Kong","Electronics Letters","20160630","2016","52","14","1272","1274","A novel algorithm for simultaneous modulation format/bit-rate classification and non-data-aided (NDA) signal-to-noise ratio (SNR) estimation in multipath fading channels by applying deep machine learning-based pattern recognition on signals’ asynchronous delay-tap plots (ADTPs) is proposed. The results for three widely-used modulation formats at two different bit-rates demonstrate classification accuracy of 99.8%. In addition, NDA SNR estimation over a wide range of 0−30 dB is shown with mean error of 1 dB. The proposed method requires low-speed, asynchronous sampling of signal and is thus ideal for low-cost multiparameter estimation under real-world channel conditions.","0013-5194;00135194","","10.1049/el.2016.0876","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500204","","","fading channels;learning (artificial intelligence);mean square error methods;modulation;multipath channels;parameter estimation;signal classification;signal sampling","NDA SNR estimation;asynchronous delay-tap plots;classification accuracy;deep machine learning;joint modulation format-bit rate classification;low-cost multiparameter estimation;low-speed asynchronous signal sampling;multipath fading channels;nondata-aided signal-to-noise ratio estimation;pattern recognition;simultaneous modulation format-bit rate classification algorithm","","","","","","","7 7 2016","","IET","IET Journals & Magazines"
"File size estimation in JPEG XR standard using machine learning","E. Öztürk; A. Mesut","Bilgisayar M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, Trakya &#220;niversitesi, Edirne, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","205","208","Although JPEG XR was developed later than JPEG2000, it has not the ability to compress to a certain size unlike JPEG2000. In this study, a machine learning algorithm is proposed in order to bring this feature to JPEG XR method. The results show that, the file size estimation algorithm gives accurate results under the circumstances of giving specific range to compression ratio.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7495713","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495713","Data Compression;JPEG XR;Machine Learning","Discrete cosine transforms;Estimation;IEC Standards;ISO Standards;Image coding;Machine learning algorithms;Transform coding","image processing;learning (artificial intelligence)","JPEG XR standard;JPEG2000;file size estimation algorithm;machine learning algorithm","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Rebooting Computers as Learning Machines","E. P. DeBenedictis","Sandia National Laboratories","Computer","20160613","2016","49","6","84","87","Artificial neural networks could become the technological driver that replaces Moore's law, boosting computers' utlity through a process akin to automatic programming--although physics and computer architecture would also factor in.","0018-9162;00189162","","10.1109/MC.2016.156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490307","ANN;Moore's law;Rebooting Computing;artificial neural network;computing","Artificial neural networks;Cancer;Computational modeling;Computers;Logic gates;Moore's Law;Neural networks","automatic programming;computer architecture;learning (artificial intelligence);neural nets","Moore law;artificial neural networks;automatic programming;boosting computer;computer architecture;machine learning;physics architecture;rebooting computers;technological driver","","1","","5","","","June 2016","","IEEE","IEEE Journals & Magazines"
"A study on the optimization of the uplink period using machine learning in the future IoT network","J. S. Jang; Y. L. Kim; J. H. Park","Network Technology R&D Center, SK telecom, Seoul, Republic of Korea","2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)","20160421","2016","","","1","3","In this paper, we propose uplink period optimization algorithm for the future IoT(Internet of Things) network system. The proposed algorithm focuses on minimizing power consumption of device and mitigating the degradation of accuracy due to a variable uplink period. In particular, by using machine learning, the proposed algorithm selects the optimal period which is expected to have the biggest F(T), accuracy divided by power consumption. Simulation results show that the proposed algorithm outperforms the conventional algorithm.","","Electronic:978-1-5090-1941-0; POD:978-1-5090-1942-7","10.1109/PERCOMW.2016.7457131","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457131","IoT network;machine learning;power consumption;uplink period","Classification algorithms;Machine learning algorithms;Network servers;Power demand;Simulation;Training;Uplink","Internet of Things;learning (artificial intelligence);wide area networks","Internet of Things network system;IoT Network;accuracy degradation mitigation;machine learning;optimal period;power consumption minimization;uplink period optimization algorithm;variable uplink period","","","","5","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
"Equivalence checking between SLM and RTL using machine learning techniques","Jian Hu; Tun Li; Sikun Li","College of Computer, National University of Defense Technology, Changsha 410073, China","2016 17th International Symposium on Quality Electronic Design (ISQED)","20160526","2016","","","129","134","The growing complexity of modern digital design makes designers shift toward starting design exploration using high-level languages, and generating register transfer level (RTL) design from system level modeling (SLM) using high-level synthesis or manual transformation. Unfortunately, this translation process is very complex and may introduce bugs into the generated design. In this paper, we propose a novel SLM and RTL sequential equivalence checking method. The proposed method bases on Finite state machines with datapath (FSMD) equivalence checking method. The proposed method recognizes the corresponding path-pairs of FSMDs using machine learning (ML) technique from all the paths. And then it compares the corresponding path-pairs by symbolic simulation. The advantage of our method is that it separates the corresponding path-pairs from all the paths and avoids blind comparisons of path-pairs. Our method can deal with greatly different SLM and RTL designs and dramatically reduce the complexity of the path-based FSMD equivalence checking problem. The promising experimental results show the efficiency and effectiveness of the proposed method.","1948-3295;19483295","Electronic:978-1-5090-1213-8; POD:978-1-5090-1214-5","10.1109/ISQED.2016.7479188","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479188","Equivalence Checking;FSMD;Formal Verification;Machine Learning;System Level Modeling","Artificial intelligence","finite state machines;formal verification;high level languages;learning (artificial intelligence)","SLM-RTL sequential equivalence checking method;design exploration;digital design;finite state machines with datapath equivalence checking method;high-level languages;high-level synthesis;machine learning techniques;path-based FSMD equivalence checking problem;register transfer level;symbolic simulation;system level modeling;translation process","","","","24","","","15-16 March 2016","","IEEE","IEEE Conference Publications"
"Machine Learning in Adversarial Settings","P. McDaniel; N. Papernot; Z. B. Celik","Pennsylvania State University","IEEE Security & Privacy","20160525","2016","14","3","68","72","Recent advances in machine learning have led to innovative applications and services that use computational structures to reason about complex phenomenon. Over the past several years, the security and machine-learning communities have developed novel techniques for constructing adversarial samples--malicious inputs crafted to mislead (and therefore corrupt the integrity of) systems built on computationally learned models. The authors consider the underlying causes of adversarial samples and the future countermeasures that might mitigate them.","1540-7993;15407993","","10.1109/MSP.2016.51","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478523","adversarial machine learning;machine learning;machine-learning security;security;system security","Autonomous automobiles;Classification algorithms;Computer security;Data models;Electronic mail;Training;Training data","learning (artificial intelligence);security of data","adversarial samples;computational structures;machine learning;security","","2","","8","","","May-June 2016","","IEEE","IEEE Journals & Magazines"
"Mesh Convolutional Restricted Boltzmann Machines for Unsupervised Learning of Features With Structure Preservation on 3-D Meshes","Z. Han; Z. Liu; J. Han; C. M. Vong; S. Bu; C. L. P. Chen","Northwestern Polytechnical University, Xi'an 710072, China.","IEEE Transactions on Neural Networks and Learning Systems","","2016","PP","99","1","14","Discriminative features of 3-D meshes are significant to many 3-D shape analysis tasks. However, handcrafted descriptors and traditional unsupervised 3-D feature learning methods suffer from several significant weaknesses: 1) the extensive human intervention is involved; 2) the local and global structure information of 3-D meshes cannot be preserved, which is in fact an important source of discriminability; 3) the irregular vertex topology and arbitrary resolution of 3-D meshes do not allow the direct application of the popular deep learning models; 4) the orientation is ambiguous on the mesh surface; and 5) the effect of rigid and nonrigid transformations on 3-D meshes cannot be eliminated. As a remedy, we propose a deep learning model with a novel irregular model structure, called mesh convolutional restricted Boltzmann machines (MCRBMs). MCRBM aims to simultaneously learn structure-preserving local and global features from a novel raw representation, local function energy distribution. In addition, multiple MCRBMs can be stacked into a deeper model, called mesh convolutional deep belief networks (MCDBNs). MCDBN employs a novel local structure preserving convolution (LSPC) strategy to convolve the geometry and the local structure learned by the lower MCRBM to the upper MCRBM. LSPC facilitates resolving the challenging issue of the orientation ambiguity on the mesh surface in MCDBN. Experiments using the proposed MCRBM and MCDBN were conducted on three common aspects: global shape retrieval, partial shape retrieval, and shape correspondence. Results show that the features learned by the proposed methods outperform the other state-of-the-art 3-D shape features.","2162-237X;2162237X","","10.1109/TNNLS.2016.2582532","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502161","3-D mesh;Laplace-Beltrami operator;mesh convolutional deep belief networks (MCDBNs);mesh convolutional restricted Boltzmann machines (MCRBMs).","Convolution;Feature extraction;Machine learning;Shape;Solid modeling;Unsupervised learning","","","","1","","","","20160630","","","IEEE","IEEE Early Access Articles"
"Paddy-Rice Phenology Classification Based on Machine-Learning Methods Using Multitemporal Co-Polar X-Band SAR Images","Ç. Küçük; G. Taşkın; E. Erten","Informatics Institute, Istanbul Technical University, Istanbul, Turkey","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20160701","2016","9","6","2509","2519","Crop monitoring and phenology estimation based on the satellite systems have become an important research area due to high demand on crops. Satellites with synthetic aperture radar (SAR) sensor are highly preferred on such studies because of not only their day/night and all weather acquisition capabilities but also their ability to detect small morphological changes in monitored target, regarding the wavelength of signals. Besides, thanks to the high temporal resolution of new generation space-based sensors, it has been possible to monitor growth cycle of crops by classification algorithms. This paper focused on building a feasible phenology classification schema for paddy-rice using multitemporal co-polar TerraSAR-X images. Phenology classification was conducted with support vector machines (SVM) with linear and nonlinear kernel, k-nearest neighbors (kNN), and decision trees (DT). The key implementation challenges such as the number of classes, the identification of the boundaries of the classes, and the selection of textural and polarimetric features were deeply analyzed. According to all the evaluations conducted, the classification schema was finalized to be used for obtaining thematic maps for two independent rice-cultivated agricultural areas located in Spain and Turkey. The results of these experiments enable one to draw a conclusion about feasibility of machine learning (ML) algorithms in operational phenology monitoring.","1939-1404;19391404","","10.1109/JSTARS.2016.2547843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473819","Agriculture;TerraSAR-X;classification;support vector machines (SVM);synthetic aperture radar (SAR)","Agriculture;Backscatter;Feature extraction;Labeling;Monitoring;Support vector machines;Synthetic aperture radar","decision trees;geophysical image processing;image classification;image resolution;image texture;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing by radar;support vector machines;synthetic aperture radar;vegetation mapping","SAR sensor;Spain;Turkey;classification algorithm;crop monitoring;decision tree;k-nearest neighbor;linear kernel;machine-learning method;monitor growth cycle;morphological change;multitemporal copolar TerraSAR-X image classification;multitemporal copolar x-band SAR image;nonlinear kernel;operational phenology monitoring;paddy-rice;paddy-rice phenology classification;phenology classification schema;phenology estimation;polarimetric feature;rice-cultivated agricultural area;satellite system;signal wavelength;synthetic aperture radar sensor;textural feature;thematic map;weather acquisition","","2","","39","","20160519","June 2016","","IEEE","IEEE Journals & Magazines"
"CryptoML: Secure outsourcing of big data machine learning applications","A. Mirhoseini; A. R. Sadeghi; F. Koushanfar","","2016 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)","20160623","2016","","","149","154","We present CryptoML, the first practical framework for provably secure and efficient delegation of a wide range of contemporary matrix-based machine learning (ML) applications on massive datasets. In CryptoML a delegating client with memory and computational resource constraints wishes to assign the storage and ML-related computations to the cloud servers, while preserving the privacy of its data. We first suggest the dominant components of delegation performance cost, and create a matrix sketching technique that aims at minimizing the cost by data pre-processing. We then propose a novel interactive delegation protocol based on the provably secure Shamir's secret sharing. The protocol is customized for our new sketching technique to maximize the client's resource efficiency. CryptoML shows a new trade-off between the efficiency of secure delegation and the accuracy of the ML task. Proof of concept evaluations corroborate applicability of CryptoML to datasets with billions of non-zero records.","","Electronic:978-1-4673-8826-9; POD:978-1-4673-8827-6","10.1109/HST.2016.7495574","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495574","","Algorithm design and analysis;Approximation algorithms;Cryptography;Decision support systems;Protocols;Sparse matrices","Big Data;cryptographic protocols;data privacy;learning (artificial intelligence);matrix algebra;public key cryptography","Big Data machine learning;CryptoML;ML-related computations;Shamir secret sharing;cloud servers;computational resource constraints;contemporary matrix-based ML;contemporary matrix-based machine learning;data pre-processing;data privacy;interactive delegation protocol;matrix sketching technique;minimisation;secure outsourcing","","","","23","","","3-5 May 2016","","IEEE","IEEE Conference Publications"
"Performance analysis of machine learning techniques in intrusion detection","Ç. Kaya; O. Yıldız; S. Ay","Bilgisayar M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, Kara Harp Okulu, Ankara, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","1473","1476","With computer and Internet to be an indispensable part of our daily lives, the number of Web applications on the Internet has increased rapidly. With the increasing number of Web applications, attacks on the disclosure of data on the internet and the number of varieties has increased. Made over the Web attacks and to detect unauthorized access requests, intrusion detection systems have been used successfully. In this study, In order to develop a more efficient STS, machine learning techniques, Bayesian networks, support vector machines, neural networks, k nearest neighbor algorithm and decision trees examined the success of the STS, the success and process time of the classifier according to the types of attacks have been analyzed. Kddcup99 data sets were used in experimental studies.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7496029","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496029","Classification;Intrusion Detection System;KDDCup99 Dataset;Machine Learning","Bayes methods;Computers;Internet;Intrusion detection;Neural networks;Principal component analysis;Probes","Bayes methods;decision trees;learning (artificial intelligence);neural nets;pattern classification;security of data;support vector machines","Bayesian networks;Kddcup99 data sets;STS;decision trees;intrusion detection;k nearest neighbor algorithm;machine learning techniques;neural networks;support vector machines","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Hand Body Language Gesture Recognition Based on Signals From Specialized Glove and Machine Learning Algorithms","P. Pławiak; T. Sośnicki; M. Niedźwiecki; Z. Tabor; K. Rzecki","Faculty of Physics, Mathematics, and Computer Science, Institute of Telecomputing, Cracow University of Technology, Krakow, Poland","IEEE Transactions on Industrial Informatics","20160602","2016","12","3","1104","1113","The man-machine interface (MMI) is one of the most exciting areas of contemporary research. To make the MMI as convenient for a human as possible, it is desirable that efficient algorithms for recognizing body language are developed. This paper presents a system for quick and effective recognition of gestures of hand body language, based on data from a specialized glove equipped with ten sensors. In the experiment, 10 people performed 22 hand body language gestures. Each of the 22 gestures was executed 10 times. Collected data were preprocessed in multiple ways and three machine learning algorithms were designed based on classifiers (probabilistic neural network, support vector machine, and k-nearest neighbors algorithm) trained and tested by a tenfold cross-validation technique. The best designed classifiers gained effectiveness of gesture recognition at κ = 98.24% with a very short time of testing, below 1 ms. The experiments confirm that efficient and quick recognition of hand body language is possible.","1551-3203;15513203","","10.1109/TII.2016.2550528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448427","Classification;Data analysis;Gesture recognition;Man-machine interface;Neural network;Pattern Recognition;Principal component analysis;Support vector machine;data analysis;gesture recognition;k-nearest neighbors (kNN) algorithm;knearest neighbors algorithm;man–machine interface (MMI);neural network;pattern recognition;principal component analysis (PCA);support vector machine (SVM)","Feature extraction;Gesture recognition;Informatics;Principal component analysis;Sensors;Thumb","data gloves;gesture recognition;learning (artificial intelligence)","MMI;cross-validation technique;hand body language gesture recognition;k-nearest neighbors algorithm;machine learning algorithms;man-machine interface;probabilistic neural network;specialized glove;support vector machine","","","","24","","20160406","June 2016","","IEEE","IEEE Journals & Magazines"
"The classification of breast cancer with Machine Learning Techniques","N. Kolay; P. Erdoğmuş","Bilgisayar M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, D&#252;zce &#220;niversitesi, T&#252;rkiye","2016 Electric Electronics, Computer Science, Biomedical Engineerings' Meeting (EBBT)","20160602","2016","","","1","4","In this study, it is aimed to classify breast cancer data attained from UCI(University of California-Irvine), Machine Learning Laboratory with some Machine Learning Techniques. With this aim, clustering performance of some distance measures in Matlab© has been compared, using breast cancer data. Later without using any pre-processing, some of the machine learning techniques are used for the clustering breast cancer data, using WEKA data mining software©. As a result, it has been seen that distance measures effects the clustering performance nearly 12 percentage and the success of the classification varies from %45 to %79, according to the methods.","","Electronic:978-1-5090-0876-6; POD:978-1-5090-0877-3","10.1109/EBBT.2016.7483683","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483683","Breast Cancer;Fuzzy C-means;K-means clustering;Machine Learning;Support Vector Machines","Breast cancer;Computers;Conferences;Expert systems;MATLAB;Support vector machines","cancer;data mining;learning (artificial intelligence);medical information systems;pattern classification;pattern clustering;support vector machines","Matlab;UCI;University-of-California-Irvine;WEKA data mining software;breast cancer data classification;breast cancer data clustering;clustering performance;distance measures;machine learning laboratory","","","","","","","26-27 April 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for computationally and energy efficient speech enhancement in binaural hearing aids","D. Ayllón; R. Gil-Pita; M. Rosa-Zurera","R&D Department, Fonetic, Spain","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","6515","6519","A binaural speech enhancement algorithm that combines superdirective beamforming with time-frequency (TF) masking is proposed. Supervised machine learning is used to design a speech/noise classifier that estimates the ideal binary mask (IBM), which is further softened to reduce musical noise. The method is energy-efficient in two ways: the computational complexity is limited and the wireless data transmission optimized. The experimental work demonstrates the ability of the method to increase the intelligibility of speech corrupted by different types of noise in low SNR scenarios.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472932","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472932","Binaural hearing aids;Machine learning;Speech enhancement;Time-frequency masking","Array signal processing;Computational efficiency;Hearing aids;Speech;Speech enhancement;Time-frequency analysis;Wireless communication","array signal processing;computational complexity;handicapped aids;hearing aids;learning (artificial intelligence);radiocommunication;signal classification;signal denoising;speech enhancement;time-frequency analysis","IBM estimation;TF masking;binaural hearing aids;computational complexity;energy efficient speech enhancement;ideal binary mask estimation;low SNR scenarios;musical noise reduction;speech intelligibility;speech-noise classifier design;superdirective beamforming;supervised machine learning approach;time-frequency masking;wireless data transmission","","","","16","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach for predicting bumps on road","M. Ghadge; D. Pandey; D. Kalbande","Sardar Patel Institute Technology, Mumbai, India 400058","2015 International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","20160421","2015","","","481","485","In today's days, due to increase in number of vehicles the probability of accidents are also increasing. The user should be aware of the road circumstances for safety purpose. Several methods requires installing dedicated hardware in vehicle which are expensive. so we have designed a Smart-phone based method which uses a Accelerometer and GPS sensors to analyze the road conditions. The designed system is called as Bumps Detection System(BDS) which uses Accelerometer for pothole detection and GPS for plotting the location of potholes on Google Map. Drivers will be informed in advance about count of potholes on road. we have assumed some threshold values on z-axis(Experimentally Derived)while designing the system. To justify these threshold values we have used a machine learning approach. The k means clustering algorithm is applied on the training data to build a model. Random forest classifier is used to evaluate this model on the test data for better prediction.","","Electronic:978-1-4673-9223-5; POD:978-1-4673-9224-2; USB:978-1-4673-9222-8","10.1109/ICATCCT.2015.7456932","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456932","Accelerometer;GPS;Random forest classifier;k-Means clustering","Accelerometers;Global Positioning System;Machine learning algorithms;Roads;Sensors;Smart phones;Vehicles","Global Positioning System;accelerometers;accident prevention;cartography;graphical user interfaces;learning (artificial intelligence);pattern classification;pattern clustering;road accidents;road safety;smart phones;traffic engineering computing","BDS;GPS sensor;Google Map;accelerometer;accident probability;bump detection system;k-means clustering algorithm;machine learning approach;pothole detection;pothole location plotting;random forest classifier;road bump prediction;road circumstances;road condition analysis;road safety;smart-phone based method;threshold values;z-axis","","","","9","","","29-31 Oct. 2015","","IEEE","IEEE Conference Publications"
"Modulation Format Identification in Coherent Receivers Using Deep Machine Learning","F. N. Khan; K. Zhong; W. H. Al-Arashi; C. Yu; C. Lu; A. P. T. Lau","Photonics Research Centre, The Hong Kong Polytechnic University, Hong Kong","IEEE Photonics Technology Letters","20160630","2016","28","17","1886","1889","We propose a novel technique for modulation format identification (MFI) in digital coherent receivers by applying deep neural network (DNN) based pattern recognition on signals' amplitude histograms obtained after constant modulus algorithm (CMA) equalization. Experimental results for three commonly-used modulation formats demonstrate MFI with an accuracy of 100% over a wide optical signal-to-noise ratio (OSNR) range. The effects of fiber nonlinearity on the performance of MFI technique are also investigated. The proposed technique is non-data-aided (NDA) and avoids any additional hardware on top of standard digital coherent receiver. Therefore, it is ideal for simple and cost-effective MFI in future heterogeneous optical networks.","1041-1135;10411135","","10.1109/LPT.2016.2574800","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482803","Modulation format identification;coherent detection;deep machine learning","Feature extraction;Histograms;Modulation;Optical attenuators;Optical noise;Receivers;Signal to noise ratio","learning (artificial intelligence);neural nets;optical fibre networks;optical modulation;optical receivers;pattern recognition;physics computing","CMA;DNN;MFI technique;NDA;OSNR;constant modulus algorithm equalization;deep machine learning;deep neural network based pattern recognition;fiber nonlinearity;heterogeneous optical networks;modulation format identification;nondata-aided technique;optical signal-to-noise ratio;signal amplitude histograms;standard digital coherent receiver","","1","","12","","20160601","Sept.1, 1 2016","","IEEE","IEEE Journals & Magazines"
"Machine learning approach for sensors validation and clustering","A. M. T. Nasser; V. P. Pawar","School of computational science, SRTM University, Nanded, Maharashtra, India","2015 International Conference on Emerging Research in Electronics, Computer Science and Technology (ICERECT)","20160627","2015","","","370","375","Wireless sensor network (WSN) is very important these days. It is used frequently in many applications in the area such as health care, security, home and military. As the sensor nodes in the domain may change over the time due to number of factors such as environment condition, lifetime of the battery (low battery power) and coverage (area of interest), then a sensor network needs periodically to check the validation of sensor in the domain. So, as needed of the sensors validations in the domain have been shown in many WSN schemes. In this paper, the validation sensors in the domain using spectral clustering technique have been proposed that is detecting a bad sensor and deleting it from the domain. Sensors have been indexed by their location using simple model of spectral clustering. Results obtained from simulation indicate that our approach is enhanced network performance in terms of detecting the bad sensor location and replace it that is improved our wireless sensor network.","","Electronic:978-1-4673-9563-2; POD:978-1-4673-9564-9","10.1109/ERECT.2015.7499043","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7499043","Distributed sensor networks (DSN);Eigenvalues;Eigenvectors;Spectral Clustering (SC);Step function;Wireless sensor network","Clustering algorithms;Laplace equations;Peer-to-peer computing;Sensors;Signal processing algorithms;Symmetric matrices;Wireless sensor networks","learning (artificial intelligence);pattern clustering;sensor placement;wireless sensor networks","WSN scheme;bad sensor location detection;machine learning approach;sensor clustering;sensor validation;spectral clustering technique;wireless sensor network","","","","9","","","17-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Dimension Reduction With Extreme Learning Machine","L. L. C. Kasun; Y. Yang; G. B. Huang; Z. Zhang","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","20160624","2016","25","8","3906","3918","Data may often contain noise or irrelevant information, which negatively affect the generalization capability of machine learning algorithms. The objective of dimension reduction algorithms, such as principal component analysis (PCA), non-negative matrix factorization (NMF), random projection (RP), and auto-encoder (AE), is to reduce the noise or irrelevant information of the data. The features of PCA (eigenvectors) and linear AE are not able to represent data as parts (e.g. nose in a face image). On the other hand, NMF and non-linear AE are maimed by slow learning speed and RP only represents a subspace of original data. This paper introduces a dimension reduction framework which to some extend represents data as parts, has fast learning speed, and learns the between-class scatter subspace. To this end, this paper investigates a linear and non-linear dimension reduction framework referred to as extreme learning machine AE (ELM-AE) and sparse ELM-AE (SELM-AE). In contrast to tied weight AE, the hidden neurons in ELM-AE and SELM-AE need not be tuned, and their parameters (e.g, input weights in additive neurons) are initialized using orthogonal and sparse random weights, respectively. Experimental results on USPS handwritten digit recognition data set, CIFAR-10 object recognition, and NORB object recognition data set show the efficacy of linear and non-linear ELM-AE and SELM-AE in terms of discriminative capability, sparsity, training time, and normalized mean square error.","1057-7149;10577149","","10.1109/TIP.2016.2570569","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7471467","Dimension reduction;Extreme Learning Machine (ELM);Extreme learning machine (ELM);Non-negative Matrix Factorization (NMF);Principal Component Analysis (PCA);auto-encoder (AE);dimension reduction;non-negative matrix factorization (NMF);principal component analysis (PCA);random projection (RP)","Machine learning;Machine learning algorithms;Mathematical model;Principal component analysis;Support vector machines","","","","","","55","","20160518","Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Credit modelling using hybrid machine learning technique","S. Dahiya; S. S. Handa; N. P. Singh","Deptt. of Computer Science & Engg., Manav Rachna International University (MRIU) Faridabad, India, And Scientist, IASRI, New Delhi","2015 International Conference on Soft Computing Techniques and Implementations (ICSCTI)","20160613","2015","","","103","106","Credit evaluation models are the important tools used by banks for the evaluation of loan customers as good or bad. These models are developed as a part of data mining projects using mainly the Classification and Clustering tasks. Their accuracy plays a very significant role as they are the backbones behind the important decisions taken by banks. The accuracy can be improved by using many factors, some of these are the use of good machine learning techniques, balanced input data, and using hybrid techniques in model development. The machine learning and statistical techniques can be combined in various ways for creating the effective hybrid models. In this paper the input data has been balanced to avoid biased model training towards the larger class. Machine learning techniques which have been proved successful in many experiments on financial data are used for this study. The machine learning techniques used are: Naïve Bayes, MLP, RBF, Logistic Regression and C4.5. First single models have been developed using these machine learning techniques and the one with highest accuracy has been found. Then this model was hybridized with others for improving the classification accuracy. The accuracy of all these models was tested on a separate test set that has not been shown to the model while training. A bench marked credit dataset has been utilized for conducting the experiments. The results of the single and hybrid models shows that the MLP outperformed all other individual models while the hybrid model developed by combining the MLP with MLP gave the best results.","","CD-ROM:978-1-4673-6790-5; Electronic:978-1-4673-6792-9; POD:978-1-4673-6793-6","10.1109/ICSCTI.2015.7489612","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489612","Balanced data;Credit Evaluation;Hybrid Models;Machine Learning","Data mining;Data models;Decision trees;Logistics;Predictive models;Radial basis function networks;Training","Bayes methods;bank data processing;credit transactions;data mining;decision trees;learning (artificial intelligence);multilayer perceptrons;pattern classification;pattern clustering;radial basis function networks;regression analysis","C4.5 decision tree algorithm;MLP;RBF;banks;classification tasks;clustering tasks;credit evaluation modelling;data mining projects;financial data;hybrid machine learning technique;loan customer evaluation;logistic regression;multilayer perceptron;naïve Bayes;radial basis function;statistical techniques","","","","12","","","8-10 Oct. 2015","","IEEE","IEEE Conference Publications"
"Towards Bridging the Gap between Machine Learning Researchers and Practitioners","H. Assem; D. O'Sullivan","Sch. of Comput. Sci. & Stat., Trinity Coll. Dublin, Dublin, Ireland","2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)","20160505","2015","","","702","708","As data keeps growing, Big Data starts to be everywhere, and there is almost an urgent need to make sense of this data. This is why Machine Learning has become crucial as it aids in improving business, decision making and it has the potential to provide solutions for a wide range of problems in computer science and other fields. Machine Learning (a.k.a. Data Mining or Predictive Analytics) algorithms can learn how to perform certain tasks by generalizing from the out of sample examples. This is a totally different paradigm than traditional programming language approaches based on writing programs that process data to produce an output. However, choosing a suitable machine learning algorithm for a particular application requires substantial amount of effort that is even hard to undertake even with text books. In order to reduce the effort, this paper introduces a recommender system that will aid machine learning researchers and practitioners to choose the optimum machine learning model to use. The system is based on an approach that is introduced in the paper called TCDC which stands for Train, Compare, Decide, and Change.","","Electronic:978-1-5090-1893-2; POD:978-1-5090-1894-9","10.1109/SmartCity.2015.151","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463805","Classification Models;Machine Learning;Predictive Modelling;Regression Models;Supervised Learning","Adaptation models;Computational modeling;Data models;Measurement;Predictive models;Recommender systems;Supervised learning","Big Data;data mining;learning (artificial intelligence);recommender systems","Big Data;TCDC;computer science;data mining;decision making;machine learning;predictive analytics;recommender system","","1","","24","","","19-21 Dec. 2015","","IEEE","IEEE Conference Publications"
"Kemy: An AQM generator based on machine learning","XinAn Lin; Dong Zhang","College of Mathematics and Computer Science, Fuzhou University, China","2015 10th International Conference on Communications and Networking in China (ChinaCom)","20160623","2015","","","556","561","With the explosion of multimedia applications, the network QoS is facing a set of challenges especially in congestion control. Active queue management(AQM), which plays an important role in network congestion control, has been proved necessary for decades. Recently, as the widespread bufferbloat being exposed, AQM has been paid more and more attention. However, traditional manually designed AQMs still exist some problems especially in parameter-tuning and scenarios adaption. Instead of designing a perfect AQM for all scenarios, which is nearly impossible, we try to make the computer generate an AQM for the scenario specified by users. We've developed a program called Kemy based on off-line machine learning technologies. The Kemy-generated AQM is evaluated in various scenarios and achieves the goals of solving bufferbloat problem. Compared to some representative human-designed AQMs, Kemy-generated AQM performs even better in some cases.","","Electronic:978-1-4799-8795-5; POD:978-1-4799-8796-2","10.1109/CHINACOM.2015.7498000","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498000","Active Queue Management;Bufferbloat;Congestion Control;Machine Learning","Bandwidth;Computers;Delays;Protocols;Quality of service;Topology;Tuning","Internet;learning (artificial intelligence);multimedia communication;quality of service;queueing theory;telecommunication congestion control","Kemy-generated AQM;QoS;active queue management;bufferbloat problem;multimedia application;network congestion control;off-line machine learning technology;parameter-tuning adaption","","","","10","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Prediction of liver fibrosis stages by machine learning model: A decision tree approach","H. Ayeldeen; O. Shaker; G. Ayeldeen; K. M. Anwar","ISI Research Lab","2015 Third World Conference on Complex Systems (WCCS)","20160602","2015","","","1","6","Using Information systems and strategic tools for medical domains is constantly growing. Automated medical models play an important role in medical decision-making, helping physicians to provide a fast and accurate diagnosis or even prediction. Making use of the knowledge or even in the early stages of knowledge acquisition, different statistical mining and machine learning tools can be used. For instance predicting whether the patient with Hepatitis C virus has also liver fibrosis or not is one of the concerns. In case the prediction result is true, in what stage is the fibrosis. To easily reach to this knowledge without costly diagnostic routine laboratory tests there should be a fully integrated system. Therefore in this study we used machine learning technique model based on decision tree classifier to predict individuals' liver fibrosis degree. Results showed that by using decision tree classifier accuracy is 93.7% which is higher range than what is reported in current researches with similar conditions.","","Electronic:978-1-4673-9669-1; POD:978-1-4673-9670-7","10.1109/ICoCS.2015.7483212","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483212","Decision Tree classification;Hepatitis C virus;Liver fibrosis;Machine Learning techniques","Biochemistry;Biomarkers;Decision trees;Liver;Medical diagnostic imaging;Predictive models","decision trees;diseases;learning (artificial intelligence);medical computing;pattern classification","Hepatitis C virus;decision tree classifier;information systems;knowledge acquisition;liver fibrosis stages prediction;machine learning model;machine learning tools;medical decision-making;medical domain;statistical mining tool","","","","15","","","23-25 Nov. 2015","","IEEE","IEEE Conference Publications"
"Android malware analysis approach based on control flow graphs and machine learning algorithms","M. A. Atici; S. Sagiroglu; I. A. Dogru","Dept. of Comp. Eng. Fac. of Eng., Gazi University, Ankara, Turkey","2016 4th International Symposium on Digital Forensic and Security (ISDFS)","20160519","2016","","","26","31","Smart devices from smartphones to wearable computers today have been used in many purposes. These devices run various mobile operating systems like Android, iOS, Symbian, Windows Mobile, etc. Since the mobile devices are widely used and contain personal information, they are subject to security attacks by mobile malware applications. In this work we propose a new approach based on control flow graphs and machine learning algorithms for static Android malware analysis. Experimental results have shown that the proposed approach achieves a high classification accuracy of 96.26% in general and high detection rate of 99.15% for DroidKungfu malware families which are very harmful and difficult to detect because of encrypting the root exploits, by reducing data dimension significantly for real time analysis.","","Electronic:978-1-4673-9865-7; POD:978-1-4673-9866-4; USB:978-1-4673-9864-0","10.1109/ISDFS.2016.7473512","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473512","Android;control flow graphs;machine learning;malware;mobile security;static analysis","Decision support systems;Flow graphs;Malware;Mobile communication;Security","Android (operating system);cryptography;data flow analysis;data flow graphs;invasive software;learning (artificial intelligence);pattern classification","Droidkungfu malware family detection rate;classification accuracy;control flow graphs;data dimension reduction;encryption;machine learning algorithms;mobile devices;mobile malware applications;mobile operating systems;security attacks;smart devices;smartphones;static Android malware analysis;wearable computers","","","","23","","","25-27 April 2016","","IEEE","IEEE Conference Publications"
"Efficient classification of Parkinson's disease using extreme learning machine and hybrid particle swarm optimization","M. K. Shahsavari; H. Rashidi; H. R. Bakhsh","Qazvin Islamic Azad University, Qazvin, Iran","2016 4th International Conference on Control, Instrumentation, and Automation (ICCIA)","20160602","2016","","","148","154","One of the most well-known problems in machine learning framework is classification of Parkinson's Disease (PD) to patient people and healthy people. Due to the importance of that problem, utilization of a novel learning method is necessary. For this purpose, this paper proposes the utilization of Extreme Learning Machine (ELM) as a type of feed-forward neural network with a single hidden layer to classify the PD patients. However ELM is known as the one of the fast and accurate learning methods, selection of relevant feature elements of PD dataset can be effective on improving the classification performance of ELM. To this end, this paper proposes Hybrid Particle Swarm Optimization (PSO) as the second innovation to efficiently select the relevant feature elements. The main advantage of Hybrid PSO is locally improving of particles in order to jump over the local optimum solution and quickly converging to the global optimal solution. Evaluation of the proposed method on PD dataset proves the superiority of the propose method on the problem of PD classification, in comparison to the other learning methods.","","Electronic:978-1-4673-8704-0; POD:978-1-4673-8705-7","10.1109/ICCIAutom.2016.7483152","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483152","Extreme Learning Machine (ELM);Parkinson's Disease (PD);Particle Swarm Optimization (PSO);classification;feature selection","Artificial neural networks;Feature extraction;Optimization;Parkinson's disease;Particle swarm optimization;Support vector machines","diseases;feedforward neural nets;learning (artificial intelligence);particle swarm optimisation;pattern classification","ELM;PD classification;PD patients;Parkinson's disease;extreme learning machine;feed-forward neural network;global optimal solution;hybrid particle swarm optimization;local optimum solution;machine learning framework","","","","31","","","27-28 Jan. 2016","","IEEE","IEEE Conference Publications"
"Association Between Imaging and XRF Sensing: A Machine Learning Approach to Discover Mineralogy in Abandoned Mine Voids","A. Rahman; G. Timms; M. S. Shahriar; C. Sennersten; A. Davie; C. A. Lindley; A. D. Hellicar; G. Smith; D. Biggins; M. Coombe","Commonwealth Scientific and Industrial Research Organisation, Sandy Bay, TAS, Australia","IEEE Sensors Journal","20160428","2016","16","11","4555","4565","For remote characterization of inaccessible underground mine voids, we are developing unmanned aerial vehicles (equipped with multiple sensors, including cameras) to fly into the mine voids to map their shape, condition, and most importantly, mineralization of the surface. The X-ray fluorescence (XRF) spectroscopy analysis is normally conducted on rock samples in order to detect the present elements (that constitutes minerals). Mining company staffs, however, are able to judge rock types based upon visual features alone. This implies that there are some associations between the XRF signatures and the visual features of rocks. Inspired by this, we have developed a machine learning approach to predict the presence of elements in rocks, for inferring probable rock and mineral types, from imaging features. Note that there exist a number of works in the literature for classifying rocks from digital images. However, to the best of our knowledge, limited attempt has been made to find association between the digital imaging features and the XRF signatures for mineralogy discovery that we have addressed in this paper. The machine learning algorithm is trained offline based on visual imaging and XRF spectroscopy analysis data of collected rock samples in a laboratory. The imaging features provide the visual cues, and the XRF data provide information on element presence/concentration. The machine learning algorithm (regression) discovered the non-linear relationship between these feature spaces and was able to predict the element presence with high accuracy as evidenced from the experimental results.","1530-437X;1530437X","","10.1109/JSEN.2016.2546241","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439740","Mineralogy;XRF spectroscopy;image processing;machine learning;mineralogy","Histograms;Image color analysis;Imaging;Minerals;Rocks;Sensors;Spectroscopy","X-ray fluorescence analysis;cameras;feature extraction;image classification;learning (artificial intelligence);minerals;mining;production engineering computing;rocks","X-ray fluorescence spectroscopy analysis;XRF sensing;XRF signature;XRF spectroscopy analysis;camera;digital imaging feature;inaccessible underground mine void remote characterization;machine learning approach;mineralogy;mining company staff;multiple sensors;rock classification;rock element prediction;rock sample;rock visual feature;surface mineralization;unmanned aerial vehicle;visual imaging","","","","28","","20160323","June1, 2016","","IEEE","IEEE Journals & Magazines"
"Ranking of machine learning algorithms based on the performance in classifying DDoS attacks","R. R. R. Robinson; C. Thomas","Dept. Electronics and Communication Engineering, College of Engineering, Trivadrum","2015 IEEE Recent Advances in Intelligent Computational Systems (RAICS)","20160613","2015","","","185","190","Network Security has become one of the most important factors to consider as the Internet evolves. The most important attack which affects the availability of service is Distributed Denial of Service. The service disruption may cause substantial financial loss as well as damage to the concerned network system. The traffic patterns exhibited by the DDoS affected traffic can be effectively captured by machine learning algorithms. This paper gives an evaluation and ranking of some of the supervised machine learning algorithms with the aim of reducing type I and type II errors, increasing precision and recall while maintaining detection accuracy. The performance evaluation is done using Multi Criteria Decision Aid software called Visual PROMETHEE. This work demonstrates the effectiveness of ensemble based classifiers especially the ensemble algorithm of Adaboost with Random Forest as the base classifier. Publicly available datasets such as DARPA scenario specific dataset, CAIDA DDoS Attack 2007 and CAIDA Conficker are used to evaluate the algorithms.","","CD-ROM:978-1-4673-6669-4; Electronic:978-1-4673-6670-0; POD:978-1-4673-6671-7","10.1109/RAICS.2015.7488411","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488411","DDoS;IDS;MCDA;Visual PROMETHEE","Classification algorithms;Computer crime;Feature extraction;Internet;Intrusion detection;Machine learning algorithms","Internet;computer network performance evaluation;computer network security;decision trees;learning (artificial intelligence);pattern classification;telecommunication traffic","Adaboost algorithm;DDoS attack classification;Internet;MultiCriteria Decision Aid software;Random Forest;Visual PROMETHEE;detection accuracy;distributed denial of service;ensemble based classifiers;machine learning algorithm ranking;network security;performance evaluation;precision;recall;service availability;service disruption;supervised machine learning algorithms;traffic patterns;type I error reduction;type II error reduction","","","","28","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"Identifying Auto-Generated Code by Using Machine Learning Techniques","K. Shimonaka; S. Sumi; Y. Higo; S. Kusumoto","Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan","2016 7th International Workshop on Empirical Software Engineering in Practice (IWESEP)","20160505","2016","","","18","23","Recently, many researchers have conducted mining source code repositories to retrieve useful information about software development. Source code repositories often include auto-generated code, and auto-generated code is usually removed in a preprocessing phase because the presence of auto-generated code is harmful to source code analysis. A usual way to remove auto-generated code is searching particular comments which exist among auto-generated code. However, we cannot identify auto-generated code automatically with such a way if comments have disappeared. In addition, it takes too much time to identify auto-generated code manually. Therefore, we propose a technique to identify auto-generated code automatically by using machine learning techniques. In our proposed technique, we can identify whether source code is auto-generated code or not by utilizing syntactic information of source code. In order to evaluate the proposed technique, we conducted experiments on source code generated by four kinds of code generators. As a result, we confirmed that the proposed technique was able to identify auto-generated code with high accuracy.","","Electronic:978-1-5090-1851-2; POD:978-1-5090-1852-9","10.1109/IWESEP.2016.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464547","auto-generated code;machine learning techniques;source code analysis","Cloning;Data models;Decision trees;Generators;Machine learning algorithms;Predictive models;Syntactics","learning (artificial intelligence);program compilers;software engineering;source code (software)","autogenerated code;code generators;machine learning techniques;software development;source code analysis;source code repositories;source code syntactic information","","","","16","","","13-13 March 2016","","IEEE","IEEE Conference Publications"
"UTD-CRSS system for the NIST 2015 language recognition i-vector machine learning challenge","C. Yu; C. Zhang; S. Ranjan; Q. Zhang; A. Misra; F. Kelly; J. H. L. Hansen","Center for Robust Speech Systems (CRSS), Erik Jonsson School of Engineering, University of Texas at Dallas, Richardson, Texas, U.S.A.","2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20160519","2016","","","5835","5839","In this paper, we present the system developed by the Center for Robust Speech Systems (CRSS), University of Texas at Dallas, for the NIST 2015 language recognition i-vector machine learning challenge. Our system includes several subsystems, based on Linear Discriminant Analysis - Support Vector Machine (LDA-SVM) and deep neural network (DNN) approaches. An important feature of this challenge is the emphasis on out-of-set language detection. As a result, our system development focuses mainly on the evaluation and comparison of two different out-of-set language detection strategies: direct out-of-set detection and indirect out-of-set detection. These out-of-set detection strategies differ mainly on whether the unlabeled development data are used or not. The experimental results indicate that indirect out-of-set detection strategies used in our system could efficiently exploit the unlabeled development data, and therefore consistently outperform the direct out-of-set detection approach. Finally, by fusing four variants of indirect out-of-set detection based subsystems, our system achieves a relative performance gain of up to 45%, compared to the baseline cosine distance scoring (CDS) system provided by organizer.","","Electronic:978-1-4799-9988-0; POD:978-1-4799-9989-7; USB:978-1-4799-9987-3","10.1109/ICASSP.2016.7472796","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7472796","deep neural network;i-vector machine learning challenge;language recognition;out-of-set detection","Feature extraction;NIST;Speech;Speech recognition;Support vector machines;Training;Training data","learning (artificial intelligence);neural nets;speech recognition;statistical analysis;support vector machines","CDS system;Center for Robust Speech Systems;DNN;LDA-SVM;NIST 2015 language recognition;UTD-CRSS system;University of Texas;cosine distance scoring system;deep neural network;direct out-of-set detection strategy;i-vector machine learning challenge;indirect out-of-set detection strategy;linear discriminant analysis;out-of-set language detection;support vector machine","","","","21","","","20-25 March 2016","","IEEE","IEEE Conference Publications"
"Supervised machine learning for document analysis and prediction","K. K. A. Ghany; H. Ayeldeen","ISI Research Lab, Faculty of Computers and Information, Beni-Suef University, Egypt","2015 Third World Conference on Complex Systems (WCCS)","20160602","2015","","","1","6","What if the data gets bigger and bigger? What if handling such huge amount of data started to be critically irritating and need much more attention? These questions became very concerning nowadays. Several organizations and industrial businesses are in need of information system and strategic organizational tool to easily handle huge data and learn the behavior of these data. In this study we proposed a model that is based on Supervised Machine learning to measure, evaluate and learn the similarity of attributes within documents. The documents are in the form of business plan executive summary that consist of several attributes that are used as parameters for evaluation. Results showed that by using similarity learning, attributes within the business plan documents are rated and furthermore the overall documents are ranked showing the effective correlation and association between attributes.","","Electronic:978-1-4673-9669-1; POD:978-1-4673-9670-7","10.1109/ICoCS.2015.7483232","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483232","Intelligence;Ranking;Rating scale;Similarity Learning","Decision trees;Mathematical model;Measurement;Ontologies;Organizations","document handling;learning (artificial intelligence)","business plan executive summary;document analysis;document prediction;similarity learning;supervised machine learning","","","","20","","","23-25 Nov. 2015","","IEEE","IEEE Conference Publications"
"Can machine learning aid in delivering new use cases and scenarios in 5G?","T. S. Buda; H. Assem; L. Xu; D. Raz; U. Margolin; E. Rosensweig; D. R. Lopez; M. I. Corici; M. Smirnov; R. Mullins; O. Uryupina; A. Mozo; B. Ordozgoiti; A. Martin; A. Alloush; P. O'Sullivan; I. G. Ben Yahia","Cognitive Computing Group, Innovation Exchange, IBM Ireland","NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium","20160704","2016","","","1279","1284","5G represents the next generation of communication networks and services, and will bring a new set of use cases and scenarios. These in turn will address a new set of challenges from the network and service management perspective, such as network traffic and resource management, big data management and energy efficiency. Consequently, novel techniques and strategies are required to address these challenges in a smarter way. In this paper, we present the limitations of the current network and service management and describe in detail the challenges that 5G is expected to face from a management perspective. The main contribution of this paper is presenting a set of use cases and scenarios of 5G in which machine learning can aid in addressing their management challenges. It is expected that machine learning can provide a higher and more intelligent level of monitoring and management of networks and applications, improve operational efficiencies and facilitate the requirements of the future 5G network.","","Electronic:978-1-5090-0223-8; POD:978-1-5090-0224-5","10.1109/NOMS.2016.7503003","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503003","","5G mobile communication;Conferences;Context;Degradation;Network topology;Resource management;Topology","5G mobile communication;learning (artificial intelligence);next generation networks;telecommunication computing","future 5G network;machine learning;next generation communication network management;next generation communication service management","","1","","19","","","25-29 April 2016","","IEEE","IEEE Conference Publications"
"The utilisation of composite machine learning models for the classification of medical datasets for sickle cell disease","M. Khalaf; A. J. Hussain; R. Keight; D. Al-Jumeily; R. Keenan; P. Fergus; I. O. Idowu","Faculty of Engineering and Technology, Liverpool John Moores University, Byrom Street, L3 3AF, UK","2016 Sixth International Conference on Digital Information Processing and Communications (ICDIPC)","20160519","2016","","","37","41","The increase growth of health information systems has provided a significant way to deliver great change in medical domains. Up to this date, the majority of medical centres and hospitals continue to use manual approaches for determining the correct medication dosage for sickle cell disease. Such methods depend completely on the experience of medical consultants to determine accurate medication dosages, which can be slow to analyse, time consuming and stressful. The aim of this paper is to provide a robust approach to various applications of machine learning in medical domain problems. The initial case study addressed in this paper considers the classification of medication dosage levels for the treatment of sickle cell disease. This study base on different architectures of machine learning in order to maximise accuracy and performance. The leading motivation for such automated dosage analysis is to enable healthcare organisations to provide accurate therapy recommendations based on previous data. The results obtained from a range of models during our experiments have shown that a composite model, comprising a Neural Network learner, trained using the Levenberg-Marquardt algorithm, combined with a Random Forest learner, produced the best results when compared to other models with an Area under the Curve of 0.995.","","CD-ROM:978-1-4673-7503-0; Electronic:978-1-4673-7504-7; POD:978-1-4673-7505-4","10.1109/ICDIPC.2016.7470788","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470788","E-Health;Machine Learning Algorithm;Real-time data;Receiver Operating Characteristic Curve;Sickle Cell Disease;The Area Under Curve","Adaptation models;Analytical models;Computer architecture;Diseases;Microprocessors;Neural networks;Training","diseases;health care;learning (artificial intelligence);neural nets","Levenberg-Marquardt algorithm;automated dosage analysis;composite machine learning models;healthcare organisations;medical consultants;medical datasets;medication dosage;neural network learner;random forest learner;sickle cell disease;therapy recommendations","","","","12","","","21-23 April 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Real Time Poses Classification Using Kinect Skeleton Data","C. Youness; M. Abdelhak","LIMIARF-OSST Lab., Mohammed V Univ., Rabat, Morocco","2016 13th International Conference on Computer Graphics, Imaging and Visualization (CGiV)","20160512","2016","","","307","311","Poses recognition is an important research topic because some situations require silent communication (sign language, surgeon poses to the nurse for assistance etc.). Traditionally, poses recognition requires high quality expensive cameras and complicated computer vision algorithms. This is not the case thanks to the Microsoft Kinect sensor which provides an inexpensive and easy way for real time user interaction. In this paper, we proposed a real time human poses classification technique, by using skeleton data provided by the Kinect sensor. Different users performed a set of tasks from a vocabulary of eighteen poses. From skeleton data of each pose, twenty features are extracted so that they are invariant with respect to the user's size and its position in the scene. We then compared the generalization performances of four machine learning algorithms, support vectors machines (SVM), artificial neural networks (ANN), k-nearest neighbors (KNN) and Bayes classifier (BC). The method used in this work shows that SVM outperforms the other algorithms.","","Electronic:978-1-5090-0811-7; POD:978-1-5090-0812-4","10.1109/CGiV.2016.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467728","Cross-validation;Kinect;Poses classification;SVM","Artificial neural networks;Feature extraction;Kernel;Prediction algorithms;Skeleton;Support vector machines;Training data","Bayes methods;computer vision;feature extraction;human computer interaction;image classification;image sensors;learning (artificial intelligence);neural nets;object recognition;pose estimation;support vector machines;user interfaces","ANN;BC;Bayes classifier;KNN;Kinect skeleton data;Microsoft Kinect sensor;SVM;artificial neural networks;complicated computer vision algorithms;feature extraction;high quality expensive cameras;k-nearest neighbors;machine learning algorithms;poses recognition;real time human poses classification technique;real time user interaction;sign language;support vectors machines","","","","6","","","March 29 2016-April 1 2016","","IEEE","IEEE Conference Publications"
"Embedded Algorithmic Noise-Tolerance for Signal Processing and Machine Learning Systems via Data Path Decomposition","S. Zhang; N. R. Shanbhag","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA","IEEE Transactions on Signal Processing","20160520","2016","64","13","3338","3350","Low overhead error-resiliency techniques such as algorithmic noise-tolerance (ANT) have been shown to be particularly effective for signal processing and machine learning kernels. However, the overhead of conventional ANT can be as large as 30% due to the use of explicit estimator block. To overcome this overhead, embedded ANT (E-ANT) is proposed [S. Zhang and N. Shanbhag, “Embedded error compensation for energy efficient DSP systems,” in Proc. GlobalSIP, Dec. 2014, pp. 30-34], where the estimator is embedded in the main computation block via data path decomposition (DPD). E-ANT reduces the logic overhead to be below 8% as compared with the 20%-30% associated with conventional reduced precision replica (RPR) ANT system while maintaining the same error compensation functionality. DPD was first proposed in our original paper [Zhang and Shanbhag, 2014] where its benefits were studied in the case of a simple multiply-accumulator (MAC) kernel. This paper builds upon [Zhang and Shanbhag, 2014] by 1) providing conditions for the existence of DPD, 2) demonstrating DPD for a variety of commonly employed kernels in signal processing and machine learning applications, and 3) evaluating the robustness improvement and energy savings of DPD at the system level for an SVM-based EEG seizure classification application. Simulation results in a commercial 45 nm CMOS process show that E-ANT can compensate for error rates up to 0.38 for errors in FE only, and 0.17 for errors in FE and CE, while maintain a true positive rate ${p}_{rm tp} > 0.9$ and a false positive rate ${p}_{rm fp}leq 0.01$. This represents a greater than 3-orders-of-magnitude improvement in error tolerance over the conventional system. This error tolerance is employed to reduce energy via the use of voltage overscaling (VOS). E-ANT is able to achieve 51% energy savings when errors are in FE only, and up to 43% savings when errors are in both FE and CE.","1053-587X;1053587X","","10.1109/TSP.2016.2546224","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439866","Algorithmic noise tolerance;data path decomposition;error resiliency;low power;machine learning;voltage overscaling","Electroencephalography;Error analysis;Iron;Kernel;Signal processing;Signal processing algorithms;Support vector machines","error correction;learning (artificial intelligence);signal processing;statistical analysis","data path decomposition;embedded algorithmic noise-tolerance;energy savings;error compensation;error tolerance;machine learning systems;signal processing;voltage overscaling","","","","24","","20160323","July1, 1 2016","","IEEE","IEEE Journals & Magazines"
"Data linearity using Kernel PCA with Performance Evaluation of Random Forest for training data: A machine learning approach","V. G. Biju; Prashant C M","Dept. of Computer Science and Engineering, Christ University Faculty of Engineering, Bangalore, India","2016 International Conference on Computer Communication and Informatics (ICCCI)","20160530","2016","","","1","5","In this study, Kernel Principal Component Analysis is applied to understand and visualize non-linear variation patterns by inverse mapping the projected data from a high-dimensional feature space back to the original input space. Performance Evaluation of Random Forest on various data sets has been compared to understand accuracy and various statistical measures of interest.","","CD-ROM:978-1-4673-6678-6; Electronic:978-1-4673-6680-9; POD:978-1-4673-6681-6","10.1109/ICCCI.2016.7479924","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479924","Kernel PCA;Machine Learning;Random Forest","Computational modeling;Computers;Eigenvalues and eigenfunctions;Kernel;Principal component analysis;Training;Vegetation","learning (artificial intelligence);principal component analysis","data linearity;kernel PCA;kernel principal component analysis;machine learning approach;nonlinear variation patterns;performance evaluation;random forest;training data","","","","15","","","7-9 Jan. 2016","","IEEE","IEEE Conference Publications"
"Adaptive real-time Trojan detection framework through machine learning","A. Kulkarni; Y. Pino; T. Mohsenin","Department of Computer Science & Electrical Engineering, University of Maryland, Baltimore County","2016 IEEE International Symposium on Hardware Oriented Security and Trust (HOST)","20160623","2016","","","120","123","Hardware Trojans inserted at the time of design or fabrication by untrustworthy design house or foundry, poses important security concerns. With the increase in attacker's resources and capabilities, we can anticipate an unexpected new attack from the attacker at run-time. Therefore, the challenge is not only to reduce hardware overhead of added security feature but also to secure design from new attacks introduced at real-time. In this work, we propose a Real-time Online Learning approach for Securing many-core design. In order to prevent unexpected attacks, many-core provides feed-back to online learning algorithm based on core information and its behavior to incoming data packet. The proposed Online Learning approach updates the model run-time at each data transfer based on feed-back from many-core. For demonstration, Online Machine Learning model is initially trained with two types of (known) attacks and Trojan free router packets and then unexpected attack is introduced later at run-time. The results show that, feedback based Online Machine Learning algorithm has 8% higher overall detection accuracy and an average of 3% higher accuracy for unexpected attacks at each interval of 1000 test records than Supervised Machine Learning algorithms. The proposed feed-back based Trojan detection framework is demonstrated using a custom many-core architecture integrated with “Modified Balanced Winnow” Online Machine Learning algorithm on Xilinx Virtex-7 FPGA. Post place and route implementation results show that, secured many-core architecture requires 4 extra cycles to complete data transfer. The proposed architecture achieves 56% reduction in area and 50% less latency overhead as compared to previous published work [1]. Furthermore, we evaluate our framework for many-core platform by employing seizure detection application as a case study.","","Electronic:978-1-4673-8826-9; POD:978-1-4673-8827-6","10.1109/HST.2016.7495568","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495568","Hardware Security;Machine Learning;Many-Core Design;Trojan Detection","Algorithm design and analysis;Detection algorithms;Hardware;Machine learning algorithms;Security;Support vector machines;Trojan horses","feedback;field programmable gate arrays;learning (artificial intelligence);multiprocessing systems;security","Trojan free router packets;Xilinx Virtex-7 FPGA;adaptive real-time Trojan detection framework;feedback based Trojan detection framework;feedback based online machine learning algorithm;hardware Trojans;hardware overhead reduction;modified balanced Winnow online machine learning algorithm;real-time online learning approach;secured many-core architecture;security concerns;seizure detection application","","","","6","","","3-5 May 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning Based Web Spam Filtering Approach","S. Kumar; X. Gao; I. Welch; M. Mansoori","Victoria Univ. of Wellington, Wellington, New Zealand","2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)","20160523","2016","","","973","980","Web spam has the effect of polluting search engine results and decreasing the usefulness of search engines.Web spam can be classified according to the methods used to raise the web page's ranking by subverting web search engine's algorithms used to rank search results. The main types are: content spam, link spam and cloaking spam. There has been little or no work on automatically classifying web spam by type. This paper has two contributions, (i) we propose a Dual-Margin Multi-Class Hypersphere Support Vector Machine (DMMH- SVM) classifier approach to automatically classifying web spam by type, (ii) we introduce novel cloaking-based spam features which help our classifier model to achieve high precision and recall rate, thereby reducing the false positive rates. The effectiveness of the proposed model is justified analytically. Our experimental results demonstrated that DMMH-SVM outperforms existing algorithms with novel cloaking features.","1550-445X;1550445X","Electronic:978-1-5090-1858-1; POD:978-1-5090-1859-8","10.1109/AINA.2016.177","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474194","","Optimization;Search engines;Support vector machines;Training;Web pages;Web search","Internet;information filtering;learning (artificial intelligence);pattern classification;search engines;support vector machines;unsolicited e-mail","DMMH-SVM classifier;Web page ranking;Web search engine algorithms;automatic Web spam classification;cloaking-based spam features;content spam;dual-margin multiclass hypersphere support vector machine classifier;false positive rates;link spam;machine learning based Web spam filtering approach;precision;recall rate","","","","45","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"False arrhythmia alarm reduction in the intensive care unit using data fusion and machine learning","Q. Zhang; X. Chen; Z. Fang; S. Xia","Institute of Electronics, Chinese Academy of Sciences, and University of Chinese Academy of Sciences, Beijing, China","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","232","235","With aim of reducing the incidence of false critical arrhythmia alarms in intensive care units, a novel data fusion and machine learning algorithm is presented in this article. The 2015 PhysioNet/Computing in Cardiology Challenge database was used in this present algorithm, with each grouped as an asystole (AS), extreme bradycardia (EB), extreme tachycardia (ET), ventricular tachycardia (VT) or ventricular flutter/fibrillation (VF) arrhythmia alarm. A 10-second segment before the onset of the alarm was truncated from available signals, namely electrocardiogram (ECG), arterial blood pressure (ABP), and/or photoplethysmogram (PPG). By first assessing signal quality of available signals, a robust estimation of beat-to-beat intervals could then be derived. Features in heart rate variability (HRV) analysis and ECG parameters such as temporal statistical parameters, spectral analysis results, wavelet transformation coefficients, and complexity measurement etc were extracted and formed a vector. After feature selection through genetic algorithm (GA), a support vector machine (SVM) model was applied to conduct the classification of alarms for the specific arrhythmia type. The overall true positive rate (TPR) of classification algorithm is 93%, with the true negative rate (TNR) 94%. According to the method of performance evaluation in the 2015 Challenge, this algorithm achieved a gross score of 84.4.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455877","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455877","","Cardiology;Classification algorithms;Electrocardiography;Feature extraction;Genetic algorithms;Heart rate variability;Support vector machines","electrocardiography;feature extraction;genetic algorithms;learning (artificial intelligence);medical signal processing;sensor fusion;signal classification;spectral analysis;support vector machines;wavelet transforms","ECG parameters;SVM;alarm classification;arterial blood pressure;asystole;beat-to-beat intervals;cardiology challenge database;complexity measurement;data fusion;electrocardiogram;extreme bradycardia;extreme tachycardia;false arrhythmia alarm reduction;genetic algorithm;heart rate variability analysis;intensive care unit;machine learning algorithm;overall true positive rate;performance evaluation;photoplethysmogram;signal quality;spectral analysis;support vector machine model;temporal statistical parameters;true negative rate;vector extraction;ventricular flutter-fibrillation arrhythmia alarm;ventricular tachycardia;wavelet transformation coefficients","","","","11","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques for cognitive decision making","A. Chandiok; D. K. Chaturvedi","Faculty of Engineering and Department of Electrical Engineering, Dayalbagh Educational Institute, Agra, UP, India 282005","2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)","20160623","2015","","","1","6","Machine learning algorithms in cognitive computing for decision making can help out how to achieve significant solutions by generalizing a learned model from environmental pattern instances. This technique is frequently practicable and economical where manual rigid rule based abstract programming is not suitable. As more training input patterns are obtainable, better-determined tasks can be attempted. As a result, machine learning is extensively used in cognitive computing and artificial intelligence for handling structured, unstructured and multimedia big data. However, evolving fruitful machine learning cognitive applications involves a considerable extent of concept that is not available in general theories. This paper will analyze primary modules of machine learning approaches and attempt to present friendly real world example of cognitive teacher appraisal. The first section will sightsee the meaning of machine learning, deliberate the cognitive abilities it can generate. The second and third section discusses the practical procedure and issues for solving cognitive problems. The next Five section will define concept of machine learning methods and its application problem domain. The last section shows comparison of machine learning algorithm capability and limitations.","","Electronic:978-1-4673-8215-1; POD:978-1-4673-8219-9","10.1109/WCI.2015.7495529","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495529","","Appraisal;Decision making;Machine learning algorithms;Prototypes;Testing;Training","appraisal;cognition;generalisation (artificial intelligence);learning (artificial intelligence)","artificial intelligence;cognitive abilities;cognitive computing;cognitive decision making;cognitive teacher appraisal;environmental pattern;machine learning technique;multimedia Big Data handling;structured Big Data handling;training input patterns;unstructured Big Data handling","","","","22","","","14-17 Dec. 2015","","IEEE","IEEE Conference Publications"
"Predicting and analyzing water quality using Machine Learning: A comprehensive model","Y. Khan; C. S. See","Faculty of Computer Science and Information Technology, Universiti Malaysia Sarawak, Kota Samarahan, Malaysia","2016 IEEE Long Island Systems, Applications and Technology Conference (LISAT)","20160620","2016","","","1","6","The deteriorating quality of natural water resources like lakes, streams and estuaries, is one of the direst and most worrisome issues faced by humanity. The effects of un-clean water are far-reaching, impacting every aspect of life. Therefore, management of water resources is very crucial in order to optimize the quality of water. The effects of water contamination can be tackled efficiently if data is analyzed and water quality is predicted beforehand. This issue has been addressed in many previous researches, however, more work needs to be done in terms of effectiveness, reliability, accuracy as well as usability of the current water quality management methodologies. The goal of this study is to develop a water quality prediction model with the help of water quality factors using Artificial Neural Network (ANN) and time-series analysis. This research uses the water quality historical data of the year of 2014, with 6-minutes time interval. Data is obtained from the United States Geological Survey (USGS) online resource called National Water Information System (NWIS). For this paper, the data includes the measurements of 4 parameters which affect and influence water quality. For the purpose of evaluating the performance of model, the performance evaluation measures used are Mean-Squared Error (MSE), Root Mean-Squared Error (RMSE) and Regression Analysis. Previous works about Water Quality prediction have also been analyzed and future improvements have been proposed in this paper.","","Electronic:978-1-4673-8490-2; POD:978-1-4673-8491-9","10.1109/LISAT.2016.7494106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494106","Artificial Neural Networks;Environmental Modeling;Machine Learning;Time-Series Analysis","Artificial neural networks;Predictive models;Testing;Time series analysis;Training;Water pollution;Water resources","contamination;environmental science computing;learning (artificial intelligence);mean square error methods;neural nets;regression analysis;time series;water quality","ANN;MSE;NWIS;National Water Information System;RMSE;USGS online resource;United States Geological Survey online resource;artificial neural network;machine learning;mean-squared error;natural water resource quality deterioration;regression analysis;root mean-squared error;time-series analysis;un-clean water effects;water contamination effects;water quality analysis;water quality prediction;water resource management","","","","18","","","29-29 April 2016","","IEEE","IEEE Conference Publications"
"A Public Cloud Based SOA Workflow for Machine Learning Based Recommendation Algorithms","R. G. Athreya; S. Thanukrishnan","Glosys Technol. Solutions Private Ltd., Chennai, India","2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)","20160505","2015","","","988","995","Over the past decade, the field of Cloud Computing has been the focus of intensive research. This paper proposes 'A Public Cloud based SOA Workflow for Machine Learning based Recommendation Algorithms' to build a framework that will simulate the architectural setup of a cloud environment and examine how it can leverage Apriori and sequential pattern based recommendation algorithms using R. Furthermore, we present a multi layered application encompassing its backend architecture, user interface built using the responsive web design technique and its development workflow. The proposed system was also exhaustively load tested using Apache JMeter to ensure its reliability at scale and the experimental results are presented.","","Electronic:978-1-5090-1893-2; POD:978-1-5090-1894-9","10.1109/SmartCity.2015.199","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463854","Cloud Computing;Machine Learning;Service Orchestration in a Cloud Environment;Service Oriented Architecture","Business;Cloud computing;Computer architecture;Databases;Machine learning algorithms;Mobile communication;Servers","cloud computing;learning (artificial intelligence);recommender systems;service-oriented architecture;user interfaces","Apache JMeter;Apriori pattern-based recommendation algorithms;backend architecture;cloud computing;cloud environment;machine learning-based recommendation algorithms;public cloud based SOA workflow;reliability;responsive Web design technique;sequential pattern based recommendation algorithms;service-oriented architecture;user interface","","","","18","","","19-21 Dec. 2015","","IEEE","IEEE Conference Publications"
"Ground-based image analysis: A tutorial on machine-learning techniques and applications","S. Dev; B. Wen; Y. H. Lee; S. Winkler","School of Electrical and Electronic Engineering, Nanyang Technological University Singapore, 639798, Singapore","IEEE Geoscience and Remote Sensing Magazine","20160607","2016","4","2","79","93","Ground-based whole-sky cameras have opened up new opportunities for monitoring the earth's atmosphere. These cameras are an important complement to satellite images by providing geoscientists with cheaper, faster, and more localized data. The images captured by whole-sky imagers (WSI) can have high spatial and temporal resolution, which is an important prerequisite for applications such as solar energy modeling, cloud attenuation analysis, local weather prediction, and more. Extracting the valuable information from the huge amount of image data by detecting and analyzing the various entities in these images is challenging. However, powerful machine-learning techniques have become available to aid with the image analysis. This article provides a detailed explanation of recent developments in these techniques and their applications in ground-based imaging, aiming to bridge the gap between computer vision and remote sensing with the help of illustrative examples. We demonstrate the advantages of using machine-learning techniques in ground-based image analysis via three primary applications: segmentation, classification, and denoising.","2473-2397;24732397","","10.1109/MGRS.2015.2510448","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486188","","Cameras;Detectors;Feature extraction;Monitoring;Remote sensing;Satellites;Spatial resolution;Tutorials","clouds;geophysical image processing;image classification;image denoising;image segmentation;learning (artificial intelligence);remote sensing","cloud attenuation analysis;computer vision;earth's atmosphere monitoring;ground-based image analysis;ground-based imaging;image classification;image data;image denoising;image segmentation;local weather prediction;machine-learning application;machine-learning technique;remote sensing;satellite image;solar energy modeling;whole-sky camera;whole-sky imager","","1","","","","","June 2016","","IEEE","IEEE Journals & Magazines"
"On User-Centric Modular QoE Prediction for VoIP Based on Machine-Learning Algorithms","P. Charonyktakis; M. Plakia; I. Tsamardinos; M. Papadopouli","Computer Science Department, University of Crete,&#160;and the Institute of Computer Science, Foundation for Research and Technology-Hellas, Heraklion, Crete, Greece","IEEE Transactions on Mobile Computing","20160504","2016","15","6","1443","1456","The impact of the network performance on the quality of experience (QoE) for various services is not well-understood. Assessing the impact of different network and channel conditions on the user experience is important for improving the telecommunication services. The QoE for various wireless services including VoIP, video streaming, and web browsing, has been in the epicenter of recent networking activities. The majority of such efforts aim to characterize the user experience, analyzing various types of measurements often in an aggregate manner. This paper proposes the MLQoE, a modular algorithm for user-centric QoE prediction. The MLQoE employs multiple machine learning (ML) algorithms, namely, Artificial Neural Networks, Support Vector Regression machines, Decision Trees, and Gaussian Naive Bayes classifiers, and tunes their hyper-parameters. It uses the Nested Cross Validation (nested CV) protocol for selecting the best classifier and the corresponding best hyper-parameter values and estimates the performance of the final model. The MLQoE is conservative in the performance estimation despite multiple induction of models. The MLQoE is modular, in that, it can be easily extended to include other ML algorithms. The MLQoE selects the ML algorithm that exhibits the best performance and its parameters automatically given the dataset used as input. It uses empirical measurements based on network metrics (e.g., packet loss, delay, and packet interarrival) and subjective opinion scores reported by actual users. This paper extensively evaluates the MLQoE using three unidirectional datasets containing VoIP calls over wireless networks under various network conditions and feedback from subjects (collected in field studies). Moreover, it performs a preliminary analysis to assess the generality of our methodology using bidirectional VoIP and video traces. The MLQoE outperforms several state-of-the-art algorithms, resulting in fairly accurate predictions.","1536-1233;15361233","","10.1109/TMC.2015.2461216","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167700","E-model;PESQ;VoIP;machine learning;modeling;quality of experience;statistical analysis;user-centric;wireless networks","Algorithm design and analysis;Estimation;Loss measurement;Prediction algorithms;Quality of service;Training","Bayes methods;Gaussian distribution;Internet telephony;access protocols;decision trees;learning (artificial intelligence);neural nets;online front-ends;quality of experience;regression analysis;support vector machines;telecommunication services;video streaming","Gaussian naive Bayes classifiers;MLQoE modular algorithm;Web browsing;artificial neural networks;bidirectional VoIP;decision trees;hyper-parameter values;multiple machine learning;nested cross validation protocol;packet delay;packet interarrival;packet loss;quality of experience;support vector regression machines;telecommunication services;unidirectional datasets;user-centric modular QoE prediction;video streaming;video traces","","1","","61","","20150727","June 1 2016","","IEEE","IEEE Journals & Magazines"
"Workload management for cloud databases via machine learning","R. Marcus; O. Papaemmanouil","Brandeis University, USA","2016 IEEE 32nd International Conference on Data Engineering Workshops (ICDEW)","20160623","2016","","","27","30","As elastic IaaS clouds continue to become more cost efficient than on-site datacenters, a wide range of data management applications are migrating to pay-as-you-go cloud computing resources. These diverse applications come with an equally diverse set of performance goals, resource demands, and budget constraints. While existing research has tackled individual tasks such as query placement, scheduling, and resource provisioning to meet these goals and constraints, these techniques fail to provide end-to-end customizable workload management solutions, leading application developers to hand-craft custom heuristics that fit their workload specifications and performance goals. In this vision paper, we argue that workload management challenges can be addressed by leveraging machine learning algorithms. These algorithms can be trained on application-specific properties and performance metrics to automatically learn how to provision resources as well as distribute and schedule the execution of incoming query workloads. Towards this goal, we sketch our vision of WiSeDB, a learning-based service that relies on supervised and reinforcement learning to generate workload management strategies for both static and dynamic workloads.","","Electronic:978-1-5090-2109-3; POD:978-1-5090-2110-9","10.1109/ICDEW.2016.7495611","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495611","","Cloud computing;Decision trees;Feature extraction;Learning (artificial intelligence);Measurement;Schedules;Supervised learning","cloud computing;database management systems;learning (artificial intelligence)","WiSeDB;cloud databases;elastic IaaS clouds;end-to-end customizable workload management solutions;machine learning algorithms;query workloads;reinforcement learning;supervised learning","","","","8","","","16-20 May 2016","","IEEE","IEEE Conference Publications"
"Hotspot detection using machine learning","K. Madkour; S. Mohamed; D. Tantawy; M. Anis","Mentor Graphics, Cairo, Egypt","2016 17th International Symposium on Quality Electronic Design (ISQED)","20160526","2016","","","405","409","As technology nodes continue shrinking, lithography hotspot detection has become a challenging task in the design flow. In this work we present a hybrid technique using pattern matching and machine learning engines for hotspot detection. In the training phase, we propose sampling techniques to correct for the hotspot/non-hotspot imbalance to improve the accuracy of the trained Support Vector Machine (SVM) system. In the detection phase, we have combined topological clustering and a novel pattern encoding technique based on pattern regularity to enhance the predictability of the system. Using the ICCAD 2012 benchmark data, our approach shows an accuracy of 88% in detecting hotspots with hit-to-extra ratio of 0.12 which are better results compared to other published techniques using the same benchmark data.","1948-3295;19483295","Electronic:978-1-5090-1213-8; POD:978-1-5090-1214-5","10.1109/ISQED.2016.7479235","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479235","Design for Manufacturability;Lithography Hotspots;Machine Learning;Pattern Matching","Encoding;Kernel;Layout;Pattern matching;Support vector machines;Training;Training data","electronic engineering computing;encoding;learning (artificial intelligence);pattern clustering;pattern matching;photolithography;support vector machines","ICCAD 2012 benchmark data;SVM system;combined topological clustering;design flow;hotspot-nonhotspot imbalance;hybrid pattern matching-machine learning technique;lithography hotspot detection;pattern encoding technique;pattern regularity;sampling techniques;support vector machine system","","","","17","","","15-16 March 2016","","IEEE","IEEE Conference Publications"
"FishAPP: A mobile App to detect fish falsification through image processing and machine learning techniques","F. Rossi; A. Benso; S. Di Carlo; G. Politano; A. Savino; P. L. Acutis","Politecnico di Torino, Control and Comp. Engineering Department, Torino, Italy","2016 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)","20160630","2016","","","1","6","Food forgery is one of the most articulated socio-economic concerns, which contributed to increase people awareness on what they eat. Identification of species represents a key aspect to expose commercial frauds implemented by substitution of valuable species with others of lower value. Fish species identification is mainly performed by morphological identification of gross anatomical features of the whole fish. However, the increasing presence on markets of new little-known species makes morphological identification of species difficult. In this paper we present FishAPP, a cloud-based infrastructure for fish species recognition. FishAPP is composed of a mobile application developed for the Android and the iOS mobile operating system enabling the user to shot pictures of a whole fish and submit them for remote analysis and a remote cloud-based processing system that implements a complex image processing pipeline and a neural network machine learning system able to analyze the obtained images and to perform classification into predefined fish classes. Preliminary results obtained from the available dataset provided encouraged results.","","Electronic:978-1-4673-8692-0; POD:978-1-4673-8693-7; USB:978-1-4673-8691-3","10.1109/AQTR.2016.7501348","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501348","","Cloud computing;Feature extraction;Mobile applications;Mobile communication;Servers;Smart phones","Android (operating system);aquaculture;cloud computing;feature extraction;image classification;learning (artificial intelligence);neural nets;smart phones","Android;FishAPP;cloud-based infrastructure;fish falsification detection;fish species identification;fish species recognition;food forgery;iOS mobile operating system;image classification;image processing;machine learning;mobile application;neural network","","","","19","","","19-21 May 2016","","IEEE","IEEE Conference Publications"
"A comparison of feature selection methods for machine learning based automatic malarial cell recognition in wholeslide images","V. Muralidharan; Y. Dong; W. David Pan","Electrical and Computer Engineering Department, University of Alabama in Huntsville, Huntsville, AL 35899, USA","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","216","219","This paper aims at investigating the best feature selection method for optimized and automated machine learning based detection of malarial parasite in wholeslide images of peripheral blood smears. We do this by extracting samples from the wholeslide images and performing feature extraction. A host of feature selection methods are used to judge the performance of the Support Vector Machine as a binary classifier. For each feature selection method, the Support Vector Machine is trained using the significant features. We perform cross-validation and grid-search for finding the best SVM parameters. The trained SVM is subsequently used to classify known instances of ""Normal"" and ""Infected"" samples that are taken randomly from the wholeslide image but unknown to the SVM. Confusion matrices are generated for each classification performed. Various performance measures of each classification task are reported. We conclude that based on our experiments, the binary SVM classifier yields a superlative accuracy of 95.5% if the feature-selection is based on Kullback-Leibler distance between the two classes.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455873","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455873","","Diseases;Feature extraction;Image resolution;Kernel;Robustness;Support vector machines;Training","biomedical optical imaging;blood;cellular biophysics;diseases;feature extraction;feature selection;image classification;medical image processing;support vector machines","Kullback-Leibler distance;SVM parameters;binary classifier;confusion matrices;feature extraction;feature selection;grid search;machine learning based automatic malarial cell recognition;malarial parasite;peripheral blood smears;support vector machine;wholeslide images","","","","27","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Automatic protocol feature word construction based on machine learning","Haifeng Li; Bin Zhang; Bo Shuai; Jian Wang; Chaojing Tang","School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China, 410073","2015 IEEE International Conference on Progress in Informatics and Computing (PIC)","20160613","2015","","","93","97","Automatic protocol reverse engineering for application protocol is becoming more and more important for many applications such as application protocol analyzer, penetration testing, intrusion prevention and detection. Unfortunately, many techniques for extracting the protocol message format specifications of unknown applications often have some limitations for few priori information or the time-consuming problem. Protocol feature words are byte subsequences within traffic payload that could help distinguish application protocols. In this paper, a new approach is proposed for extracting the protocol message format specifications of unknown applications which is based on the Latent Dirichlet Allocation (LDA) model and Huffman Tree Support Vector Machine (HT-SVM). Firstly, the key words are extracted by utilizing the LDA model, which is a kind of machine learning in document library to extract the theme structure named topic. Secondly, the HT-SVM method is applied to constructing the feature words based on the above process. The proposed approach is implemented and evaluated to infer message format specifications of SMTP binary protocol. Experimental results show that the approach accurately parses and infers SMTP protocol with highly recall rate.","","CD-ROM:978-1-4673-8085-0; Electronic:978-1-4673-9088-0; POD:978-1-4673-9089-7","10.1109/PIC.2015.7489816","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489816","HT-SVM;LDA;feature word;protocol reverse","Artificial neural networks;Protocols;Support vector machines","feature extraction;learning (artificial intelligence);protocols;reverse engineering;statistical analysis;support vector machines;telecommunication traffic;trees (mathematics)","HT-SVM method;Huffman tree support vector machine;LDA model;SMTP binary protocol;application protocol analyzer;automatic protocol feature word construction;automatic protocol reverse engineering;byte subsequences;document library;intrusion detection;intrusion prevention;latent Dirichlet allocation model;machine learning;penetration testing;protocol message format specifications;traffic payload","","","","20","","","18-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection","A. L. Buczak; E. Guven","The Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA","IEEE Communications Surveys & Tutorials","20160520","2016","18","2","1153","1176","This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.","1553-877X;1553877X","","10.1109/COMST.2015.2494502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307098","Cyber Analytics;Cyber analytics;Data Mining;Machine Learning;data mining;machine learning","Computer security;Data mining;Data models;IP networks;Measurement;Ports (Computers);Protocols","computer network security;data mining;learning (artificial intelligence)","DM methods;ML-DM algorithms;ML-DM method;cyber analytics;cyber data sets;cyber security intrusion detection;data mining;machine learning methods","","4","","113","","20151026","Secondquarter 2016","","IEEE","IEEE Journals & Magazines"
"Distributed-neuron-network based machine learning on smart-gateway network towards real-time indoor data analytics","H. Huang; Y. Cai; H. Yu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)","20160428","2016","","","720","725","Indoor data analytics is one typical example of ambient intelligence with behaviour or feature extraction from environmental data. It can be utilized to help improve comfort level in building and room for occupants. To address dynamic ambient change in a large-scaled space, real-time and distributed data analytics is required on sensor (or gateway) network, which however has limited computing resources. This paper proposes a computationally efficient data analytics by distributed-neuron-network (DNN) based machine learning with application for indoor positioning. It is based on one incremental L<sub>2</sub>-norm based solver for learning collected WiFi-data at each gateway and is further fused for all gateways in the network to determine the location. Experimental results show that with multiple distributed gateways running in parallel, the proposed algorithm can achieve 50x and 38x speedup during data testing and training time respectively with comparable positioning accuracy, when compared to traditional support vector machine (SVM) method.","","Electronic:978-3-9815-3707-9; POD:978-1-4673-9228-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459402","","IEEE 802.11 Standard;Logic gates;Neurons;Support vector machines;Training;Training data","LAN interconnection;ambient intelligence;home automation;human factors;indoor navigation;learning (artificial intelligence);neural nets;wireless LAN;wireless sensor networks","DNN-based machine learning;Wi-Fi;ambient intelligence;data testing time;data training time;distributed-neuron-network based machine learning;dynamic ambient change;incremental L<sub>2</sub>-norm based solver;indoor positioning;large-scaled space-real-time distributed data analytics;multiple distributed gateways;occupant comfort level improvement;sensor network;smart home;smart-gateway network","","","","20","","","14-18 March 2016","","IEEE","IEEE Conference Publications"
