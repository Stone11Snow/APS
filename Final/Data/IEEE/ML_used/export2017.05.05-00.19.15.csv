"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6331071,6331073,6331078,6331075,6331080,6331086,6331072,6331077,6284963,6284941,6279036,6279034,6279035,6279037,6279038,6279023,6279381,6279517,6279350,6279346,6279396,6279382,6279345,6279344,6279560,6279380,6279348,6279384,6279347,6279379,6282652,6282653,6282654,6282655,6282656,6282658,6284950,6284961,6279024,6284942,6279358,6282659,6284957,6279031,6284945,6279030,6279356,6284949,6284960,6279033,6284944,6279359,6279029,6279027,6279354,6284956,6279351,6279025,6284948,6279352,6284959,6284954,6279360,6279032,6284946,6279357,6284943,6279028,6279355,6279026,6279353,6284955,6284952,6284947,6279562,6279561,6284958,6284953,6284964,6278037,6278053,6279529,6278039,6279495,6278038,6278036,6278035,6270038,6270039,6270032,6278051,6270047,6278040,6278048,6278047,6278046,6270052,6278045,6270051,6278044",2017/05/05 00:19:15
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Introduction and Problem Formulation","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","3","17","This chapter contains sections titled: Machine Learning under Covariate Shift, Quick Tour of Covariate Shift Adaptation, Problem Formulation, Structure of This Book","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278051.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Learning Under Covariate Shift","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","19","19","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278037.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Function Approximation","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","21","45","This chapter contains sections titled: Importance-Weighting Techniques for Covariate Shift Adaptation, Examples of Importance-Weighted Regression Methods, Examples of Importance-Weighted Classification Methods, Numerical Examples, Summary and Discussion","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278048.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"References","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","409","445","An unprecedented wealth of data is being generated by genome sequencing projects and other experimental efforts to determine the structure and function of biological molecules. The demands and opportunities for interpreting these data are expanding rapidly. Bioinformatics is the development and application of computer methods for management, analysis, interpretation, and prediction, as well as for the design of experiments. Machine learning approaches (e.g., neural networks, hidden Markov models, and belief networks) are ideally suited for areas where there is a lot of data but little theory, which is the situation in molecular biology. The goal in machine learning is to extract useful information from a body of data by building good probabilistic models--and to automate the process as much as possible.In this book Pierre Baldi and SÃ¿ren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.This new second edition contains expanded coverage of probabilistic graphical models and of the applications of neural networks, as well as a new chapter on microarrays and gene expression. The entire text has been extensively revised.","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270038.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Model Selection","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","47","71","This chapter contains sections titled: Importance-Weighted Akaike Information Criterion, Importance-Weighted Subspace Information Criterion, Importance-Weighted Cross-Validation, Numerical Examples, Summary and Discussion","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278047.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Enabling Technologies","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","39","39","The Internet gives us access to a wealth of information in languages we don't understand. The investigation of automated or semi-automated approaches to translation has become a thriving research field with enormous commercial potential. This volume investigates how Machine Learning techniques can improve Statistical Machine Translation, currently at the forefront of research in the field. The book looks first at enabling technologies--technologies that solve problems that are not Machine Translation proper but are linked closely to the development of a Machine Translation system. These include the acquisition of bilingual sentence-aligned data from comparable corpora, automatic construction of multilingual name dictionaries, and word alignment. The book then presents new or improved statistical Machine Translation techniques, including a discriminative training framework for leveraging syntactic information, the use of semi-supervised and kernel-based learning methods, and the combination of multiple Machine Translation outputs in order to improve overall translation quality.ContributorsSrinivas Bangalore, Nicola Cancedda, Josep M. Crego, Marc Dymetman, Jakob Elming, George Foster, JesÃºs GimÃ©nez, Cyril Goutte, Nizar Habash, Gholamreza Haffari, Patrick Haffner, Hitoshi Isahara, Stephan Kanthak, Alexandre Klementiev, Gregor Leusch, Pierre MahÃ©, LluÃ­s MÃ rquez, Evgeny Matusov, I. Dan Melamed, Ion Muslea, Hermann Ney, Bruno Pouliquen, Dan Roth, Anoop Sarkar, John Shawe-Taylor, Ralf Steinberger, Joseph Turian, Nicola Ueffing, Masao Utiyama, Zhuoran Wang, Benjamin Wellington, Kenji Yamada","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279381.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Index","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","447","452","An unprecedented wealth of data is being generated by genome sequencing projects and other experimental efforts to determine the structure and function of biological molecules. The demands and opportunities for interpreting these data are expanding rapidly. Bioinformatics is the development and application of computer methods for management, analysis, interpretation, and prediction, as well as for the design of experiments. Machine learning approaches (e.g., neural networks, hidden Markov models, and belief networks) are ideally suited for areas where there is a lot of data but little theory, which is the situation in molecular biology. The goal in machine learning is to extract useful information from a body of data by building good probabilistic models--and to automate the process as much as possible.In this book Pierre Baldi and SÃ¿ren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.This new second edition contains expanded coverage of probabilistic graphical models and of the applications of neural networks, as well as a new chapter on microarrays and gene expression. The entire text has been extensively revised.","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270039.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Importance Estimation","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","73","102","This chapter contains sections titled: Kernel Density Estimation, Kernel Mean Matching, Logistic Regression, Kullback-Leibler Importance Estimation Procedure, Least-Squares Importance Fitting, Unconstrained Least-Squares Importance Fitting, Numerical Examples, Experimental Comparison, Summary","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278046.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Appendix: List of Symbols and Abbreviations","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","243","245","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278053.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Hidden Markov Models: Applications","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","189","223","This chapter contains sections titled: Protein Applications, DNA and RNA Applications, Advantages and Limitations of HMMs","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270052.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Classification","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","33","77","This chapter contains sections titled: Classification Problems, Linear Models for Classification, Gaussian Process Classification, The Laplace Approximation for the Binary GP Classifier, Multi-class Laplace Approximation, Expectation Propagation, Experiments, Discussion, Appendix: Moment Derivations, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279031.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Direct Density-Ratio Estimation with Dimensionality Reduction","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","103","123","This chapter contains sections titled: Density Difference in Hetero-Distributional Subspace, Characterization of Hetero-Distributional Subspace, Identifying Hetero-Distributional Subspace by Supervised Dimensionality Reduction, Using LFDA for Finding Hetero-Distributional Subspace, Density-Ratio Estimation in the Hetero-Distributional Subspace, Numerical Examples, Summary","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278045.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Bibliography","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","247","257","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279529.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Supervised Learning","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","21","45","This chapter contains sections titled: 2.1 Learning a Class from Examples, 2.2 Vapnik-Chervonenkis (VC) Dimension, 2.3 Probably Approximately Correct (PAC) Learning, 2.4 Noise, 2.5 Learning Multiple Classes, 2.6 Regression, 2.7 Model Selection and Generalization, 2.8 Dimensions of a Supervised Machine Learning Algorithm, 2.9 Notes, 2.10 Exercises, 2.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284950.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Datasets and Code","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","221","221","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279036.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Clustering","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","143","162","This chapter contains sections titled: 7.1 Introduction, 7.2 Mixture Densities, 7.3 k-Means Clustering, 7.4 Expectation-Maximization Algorithm, 7.5 Mixtures of Latent VariableModels, 7.6 Supervised Learning after Clustering, 7.7 Hierarchical Clustering, 7.8 Choosing the Number of Clusters, 7.9 Notes, 7.10 Exercises, 7.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284945.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Probabilistic Graphical Models in Bioinformatics","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","225","263","This chapter contains sections titled: The Zoo of Graphical Models in Bioinformatics, Markov Models and DNA Symmetries, Markov Models and Gene Finders, Hybrid Models and Neural Network Parameterization of Graphical Models, The Single-Model Case, Bidirectional Recurrent Neural Networks for Protein Secondary Structure Prediction","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270051.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Covariance functions","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","79","104","This chapter contains sections titled: Preliminaries, Examples of Covariance Functions, Eigenfunction Analysis of Kernels, Kernels for Non-vectorial Inputs, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279030.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"On the Training/Test Distributions Gap: A Data Representation Learning Framework","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","73","84","This chapter contains sections titled: Introduction, Formal Framework and Notation, A Basic Taxonomy of Tasks and Paradigms, Error Bounds for Conservative Domain Adaptation Prediction, Adaptive Predictors","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279356.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Relation to Sample Selection Bias","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","125","136","This chapter contains sections titled: Heckman's Sample Selection Model, Distributional Change and Sample Selection Bias, The Two-Step Algorithm, Relation to Covariate Shift Approach","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278044.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Index","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","259","261","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278039.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Bayesian Decision Theory","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","47","59","This chapter contains sections titled: 3.1 Introduction, 3.2 Classification, 3.3 Losses and Risks, 3.4 Discriminant Functions, 3.5 Utility Theory, 3.6 Association Rules, 3.7 Notes, 3.8 Exercises, 3.9 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284949.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Front Matter","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","i","xviii","This chapter contains sections titled: Half Title, Adaptive Computation and Machine Learning, Title, Copyright, Dedication, Contents, Series Foreword, Preface","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282659.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Linear Discrimination","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","209","231","This chapter contains sections titled: 10.1 Introduction, 10.2 Generalizing the Linear Model, 10.3 Geometry of the Linear Discriminant, 10.4 Pairwise Separation, 10.5 Parametric Discrimination Revisited, 10.6 Gradient Descent, 10.7 Logistic Discrimination, 10.8 Discrimination by Regression, 10.9 Notes, 10.10 Exercises, 10.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284960.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Bibliography","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","223","238","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279034.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Introduction","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","1","6","This chapter contains sections titled: A Pictorial Introduction to Bayesian Modelling, Roadmap","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279033.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Nonparametric Methods","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","163","183","This chapter contains sections titled: 8.1 Introduction, 8.2 Nonparametric Density Estimation, 8.3 Generalization to Multivariate Data, 8.4 Nonparametric Classification, 8.5 Condensed Nearest Neighbor, 8.6 Nonparametric Regression: Smoothing Models, 8.7 How to Choose the Smoothing Parameter, 8.8 Notes, 8.9 Exercises, 8.10 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284944.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Author Comments","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","201","205","Hidetoshi Shimodaira, Masashi Sugiyama, Amos Storkey, Arthur Gretton, Shai Ben-David","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279359.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Notation and Symbols","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","219","221","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279517.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Model Selection and Adaptation of Hyperparameters","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","105","128","This chapter contains sections titled: The Model Selection Problem, Bayesian Model Selection, Cross-validation, Model Selection for GP Regression, Model Selection for GP Classification, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279029.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Index","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","227","229","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279350.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Algorithms for Covariate Shift","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","85","85","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279346.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Theoretical Perspectives","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","151","170","This chapter contains sections titled: The Equivalent Kernel, Asymptotic Analysis, Average-Case Learning Curves, PAC-Bayesian Analysis, Comparison with Other Supervised Learning Methods, Appendix: Learning Curve for the Ornstein-Uhlenbeck Process, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279027.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"A Conditional Expectation Approach to Model Selection and Active Learning under Covariate Shift","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","107","130","This chapter contains sections titled: Conditional Expectation Analysis of Generalization Error, Linear Regression under Covariate Shift, Model Selection, Active Learning, Active Learning with Model Selection, Conclusions","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279354.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Bayesian Estimation","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","341","361","This chapter contains sections titled: 14.1 Introduction, 14.2 Estimating the Parameter of a Distribution, 14.3 Bayesian Estimation of the Parameters of a function, 14.4 Gaussian Processes, 14.5 Notes, 14.6 Exercises, 14.7 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284956.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Front Matter","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","i","xv","This chapter contains sections titled: Half Title, Neural Information Processing Series, Title, Copyright, Contents, Series Foreword, Preface","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279351.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Back Matter","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","263","263","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279495.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Further Issues and Conclusions","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","189","198","This chapter contains sections titled: multiple Outputs, Noise Models with Dependencies, Non-Gaussian Likelihoods, Derivative Observations, Prediction with Uncertain Inputs, Mixtures of Gaussian Processes, Global Optimization, Evaluation of Integrals, Student's t Process, Invariances, Latent Variable Models, Conclusions and Future Directions","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279025.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Parametric Methods","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","61","86","This chapter contains sections titled: 4.1 Introduction, 4.2 Maximum Likelihood Estimation, 4.3 Evaluating an Estimator: Bias and Variance, 4.4 The Bayes' Estimator, 4.5 Parametric Classification, 4.6 Regression, 4.7 Tuning Model Complexity: Bias/Variance Dilemma, 4.8 Model Selection Procedures, 4.9 Notes, 4.10 Exercises, 4.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284948.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Discriminative Learning under Covariate Shift with a Single Optimization Problem","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","131","160","This chapter contains sections titled: Introduction, Sample Reweighting, Distribution Matching, Risk Estimates, The Connection to Single Class Support Vector Machines, Experiments, Conclusion, Appendix: Proofs","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279352.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Multilayer Perceptrons","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","233","277","This chapter contains sections titled: 11.1 Introduction, 11.2 The Perceptron, 11.3 Training a Perceptron, 11.4 Learning Boolean Functions, 11.5 Multilayer Perceptrons, 11.6 MLP as a Universal Approximator, 11.7 Backpropagation Algorithm, 11.8 Training Procedures, 11.9 Tuning the Network Size, 11.10 Bayesian View of Learning, 11.11 Dimensionality Reduction, 11.12 Learning Time, 11.13 Notes, 11.14 Exercises, 11.15 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284959.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Front Matter","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","i","xxiii","This chapter contains sections titled: Half Title, Adaptive Computation and Machine Learning, Title, Copyright, Contents, Series Foreword, Preface","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270047.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Author Index","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","239","243","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279035.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Graphical Models","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","387","418","This chapter contains sections titled: 16.1 Introduction, 16.2 Canonical Cases for Conditional Independence, 16.3 Example Graphical Models, 16.4 d-Separation, 16.5 Belief Propagation, 16.6 Undirected Graphs: Markov Random Fields, 16.7 Learning the Structure of a Graphical Model, 16.8 Influence Diagrams, 16.9 Notes, 16.10 Exercises, 16.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284954.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"References","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","592","616","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282652.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"When Training and Test Sets Are Different: Characterizing Learning Transfer","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","3","28","This chapter contains sections titled: Introduction, Conditional and Generative Models, Real-Life Reasons for Dataset Shift, Simple Covariate Shift, Prior Probability Shift, Sample Selection Bias, Imbalanced Data, Domain Shift, Source Component Shift, Gaussian Process Methods for Dataset Shift, Shift or No Shift?, Dataset Shift and Transfer Learning, Conclusions","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279360.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Introduction","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","1","1","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278038.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Symbols and Abbreviations","P. Baldi; S. Brunak","","Bioinformatics, second edition:The Machine Learning Approach","20120904","2001","","","399","407","An unprecedented wealth of data is being generated by genome sequencing projects and other experimental efforts to determine the structure and function of biological molecules. The demands and opportunities for interpreting these data are expanding rapidly. Bioinformatics is the development and application of computer methods for management, analysis, interpretation, and prediction, as well as for the design of experiments. Machine learning approaches (e.g., neural networks, hidden Markov models, and belief networks) are ideally suited for areas where there is a lot of data but little theory, which is the situation in molecular biology. The goal in machine learning is to extract useful information from a body of data by building good probabilistic models--and to automate the process as much as possible.In this book Pierre Baldi and SÃ¿ren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.This new second edition contains expanded coverage of probabilistic graphical models and of the applications of neural networks, as well as a new chapter on microarrays and gene expression. The entire text has been extensively revised.","","97802622557","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6270032.pdf&bkn=6267217&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"A Statistical Machine Translation Primer","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","1","37","This first chapter is a short introduction to the main aspects of statistical machine translation (SMT). In particular, we cover the issues of automatic evaluation of machine translation output, language modeling, word-based and phrase-based translation models, and the use of syntax in machine translation. We will also do a quick roundup of some more recent directions that we believe may gain importance in the future. We situate statistical machine translation in the general context of machine learning research, and put the emphasis on similarities and differences with standard machine learning problems and practice.","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279396.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Regression","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","7","31","This chapter contains sections titled: Weight-space View, Function-space View, Varying the Hyperparameters, Decision Theory for Regression, An Example Application, Smoothing, Weight Functions and Equivalent Kernels, Incorporating Explicit Basis Functions, History and Related Work, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279032.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Introduction","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","1","19","This chapter contains sections titled: 1.1 What Is Machine Learning?, 1.2 Examples of Machine Learning Applications, 1.3 Notes, 1.4 Relevant Resources, 1.5 Exercises, 1.6 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284961.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Kernel Methods","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","405","406","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282653.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Gaussian Markov Processes","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","207","219","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279037.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Bibliography","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","267","277","No abstract.","","97811184379","10.1002/9781118437957.biblio","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331071.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Dimensionality Reduction","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","109","141","This chapter contains sections titled: 6.1 Introduction, 6.2 Subset Selection, 6.3 Principal Components Analysis, 6.4 Factor Analysis, 6.5 Multidimensional Scaling, 6.6 Linear Discriminant Analysis, 6.7 Isomap, 6.8 Locally Linear Embedding, 6.9 Notes, 6.10 Exercises, 6.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284946.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"On Bayesian Transduction: Implications for the Covariate Shift Problem","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","65","72","This chapter contains sections titled: Introduction, Generalization Optimal Least Squares Predictions, Bayesian Transduction, Bayesian Semisupervised Learning, Implications for Covariate Shift and Dataset Shift, Learning Transfer under Covariate and Dataset Shift: An Example, Conclusion","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279357.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Index","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","313","316","The Internet gives us access to a wealth of information in languages we don't understand. The investigation of automated or semi-automated approaches to translation has become a thriving research field with enormous commercial potential. This volume investigates how Machine Learning techniques can improve Statistical Machine Translation, currently at the forefront of research in the field. The book looks first at enabling technologies--technologies that solve problems that are not Machine Translation proper but are linked closely to the development of a Machine Translation system. These include the acquisition of bilingual sentence-aligned data from comparable corpora, automatic construction of multilingual name dictionaries, and word alignment. The book then presents new or improved statistical Machine Translation techniques, including a discriminative training framework for leveraging syntactic information, the use of semi-supervised and kernel-based learning methods, and the combination of multiple Machine Translation outputs in order to improve overall translation quality.ContributorsSrinivas Bangalore, Nicola Cancedda, Josep M. Crego, Marc Dymetman, Jakob Elming, George Foster, JesÃºs GimÃ©nez, Cyril Goutte, Nizar Habash, Gholamreza Haffari, Patrick Haffner, Hitoshi Isahara, Stephan Kanthak, Alexandre Klementiev, Gregor Leusch, Pierre MahÃ©, LluÃ­s MÃ rquez, Evgeny Matusov, I. Dan Melamed, Ion Muslea, Hermann Ney, Bruno Pouliquen, Dan Roth, Anoop Sarkar, John Shawe-Taylor, Ralf Steinberger, Joseph Turian, Nicola Ueffing, Masao Utiyama, Zhuoran Wang, Benjamin Wellington, Kenji Yamada","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279382.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Decision Trees","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","185","208","This chapter contains sections titled: 9.1 Introduction, 9.2 Univariate Trees, 9.3 Pruning, 9.4 Rule Extraction from Trees, 9.5 Learning Rules from Data, 9.6 Multivariate Trees, 9.7 Notes, 9.8 Exercises, 9.9 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284943.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Front Matter","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","i","xviii","This chapter contains sections titled: Half Title, Adaptive Computation and Machine Learning, Title, Copyright, Dedication, Contents, Series Foreword, Symbols and Notation","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279024.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Support Vector Machines","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","187","188","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282654.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Discussion","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","199","199","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279345.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"References","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","207","218","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279344.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Contributors","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","223","225","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279560.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Relationships between GPs and Other Models","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","129","150","This chapter contains sections titled: Reproducing Kernel Hilbert Spaces, Regularization, Spline Models, Support Vector Machines, Least-Squares Classification, Relevance Vector Machines, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279028.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Geometry of Covariate Shift with Applications to Active Learning","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","87","105","This chapter contains sections titled: Introduction, Statistical Inference under Covariate Shift, Information Criterion for Weighted Estimator, Active Learning and Covariate Shift, Pool-Based Active Leaning, Information Geometry of Active Learning, Conclusions","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279355.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Steganography and Steganalysis","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","7","24","This chapter contains sections titled: <br> Cryptography versus Steganography <br> Steganography <br> Steganalysis <br> Summary and Notes","","97811184379","10.1002/9781118437957.ch2","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331073.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Kernel Machines","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","309","340","This chapter contains sections titled: 13.1 Introduction, 13.2 Optimal Separating Hyperplane, 13.3 The Nonseparable Case: Soft Margin Hyperplane, 13.4 v-SVM, 13.5 Kernel Trick, 13.6 Vectorial Kernels, 13.7 Defining Kernels, 13.8 Multiple Kernel Learning, 13.9 Multiclass Kernel Machines, 13.10 Kernel Machines for Regression, 13.11 One-Class Kernel Machines, 13.12 Kernel Dimensionality Reduction, 13.13 Notes, 13.14 Exercises, 13.15 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284957.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"The Wavelets Domain","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","89","106","This chapter contains sections titled: <br> A Visual View <br> The Wavelet Domain <br> Farid's Features <br> HCF in the Wavelet Domain <br> Denoising and the WAM Features <br> Experiment and Comparison","","97811184379","10.1002/9781118437957.ch7","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331078.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Approximation Methods for Large Datasets","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","171","188","This chapter contains sections titled: Reduced-rank Approximations of the Gram Matrix, Greedy Approximation, Approximations for GPR with Fixed Hyperparameters, Approximations for GPC with Fixed Hyperparameters, Approximating the Marginal Likelihood and its Derivatives, Appendix: Equivalence of SR and GPR using the Nyström Approximate Kernel, Exercises","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279026.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Covariate Shift by Kernel Mean Matching","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","131","160","This chapter contains sections titled: Introduction, Sample Reweighting, Distribution Matching, Risk Estimates, The Connection to Single Class Support Vector Machines, Experiments, Conclusion, Appendix: Proofs","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279353.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Histogram Analysis","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","43","63","This chapter contains sections titled: <br> Early Histogram Analysis <br> Notation <br> Additive Independent Noise <br> Multi-dimensional Histograms <br> Experiment and Comparison","","97811184379","10.1002/9781118437957.ch4","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331075.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Hidden Markov Models","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","363","385","This chapter contains sections titled: 15.1 Introduction, 15.2 Discrete Markov Processes, 15.3 Hidden Markov Models, 15.4 Three Basic Problems of HMMs, 15.5 Evaluation Problem, 15.6 Finding the State Sequence, 15.7 Learning Model Parameters, 15.8 Continuous Observations, 15.9 The HMM with Input, 15.10 Model Selection in HMM, 15.11 Notes, 15.12 Exercises, 15.13 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284955.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Machine Translation","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","129","129","The Internet gives us access to a wealth of information in languages we don't understand. The investigation of automated or semi-automated approaches to translation has become a thriving research field with enormous commercial potential. This volume investigates how Machine Learning techniques can improve Statistical Machine Translation, currently at the forefront of research in the field. The book looks first at enabling technologies--technologies that solve problems that are not Machine Translation proper but are linked closely to the development of a Machine Translation system. These include the acquisition of bilingual sentence-aligned data from comparable corpora, automatic construction of multilingual name dictionaries, and word alignment. The book then presents new or improved statistical Machine Translation techniques, including a discriminative training framework for leveraging syntactic information, the use of semi-supervised and kernel-based learning methods, and the combination of multiple Machine Translation outputs in order to improve overall translation quality.ContributorsSrinivas Bangalore, Nicola Cancedda, Josep M. Crego, Marc Dymetman, Jakob Elming, George Foster, JesÃºs GimÃ©nez, Cyril Goutte, Nizar Habash, Gholamreza Haffari, Patrick Haffner, Hitoshi Isahara, Stephan Kanthak, Alexandre Klementiev, Gregor Leusch, Pierre MahÃ©, LluÃ­s MÃ rquez, Evgeny Matusov, I. Dan Melamed, Ion Muslea, Hermann Ney, Bruno Pouliquen, Dan Roth, Anoop Sarkar, John Shawe-Taylor, Ralf Steinberger, Joseph Turian, Nicola Ueffing, Masao Utiyama, Zhuoran Wang, Benjamin Wellington, Kenji Yamada","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279380.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Learning Causing Covariate Shift","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","181","181","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278036.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Introduction to Dataset Shift","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","1","1","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279348.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Reinforcement Learning","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","447","474","This chapter contains sections titled: 18.1 Introduction, 18.2 Single State Case: K-Armed Bandit, 18.3 Elements of Reinforcement Learning, 18.4 Model-Based Learning, 18.5 Temporal Difference Learning, 18.6 Generalization, 18.7 Partially Observable States, 18.8 Notes, 18.9 Exercises, 18.10 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284952.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Front Matter","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","i","xiv","This chapter contains sections titled: Half Title, Adaptive Computation and Machine Learning, Title, Copyright, Contents, Foreword, Preface","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278040.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Conclusions","M. Sugiyama; M. Kawanabe","","Machine Learning in Non-Stationary Environments:Introduction to Covariate Shift Adaptation","20120904","2012","","","239","239","As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.","","97802623012","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6278035.pdf&bkn=6267539&pdfType=chapter","","","","","","","","","2012","","","","MIT Press","MIT Press eBook Chapters"
"Calibration Techniques","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","131","149","This chapter contains sections titled: <br> Calibrated Features <br> JPEG Calibration <br> Calibration by Downsampling <br> Calibration in General <br> Progressive Randomisation","","97811184379","10.1002/9781118437957.ch9","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331080.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Front Matter","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","i","xl","This chapter contains sections titled: Half Title, Adaptive Computation and Machine Learning, Title, Copyright, Brief Contents, Contents, Series Foreword, Figures, Tables, Preface, Acknowledgments, Notes for the Second Edition, Notations","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284942.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Mathematical Background","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","199","206","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279038.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Future of the Field","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","263","266","This chapter contains sections titled: <br> Image Forensics <br> Conclusions and Notes","","97811184379","10.1002/9781118437957.ch15","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331086.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Multivariate Methods","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","87","107","This chapter contains sections titled: 5.1 Multivariate Data, 5.2 Parameter Estimation, 5.3 Estimation of Missing Values, 5.4 Multivariate Normal Distribution, 5.5 Multivariate Classification, 5.6 Tuning Complexity, 5.7 Discrete Features, 5.8 Multivariate Regression, 5.9 Notes, 5.10 Exercises, 5.11 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284947.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Concepts and Tools","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","23","24","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282655.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Binary Classification under Sample Selection Bias","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","41","64","This chapter contains sections titled: Introduction, Model for Sample Selection Bias, Necessary and Sufficient Conditions for the Equivalence of the Bayes Classifier, Bounding the Selection Index via Unlabeled Data, Classifiers of Small and Large Capacity, A Nonparametric Framework for General Sample Selection Bias Using Adaptive Regularization, Experiments, Conclusion","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279562.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Contributors","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","307","311","The Internet gives us access to a wealth of information in languages we don't understand. The investigation of automated or semi-automated approaches to translation has become a thriving research field with enormous commercial potential. This volume investigates how Machine Learning techniques can improve Statistical Machine Translation, currently at the forefront of research in the field. The book looks first at enabling technologies--technologies that solve problems that are not Machine Translation proper but are linked closely to the development of a Machine Translation system. These include the acquisition of bilingual sentence-aligned data from comparable corpora, automatic construction of multilingual name dictionaries, and word alignment. The book then presents new or improved statistical Machine Translation techniques, including a discriminative training framework for leveraging syntactic information, the use of semi-supervised and kernel-based learning methods, and the combination of multiple Machine Translation outputs in order to improve overall translation quality.ContributorsSrinivas Bangalore, Nicola Cancedda, Josep M. Crego, Marc Dymetman, Jakob Elming, George Foster, JesÃºs GimÃ©nez, Cyril Goutte, Nizar Habash, Gholamreza Haffari, Patrick Haffner, Hitoshi Isahara, Stephan Kanthak, Alexandre Klementiev, Gregor Leusch, Pierre MahÃ©, LluÃ­s MÃ rquez, Evgeny Matusov, I. Dan Melamed, Ion Muslea, Hermann Ney, Bruno Pouliquen, Dan Roth, Anoop Sarkar, John Shawe-Taylor, Ralf Steinberger, Joseph Turian, Nicola Ueffing, Masao Utiyama, Zhuoran Wang, Benjamin Wellington, Kenji Yamada","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279384.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Back Matter","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","538","539","The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. The second edition of Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284963.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"An Adversarial View of Covariate Shift and a Minimax Approach","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","179","197","Building Robust Classifiers, Minimax Problem Formulation, Finding the Minimax Optimal Features, A Convex Dual for the Minimax Problem, An Alternate Setting: Uniform Feature Deletion, Related Frameworks, Experiments, Discussion and Conclusions","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279561.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Introduction","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","1","6","This chapter contains sections titled: <br> Real Threat or Hype? <br> Artificial Intelligence and Learning <br> How to Read this Book","","97811184379","10.1002/9781118437957.ch1","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331072.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Local Models","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","279","308","This chapter contains sections titled: 12.1 Introduction, 12.2 Competitive Learning, 12.3 Radial Basis Functions, 12.4 Incorporating Rule-Based Knowledge, 12.5 Normalized Basis Functions, 12.6 Competitive Basis Functions, 12.7 Learning Vector Quantization, 12.8 Mixture of Experts, 12.9 Hierarchical Mixture of Experts, 12.10 Notes, 12.11 Exercises, 12.12 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284958.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"More Spatial Domain Features","H. G. Schaathun","","Machine Learning in Image Steganalysis","20121024","2012","","","75","87","This chapter contains sections titled: <br> The Difference Matrix <br> Image Quality Measures <br> Colour Images <br> Experiment and Comparison","","97811184379","10.1002/9781118437957.ch6","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6331077.pdf&bkn=6331046&pdfType=chapter","","","","","","","","","2012","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Subject Index","C. E. Rasmussen; C. K. I. Williams","","Gaussian Processes for Machine Learning","20120924","2005","","","244","247","Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics.The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.","","97802622568","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279023.pdf&bkn=6267323&pdfType=chapter","","","","","","","","","2005","","","","MIT Press","MIT Press eBook Chapters"
"Combining Multiple Learners","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","419","445","This chapter contains sections titled: 17.1 Rationale, 17.2 Generating Diverse Learners, 17.3 Model Combination Schemes, 17.4 Voting, 17.5 Error-Correcting Output Codes, 17.6 Bagging, 17.7 Boosting, 17.8 Mixture of Experts Revisited, 17.9 Stacked Generalization, 17.10 Fine-Tuning an Ensemble, 17.11 Cascading, 17.12 Notes, 17.13 Exercises, 17.14 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284953.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Notation and Symbols","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","625","626","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282656.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Theoretical Views on Dataset and Covariate Shift","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","39","39","Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors [cut for catalog if necessary]Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael BrÃ¿ckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui Teo, Takafumi Kanamori, Klaus-Robert MÃ¿ller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard SchÃ¶lkopf Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279347.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"References","C. Goutte; N. Cancedda; M. Dymetman; G. Foster","","Learning Machine Translation","20120924","2009","","","277","306","The Internet gives us access to a wealth of information in languages we don't understand. The investigation of automated or semi-automated approaches to translation has become a thriving research field with enormous commercial potential. This volume investigates how Machine Learning techniques can improve Statistical Machine Translation, currently at the forefront of research in the field. The book looks first at enabling technologies--technologies that solve problems that are not Machine Translation proper but are linked closely to the development of a Machine Translation system. These include the acquisition of bilingual sentence-aligned data from comparable corpora, automatic construction of multilingual name dictionaries, and word alignment. The book then presents new or improved statistical Machine Translation techniques, including a discriminative training framework for leveraging syntactic information, the use of semi-supervised and kernel-based learning methods, and the combination of multiple Machine Translation outputs in order to improve overall translation quality.ContributorsSrinivas Bangalore, Nicola Cancedda, Josep M. Crego, Marc Dymetman, Jakob Elming, George Foster, JesÃºs GimÃ©nez, Cyril Goutte, Nizar Habash, Gholamreza Haffari, Patrick Haffner, Hitoshi Isahara, Stephan Kanthak, Alexandre Klementiev, Gregor Leusch, Pierre MahÃ©, LluÃ­s MÃ rquez, Evgeny Matusov, I. Dan Melamed, Ion Muslea, Hermann Ney, Bruno Pouliquen, Dan Roth, Anoop Sarkar, John Shawe-Taylor, Ralf Steinberger, Joseph Turian, Nicola Ueffing, Masao Utiyama, Zhuoran Wang, Benjamin Wellington, Kenji Yamada","","97802622550","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279379.pdf&bkn=6267198&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Index","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","529","537","The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. The second edition of Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284941.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
"Index","B. Schölkopf; A. J. Smola","","Learning with Kernels:Support Vector Machines, Regularization, Optimization, and Beyond","20120924","2001","","","617","624","In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs -- -kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.","","97802622569","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6282658.pdf&bkn=6267332&pdfType=chapter","","","","","","","","","2001","","","","MIT Press","MIT Press eBook Chapters"
"Projection and Projectability","J. Quiñonero-Candela; M. Sugiyama; A. Schwaighofer; N. D. Lawrence","","Dataset Shift in Machine Learning","20120924","2009","","","29","38","This chapter contains sections titled: Introduction, Data and Its Distributions, Data Attributes and Projection, The New Riddle of Induction, Natural Kinds and Causes, Machine Learning, Conclusion","","97802622551","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6279358.pdf&bkn=6267199&pdfType=chapter","","","","","","","","","2009","","","","MIT Press","MIT Press eBook Chapters"
"Probability","E. Alpaydin","","Introduction to Machine Learning","20120924","2010","","","517","527","This chapter contains sections titled: A.1 Elements of Probability, A.2 Random Variables, A.3 Special Random Variables, A.4 References","","97802622670","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=6284964.pdf&bkn=6267367&pdfType=chapter","","","","","","","","","2010","","","","MIT Press","MIT Press eBook Chapters"
