"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6819238,6818488,6819237,6818791,6819140,6818766,6796914,6817944,6816427,6817936,6796437,6797627,6789163,6797397,6792514,6788938,6792369,6570512,6815206,6741541,6810420,6809335,6806033,6805715,6804508,6727447,6701351,6798945,6799098,6787254,6786762,6784605,6786079,6786142,6784636,6785439,6786081,6786103,6786087,6784606,6784592,6784632,6784594,6783327,6782960,6782561,6782554,6781387,6682887,6754764,6779503,6779289,6779514,6778339,6712138,6645367,6775877,6632881,6761586,6615928,6758529,6758697,6758930,6759171,6760837,6761165,6755850,6756290,6757532,6755716,6755296,6753954,6750202,6742888,6745453,6742890,6740235,6740992,6547983,6671378,6738088,6738324,6737950,6736684,6736326,6735969,6735899,6735264,6731669,6733226,6732733,6732588,6732048,6733057,6730683,6730744,6729619,6726074,6726818,6725908",2017/05/04 23:34:40
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Sentiment analysis in twitter using machine learning techniques","M. S. Neethu; R. Rajasree","Dept. of Comput. Sci. & Eng., Coll. of Eng., Trivandrum, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20140130","2013","","","1","5","Sentiment analysis deals with identifying and classifying opinions or sentiments expressed in source text. Social media is generating a vast amount of sentiment rich data in the form of tweets, status updates, blog posts etc. Sentiment analysis of this user generated data is very useful in knowing the opinion of the crowd. Twitter sentiment analysis is difficult compared to general sentiment analysis due to the presence of slang words and misspellings. The maximum limit of characters that are allowed in Twitter is 140. Knowledge base approach and Machine learning approach are the two strategies used for analyzing sentiments from the text. In this paper, we try to analyze the twitter posts about electronic products like mobiles, laptops etc using Machine Learning approach. By doing sentiment analysis in a specific domain, it is possible to identify the effect of domain information in sentiment classification. We present a new feature vector for classifying the tweets as positive, negative and extract peoples' opinion about products.","","Electronic:978-1-4799-3926-8; POD:978-1-4799-3927-5","10.1109/ICCCNT.2013.6726818","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726818","Machine Learning Techniques;Sentiment Analysis;Twitter","Entropy;Feature extraction;Speech;Support vector machines;Training;Twitter;Vectors","information retrieval;knowledge based systems;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis","Twitter sentiment analysis;crowd opinion extraction;domain information;electronic products;feature vector;knowledge base approach;machine learning techniques;opinion classification;opinion identification;sentiment analysis;sentiment classification;sentiment identification;sentiment rich data;social media;source text","","15","1","17","","","4-6 July 2013","","IEEE","IEEE Conference Publications"
"Detection of phishing URLs using machine learning techniques","J. James; Sandhya L.; C. Thomas","SCT Coll. of Eng., Trivandrum, India","2013 International Conference on Control Communication and Computing (ICCC)","20140206","2013","","","304","309","Phishing costs Internet users billions of dollars per year. It refers to luring techniques used by identity thieves to fish for personal information in a pond of unsuspecting Internet users. Phishers use spoofed e-mail, phishing software to steal personal information and financial account details such as usernames and passwords. This paper deals with methods for detecting phishing Web sites by analyzing various features of benign and phishing URLs by Machine learning techniques. We discuss the methods used for detection of phishing Web sites based on lexical features, host properties and page importance properties. We consider various data mining algorithms for evaluation of the features in order to get a better understanding of the structure of URLs that spread phishing. The fine-tuned parameters are useful in selecting the apt machine learning algorithm for separating the phishing sites from benign sites.","","Electronic:978-1-4799-0575-1; POD:978-1-4799-0574-4","10.1109/ICCC.2013.6731669","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6731669","Page rank;Phishing;URL;WHOIS;benign","Classification algorithms;Electronic mail;Feature extraction;Google;Internet;MATLAB;Web pages","Web sites;authorisation;data mining;learning (artificial intelligence);unsolicited e-mail","Internet users;benign sites;data mining algorithms;financial account stealing;host properties;lexical feature analysis;machine learning algorithm selection;page importance properties;passwords;personal information stealing;phishing URL detection;phishing Web site detection;phishing software;spoofed e-mail;usernames","","4","","24","","","13-15 Dec. 2013","","IEEE","IEEE Conference Publications"
"Applying Machine Learning and Audio Analysis Techniques to Insect Recognition in Intelligent Traps","D. F. Silva; V. M. A. D. Souza; G. E. A. P. A. Batista; E. Keogh; D. P. W. Ellis","ICMC, USP - Sao Carlos, Sao Carlos, Brazil","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","99","104","Throughout the history, insects have had an intimate relationship with humanity, both positive and negative. Insects are vectors of diseases that kill millions of people every year and, at the same time, insects pollinate most of the world's food production. Consequently, there is a demand for new devices able to control the populations of harmful insects while having a minimal impact on beneficial insects. In this paper, we present an intelligent trap that uses a laser sensor to selectively classify and catch insects. We perform an extensive evaluation of different feature sets from audio analysis and machine learning algorithms to construct accurate classifiers for the insect classification task. Support Vector Machines achieved the best results with a MFCC feature set, which consists of coefficients from frequencies scaled according to the human auditory system. We evaluate our classifiers in multiclass and binary class settings, and show that a binary class classifier that recognizes the mosquito species achieved almost perfect accuracy, assuring the applicability of the proposed intelligent trap.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784594","feature extraction;insect classification;intelligent trap;sensor data","Accuracy;Feature extraction;Insects;Mel frequency cepstral coefficient;Radio frequency;Support vector machines;Vectors","diseases;learning (artificial intelligence);optical sensors;pattern classification;pest control;support vector machines","MFCC feature set;audio analysis techniques;beneficial insects;binary class classifier;disease vectors;food production;harmful insects;humanity;insect classification task;insect recognition;intelligent traps;laser sensor;machine learning;support vector machines","","3","","22","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"A machine learning approach for linux malware detection","K. A. Asmitha; P. Vinod","Dept. of Comput. Sci. & Eng., SCMS Sch. of Eng. & Technol., Ernakulam, India","2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT)","20140403","2014","","","825","830","The increasing number of malware is becoming a serious threat to the private data as well as to the expensive computer resources. Linux is a Unix based machine and gained popularity in recent years. The malware attack targeting Linux has been increased recently and the existing malware detection methods are insufficient to detect malware efficiently. We are introducing a novel approach using machine learning for identifying malicious Executable Linkable Files. The system calls are extracted dynamically using system call tracer Strace. In this approach we identified best feature set of benign and malware specimens to built classification model that can classify malware and benign efficiently. The experimental results are promising which depict a classification accuracy of 97% to identify malicious samples.","","DVD:978-1-4799-2899-6; Electronic:978-1-4799-2900-9; POD:978-1-4799-2901-6","10.1109/ICICICT.2014.6781387","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781387","dynamic analysis;feature selection;system call","Accuracy;Malware;Testing","Linux;invasive software;learning (artificial intelligence);pattern classification","Linux malware detection;Unix based machine;benign specimens;classification model;machine learning approach;malicious executable linkable files identification;malware specimens;system call tracer Strace","","4","","17","","","7-8 Feb. 2014","","IEEE","IEEE Conference Publications"
"Nonlinear Appraisal Modeling: An Application of Machine Learning to the Study of Emotion Production","B. Meuleman; K. R. Scherer","Swiss Center for Affective Sci., Univ. of Geneva, Geneva, Switzerland","IEEE Transactions on Affective Computing","20140320","2013","4","4","398","411","Appraisal theory of emotion claims that emotions are not caused by “raw” stimuli, as such, but by the subjective evaluation (appraisal) of those stimuli. Studies that analyzed this relation have been dominated by linear models of analysis. These methods are not ideally suited to examine a basic assumption of many appraisal theories, which is that appraisal criteria interact to differentiate emotions, and hence show nonlinear effects. Studies that did model interactions were either limited in scope or exclusively theory-driven simulation attempts. In the present study, we improve on these approaches using data-driven methods from the field of machine learning. We modeled a categorical emotion response as a function of 25 appraisal predictors, using a large data set on recalled emotion experiences (5,901 cases). A systematic comparison of machine learning models on these data supported the interactive nature of the appraisal-emotion relationship, with the best nonlinear model significantly outperforming the best linear model. The interaction structure was found to be moderately hierarchical. Strong main effects of intrinsic valence and goal compatibility appraisal differentiated positive from negative emotions, while more specific emotions (e.g., pride, irritation, despair) were differentiated by interactions involving agency appraisal and norm appraisal.","1949-3045;19493045","","10.1109/T-AFFC.2013.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645367","Emotion;appraisal theory;interactions;machine learning;modeling","Appraisal;Computational modeling;Data analysis;Decision support systems;Handheld computers;Software","behavioural sciences computing;learning (artificial intelligence)","agency appraisal;appraisal criteria;appraisal-emotion relationship;categorical emotion response;emotion experiences;emotion production;goal compatibility appraisal;intrinsic valence;machine learning;negative emotions;nonlinear appraisal modeling;nonlinear effects;norm appraisal;positive emotions;theory-driven simulation attempts","","5","","42","","20131023","Oct.-Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Machine Learning-Based Runtime Scheduler for Mobile Offloading Framework","H. Eom; P. S. Juste; R. Figueiredo; O. Tickoo; R. Illikkal; R. Iyer","Adv. Comput. & Inf. Syst. Lab., Univ. of Florida, Gainesville, FL, USA","2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing","20140505","2013","","","17","25","Remote offloading techniques have been proposed to overcome the limited resources of mobile platforms by leveraging external powerful resources such as personal work-stations or cloud servers. Prior studies have primarily focused on core mechanisms for offloading. Yet, adaptive scheduling in such systems is important because offloading effectiveness can be influenced by varying network conditions, workload requirements, and load at the target device. In this paper, we present a study on the feasibility of applying machine learning techniques to address the adaptive scheduling problem in mobile offloading framework. The study considers 19 different machine learning algorithms and four workloads, with a dataset obtained through the deployment of an Android-based remote offloading framework prototype on actual mobile and cloud resources. From this set, a subset of machine learning algorithms, which have relatively high scheduling accuracy, is selected to implement an offline offloading scheduler. Finally, by taking computational cost and the scheduling performance into account, we use Instance-Based Learning to evaluate an online adaptive scheduler for mobile offloading. In our evaluation, we observe that an Instance Learning-based online offloading scheduler selects the best scheduling decision in 87.5% instances, in an experiment setup in which an image processing workload is offloaded while subject to varying network bandwidth conditions and the amount of data transfer.","","Electronic:978-0-7695-5152-4; POD:978-1-4799-2574-2","10.1109/UCC.2013.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6809335","cloud;energy consumption;machine learning;mobile platform;offloading;scheduling","Bandwidth;Mobile communication;Mobile computing;Processor scheduling;Runtime;Scheduling;Servers","Android (operating system);learning (artificial intelligence);mobile computing;scheduling","Android-based remote offloading framework prototype;adaptive scheduling problem;cloud resource;cloud servers;computational cost;data transfer;external powerful resources;image processing workload;instance learning-based online offloading scheduler;instance-based learning;machine learning;mobile offloading framework;mobile platforms;mobile resource;offline offloading scheduler;offloading effectiveness;online adaptive scheduler evaluation;personal work-stations;runtime scheduler;scheduling performance","","7","","21","","","9-12 Dec. 2013","","IEEE","IEEE Conference Publications"
"Machine-learning methodology for energy efficient routing","M. Masikos; K. Demestichas; E. Adamopoulou; M. Theologou","Inst. of Commun. & Comput. Syst., Athens, Greece","IET Intelligent Transport Systems","20140522","2014","8","3","255","265","Eco-driving assistance systems encourage economical driving behaviour and support the driver in optimising his/her driving style to achieve fuel economy and consequently, emission reductions. Energy efficiency is also one of the most pertinent issues related to the autonomy of fully electric vehicles. This study introduces a novel methodology for energy efficient routing, based on the realisation of dependable energy consumption predictions for the various road segments constituting an actual or potential vehicle route, performed mainly by means of machine-learning functionality. This proposed innovative methodology, the functional architecture implementing it, as well as demonstrative experimental results are presented in this study.","1751-956X;1751956X","","10.1049/iet-its.2013.0006","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818488","","","electric vehicles;electrical engineering computing;energy conservation;learning (artificial intelligence);road traffic;road vehicles;traffic engineering computing","ecodriving assistance systems;economical driving behaviour;electric vehicles;emission reductions;energy consumption predictions;energy efficient routing;fuel economy;machine learning functionality;machine learning methodology;road segments;vehicle route","","1","","","","","May 2014","","IET","IET Journals & Magazines"
"An Evaluation of Machine Learning Methods to Detect Malicious SCADA Communications","J. M. Beaver; R. C. Borges-Hink; M. A. Buckner","Oak Ridge Nat. Lab., Oak Ridge, TN, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","54","59","Critical infrastructure Supervisory Control and Data Acquisition (SCADA) systems have been designed to operate on closed, proprietary networks where a malicious insider posed the greatest threat potential. The centralization of control and the movement towards open systems and standards has improved the efficiency of industrial control, but has also exposed legacy SCADA systems to security threats that they were not designed to mitigate. This work explores the viability of machine learning methods in detecting the new threat scenarios of command and data injection. Similar to network intrusion detection systems in the cyber security domain, the command and control communications in a critical infrastructure setting are monitored, and vetted against examples of benign and malicious command traffic, in order to identify potential attack events. Multiple learning methods are evaluated using a dataset of Remote Terminal Unit communications, which included both normal operations and instances of command and data injection attack scenarios.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.105","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786081","SCADA;critical infrastructure protection;intrusion detection;machine learning;network","Intrusion detection;Learning systems;Machine learning algorithms;Pipelines;SCADA systems;Telemetry","SCADA systems;computer network security;critical infrastructures;industrial control;learning (artificial intelligence);open systems","command and control communication;critical infrastructure monitoring;critical infrastructure systems;cyber security domain;data injection attack;industrial control;machine learning method;malicious SCADA communication detection;network intrusion detection system;open standards;open systems;potential attack event identification;remote terminal unit communication;security threat potential;supervisory control and data acquisition","","5","","23","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Extreme Learning Machines [Trends & Controversies]","E. Cambria; G. B. Huang; L. L. C. Kasun; H. Zhou; C. M. Vong; J. Lin; J. Yin; Z. Cai; Q. Liu; K. Li; V. C. M. Leung; L. Feng; Y. S. Ong; M. H. Lim; A. Akusok; A. Lendasse; F. Corona; R. Nian; Y. Miche; P. Gastaldo; R. Zunino; S. Decherchi; X. Yang; K. Mao; B. S. Oh; J. Jeon; K. A. Toh; A. B. J. Teoh; J. Kim; H. Yu; Y. Chen; J. Liu","MIT Media Laboratory","IEEE Intelligent Systems","20140206","2013","28","6","30","59","This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In ""Representational Learning with ELMs for Big Data,"" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In ""A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,"" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In ""ELM-Guided Memetic Computation for Vehicle Routing,"" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In ""ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,"" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In ""Combining ELMs with Random Projections,"" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In ""Reduced ELMs for Causal Relation Extraction from Unstructured Text,"" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In ""A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,"" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry- for touchless applications without the need for handheld devices. Finally, in ""An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,"" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.","1541-1672;15411672","","10.1109/MIS.2013.140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733226","ELM;Kinect;OS-ELM;cloud computing;computation outsourcing;deep networks;ensemble;evolutionary optimization;extreme learning machine;hand gesture signature verification;human-computer interaction;knowledge extraction;memetic computation;meta meme;online sequential ELM;partitioned ELM;random permutations;random projections;representational learning;signature biometrics;text mining;visualization","Adaptive learning;Artificial intelligence;Big data;Biological neural networks;Data visualization;Gesture recognition;Learning systems;Machine learning;Nonhomogeneous media;Special issues and sections","","","","84","","4","","","Nov.-Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Automatic Grading of Computer Programs: A Machine Learning Approach","S. Srikant; V. Aggarwal","","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","85","92","The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a machine learning framework to automatically grade computer programs. We propose a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts on the basis of a rubric. We show that regression modeling based on the given features provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading. We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem. Our preliminary investigations in the same show promising results and suggest an implicit correlation of our features with the proposed grading-levels (rubric). To the best of the authors' knowledge, this is the first time machine learning has been applied to the problem of grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784592","Abstract Syntax Trees;Assessment of Computer Programs;MOOC;One-Class Modeling;Regression;Rubric","Abstracts;Computers;Context;Feature extraction;Grammar;Measurement;Programming","courseware;learning (artificial intelligence);program testing;regression analysis;ubiquitous computing","MOOC;abstract representations;automatic computer program evaluation;automatic computer-program grading;expression count features;extant program assessment system score;hand-graded digitized data;highly-informative features;keyword count features;machine learning framework;massively online open courseware;one-class modeling problem;open-response problems;program functionality;regression modeling;ubiquitous test-case-pass based grading","","0","","16","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Machine Learning in Virtualization: Estimate a Virtual Machine's Working Set Size","A. Melekhova","Parallels, Moscow, Russia","2013 IEEE Sixth International Conference on Cloud Computing","20140217","2013","","","863","870","Achieving high density of virtual machines on a node while maintaining their performance strongly depends on the correct calculation of a virtual machine's working set. Different strategies are applied to solve the problem. Some researchers interpret a virtual machine as an unpredictable memory consumer, while others try to introspect a guest OS's knowledge of memory pressure. This paper introduces a new approach to calculation of the working set size - regression analysis. The technique estimates the memory consumption using a set of virtualization events. In this investigation, we discuss a correlation between the working set size and virtualization events, demonstrate the applicability of the approach and state its limitations. The argued choice of mathematical instrumentation is given. The collecting of control and learning samples is described in details. The results of final evaluation demonstrate significant resource gain.","2159-6182;21596182","Electronic:978-0-7695-5028-2; POD:978-1-4799-0490-7","10.1109/CLOUD.2013.91","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740235","density;memory;regression;virtual machine;working set","Abstracts;Cloud computing;Conferences;Memory management;Regression analysis;Virtual machining;Virtualization","learning (artificial intelligence);regression analysis;virtual machines","machine learning;mathematical instrumentation;unpredictable memory consumer;virtual machines;virtualization events;working set size-regression analysis","","0","","22","","","June 28 2013-July 3 2013","","IEEE","IEEE Conference Publications"
"Toward a Semiautomatic Machine Learning Retrieval of Biophysical Parameters","J. P. R. Caicedo; J. Verrelst; J. Muñoz-Marí; J. Moreno; G. Camps-Valls","Image Process. Lab. (IPL), Univ. de Valencia, Vale&#x0300;ncia, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20140422","2014","7","4","1249","1259","Biophysical parameters such as leaf chlorophyll content (LCC) and leaf area index (LAI) are standard vegetation products that can be retrieved from Earth observation imagery. This paper introduces a new machine learning regression algorithms (MLRAs) toolbox into the scientific Automated Radiative Transfer Models Operator (ARTMO) software package. ARTMO facilitates retrieval of biophysical parameters from remote observations in a MATLAB graphical user interface (GUI) environment. The MLRA toolbox enables analyzing the predictive power of various MLRAs in a semiautomatic and systematic manner, and applying a selected MLRA to multispectral or hyperspectral imagery for mapping applications. It contains both linear and nonlinear state-of-the-art regression algorithms, in particular linear feature extraction via principal component regression (PCR), partial least squares regression (PLSR), decision trees (DTs), neural networks (NNs), kernel ridge regression (KRR), and Gaussian processes regression (GPR). The performance of multiple implemented regression strategies has been evaluated against the SPARC dataset (Barrax, Spain) and simulated Sentinel-2 (8 bands), CHRIS (62 bands) and HyMap (125 bands) observations. In general, nonlinear regression algorithms (NN, KRR, and GPR) outperformed linear techniques (PCR and PLSR) in terms of accuracy, bias, and robustness. Most robust results along gradients of training/validation partitioning and noise variance were obtained by KRR while GPR delivered most accurate estimations. We applied a GPR model to a hyperspectral HyMap flightline to map LCC and LAI. We exploited the associated uncertainty intervals to gain insight in the per-pixel performance of the model.","1939-1404;19391404","","10.1109/JSTARS.2014.2298752","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727447","Biophysical parameter retrieval;CHRIS;HyMap;Sentinel-2 (S2);graphical user interface (GUI) toolbox;leaf area index (LAI);leaf chlorophyll content (LCC);machine learning;nonparametric regression","Artificial neural networks;Biological system modeling;Graphical user interfaces;Ground penetrating radar;Kernel;Remote sensing;Training","Gaussian processes;decision trees;feature extraction;geophysical image processing;graphical user interfaces;hyperspectral imaging;learning (artificial intelligence);least squares approximations;mathematics computing;neural nets;principal component analysis;regression analysis;vegetation mapping","CHRIS observation;Gaussian processes regression;MATLAB graphical user interface;SPARC dataset;biophysical parameters;decision trees;hyperspectral HyMap flightline;hyperspectral imagery;kernel ridge regression;leaf area index;leaf chlorophyll content;linear feature extraction;machine learning regression algorithms;multispectral imagery;neural networks;noise variance;nonlinear regression algorithms;partial least squares regression;partitioning;principal component regression;scientific Automated Radiative Transfer Models Operator software package;semiautomatic machine learning retrieval;simulated Sentinel-2 observation;vegetation","","16","","58","","20140128","April 2014","","IEEE","IEEE Journals & Magazines"
"An adaptive machine learning on Map-Reduce framework for improving performance of large-scale data analysis on EC2","W. Romsaiyud; W. Premchaiswadi","Grad. Sch. of Inf. Technol., Siam Univ., Bangkok, Thailand","2013 Eleventh International Conference on ICT and Knowledge Engineering","20140306","2013","","","1","7","Map-Reduce is a programming for writing applications that rapidly process vast amounts of data in parallel on large cluster of computer nodes and can be deployed on cloud computing. However, to run a Map-Reduce job, many configuration parameters are required for tuning and improving the performance to set up such as number of running mappers and maximum number of reduce slots in the cluster in order to minimize the data transferred between map and reduce tasks. To say simple, the main emphasis is on reducing the job execution time as well as shuffling tweaks to tune parameters for memory management. In this paper, we introduce a machine learning model on top of Map-Reduce for automate setting of tuning parameters for Map-Reduce programs. Our model consists of three main steps; 1) describe the plan baseline marked for verification. 2) Propose a ML algorithm for learning and predicting the model, and 3) develop our automated method to run the program automatically at a specific time. In our experiments, we run Hadoop on 20-nodes cluster on EC2.","2157-0981;21570981","Electronic:978-1-4799-2295-6; POD:978-1-4799-2296-3","10.1109/ICTKE.2013.6756290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6756290","Amazon Web Services (AWS);Job Scheduling;Machine Learning;Map-Reduce;Performance Tuning","Computational modeling;File systems;Logistics;Machine learning algorithms;Optimization;Runtime;Tuning","cloud computing;data analysis;learning (artificial intelligence);parallel programming","EC2;Hadoop;ML algorithm;MapReduce framework;adaptive machine learning;cloud computing;computer nodes;configuration parameters;job execution time reduction;large-scale data analysis;memory management;reduce slots;running mappers;tuning parameters","","1","","27","","","20-22 Nov. 2013","","IEEE","IEEE Conference Publications"
"Automatic working area localization in blood smear microscopic images using machine learning algorithms","E. A. Mohammed; B. H. Far; M. M. A. Mohamed; C. Naugler","Schulich Sch. of Eng., Univ. of Calgary, Calgary, AB, Canada","2013 IEEE International Conference on Bioinformatics and Biomedicine","20140206","2013","","","43","50","Microscopic examination of a properly prepared blood smear is valuable in complete blood count (CBC) and differential blood count (DBC). A hematopathologist may spend enormous time manually inspecting the good working area (GWA) of the blood smear under a light microscope system to perform CBC or DBC. In this paper we focus on automatic localization of the GWA by classifying microscopic images of blood smears using different machine learning algorithms into three areas: Clumped, Good, and Sparse. The features used are the statistical and texture features. This approach yields a good localization of GWA in images acquired by a low cost light microscope system, scanned under magnifying power of x100 oil-immersed objective. Our experiment using images with resolution (3488×2616 pixels) of Giemsa-stained blood smears shows that the proposed method has an accuracy of 82% for the localizing the GWA and 79.73% for all areas in a validation set of 301 images.","","Electronic:978-1-4799-1309-1; POD:978-1-4799-1311-4","10.1109/BIBM.2013.6732733","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732733","AdaBoost;Blood Smear Examination;Decision tree;Good Working Area (GWA);KNN Classifier;Microscopic Images;White Blood Cells (WBCs)","Blood;Classification algorithms;Decision support systems;Decision trees;Feature extraction;Machine learning algorithms;Microscopy","biomedical optical imaging;blood;feature extraction;image classification;image resolution;image texture;learning (artificial intelligence);medical image processing;optical microscopy;statistical analysis","CBC;DBC;GWA;Giemsa-stained blood smears;automatic working area localization;blood smear microscopic images;complete blood count;differential blood count;good working area;hematopathologist;image resolution;light microscope system;machine learning algorithms;microscopic image classification;oil-immersed objective;statistical features;texture features","","0","","21","","","18-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"Web search personalization using machine learning techniques","T. Bibi; P. Dixit; R. Ghule; R. Jadhav","Comput. Dept., PICT, Pune, India","2014 IEEE International Advance Computing Conference (IACC)","20140327","2014","","","1296","1299","Information on the web is increasing at an enormous speed. Every user has a distinct background and aspecific goal when searching for information on the web. Present search engines produce results that are best suited to given query. But these engines are unaware of user's individual preferences which in turn can vary with individual interest and these interests most of the time change with individual working environment time. To provide such personalized results, user's topical preferences could be stored and utilized for the purpose. Different approaches have been implemented for the same such as, Collaborative Filtering, Document-Based or Concept based profiling etc. We are proposing hybrid approach based on Document Based as well as Concept Based Profiling. Proposed framework aims to re-rank results for a given query obtained from existing search engines. Thus this system would provide an adaptive methodology for learning changing user preferences to re-rank results according to one's individual interests.","","CD-ROM:978-1-4799-2571-1; Electronic:978-1-4799-2572-8; POD:978-1-4799-2573-5","10.1109/IAdCC.2014.6779514","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779514","Web Personalization;click-through data;concept;ontology;user preference;user profile","Browsers;Computers;Conferences;Data mining;Search engines;Taxonomy;Web search","Internet;information retrieval;learning (artificial intelligence);search engines","Web search personalization;World Wide Web;concept based profiling;document based profiling;machine learning;search engine","","0","","12","","","21-22 Feb. 2014","","IEEE","IEEE Conference Publications"
"Theory Identity: A Machine-Learning Approach","K. R. Larsen; D. Hovorka; J. West; J. Birt; J. R. Pfaff; T. W. Chambers; Z. R. Sampedro; N. Zager; B. Vanstone","Univ. of Colorado, Boulder, CO, USA","2014 47th Hawaii International Conference on System Sciences","20140310","2014","","","4639","4648","Theory identity is a fundamental problem for researchers seeking to determine theory quality, create theory ontologies and taxonomies, or perform focused theory-specific reviews and meta-analyses. We demonstrate a novel machine-learning approach to theory identification based on citation data and article features. The multi-disciplinary ecosystem of articles which cite a theory's originating paper is created and refined into the network of papers predicted to contribute to, and thus identify, a specific theory. We provide a 'proof-of-concept' for a highly-cited theory. Implications for cross-disciplinary theory integration and the identification of theories for a rapidly expanding scientific literature are discussed.","1530-1605;15301605","Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6","10.1109/HICSS.2014.564","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6759171","","Abstracts;Ecosystems;Educational institutions;Ontologies;Portals;Subscriptions;Testing","citation analysis;ontologies (artificial intelligence)","citation data;cross-disciplinary theory integration;focused theory-specific reviews;fundamental problem;highly-cited theory;machine-learning approach;meta-analyses;multidisciplinary ecosystem;scientific literature;taxonomies;theory identification;theory identity;theory ontologies;theory quality","","3","","29","","","6-9 Jan. 2014","","IEEE","IEEE Conference Publications"
"Learning Finite-State Machines: Conserving Fitness Function Evaluations by Marking Used Transitions","D. Chivilikhin; V. Ulyantsev","Univ. ITMO, St. Petersburg, Russia","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","90","95","This paper is dedicated to the problem of learning finite-state machines (FSMs), which plays a key role in automata-based programming. Metaheuristic algorithms commonly applied to this problem often use FSM mutations (small changes in the FSM structure) for solution construction. Most of them do not employ the specifics of FSMs in their work. We propose a new simple method for improving performance of these algorithms. The basic idea is to mark those transitions of FSMs that were used during fitness evaluation. Then, if a FSM mutation changes a transition that was not used in fitness evaluation, the fitness function value need not be calculated for the mutated FSM. This observation allows to conserve fitness evaluations, which often have high computational costs. The proposed method has been incorporated into several traditional and recent FSM learning algorithms based on evolutionary strategies, genetic algorithms and ant colony optimization. Experimental results are reported showing that the new method significantly improves performance of two methods based on evolutionary strategies and ant colony optimization.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786087","automata;inference;machine learning;optimization","Algorithm design and analysis;Ant colony optimization;Genetic algorithms;Programming;Sociology;Statistics;Tuning","ant colony optimisation;finite state machines;genetic algorithms;inference mechanisms;learning (artificial intelligence);software performance evaluation","FSM learning algorithms;FSM mutations;FSM structure;ant colony optimization;automata-based programming;evolutionary strategies;finite-state machine learning;fitness function evaluation conservation;genetic algorithms;inference;machine learning;metaheuristic algorithms;performance improvement;used transition marking","","0","","17","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Towards Machine Learning-Based Auto-tuning of MapReduce","N. Yigitbasi; T. L. Willke; G. Liao; D. Epema","Intel Labs., Hillsboro, OR, USA","2013 IEEE 21st International Symposium on Modelling, Analysis and Simulation of Computer and Telecommunication Systems","20140203","2013","","","11","20","MapReduce, which is the de facto programming model for large-scale distributed data processing, and its most popular implementation Hadoop have enjoyed widespread adoption in industry during the past few years. Unfortunately, from a performance point of view getting the most out of Hadoop is still a big challenge due to the large number of configuration parameters. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. Even worse, the parameters have to be re-tuned for different MapReduce applications and clusters. To make the parameter tuning process more effective, in this paper we explore machine learning-based performance models that we use to auto-tune the configuration parameters. To this end, we first evaluate several machine learning models with diverse MapReduce applications and cluster configurations, and we show that support vector regression model (SVR) has good accuracy and is also computationally efficient. We further assess our auto-tuning approach, which uses the SVR performance model, against the Starfish auto tuner, which uses a cost-based performance model. Our findings reveal that our auto-tuning approach can provide comparable or in some cases better performance improvements than Starfish with a smaller number of parameters. Finally, we propose and discuss a complete and practical end-to-end auto-tuning flow that combines our machine learning-based performance models with smart search algorithms for the effective training of the models and the effective exploration of the parameter space.","1526-7539;15267539","Electronic:978-0-7695-5102-9; POD:978-1-4799-1209-4","10.1109/MASCOTS.2013.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730744","big data;distributed systems;hadoop;performance modeling","Accuracy;Benchmark testing;Computational modeling;Data models;Training;Training data;Tuning","distributed programming;learning (artificial intelligence);public domain software;regression analysis;search problems;support vector machines","Hadoop;MapReduce;SVR performance model;cluster configurations;configuration parameters;cost-based performance model;de facto programming model;end-to-end autotuning flow;large-scale distributed data processing;machine learning-based autotuning approach;machine learning-based performance models;parameter tuning process;smart search algorithms;starfish autotuner;support vector regression model","","8","","22","","","14-16 Aug. 2013","","IEEE","IEEE Conference Publications"
"Machine learning in district heating system energy optimization","S. Idowu; C. Åhlund; O. Schelén","Lulea Univ. of Technol., Lulea, Sweden","2014 IEEE International Conference on Pervasive Computing and Communication Workshops (PERCOM WORKSHOPS)","20140515","2014","","","224","227","This paper introduces a work in progress, where we intend to investigate the application of Reinforcement Learning (RL) and online Supervised Learning (SL) to achieve energy optimization in District-Heating (DH) systems. We believe RL is an ideal approach since this task falls under the control-optimization problem where RL has yielded optimal results in previous work. The magnitude and scale of a DH system complexity incurs the curse of dimensionalities and model, hereby making RL a good choice since it provides a solution for the problem. To assist RL even further with the curse of dimensionalities, we intend to investigate the use of SL to reduce the state space. To achieve this, we shall use historical data to generate a heat load sub-model for each home. We believe using the output of these sub-models as feedback to the RL algorithm could significantly reduce the complexity of the learning task. Also, it could reduce convergence time for the RL algorithm. The desired goal is to achieve a realtime application, which takes operational actions when it receives new direct feedback. However, considering the dynamics of DH system such as large time delay and dissipation in DH network due to various factors, we hope to investigate things such as the appropriate data sampling rate and new parameters / sensors that could improve knowledge about the state of the system, especially on the consumer side of the DH network.","","Electronic:978-1-4799-2736-4; POD:978-1-4799-2737-1","10.1109/PerComW.2014.6815206","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815206","Pervasive computing;district heating system;online supervised learning;reinforcement learning;smart city","Cogeneration;DH-HEMTs;Learning (artificial intelligence);Load modeling;Space heating;Water heating","district heating;learning (artificial intelligence);power engineering computing","DH network dissipation;DH systems;RL;SL;control-optimization problem;curse-of-dimensionalities;district heating system energy optimization;heat load submodel;learning task complexity;machine learning;online supervised learning;reinforcement learning;time delay","","2","","15","","","24-28 March 2014","","IEEE","IEEE Conference Publications"
"Virtual Metrology in Semiconductor Manufacturing by Means of Predictive Machine Learning Models","B. Lenz; B. Barak; J. Mührwald; C. Leicht; B. Lenz","Unit Process Dev., Infineon Technol. AG, Regensburg, Germany","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","174","177","Advanced Process Control (APC) is an important research area in Semiconductor Manufacturing (SM) to improve process stability crucial for product quality. In low-volume-high-mixture fabrication plants (fabs), Knowledge Discovery in Databases is extremely challenging due to complex technology mixtures and reduced availability of data for comparable process steps. High Density Plasma Chemical Vapor Deposition (HDP CVD) appears to be a process area in SM predestinated for application of Data Mining (DM). Enhancing physical metrology by predictive models leads to smart future fabs. Actual research focuses on Virtual Metrology (VM) using high sophisticated Machine Learning (ML) methods to model unknown functional interrelations and to predict the thickness of dielectric layers deposited onto a metallization layer of the manufactured wafers. Decision Trees (DT), Neural Networks (NN) and Support Vector Regression (SVR) have been investigated to maximize the accuracy of the regression. For data of various logistical granularities promising results have been achieved by implementing these statistical models.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.186","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786103","Advanced Process Control;Data Mining;Decision Trees;Machine Learning;Neural Networks;Semiconductor Manufacturing;Support Vector Regression;Virtual Metrology","Accuracy;Artificial neural networks;Manufacturing;Metrology;Predictive models;Process control;Semiconductor device measurement","data mining;decision trees;learning (artificial intelligence);neural nets;plasma CVD;process control;product quality;production engineering computing;regression analysis;semiconductor device metallisation;semiconductor industry;support vector machines","APC;DM;DT;HDP CVD;ML method;NN;SM;SVR;VM;advanced process control;data mining;databases;decision trees;dielectric layers;fabs;high density plasma chemical vapor deposition;knowledge discovery;low-volume-high-mixture fabrication plants;machine learning methods;metallization layer;neural networks;predictive machine learning models;product quality;semiconductor manufacturing;statistical models;support vector regression;virtual metrology","","1","","13","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Retail pricing for stochastic demand with unknown parameters: An online machine learning approach","L. Jia; Q. Zhao; L. Tong","Sch. of Electr. & Comput. Eng., Cornell Univ., Ithaca, NY, USA","2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20140213","2013","","","1353","1358","The problem of dynamically pricing of electricity by a retailer for customers in a demand response program is considered. It is assumed that the retailer obtains electricity in a two-settlement wholesale market consisting of a day ahead market and a real-time market. Under a day ahead dynamic pricing mechanism, the retailer aims to learn the aggregated demand function of its customers while maximizing its retail profit. A piecewise linear stochastic approximation algorithm is proposed. It is shown that the accumulative regret of the proposed algorithm grows with the learning horizon T at the order of O(log T). It is also shown that the achieved growth rate cannot be reduced by any piecewise linear policy.","","CD-ROM:978-1-4799-3409-6; Electronic:978-1-4799-3410-2; POD:978-1-4799-3411-9","10.1109/Allerton.2013.6736684","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6736684","Demand response;electricity retail pricing;online learning;optimal stochastic thermal control;stochastic approximation","Approximation methods;Electricity;Load management;Piecewise linear approximation;Pricing;Real-time systems;Stochastic processes","electricity supply industry;learning (artificial intelligence);pricing;retailing","aggregated demand function;customers;day ahead market;demand response program;dynamic pricing mechanism;electricity;online machine learning;piecewise linear policy;piecewise linear stochastic approximation algorithm;real-time market;retail pricing;retail profit;retailer;stochastic demand;unknown parameters;wholesale market","","1","","25","","","2-4 Oct. 2013","","IEEE","IEEE Conference Publications"
"Can smartphones be used to detect an earthquake? Using a machine learning approach to identify an earthquake event","A. Fikri Aji; I. P. E. S. Putra; P. Mursanto; S. Yazid","Fac. of Comput. Sci., Univ. Indonesia, Depok, Indonesia","2014 IEEE International Systems Conference Proceedings","20140522","2014","","","72","77","The possibility of using smart phone accelerometer to detect earthquake is investigated in this research. Experiments are designed to learn the pattern of an earthquake signal recorded from smart phone's accelerometer. The signal is processed using N-gram modeling as feature extractor for machine learning. For the classifier, this study use Naïve Bayes, Multi-Layer Perceptron (MLP), and Random Forest. Our result shows that the best classification accuracy is achieved by Random Forest method. Its accuracy reached 93.15%. It can be concluded that smart phones can be used as an earthquake detector.","","CD-ROM:978-1-4799-2087-7; Electronic:978-1-4799-2086-0; POD:978-1-4799-2089-1","10.1109/SysCon.2014.6819238","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6819238","earthquake;machine learning;n-gram;signal processing","Accelerometers;Accuracy;Data models;Earthquakes;Feature extraction;Fractals;Smart phones","earthquakes;feature extraction;geophysical signal processing;learning (artificial intelligence);multilayer perceptrons;signal classification;smart phones","MLP;N-gram modeling;classification accuracy;earthquake detection;earthquake event identification;earthquake signal pattern;feature extractor;machine learning;machine learning approach;multilayer perceptron;naive Bayes;random forest;signal processing;smart phone accelerometer","","0","","12","","","March 31 2014-April 3 2014","","IEEE","IEEE Conference Publications"
"An integrated approach to spam classification on Twitter using URL analysis, natural language processing and machine learning techniques","K. Kandasamy; P. Koroth","Amrita Center for Cyber Security, Amrita Vishwa Vidyapeetham, Kollam, Kerala, India","Electrical, Electronics and Computer Science (SCEECS), 2014 IEEE Students' Conference on","20140424","2014","","","1","5","In the present day world, people are so much habituated to Social Networks. Because of this, it is very easy to spread spam contents through them. One can access the details of any person very easily through these sites. No one is safe inside the social media. In this paper we are proposing an application which uses an integrated approach to the spam classification in Twitter. The integrated approach comprises the use of URL analysis, natural language processing and supervised machine learning techniques. In short, this is a three step process.","","CD-ROM:978-1-4799-2524-7; Electronic:978-1-4799-2526-1; POD:978-1-4799-2527-8","10.1109/SCEECS.2014.6804508","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804508","URLs;machine learning;natural language processing;tweets","Accuracy;Machine learning algorithms;Natural language processing;Training;Twitter;Unsolicited electronic mail","classification;learning (artificial intelligence);natural language processing;social networking (online);unsolicited e-mail","Twitter;URL analysis;natural language processing;social media;social networks;spam classification;spam contents;supervised machine learning techniques","","3","","12","","","1-2 March 2014","","IEEE","IEEE Conference Publications"
"A 48.6-to-105.2 µW Machine Learning Assisted Cardiac Sensor SoC for Mobile Healthcare Applications","S. Y. Hsu; Y. Ho; P. Y. Chang; C. Su; C. Y. Lee","Dept. of Electron. Eng., Nat. Chiao Tung Univ., Hsinchu, Taiwan","IEEE Journal of Solid-State Circuits","20140324","2014","49","4","801","811","A machine-learning (ML) assisted cardiac sensor SoC (CS-SoC) is designed for mobile healthcare applications. The heterogeneous architecture realizes the cardiac signal acquisition, filtering with versatile feature extractions and classifications, and enables the higher order analysis over traditional DSPs. Besides, the asynchronous architecture with dynamic standby controller further suppresses the system active duty and the leakage power dissipation. The proposed chip is fabricated in a 90-nm standard CMOS technology and operates at 0.5 V-1.0 V (0.7 V-1.0 V for SRAM and I/O interface). Examined with healthcare monitoring applications, the CS-SoC dissipates 48.6/105.2 μW for real-time syndrome detections of ECG-based arrhythmia/VCG-based myocardial infarction with 95.8/99% detection accuracy, respectively.","0018-9200;00189200","","10.1109/JSSC.2013.2297406","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6712138","Arrhythmia;ECG;VCG;biomedical signal processor;classification;feature extraction;machine learning;myocardial infarction","Computer architecture;Feature extraction;Medical services;Mobile communication;Noise;Power dissipation;System-on-chip","electrocardiography;feature extraction;health care;learning (artificial intelligence);medical signal processing;system-on-chip","CS-SoC;ECG based arrhythmia;VCG based myocardial infarction;cardiac sensor SoC;cardiac signal acquisition;dynamic standby controller;feature classification;feature extraction;heterogeneous architecture;machine learning;mobile healthcare applications;standard CMOS technology;voltage 0.5 V to 1.0 V","","5","","25","","20140114","April 2014","","IEEE","IEEE Journals & Magazines"
"Machine Learning for Android Malware Detection Using Permission and API Calls","N. Peiravian; X. Zhu","Dept. of Comput. & Electr. Eng. & Comput. Sci., Florida Atlantic Univ., Boca Raton, FL, USA","2013 IEEE 25th International Conference on Tools with Artificial Intelligence","20140210","2013","","","300","305","The Google Android mobile phone platform is one of the most anticipated smartphone operating systems on the market. The open source Android platform allows developers to take full advantage of the mobile operation system, but also raises significant issues related to malicious applications. On one hand, the popularity of Android absorbs attention of most developers for producing their applications on this platform. The increased numbers of applications, on the other hand, prepares a suitable prone for some users to develop different kinds of malware and insert them in Google Android market or other third party markets as safe applications. In this paper, we propose to combine permission and API (Application Program Interface) calls and use machine learning methods to detect malicious Android Apps. In our design, the permission is extracted from each App's profile information and the APIs are extracted from the packed App file by using packages and classes to represent API calls. By using permissions and API calls as features to characterize each Apps, we can learn a classifier to identify whether an App is potentially malicious or not. An inherent advantage of our method is that it does not need to involve any dynamical tracing of the system calls but only uses simple static analysis to find system functions involved in each App. In addition, because permission settings and APIs are alwaysavailable for each App, our method can be generalized to all mobile applications. Experiments on real-world Apps with more than 1200 malware and 1200 benign samples validate the algorithm performance.","1082-3409;10823409","Electronic:978-1-4799-2972-6; POD:978-1-4799-2973-3; USB:978-1-4799-2971-9","10.1109/ICTAI.2013.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735264","API calls;Android;Malware detection;Permissions;Smartphone Security","Androids;Feature extraction;Google;Humanoid robots;Malware;Smart phones","Android (operating system);application program interfaces;invasive software;learning (artificial intelligence);mobile computing;pattern classification;public domain software;smart phones","API calls;Android malware detection;App profile information;Google Android market;Google Android mobile phone platform;application program interface calls;classifier;machine learning;malicious Android Apps detection;mobile operation system;open source Android platform;permission;smartphone operating systems;static analysis","","26","","22","","","4-6 Nov. 2013","","IEEE","IEEE Conference Publications"
"A novel lie detection method based on extreme learning machine using P300","Yijun Xiong; Yong Yang; Junfeng Gao","Coll. of Mech. & Electr. Eng., Wuhan Donghu Univ., Wuhan, China","IET International Conference on Information Science and Control Engineering 2012 (ICISCE 2012)","20140306","2012","","","1","4","Machine learning-based lie detection has drawn much attention recently. In this paper, we used extreme learning machine (ELM), a recently-proposed machine learning method based on a single layer feedforward network (SLFN), to classify P300 potentials from guilty subject and non-P300 potentials from innocent subject. Back-propagation network and support vector machine classifiers were also used to compare with the proposed method. The number of hidden nodes in ELM was tuned using training with the 10-fold cross validation. The experimental results show that the proposed method reaches the highest classification accuracy with extremely less training and testing time, compared with the other classification models.","","Electronic:978-1-84919-641-3","10.1049/cp.2012.2471","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755850","Lie detection;P300;Probe stimuli;extreme learning machine","","backpropagation;bioelectric potentials;feedforward neural nets;medical signal processing;support vector machines","ELM;P300 potential;SLFN;backpropagation network;extreme learning machine;lie detection method;single layer feedforward network;support vector machine classifier","","0","","","","","7-9 Dec. 2012","","IET","IET Conference Publications"
"Characterizing genetic interactions using a machine learning approach in Colombian patients with Alzheimer's disease","E. A. O. Granados; L. F. N. Vásquez; H. A. Granados","Ind. & Syst. Eng. Dept., Nat. Univ. of Colombia, Medelli&#x0301;n, Colombia","2013 IEEE International Conference on Bioinformatics and Biomedicine","20140206","2013","","","1","2","A main goal of human genetics is to understand the relationship between variations in DNA sequences and the susceptibility to certain illnesses. In this particular work, genetic information is analyzed in relation to the Alzheimer's disease (AD) in order to improve its diagnosis, prevention and treatment. In Colombia, this disease currently requires special attention because its incidence has increased significantly in recent years. Thus, this work analyzes a set of twelve genetic markers or single nucleotide polymorphisms (SNPs) in a set of Colombian patients through a constructive induction method based on a machine learning approach, namely, multifactor dimensionality reduction (MDR). Also, some statistical epistasis analysis is carried out. Particularly, epistasis is obtained based on information gain from AD related genes, providing a simple methodology to characterize interactions in genetic association studies and capturing important traits that describe the behavior of the disease.","","Electronic:978-1-4799-1309-1; POD:978-1-4799-1311-4","10.1109/BIBM.2013.6732588","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732588","","Alzheimer's disease;Educational institutions;Filtering algorithms;Genetics;Machine learning algorithms;Predictive models","DNA;diseases;genetics;genomics;learning (artificial intelligence);medical computing;polymorphism;statistical analysis","AD diagnosis;AD prevention;AD related genes;AD treatment;Alzheimer's disease;Colombian patients;DNA sequences;MDR;SNP;constructive induction method;genetic association studies;genetic information;genetic interaction;genetic markers;illness susceptibility;machine learning approach;multifactor dimensionality reduction;single nucleotide polymorphism;statistical epistasis analysis","","0","","10","","","18-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"Aiding Intrusion Analysis Using Machine Learning","L. Zomlot; S. Chandran; D. Caragea; X. Ou","HP Labs., Princeton, NJ, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","40","47","Intrusion analysis, i.e., the process of combing through IDS alerts and audit logs to identify real successful and attempted attacks, remains a difficult problem in practical network security defense. The major contributing cause to this problem is the high false-positive rate in the sensors used by IDS systems to detect malicious activities. The goal of our work is to examine whether a machine-learned classifier can help a human analyst filter out non-interesting scenarios reported by an IDS alert correlator, so that analysts' time can be saved. This research is conducted in the open-source SnIPS intrusion analysis framework. Throughout observing the output of SnIPS running on our departmental network, we found that an analyst would need to perform repetitive tasks in pruning out the false positives in the correlation graphs produced by it. We hypothesized that such repetitive tasks can yield (limited) labeled data that can enable the use of a machine learning-based approach to prune SnIPS' output based on the human analysts' feedback, much similar to spam filters that can learn from users' past judgment to prune emails. Our goal is to classify the correlation graphs produced from SnIPS into ""interesting"" and ""non-interesting"", where ""interesting"" means that a human analyst would want to conduct further analysis on the events. We spent significant amount of time manually labeling SnIPS' output correlations based on this criterion, and built prediction models using both supervised and semi-supervised learning approaches. Our experiments revealed a number of interesting observations that give insights into the pitfalls and challenges of applying machine learning in intrusion analysis. The experimentation results also indicate that semi-supervised learning is a promising approach towards practical machine learning-based tools that can aid human analysts, when a limited amount of labeled data is available.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786079","Enterprise Network Security;Intrusion Detection and Analysis;Machine Learning","Correlation;Labeling;Predictive models;Production;Security;Sensors;Servers","computer network security;correlation theory;e-mail filters;learning (artificial intelligence);network theory (graphs);pattern classification;public domain software;unsolicited e-mail","IDS alert correlator;IDS system;aiding intrusion analysis;attack identification;audit logs;correlation graph;correlation graph classification;email pruning;false positive rate;human analyst feedback;machine learned classifier;machine learning-based tools;malicious activity detection;manual SnIPS labeling;network security defense;open source SnIPS intrusion analysis;prediction model;semi-supervised learning approach;sensor;spam filters","","1","","34","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Automated human behavioral analysis framework using facial feature extraction and machine learning","D. Smirnov; S. Banger; S. Davis; R. Muraleedharan; R. P. Ramachandran","Dept. of Electr. & Comput. Eng., Rowan Univ., Glassboro, NJ, USA","2013 Asilomar Conference on Signals, Systems and Computers","20140508","2013","","","911","914","Emotional intelligence is essential in understanding and predicting human behavior. Although human emotion is best captured using non-intrusive methods, due to factors such as system complexity, computation time and decision response time, the reality of automated behavioral analysis is hindered. In this paper, we propose a framework capable of recognizing emotions of an individual to identify any suspicious behavior. Our research shows 91.1% of emotion classification accuracy for cooperative individuals using facial feature extraction and machine learning techniques, thus outperforming existing state-of-the-art approaches.","","CD-ROM:978-1-4799-2388-5; Electronic:978-1-4799-2390-8; POD:978-1-4799-2391-5","10.1109/ACSSC.2013.6810420","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6810420","","Cameras;Complexity theory;Discrete cosine transforms;Emotion recognition;Face;Facial features;Feature extraction","behavioural sciences computing;computational complexity;emotion recognition;face recognition;feature extraction;image classification;learning (artificial intelligence)","automated human behavioral analysis framework;computation time;cooperative individuals;decision response time;emotion classification accuracy;emotional intelligence;facial feature extraction;human behavior;human emotion;machine learning;nonintrusive methods;system complexity","","0","","28","","","3-6 Nov. 2013","","IEEE","IEEE Conference Publications"
"Machine Learning Techniques Applied to Sensor Data Correction in Building Technologies","M. K. Smith; C. C. Castello; J. R. New","Dept. of Electr. & Comput. Eng., Univ. of Alabama, Tuscaloosa, AL, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","305","308","Since commercial and residential buildings account for nearly half of the United States' energy consumption, making them more energy-efficient is a vital part of the nation's overall energy strategy. Sensors play an important role in this research by collecting data needed to analyze performance of components, systems, and whole-buildings. Given this reliance on sensors, ensuring that sensor data are valid is a crucial problem. The solution we are researching is machine learning techniques, namely: artificial neural networks and Bayesian Networks. Types of data investigated in this study are: (1) temperature, (2) humidity, (3) refrigerator energy consumption, (4) heat pump liquid pressure, and (5) water flow. These data are taken from Oak Ridge National Laboratory's (ORNL) ZEBRAlliance research project which is composed of four single-family homes in Oak Ridge, TN. Results show that for the temperature, humidity, pressure, and flow sensors, data can mostly be predicted with root-mean-square error of less than 10% of the respective sensor's mean value. Results for the energy sensor were not as good, root-mean-square errors were centered about 100% of the mean value and were often well above 200%. Bayesian networks had smaller errors, but took substantially longer to train.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784632","Bayesian networks;artificial neural networks;building technologies;machine learning;sensor data validation","Bayes methods;Buildings;Humidity;Liquids;Refrigerators;Robot sensing systems;Temperature sensors","belief networks;building management systems;computerised instrumentation;energy conservation;energy consumption;flow sensors;heat pumps;humidity sensors;learning (artificial intelligence);mean square error methods;neural nets;power engineering computing;pressure sensors;refrigerators;temperature sensors","Bayesian networks;United States;artificial neural networks;building technologies;commercial buildings;energy efficiency;energy sensor;flow sensor;heat pump liquid pressure;humidity sensor;machine learning techniques;performance analysis;pressure sensor;refrigerator energy consumption;residential buildings;root-mean-square error;sensor data correction;temperature sensor;water flow","","2","","11","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Detection of underwater objects based on machine learning","Y. Tan; J. K. Tan; H. Kim; S. Ishikawa","Kyushu Institute of Technology, Japan","The SICE Annual Conference 2013","20140213","2013","","","2104","2109","Side-scan and forward-looking sonars are some of the most widely used imaging systems for obtaining large scale images of the seafloor, and their use continues to expand rapidly with their increased deployment on autonomous underwater vehicles. However, it is difficult to extract quantitative information from the images generated from these processes, particularly for the detection and extraction of information on the objects within these images. We propose in this paper an algorithm for automatic detection of underwater objects in side-scan images based on machine learning employing adaptive boosting. Experimental results show that the method produces consistent maps of the seafloor.","","Electronic:978-4-907764-43-2; POD:978-1-5090-0008-1","10.2534/jjasnaoe.18.115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6736326","Haar-like features;Side-scan sonar;underwater objects","Accuracy;Feature extraction;Global Positioning System;Image edge detection;Sonar detection;Sonar navigation","","","","0","","10","","","14-17 Sept. 2013","","IEEE","IEEE Conference Publications"
"Hankel based maximum margin classifiers: A connection between machine learning and Wiener systems identification","F. Xiong; Y. Cheng; O. Camps; M. Sznaier; C. Lagoa","Dept. of Electr. & Comput. Eng., Northeastern Univ., Boston, MA, USA","52nd IEEE Conference on Decision and Control","20140310","2013","","","6005","6010","This paper considers the problem of nonparametric identification of Wiener systems in cases where there is no a-priori available information on the dimension of the output of the linear dynamics. Thus, it can be considered as a generalization to the case of dynamical systems of non-linear manifold embedding methods recently proposed in the machine learning community. A salient feature of this framework is its ability to exploit both positive and negative examples, as opposed to classical identification techniques where usually only data known to have been produced by the unknown system is used. The main result of the paper shows that while in principle this approach leads to challenging non-convex optimization problems, tractable convex relaxations can be obtained by exploiting a combination of recent developments in polynomial optimization and matrix rank minimization. Further, since the resulting algorithm is based on identifying kernels, it uses only information about the covariance matrix of the observed data (as opposed to the data itself). Thus, it can comfortably handle cases such as those arising in computer vision applications where the dimension of the output space is very large (since each data point is a frame from a video sequence with thousands of pixels).","0191-2216;01912216","Electronic:978-1-4673-5717-3; POD:978-1-4673-5714-2","10.1109/CDC.2013.6760837","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6760837","","Computational complexity;Heuristic algorithms;Kernel;Manifolds;Optimization;Polynomials;Vectors","Hankel matrices;concave programming;covariance matrices;identification;learning (artificial intelligence);pattern classification","Hankel based maximum margin classifiers;Wiener systems identification;covariance matrix;kernels;machine learning;matrix rank minimization;nonconvex optimization problems;nonlinear manifold embedding methods;polynomial optimization;tractable convex relaxations","","4","","26","","","10-13 Dec. 2013","","IEEE","IEEE Conference Publications"
"Research of machine learning methods for student management performance evaluation","Xiaobo Huang; Gang Zhang; Qiang Pan; Jianrong Huang; Ying Huang","Sch. of Autom., Guangdong Univ. of Technol., Guangzhou, China","IET International Conference on Information Science and Control Engineering 2012 (ICISCE 2012)","20140306","2012","","","1","5","We constructed a student management evaluation framework oriented information system through Java Web technology, which performs in-depth analysis for recent evaluation framework and processing un-structured data and semi-structured data. Meanwhile, considering the complexity of information for student management and subjectivity of measurements, we propose an algorithm framework based on Artificial Neural Network (ANN) aiming at constructing an intelligent integrated model through data mining technology. Evaluation on a student management data set collected from a polytechnic school shows the effectiveness of the proposed algorithm framework.","","Electronic:978-1-84919-641-3","10.1049/cp.2012.2337","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755716","artificial neural network;data mining;higher professional college;student management evaluation","","Internet;Java;computational complexity;computer aided instruction;data mining;data structures;learning (artificial intelligence);neural nets","ANN;Java Web technology;artificial neural network;data mining technology;in-depth analysis;information complexity;intelligent integrated model;machine learning methods;polytechnic school;semistructured data;student management evaluation framework oriented information system;student management performance evaluation;unstructured data","","0","","","","","7-9 Dec. 2012","","IET","IET Conference Publications"
"Determination of Weights for Multiobjective Decision Making or Machine Learning","P. Wang; H. Zhu; M. Wilamowska-Korsak; Z. Bi; L. Li","Sch. of Autom. & the Inst. of Syst. Sci. & Eng., Wuhan Univ. of Technol., Wuhan, China","IEEE Systems Journal","20140214","2014","8","1","63","72","Decision-making processes in complex systems generally require the mechanisms to make the tradeoff among contradicting design criteria. When multiple objectives are involved in decision making or machine learning, a crucial step is to determine the weights of individual objectives to the system-level performance. Determining the weights of multiobjectives is an evaluation process, and it has been often treated as an optimization problem. However, our preliminary investigation has shown that existing methodologies in dealing with the weights of multiobjectives have some obvious limitations in the sense that the determination of weights is tackled as a single optimization problem, a result based on such an optimization is incomprehensive, and it can even be unreliable when the information about multiple objectives is incomplete such as an incompleteness caused by poor data. The constraints of weights are also discussed. Variable weights are natural in decision-making processes. Therefore, we are motivated to develop a systematic methodology in determining variable weights of multiobjectives. The roles of weights in an original multiobjective decision-making or machine-learning problem are analyzed, and the weights are determined with the aid of a modular neural network. The inconsistency issue of weights is particularly discussed.","1932-8184;19328184","","10.1109/JSYST.2013.2265663","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547983","Consistency;multidisciplinary design optimization (MDO);multifunctional machine learning (MFML);multiobjective decision making (MODM);neural network;tradeoff;variable weights","","decision making;learning (artificial intelligence);neural nets;optimisation","machine learning;modular neural network;multiobjective decision making;optimization problem;weight determination","","5","","66","","20130627","March 2014","","IEEE","IEEE Journals & Magazines"
"IoT service platform enhancement through ‘in-situ’ machine learning of real-world knowledge","M. Roelands","Bell Labs., Alcatel-Lucent, Antwerp, Belgium","38th Annual IEEE Conference on Local Computer Networks - Workshops","20140310","2013","","","896","903","With Machine-to-Machine and Internet of Things getting beyond hype, including an ever wider range of connected device types in ever more value-added services, a new era of data (and multimedia) stream-intensive services is emerging. While live data is massively becoming available, turning it into meaningful information that is not only actionable for decision makers, but also can be leveraged as a behavioral service property, or even reused across services, is a challenge that demands a systematic approach. In this paper we propose such systematic approach, towards establishing an Internet of Things service platform architecture that leverages real-world knowledge for faster service creation and more efficient execution. Illustrated by example scenarios, we go further beyond this, proposing a method to systematically leverage machine learning techniques for revising, improving or ultimately semi-automatically extending this real-world knowledge `in-situ', i.e. during system operation, leveraging real-world observation in-context of requested service execution.","","Electronic:978-1-4799-0540-9; POD:978-1-4799-0538-6","10.1109/LCNW.2013.6758529","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758529","IoT service platform;cognitive feedback loop;machine learning;real world phenomena;service creation","Cameras;Computer architecture;Context;Hidden Markov models;Optimization;Sensors;Streaming media","Internet of Things;learning (artificial intelligence);media streaming;telecommunication networks","Internet-of-things service platform architecture;IoT service platform enhancement;behavioral service property;data stream-intensive services;in-situ machine learning techniques;machine-to-machine industry;multimedia stream-intensive services;real-world knowledge;service creation;value-added services","","1","","27","","","21-24 Oct. 2013","","IEEE","IEEE Conference Publications"
"Simplifying the Utilization of Machine Learning Techniques for Bioinformatics","D. J. Dittman; T. M. Khoshgoftaar; R. Wald; A. Napolitano","Florida Atlantic Univ., Boca Raton, FL, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","2","","396","403","The domain of bioinformatics has a number of challenges such as handling datasets which exhibit extreme levels of high dimensionality (large number of features per sample) and datasets which are particularly difficult to work with. These datasets contain many pieces of data (features) which are irrelevant and redundant to the problem being studied, which makes analysis quite difficult. However, techniques from the domain of machine learning and data mining are well suited to combating these difficulties. Techniques like feature selection (choosing an optimal subset of features for subsequent analysis by removing irrelevant or redundant features) and classifiers (used to build inductive models in order to classify unknown instances) can assist researchers in working with such difficult datasets. Unfortunately, many practitioners of bioinformatics do not have the machine learning knowledge to choose the correct techniques in order to achieve good classification results. If the choices could be simplified or predetermined then it would be easier to apply the techniques. This study is a comprehensive analysis of machine learning techniques on twenty-five bioinformatics datasets using six classifiers, and twenty-four feature rankers. We analyzed the factors at each of four feature subset sizes chosen for being large enough to be effective in creating inductive models but small enough to be of use for further research. Our results shows that Random Forest with 100 trees is the top performing classifier and that the choice of feature ranker is of little importance as long as feature selection occurs. Statistical analysis confirms our results. By choosing these parameters, machine learning techniques are more accessible to bioinformatics.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.155","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786142","Bioinformatics;Classification;Feature Selection","Bioinformatics;Biological system modeling;DNA;Logistics;Lungs;Support vector machines;Vegetation","bioinformatics;data mining;feature selection;learning (artificial intelligence);pattern classification;statistical analysis","bioinformatics datasets;comprehensive analysis;data mining;feature rankers;feature selection;high-dimensional data;inductive models;machine learning techniques;optimal feature subset;random forest classifier;statistical analysis;unknown instance classification","","9","","20","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Machine learning for understanding the contextual semantics of tabular web sources","J. Weerasinghe; S. Weerasinghe; A. Panditha; V. Weerasinghe","","2013 IEEE 8th International Conference on Industrial and Information Systems","20140206","2013","","","577","582","Tables are frequently used in web sources to present relational data in a human friendly manner. Because they are intended for humans, using machines to extract such information is difficult. There are approaches such as wrappers that attempt to solve this problem, but they lack adaptability and require high maintenance. Identifying and extracting information from web tables is not a trivial task, and understanding the semantics of a web table proves to be even harder. In this paper, we introduce a machine learning based approach to understand the semantics in the data residing in tabular web sources. We suggest features that reflect the characteristics of the content in the tables and analyze their impact on the accuracy of the classification process.","2164-7011;21647011","Electronic:978-1-4799-0910-0; POD:978-1-4799-0907-0","10.1109/ICIInfS.2013.6732048","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732048","Artificial Intelligence;Machine Learning;Semantics;Web Tables","Accuracy;Data mining;Feature extraction;HTML;Pricing;Semantics;Web pages","information resources;learning (artificial intelligence);semantic Web","Web tables;classification process;contextual semantics;machine learning based approach;tabular Web sources","","0","","9","","","17-20 Dec. 2013","","IEEE","IEEE Conference Publications"
"Removing JPEG blocking artifacts using machine learning","J. Quijas; O. Fuentes","Dept. of Comput. Sci., Univ. of Texas at El Paso, El Paso, TX, USA","2014 Southwest Symposium on Image Analysis and Interpretation","20140501","2014","","","77","80","JPEG is a commonly used image compression method. While it normally yields very good compression ratios, it also introduces blocking artifacts and quantization noise. In this paper, we present a method to remove noise and blocking effects from JPEG-compressed images. We use machine learning techniques to predict DCT coefficients and pixel values in a compressed image. Results show a decrease in mean square error between our predicted images and the original uncompressed images when compared to the compressed images, as well as a clear reduction of blocking artifacts.","","Electronic:978-1-4799-4053-0; POD:978-1-4799-4052-3; USB:978-1-4799-4054-7","10.1109/SSIAI.2014.6806033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6806033","Image compression;JPEG;artifact removal;feed-forward neural networks","Image coding;Image color analysis;Image edge detection;PSNR;Smoothing methods;Transform coding;Visualization","data compression;discrete cosine transforms;image coding;image denoising;learning (artificial intelligence);mean square error methods","DCT coefficient prediction;JPEG blocking artifact removal;JPEG-compressed images;compression ratios;image compression method;machine learning techniques;mean square error;pixel value prediction;quantization noise removal;uncompressed images","","0","","6","","","6-8 April 2014","","IEEE","IEEE Conference Publications"
"Cost-optimal, robust charging of electrically-fueled commercial vehicle fleets via machine learning","J. Shah; M. Nielsen; A. Reid; C. Shane; K. Mathews; D. Doerge; R. Piel; R. Anderson; A. Boulanger; L. Wu; V. Bhandari; A. Gagneja; A. Kressner; X. Li; S. Sarkar","Controls, Electron. & Signal Process., GE Global Res., Niskayuna, NY, USA","2014 IEEE International Systems Conference Proceedings","20140522","2014","","","65","71","Electrification for commercial vehicle fleets presents opportunity to cut emissions, reduce fuel costs, and improve operational metrics. However, infrastructure limitations in urban areas often inhibit the ability to charge a significant number of electric vehicles, especially under one roof. This paper highlights a novel controls approach developed at GE Global Research in conjunction with Columbia University to fulfill the stated needs for intelligent charging of a commercial fleet of electric vehicles. This novel approach combines traditional control techniques with machine learning algorithms to adapt to customer behavior over time. The stated controls system is designed to regulate the charging rate of multiple electric vehicle supply equipment devices (EVSEs) to facilitate cost-optimal charging subject to past and predicted building load, vehicle energy requirements, and current conditions. In this embodiment, the system is primarily designed to mitigate electric demand charges that may otherwise occur due to charging at inopportune times. The system will be deployed at a New York City FedEx Express delivery depot in partnership with the local utility, Consolidated Edison Company of New York.","","CD-ROM:978-1-4799-2087-7; Electronic:978-1-4799-2086-0; POD:978-1-4799-2089-1","10.1109/SysCon.2014.6819237","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6819237","Electric vehicle (EV);artificial intelligence;controls;infrastructure;machine learning;optimization;peak demand;smart grid;supervisory control and data acquisition (SCADA);support vector machine (SVM);support vector regression (SVR)","Buildings;Databases;Electric vehicles;Fuels;Learning systems;Load modeling","automotive electrics;control engineering computing;electric vehicles;learning (artificial intelligence);mechanical engineering computing;power engineering computing","Columbia University;Consolidated Edison Company;EVSE;GE Global Research;New York City FedEx Express delivery depot;cost-optimal charging;customer behavior;electric demand charge mitigation;electrically-fueled commercial vehicle fleets;fuel cost reduction;intelligent charging;machine learning algorithms;multiple electric vehicle supply equipment devices;operational metrics improvement;robust charging","","3","","10","","","March 31 2014-April 3 2014","","IEEE","IEEE Conference Publications"
"Machine learning based internet traffic recognition with statistical approach","R. C. Jaiswal; S. D. Lokhande","Dept. of Electron. & Telecommun., Pune Inst. of Comput. Technol., Pune, India","2013 Annual IEEE India Conference (INDICON)","20140130","2013","","","1","6","The researchers have started looking for Internet traffic recognition techniques that are independent of `well known' TCP or UDP port numbers, or interpreting the contents of packet payloads. Newer approaches classify traffic by recognizing statistical patterns in externally observable attributes of the traffic (such as typical packet lengths and inter-arrival times). The main goal is to cluster or classify the Internet traffic flows into groups that have identical statistical properties. The need to deal with Traffic patterns, large datasets and Multidimensional spaces of flow and packet attributes is one of the reasons for the introduction of Machine Learning (ML) techniques in this field. ML techniques are subset of Artificial Intelligence used for traffic recognition. Further, there are four types of Machine Learning, i.e. Classification (Supervised learning), clustering (Un-Supervised learning), Numeric prediction and Association. In this research paper IP traffic recognition through classification process is implemented. Different researchers are calling this process as IP traffic Recognition, IP traffic Identification, and sometimes IP traffic classification. Here Real time internet traffic has been captured using packet capturing tool and datasets has been developed. Also few standard datasets have been used in this research work. Then using standard attribute selection algorithms, a reduced statistical feature dataset has been developed. After that, Six ML algorithms AdaboostM1, C4.5, Random Forest tree, MLP, RBF and SVM with Polykernel function classifiers are used for IP traffic classification. This implementation and analysis shows that Tree based algorithms are effective ML techniques for Internet traffic classification with accuracy up to of 99.7616 %.","2325-940X;2325940X","CD-ROM:978-1-4799-2274-1; Electronic:978-1-4799-2275-8; POD:978-1-4799-2276-5","10.1109/INDCON.2013.6726074","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726074","AdaboostM1;C4.5;Internet Traffic Classification;MLP;Machine Learning;RBF and SVM with Polykernel function;Random Forest tree","Accuracy;Classification algorithms;IP networks;Internet;Support vector machines;Training;Vegetation","IP networks;Internet;learning (artificial intelligence);pattern recognition;statistical analysis;telecommunication traffic","IP traffic classification;IP traffic identification;IP traffic recognition;Internet traffic classification;Internet traffic flows;MLP;RBF;SVM;TCP port numbers;UDP port numbers;artificial intelligence;classification process;machine learning based Internet traffic recognition;packet attributes;packet capturing tool;packet payloads;polykernel function classifiers;random forest tree;real time Internet traffic;reduced statistical feature dataset;standard attribute selection algorithms;statistical pattern recognition;tree based algorithms;unsupervised learning","","2","","34","","","13-15 Dec. 2013","","IEEE","IEEE Conference Publications"
"Using Machine Learning for Behavior-Based Access Control: Scalable Anomaly Detection on TCP Connections and HTTP Requests","A. Adler; M. J. Mayhew; J. Cleveland; M. Atighetchi; R. Greenstadt","Raytheon BBN Technol., Cambridge, MA, USA","MILCOM 2013 - 2013 IEEE Military Communications Conference","20140210","2013","","","1880","1887","Today's business processes are more connected than ever before, driven by the ability to share the right information with the right partners at the right time. While this interconnectedness and situational awareness is crucial to success, it also opens the possibility for misuse of the same capabilities by sophisticated adversaries to spread attacks and exfiltrate or corrupt critical sensitive information. We have been investigating means to analyze behaviors of actors and assess trustworthiness of information to support real-time cyber security decision making through a concept called Behavior-Based Access Control (BBAC). The work described in this paper focuses on the statistical machine learning techniques used in BBAC to make predictions about the intent of actors establishing TCP connections and issuing HTTP requests. We discuss pragmatic challenges and solutions we encountered in implementing and evaluating BBAC, discussing (a) the general concepts underlying BBAC, (b) challenges we have encountered in identifying suitable datasets, (c) mitigation strategies to cope with shortcomings in available data, (d) the combination of clustering and support vector machines for performing classification at scale, and (e) results from a number of scientific experiments. We also include expert commentary from Air Force stakeholders and describe current plans for transitioning BBAC capabilities into the Department of Defense together with lessons learned for the machine learning community.","2155-7578;21557578","Electronic:978-0-7695-5124-1; POD:978-1-4799-4899-4","10.1109/MILCOM.2013.317","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735899","","Access control;Accuracy;Computer security;Face;Intrusion detection;Servers;Training","authorisation;hypermedia;learning (artificial intelligence);statistical analysis;transport protocols","BBAC;Department of Defense;HTTP requests;TCP connections;behavior-based access control;critical sensitive information;mitigation strategies;real-time cyber security decision making;scalable anomaly detection;situational awareness;statistical machine learning techniques;support vector machines","","1","","23","","","18-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems","A. Groce; T. Kulesza; C. Zhang; S. Shamasunder; M. Burnett; W. K. Wong; S. Stumpf; S. Das; A. Shinsel; F. Bice; K. McIntosh","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis","IEEE Transactions on Software Engineering","20140331","2014","40","3","307","323","How do you test a program when only a single user, with no expertise in software testing, is able to determine if the program is performing correctly? Such programs are common today in the form of machine-learned classifiers. We consider the problem of testing this common kind of machine-generated program when the only oracle is an end user: e.g., only you can determine if your email is properly filed. We present test selection methods that provide very good failure rates even for small test suites, and show that these methods work in both large-scale random experiments using a “gold standard” and in studies with real users. Our methods are inexpensive and largely algorithm-independent. Key to our methods is an exploitation of properties of classifiers that is not possible in traditional software testing. Our results suggest that it is plausible for time-pressured end users to interactively detect failures-even very hard-to-find failures-without wading through a large number of successful (and thus less useful) tests. We additionally show that some methods are able to find the arguably most difficult-to-detect faults of classifiers: cases where machine learning algorithms have high confidence in an incorrect result.","0098-5589;00985589","","10.1109/TSE.2013.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682887","Machine learning;end-user testing;test suite size","Electronic mail;Machine learning algorithms;Software;Software algorithms;Testing;Training;Training data","interactive systems;learning (artificial intelligence);program testing","effective test selection;email;end users;hard-to-find failures;interactive failure detection;interactive machine learning systems;machine generated program;machine learned classifiers;program testing;software testing","","6","","63","","20131212","March 2014","","IEEE","IEEE Journals & Magazines"
"Prediction of horizontal gene transfer in escherichia coil using machine learning","P. G. Sudasinghe; C. R. Wijesinghe; A. R. Weerasinghe","Sch. of Comput., Univ. of Colombo, Colombo, Sri Lanka","2013 International Conference on Advances in ICT for Emerging Regions (ICTer)","20140310","2013","","","118","124","Horizontal Gene Transfer (HGT), also known as Lateral Gene Transfer is a process where an organism acquires genetic material from another organism without being a descendant of that organism. Horizontal gene transfer is said to be the predominant method of evolution in prokaryotic organisms. This study is focused on constructing a method that employs genome comparison and semi supervised learning to identify genes that are horizontally transferred to Escherichia coli 0157:H7 and attempting to find a link between these genes and other organisms that display pathogenic behaviour. E.coli 0157:H7 is compared to E.coli K-12 which is a harmless strain of the same organism. This comparison yields the set of genes that has not originated from the same ancestor (non-homologous) and is the possible cause of its pathogenic properties. A supervised self-organizing map was constructed to classify the non-homologous genes as either horizontally or vertically transferred. Most of the obtained horizontally transferred genes have shown a striking similarity to other pathological bacteria and Achaea. The results have indicated that, while it is possible to discern the mode of transfer of a gene based on compositional feature to a certain degree, it is better to combine several other features to further refine the findings.","","Electronic:978-1-4799-1276-6; POD:978-1-4799-1273-5","10.1109/ICTer.2013.6761165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761165","Codon adaptation index;Escherichia coli;GC content;Horizontal gene transfer;Lateral Gene Transfer;Self-organizing map;Supervised Self-organizing map;Unsupervised learning","Amino acids;Bioinformatics;Genomics;Microorganisms;Strain;Vegetation","biology computing;genetics;learning (artificial intelligence);microorganisms;pattern classification;self-organising feature maps","Achaea;E coli K-12;E coli O157:H7;Escherichia coli;genome comparison;horizontal gene transfer prediction;lateral gene transfer;machine learning;nonhomologous genes classification;pathogenic behaviour;pathogenic properties;pathological bacteria;prokaryotic organisms;semisupervised learning;supervised self-organizing map","","0","","17","","","11-15 Dec. 2013","","IEEE","IEEE Conference Publications"
"Preliminary design of estimation heart disease by using machine learning ANN within one year","R. Wijaya; A. S. Prihatmanto; Kuspriyanto","Electr. Enginering, Inst. Teknol. Bandung, Bandung, Indonesia","2013 Joint International Conference on Rural Information & Communication Technology and Electric-Vehicle Technology (rICT & ICeV-T)","20140508","2013","","","1","4","In this paper discussed the development of heart disease prediction using machine learning (in this case the Artificial Neural Network or ANN). There are 13 variables that can determine heart disease according to Miss Chaitrali paper. Prediction of a person's heart disease one year ahead is performed by studying the model heart rate data. Data is taken by using tool such as smart mirror, smart mouse, smart phones and smart chair. Heart rate data were collected through the Internet and collected in a server. Learning in this system is performed for a period of one year to get enough data to make predictions. Predictive of future heart disease in one year can increase a person's awareness of heart disease itself. The system is also expected to reduce the number of patients and the number of deaths from heart disease.","","CD-ROM:978-1-4799-3364-8; Electronic:978-1-4799-3365-5; POD:978-1-4799-3366-2","10.1109/rICT-ICeVT.2013.6741541","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6741541","Artificial Neural Network;Heart disease;Prediction","Artificial neural networks;Diseases;Heart rate;Mice;Prediction algorithms;Servers","cardiology;diseases;learning (artificial intelligence);medical computing;neural nets","ANN;Internet;artificial neural network;heart disease determination;heart disease estimation;machine learning;model heart rate data;smart chair;smart mirror;smart mouse;smart phones","","0","","17","","","26-28 Nov. 2013","","IEEE","IEEE Conference Publications"
"Fast transrating for high efficiency video coding based on machine learning","L. P. Van; J. De Cock; G. Van Wallendael; S. Van Leuven; R. Rodriguez-Sánchez; J. L. Martínez; P. Lambert; R. Van de Walle","ELIS - Multimedia Lab., Ghent Univ. - iMinds, Ghent, Belgium","2013 IEEE International Conference on Image Processing","20140213","2013","","","1573","1577","To incorporate the newly developed High Efficiency Video Coding (HEVC) standard in real-life network applications, efficient transrating algorithms are required. We propose a fast transrating scheme, based on the early prediction of the partition split-flags in P pictures. Using machine learning techniques, the correlation between co-located partitions at different quantizations is investigated. This results in a model which predicts the split-flag and gives the associated prediction accuracy so that the splitting process in the transcoder is optimized. At each partition depth, the model indicates whether the full rate-distortion cost evaluations should be performed at the current depth, or if the partition can be split immediately. Experimental results show that the proposed transcoder reduces the complexity of the transrating process by 76.04%, while maintaining the coding efficiency of a cascaded decoder-encoder.","1522-4880;15224880","Electronic:978-1-4799-2341-0; POD:978-1-4799-2342-7; USB:978-1-4799-2340-3","10.1109/ICIP.2013.6738324","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738324","HEVC;Transrating;low complexity","","learning (artificial intelligence);quantisation (signal);video coding","HEVC standard;cascaded decoder-encoder;co-located partitions;coding efficiency;fast transrating scheme;full rate-distortion cost evaluations;high efficiency video coding;machine learning techniques;partition depth;partition split-flags;prediction accuracy;quantizations;splitting process;transcoder","","6","","17","","","15-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Detecting spongiosis in stained histopathological specimen using multispectral imaging and machine learning","S. Abeysekera; M. P. L. Ooi; Y. C. Kuang; C. P. Tan; S. S. Hassan","Monash Univ., Bandar Sunway, Malaysia","2014 IEEE Sensors Applications Symposium (SAS)","20140417","2014","","","195","200","Pathologists spend nearly 80% of their time analysing pathological tissue samples. In addition, the diagnosis is subject to inter/intra-observer variability. Thus to increase productivity and repeatability, a new field known as Computational Pathology has emerged which combines the field of pathology with computer vision, pattern recognition and machine learning. This research develops a new computational pathology framework specifically to aid with detecting a condition known as spongiosis caused by Newcastle Disease Virus infection in poultry. It combines the use of multispectral imaging with feature extraction and classification to detect areas of spongiosis in tissue of infected poultry. The success of this framework is the first step towards a completely automated diagnosis tool for histopathology.","","CD-ROM:978-1-4799-2180-5; Electronic:978-1-4799-2179-9; POD:978-1-4799-4153-7","10.1109/SAS.2014.6798945","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798945","computer vision;detection;machine learning;multispectral imaging;pattern recognition","Cameras;Feature extraction;Multispectral imaging;Pathology;Support vector machines;Training","biological tissues;biomedical optical imaging;cellular biophysics;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;microorganisms","computational pathology;computer vision;feature extraction;image classification;infected poultry;interobserver variability;intraobserver variability;machine learning;multispectral imaging;newcastle disease virus infection;pathological tissue samples;pattern recognition;spongiosis detection;stained histopathological specimen","","1","","15","","","18-20 Feb. 2014","","IEEE","IEEE Conference Publications"
"Innovative practices session 5C: Machine learning and data analysis in test","S. Biswas; J. Carulli; D. G. Drmanac; A. Bhattacherjee","Nvidia","2014 IEEE 32nd VLSI Test Symposium (VTS)","20140522","2014","","","1","1","Finding the cause of yield and reliability issues has never been an easy task for the product and test engineer. The challenge continues to grow as processes add more steps and contain more complicated interactions, as designs are pushed to the limits of the process capabilities to meet their market requirements, as DPPM requirements continue to lower, and as test costs do not scale well. Test data has continued to become more critical in quickly resolving issues for fast ramps and maintenance of quality levels. However, the analysis methods are becoming more sophisticated and data intensive. This presentation will review some case studies, what was learned, and some observations.","1093-0167;10930167","Electronic:978-1-4799-2611-4; POD:978-1-4799-2612-1","10.1109/VTS.2014.6818766","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818766","","Abstracts;Assembly;Data analysis;Maintenance engineering;Reliability engineering;Very large scale integration","","","","0","","","","","13-17 April 2014","","IEEE","IEEE Conference Publications"
"Using Support Vector Machines to Classify Student Attentiveness for the Development of Personalized Learning Systems","M. Ross; C. A. Graves; J. W. Campbell; J. H. Kim","Dept. of Electr. & Comput. Eng., North Carolina A&T State Univ., Greensboro, NC, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","325","328","There have been many studies in which researchers have attempted to classify student attentiveness. Many of these approaches depended on a qualitative analysis and lacked any quantitative analysis. Therefore, this work is focused on bridging the gap between qualitative and quantitative approaches to classify student attentiveness. Thus, this research applies machine learning algorithms (K-means and SVM) to automatically classify students as attentive or inattentive using data from a consumer RGB-D sensor. Results of this research can be used to improve teaching strategies for instructors at all levels and can aid instructors in implementing personalized learning systems, which is a National Academy of Engineering Grand Challenge. This research applies machine learning algorithms to an educational setting. Data from these algorithms can be used by instructors to provide valuable feedback on the effectiveness of their instructional strategies and pedagogies. Instructors can use this feedback to improve their instructional strategies, and students will benefit by achieving improved learning and subject mastery. Ultimately, this will result in the students' increased ability to do work in their respective areas. Broadly, this work can help advance efforts in many areas of education and instruction. It is expected that improving instructional strategies and implementing personalized learning will help create more competent, capable, and prepared persons available for the future workforce.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784636","K-means;Kinect;Support vector machines;personalized learning systems","Classification algorithms;Clustering algorithms;Databases;Education;Learning systems;Machine learning algorithms;Support vector machines","learning (artificial intelligence);learning management systems;pattern classification;support vector machines;teaching","consumer RGB-D sensor;instructional strategy;machine learning algorithm;pedagogy;personalized learning systems;student attentiveness classification;subject mastery;support vector machine;teaching strategy improvement","","1","","15","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"DeepQA Jeopardy! Gamification: A Machine-Learning Perspective","A. K. Baughman; W. Chuang; K. R. Dixon; Z. Benz; J. Basilico","IBM Special Events, Research Triangle Park, NC, USA","IEEE Transactions on Computational Intelligence and AI in Games","20140313","2014","6","1","55","66","DeepQA is a large-scale natural language processing (NLP) question-and-answer system that responds across a breadth of structured and unstructured data, from hundreds of analytics that are combined with over 50 models, trained through machine learning. After the 2011 historic milestone of defeating the two best human players in the Jeopardy! game show, the technology behind IBM Watson, DeepQA, is undergoing gamification into real-world business problems. Gamifying a business domain for Watson is a composite of functional, content, and training adaptation for nongame play. During domain gamification for medical, financial, government, or any other business, each system change affects the machine-learning process. As opposed to the original Watson Jeopardy!, whose class distribution of positive-to-negative labels is 1:100, in adaptation the computed training instances, question-and-answer pairs transformed into true-false labels, result in a very low positive-to-negative ratio of 1:100 000. Such initial extreme class imbalance during domain gamification poses a big challenge for the Watson machine-learning pipelines. The combination of ingested corpus sets, question-and-answer pairs, configuration settings, and NLP algorithms contribute toward the challenging data state. We propose several data engineering techniques, such as answer key vetting and expansion, source ingestion, oversampling classes, and question set modifications to increase the computed true labels. In addition, algorithm engineering, such as an implementation of the Newton-Raphson logistic regression with a regularization term, relaxes the constraints of class imbalance during training adaptation. We conclude by empirically demonstrating that data and algorithm engineering are complementary and indispensable to overcome the challenges in this first Watson gamification for real-world business problems.","1943-068X;1943068X","","10.1109/TCIAIG.2013.2285651","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632881","Gamification;machine learning;natural language processing (NLP);pattern recognition","Accuracy;Games;Logistics;Machine learning algorithms;Pipelines;Training","business data processing;computer games;learning (artificial intelligence);natural language processing;question answering (information retrieval);text analysis","DeepQA Jeopardy! gamification;NLP algorithms;NLP question-and-answer system;Newton-Raphson logistic regression;Watson gamification;Watson machine-learning pipelines;algorithm engineering;business domain;configuration settings;data engineering techniques;domain gamification;extreme class imbalance;ingested corpus sets;large-scale natural language processing question-and-answer system;machine-learning process;nongame play;positive-to-negative ratio;question-and-answer pairs;real-world business problems;regularization term;structured data;training instances;true-false labels;unstructured data","","2","","31","","20131017","March 2014","","IEEE","IEEE Journals & Magazines"
"Machine learning tools for content-based search in large multimedia databases","M. Gabbouj","Dept. of Signal Process., Tampere Univ. of Technol., Tampere, Finland","2013 International Conference on Computer, Control, Informatics and Its Applications (IC3INA)","20140522","2013","","","15","16","Summary form only given. The talk deals with a new paradigm for multimedia search based on content. We present an alternative approach to classical search engines for information retrieval which can be used for large and generic multimedia repositories. We introduce an incremental evolution scheme within a collective network of (evolutionary) binary classifier (CNBC) framework. The proposed framework addresses the problems of feature/class scalability and achieves high classification and content-based retrieval performances over dynamic image repositories. The secret behind the success of CNBC is a novel design to implement the backbone of CNBC, namely the binary classifier. This is a special neural network which is optimally designed using the recently developed evolutionary optimization algorithm called multi-dimensional particle swarm optimization.","","CD-ROM:978-1-4799-1076-2; Electronic:978-1-4799-1078-6; POD:978-1-4799-1077-9","10.1109/IC3INA.2013.6819140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6819140","","Awards activities;Computers;Educational institutions;Information technology;Multimedia communication;Signal processing;Signal processing algorithms","content-based retrieval;learning (artificial intelligence);multimedia databases;neural nets;particle swarm optimisation;pattern classification","CNBC framework;classification performance;collective network of binary classifier;content-based retrieval performance;content-based search;evolutionary optimization algorithm;feature-class scalability;image repositories;incremental evolution scheme;information retrieval;machine learning tools;multidimensional particle swarm optimization;multimedia databases;multimedia repositories;multimedia search paradigm;neural network;search engines","","0","","1","","","19-21 Nov. 2013","","IEEE","IEEE Conference Publications"
"Zebrafish Larva Locomotor Activity Analysis Using Machine Learning Techniques","H. Zhang; S. C. Lenaghan; M. H. Connolly; L. E. Parker","Dept. of Electr. Eng. & Comput. Sci., Univ. of Tennessee, Knoxville, TN, USA","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","161","166","Zebra fish larvae have become a popular model organism to investigate genetic and environmental factors affecting behavior. However, difficulties exist in the analysis of complex behaviors from a large array of larvae. In this paper, we present the new application of machine learning techniques in bioinformatics to automatically detect and investigate the locomotor activities of zebra fish larvae. To achieve this, twelve features were defined and seven unsupervised learning methods were implemented. Next, seven performance measures were applied to evaluate and compare these methods. In order to empirically evaluate the machine learning algorithms, a large dataset was collected that contained 6847 valid instances. Using this dataset, the characteristics of the features were analyzed and the most appropriate unsupervised learning algorithm, i.e., Unweighted Pair Group Method with Arithmetic mean (UPGMA), for locomotor activity analysis was identified. In addition, UPGMA's ability to reveal underlying patterns of zebra fish locomotor activities was demonstrated. In general, this study shows that machine learning techniques have the potential to construct effective, high-throughput systems to automate the process of identifying zebra fish behaviors influenced by genetic manipulation, pharmaceuticals, and environmental toxins.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784605","Bioinformatics;locomotor behavior analysis;machine learning;zebrafish larva","Clustering algorithms;Machine learning algorithms;Measurement;Stability analysis;Turning;Unsupervised learning;Videos","bioinformatics;biomechanics;genetics;unsupervised learning;zoology","UPGMA ability;arithmetic mean;bioinformatics;complex zebrafish behavior identification;environmental factors;environmental toxins;feature characteristics;genetic factors;genetic manipulation;high-throughput systems;machine learning techniques;pharmaceuticals;unsupervised learning method;unweighted pair group method;zebrafish larva locomotor activity analysis","","1","","13","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Breast cancer mass localization based on machine learning","A. Qasem; S. N. H. S. Abdullah; S. Sahran; T. S. M. T. Wook; R. I. Hussain; N. Abdullah; F. Ismail","Pattern Recognition Res. Group, Univ. Kebangsaan Malaysia, Bangi, Malaysia","2014 IEEE 10th International Colloquium on Signal Processing and its Applications","20140428","2014","","","31","36","According to Breast Cancer Institute (BCI), Breast cancer is one of the most dangerous types of cancer that affects women all around the world. Based on clinical guidelines, the use of mammogram for an early detection of this cancer is an important step in reducing its danger. Thus, computer aided detection using image processing techniques in analyzing mammogram images and localizing abnormalities such as mass has been used. A False Positive (FP) rate is considered a challenge in localizing mass in mammogram images. Hence, in this paper, the rejection model based on the Support Vector Machine (SVM) has been used in reducing the FP rate of segmented mammogram images using the Chan-Vese method, initialized by the Marker Controller Watershed (MCWS) algorithm. Firstly, a mammogram image is segmented using the MCWS algorithm. Then, the segmentation is refined using Chan-Vese. After that, the SVM rejection model is built and is used in rejecting the non-correct segmented nodules. The dataset which consists of 16 nodules and 28 non-nodules has been obtained from the UKM Medical Centre. The experiment has shown the effectiveness of the SVM rejection model in reducing the FP rate compared to the result without the use of the SVM rejection model.","","CD-ROM:978-1-4799-3090-6; Electronic:978-1-4799-3091-3; POD:978-1-4799-3092-0","10.1109/CSPA.2014.6805715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805715","Breast Cancer;Chan-Vese;MCWS;Mammogram;SVM","Breast cancer;Image segmentation;Predictive models;Signal processing algorithms;Support vector machines","image recognition;image segmentation;learning (artificial intelligence);mammography;medical image processing;support vector machines","Chan-Vese method;MCWS algorithm;SVM;breast cancer mass localization;computer aided detection;false positive rate;image processing techniques;image segmentation;machine learning;mammogram image;marker controller watershed algorithm;rejection model;support vector machine","","0","","17","","","7-9 March 2014","","IEEE","IEEE Conference Publications"
"Leveraging the error resilience of machine-learning applications for designing highly energy efficient accelerators","Z. Du; K. Palem; A. Lingamneni; O. Temam; Y. Chen; C. Wu","CARCH, ICT, China","2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)","20140220","2014","","","201","206","In recent years, inexact computing has been increasingly regarded as one of the most promising approaches for reducing energy consumption in many applications that can tolerate a degree of inaccuracy. Driven by the principle of trading tolerable amounts of application accuracy in return for significant resource savings - the energy consumed, the (critical path) delay and the (silicon) area being the resources - this approach has been limited to certain application domains. In this paper, we propose to expand the application scope, error tolerance as well as the energy savings of inexact computing systems through neural network architectures. Such neural networks are fast emerging as popular candidate accelerators for future heterogeneous multi-core platforms, and have flexible error tolerance limits owing to their ability to be trained. Our results based on simulated 65nm technology designs demonstrate that the proposed inexact neural network accelerator could achieve 43.91%-62.49% savings in energy consumption (with corresponding delay and area savings being 18.79% and 31.44% respectively) when compared to existing baseline neural network implementation, at the cost of an accuracy loss (quantified as the Mean Square Error (MSE) which increases from 0.14 to 0.20 on average).","2153-6961;21536961","Electronic:978-1-4799-2816-3; POD:978-1-4799-2817-0; USB:978-1-4799-2815-6","10.1109/ASPDAC.2014.6742890","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742890","","Accuracy;Benchmark testing;Biological neural networks;Hardware;Neurons;Training","learning (artificial intelligence);mean square error methods;multiprocessing systems;neural nets","MSE;application accuracy;application scope;critical path delay;energy consumption;energy efficienct accelerators;energy savings;flexible error tolerance limits;future heterogeneous multicore platforms;inexact computing systems;machine-learning applications;mean square error;neural network architectures;resource savings;silicon area","","0","","25","","","20-23 Jan. 2014","","IEEE","IEEE Conference Publications"
"Tactile sensing based softness classification using machine learning","I. Bandyopadhyaya; D. Babu; A. Kumar; J. Roychowdhury","Embedded Syst. Lab., CMERI, Durgapur, India","2014 IEEE International Advance Computing Conference (IACC)","20140327","2014","","","1231","1236","The research on tactile sensors and its wide applications have received extensive attention among researchers very recently, especially in the two fields-Medical Surgery (Minimally Invasive Surgery-MIS) and Fruit and Vegetable Grading Industry. This paper proposes the implementation of a robotic system which can distinguish objects of different softness using machine learning approach, based on different parameters. Two piezoresistive flexible tactile sensors are mounted on a two fingered robotic gripper, as robotic arm can perform repetitive tasks under a controlled environment. A PIC32 microcontroller is used to control the gripping action and to acquire pressure data. Decision Tree and Naive Bayes methods are used as intelligent classifiers using feature vectors, obtained from the time series response of tactile sensors during grasping action for grading the objects. From the analytical point of view it is observed that Decision Tree based approach is better than the Bayesian approach.","","CD-ROM:978-1-4799-2571-1; Electronic:978-1-4799-2572-8; POD:978-1-4799-2573-5","10.1109/IAdCC.2014.6779503","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779503","classifier;flexiforce;fruit grading;gripper;machine learning;sensor;tactile","Conferences","Bayes methods;control engineering computing;decision trees;grippers;learning (artificial intelligence);microcontrollers;tactile sensors;time series","PIC32 microcontroller;decision tree;feature vector;grasping action;gripping action;intelligent classifier;machine learning;naive Bayes method;piezoresistive flexible tactile sensor;pressure data;robotic arm;robotic system;softness classification;tactile sensing;time series;two fingered robotic gripper","","2","","18","","","21-22 Feb. 2014","","IEEE","IEEE Conference Publications"
"MaSiF: Machine learning guided auto-tuning of parallel skeletons","A. Collins; C. Fensch; H. Leather; M. Cole","School of Informatics, University of Edinburgh, EH8 9AB, UK","20th Annual International Conference on High Performance Computing","20140417","2013","","","186","195","Parallel skeletons provide a predefined set of parallel templates that can be combined, nested and parameterized with sequential code to produce complex parallel programs. The implementation of each skeleton includes parameters that have a significant effect on performance; so carefully tuning them is vital. The optimization space formed by these parameters is complex, non-linear, exhibits multiple local optima and is program dependent. This makes manual tuning impractical. Effective automatic tuning is therefore essential for the performance of parallel skeleton programs. In this paper we present MaSiF, a novel tool to auto-tune the parallelization parameters of skeleton parallel programs. It reduces the size of the parameter space using a combination of machine learning, via nearest neighbor classification, and linear dimensionality reduction using Principal Components Analysis. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. Good parallelization parameters are found quickly by searching this smaller search space. We evaluate MaSiF for two existing parallel frameworks: Threading Building Blocks and FastFlow. MaSiF achieves 89% of the performance of the oracle on average. This exploration requires just 45 parameters values on average, which is ~0.05% of the optimization space. In contrast, a state-of-the-art machine learning approach achieves 51%. MaSiF achieves an average speedup of 1.32× over parallelization parameters chosen by human experts.","1094-7256;10947256","Electronic:978-1-4799-0730-4; POD:978-1-4799-0728-1","10.1109/HiPC.2013.6799098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799098","","Feature extraction;Optimization;Principal component analysis;Skeleton;Training;Tuning;Vectors","","","","0","","24","","","18-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"Contact state estimation using machine learning","N. Jamali; P. Kormushev; D. G. Caldwell","Department of Advanced Robotics, Istituto Italiano di Technologia, via Morego 30, 16163 Genoa, Italy","2013 OCEANS - San Diego","20140217","2013","","","1","4","In this paper we present an approach that uses machine learning to determine the location of a contact between a gripper and a T-bar valve based on force/torque sensor data. The robot performs an exploratory behaviour that produces distinct force/torque data for each contact location of interest: no contact, a contact aligned with the central axis of the valve, and an off-center contact. Probabilistic clustering is utilised to transform the multidimensional data into a one-dimensional sequence of symbols, which is then used to train a hidden Markov model classifier. We present the results of an experiment where the learned classifier can predict a contact location with an accuracy of 97% on an unseen dataset.","0197-7385;01977385","Electronic:978-0-933957-40-4","10.23919/OCEANS.2013.6740992","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740992","","Force;Grippers;Hidden Markov models;Robot sensing systems;Torque;Valves","dexterous manipulators;grippers;hidden Markov models;learning (artificial intelligence);manipulator kinematics;mechanical contact;mechanical engineering computing;pattern classification","T-bar valve;contact behaviour;contact location;contact state estimation;exploratory behaviour;force-torque sensor data;gripper;hidden Markov model classifier;machine learning;multidimensional data;no-contact behaviour;off-center contact behaviour;one-dimensional symbol sequence;probabilistic clustering","","0","1","13","","","23-27 Sept. 2013","","IEEE","IEEE Conference Publications"
"30.10 A 1TOPS/W analog deep machine-learning engine with floating-gate storage in 0.13μm CMOS","J. Lu; S. Young; I. Arel; J. Holleman","Univ. of Tennessee, Knoxville, TN, USA","2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)","20140306","2014","","","504","505","Direct processing of raw high-dimensional data such as images and video by machine learning systems is impractical both due to prohibitive power consumption and the “curse of dimensionality,” which makes learning tasks exponentially more difficult as dimension increases. Deep machine learning (DML) mimics the hierarchical presentation of information in the human brain to achieve robust automated feature extraction, reducing the dimension of such data. However, the computational complexity of DML systems limits large-scale implementations in standard digital computers. Custom analog or mixed-mode signal processors have been reported to yield much higher energy efficiency than DSP [1-4], presenting the means of overcoming these limitations. However, the use of volatile digital memory in [1-3] precludes their use in intermittently-powered devices, and the required interfacing and internal A/D/A conversions add power and area overhead. Nonvolatile storage is employed in [4], but the lack of learning capability requires task-specific programming before operation, and precludes online adaptation.","0193-6530;01936530","Electronic:978-1-4799-0920-9; POD:978-1-4799-0917-9","10.1109/ISSCC.2014.6757532","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6757532","","Accuracy;Computer architecture;Energy efficiency;Engines;Feature extraction;Nonvolatile memory;Training","CMOS integrated circuits;analogue-digital conversion;computational complexity;digital-analogue conversion;feature extraction;learning (artificial intelligence);mixed analogue-digital integrated circuits;random-access storage","CMOS technology;DML;analog deep machine learning engine;computational complexity;floating-gate storage;internal A-D-A conversions;mixed mode signal processors;nonvolatile storage;robust automated feature extraction;size 0.13 mum;volatile digital memory","","2","","6","","","9-13 Feb. 2014","","IEEE","IEEE Conference Publications"
"POSTECH BCIs with machine learning","S. Choi","Department of Computer Science and Engineering Pohang University of Science and Technology 77 Cheongam-ro, Nam-gu, Pohang 790-784, Korea","2014 International Winter Workshop on Brain-Computer Interface (BCI)","20140403","2014","","","1","1","This paper outlines a brief overview of brain computer interfaces (BCIs), the research on which has been conducted at POSTECH machine learning lab. It has three folds. First, matrix factorization methods are introduced, which are used to learn spectral features for automatic classification of brain waves. Second, Bayesian multi-task learning methods are presented, which are applied to multi-subject EEG classification where subject-to-subject transfer is often considered to improve EEG classification. Third, tongue-machine interface is presented, where glossokinetic potentials involving tongue movements are analyzed to predict where tongue touches around gum line.","","Electronic:978-1-4799-2588-9","10.1109/iww-BCI.2014.6782554","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782554","","Bayes methods;Brain-computer interfaces;Electric potential;Electroencephalography;Probabilistic logic;Tensile stress;Tongue","brain-computer interfaces;learning (artificial intelligence);matrix decomposition","Bayesian multi-task learning methods;POSTECH BCI;POSTECH machine learning lab;brain computer interfaces;brain waves automatic classification;glossokinetic potentials;matrix factorization methods;multisubject EEG classification;tongue-machine interface","","0","","13","","","17-19 Feb. 2014","","IEEE","IEEE Conference Publications"
"When brain and behavior disagree: Tackling systematic label noise in EEG data with machine learning","A. K. Porbadnigk; N. Görnitz; C. Sannelli; A. Binder; M. Braun; M. Kloft; K. R. Müller","Machine Learning Group, Berlin Institute of Technology (TU Berlin), Berlin, Germany","2014 International Winter Workshop on Brain-Computer Interface (BCI)","20140403","2014","","","1","4","Conventionally, neuroscientific data is analyzed based on the behavioral response of the participant. This approach assumes that behavioral errors of participants are in line with the neural processing. However, this may not be the case, in particular in experiments with time pressure or studies investigating the threshold of perception. In these cases, the error distribution deviates from uniformity due to the heteroscedastic nature of the underlying experimental set-up. This problem of systematic and structured (non-uniform) label noise is ignored when analysis are based on behavioral data, as is being done typically. Thus, we run the risk to arrive at wrong conclusions in our analysis. This paper proposes a remedy to handle this crucial problem: we present a novel approach for a) measuring label noise and b) removing structured label noise. We show its usefulness for an EEG data set recorded during a standard d2 test for visual attention.","","Electronic:978-1-4799-2588-9","10.1109/iww-BCI.2014.6782561","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782561","Applied Cognitive Neuroscience;EEG;Label Noise;Machine Learning;Unsupervised Learning","Brain modeling;Electrodes;Electroencephalography;Kernel;Noise;Support vector machines;Visualization","behavioural sciences computing;electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology;signal denoising","EEG data set;behavior disagree;behavioral data;brain;error distribution;heteroscedastic nature;label noise measurement;machine learning;neural processing;neuroscientific data;participant behavioral errors;participant behavioral response;perception threshold;standard d2 test;structured label noise removal;systematic label noise;visual attention","","1","","18","","","17-19 Feb. 2014","","IEEE","IEEE Conference Publications"
"Atomic model learning: A machine learning paradigm for post silicon debug of RF/analog circuits","S. Deyati; B. J. Muldrey; A. Banerjee; A. Chatterjee","Electr. & Comput. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","2014 IEEE 32nd VLSI Test Symposium (VTS)","20140522","2014","","","1","6","As RF design scales to the 28nm technology node and beyond, pre-silicon simulation and verification of complex mixed-signal/RF SoCs is becoming intractable due to the difficulties associated with simulating diverse electrical effects and design bugs. As a consequence, there is increasing pressure to develop comprehensive post-silicon test and debug tools that can be used to identify design bugs and improve modeling of complex electrical nonidealities observed in silicon. Often, it is not known a-priori what these bugs are and how they can be modeled, significantly complicating the debug process. In this research, a new atomic model learning approach is proposed that uses supervised learning techniques to diagnose design bugs and learn unknown module-level behaviors. Nonideality modeling artifacts called model atoms are inserted into different nodes of the design signal flow paths to learn unknown behaviors along those paths. Under the assumption that the design bug is localized, it is shown that the source of the bug can be identified with high resolution even when the nature of the bug is unknown. The method has been applied to a conventional wireless as well as a polar radio transmitter and key results that demonstrate usefulness and feasibility of the proposed approach are presented.","1093-0167;10930167","Electronic:978-1-4799-2611-4; POD:978-1-4799-2612-1","10.1109/VTS.2014.6818791","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818791","Artificial neural networks;Fault diagnosis;Mixers;Model checking;Power amplifiers;Preamplifiers;Radio transmitters","Integrated circuit modeling;Mathematical model;Mixers;Radio frequency;Radio transmitters;Silicon;Tuning","analogue integrated circuits;electronic engineering computing;integrated circuit design;integrated circuit modelling;learning (artificial intelligence);radiofrequency integrated circuits","RF SoC;RF circuits;analog circuits;atomic model learning;design bug diagnosis;machine learning paradigm;mixed-signal SoC;module level behavior;post silicon debug;supervised learning","","2","","18","","","13-17 April 2014","","IEEE","IEEE Conference Publications"
"Machine-Learning-Based Coadaptive Calibration for Brain-Computer Interfaces","C. Vidaurre; C. Sannelli; K. R. Müller; B. Blankertz","Machine Learning Department, Berlin Institute of Technology, Berlin 10587, Germany carmen.vidaurre@tu-berlin.de","Neural Computation","20140519","2011","23","3","791","816","Brain-computer interfaces (BCIs) allow users to control a computer application by brain activity as acquired (e.g., by EEG). In our classic machine learning approach to BCIs, the participants undertake a calibration measurement without feedback to acquire data to train the BCI system. After the training, the user can control a BCI and improve the operation through some type of feedback. However, not all BCI users are able to perform sufficiently well during feedback operation. In fact, a nonnegligible portion of participants (estimated 15%–30%) cannot control the system (a BCI illiteracy problem, generic to all motor-imagery-based BCIs). We hypothesize that one main difficulty for a BCI user is the transition from offline calibration to online feedback. In this work, we investigate adaptive machine learning methods to eliminate offline calibration and analyze the performance of 11 volunteers in a BCI based on the modulation of sensorimotor rhythms. We present an adaptation scheme that individually guides the user. It starts with a subject-independent classifier that evolves to a subject-optimized state-of-the-art classifier within one session while the user interacts continuously. These initial runs use supervised techniques for robust coadaptive learning of user and machine. Subsequent runs use unsupervised adaptation to track the features’ drift during the session and provide an unbiased measure of BCI performance. Using this approach, without any offline calibration, six users, including one novice, obtained good performance after 3 to 6 minutes of adaptation. More important, this novel guided learning also allows participants with BCI illiteracy to gain significant control with the BCI in less than 60 minutes. In addition, one volunteer without sensorimotor idle rhythm peak at the beginning of the BCI experiment developed it during the course of the session and used voluntary modulation of its amplitude to control the feedback application.","0899-7667;08997667","","10.1162/NECO_a_00089","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6796914","","","","","","29","","","","","March 2011","","MIT Press","MIT Press Journals"
"A Machine Learning Method for Extracting Symbolic Knowledge from Recurrent Neural Networks","A. Vahed; C. W. Omlin","Department of Computer Science, University of the Western Cape, 7535 Bellville, South Africa, <email xlink:href="mailto:avahed@uwc.ac.za" xlink:type="simple">avahed@uwc.ac.za</email>","Neural Computation","20140519","2004","16","1","59","71","Neural networks do not readily provide an explanation of the knowledge stored in their weights as part of their information processing. Until recently, neural networks were considered to be black boxes, with the knowledge stored in their weights not readily accessible. Since then, research has resulted in a number of algorithms for extracting knowledge in symbolic form from trained neural networks. This article addresses the extraction of knowledge in symbolic form from recurrent neural networks trained to behave like deterministic finite-state automata (DFAs). To date, methods used to extract knowledge from such networks have relied on the hypothesis that networks' states tend to cluster and that clusters of network states correspond to DFA states. The computational complexity of such a cluster analysis has led to heuristics that either limit the number of clusters that may form during training or limit the exploration of the space of hidden recurrent state neurons. These limitations, while necessary, may lead to decreased fidelity, in which the extracted knowledge may not model the true behavior of a trained network, perhaps not even for the training set. The method proposed here uses a polynomial time, symbolic learning algorithm to infer DFAs solely from the observation of a trained network's input-output behavior. Thus, this method has the potential to increase the fidelity of the extracted knowledge.","0899-7667;08997667","","10.1162/08997660460733994","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6788938","","","","","","1","","","","","Jan. 1 2004","","MIT Press","MIT Press Journals"
"Providing Transaction Class-Based QoS in In-Memory Data Grids via Machine Learning","P. D. Sanzo; F. Molfese; D. Rughetti; B. Ciciani","DIAG, Sapienza Univ. of Rome, Rome, Italy","2014 IEEE 3rd Symposium on Network Cloud Computing and Applications (ncca 2014)","20140414","2014","","","46","53","Elastic architectures and the ""pay-as-you-go"" resource pricing model offered by many cloud infrastructure providers may seem the right choice for companies dealing with data centric applications characterized by high variable workload. In such a context, in-memory transactional data grids have demonstrated to be particularly suited for exploiting advantages provided by elastic computing platforms, mainly thanks to their ability to be dynamically (re-)sized and tuned. Anyway, when specific QoS requirements have to be met, this kind of architectures have revealed to be complex to be managed by humans. Particularly, their management is a very complex task without the stand of mechanisms supporting run-time automatic sizing/tuning of the data platform and the underlying (virtual) hardware resources provided by the cloud. In this paper, we present a neural network-based architecture where the system is constantly and automatically re-configured, particularly in terms of computing resources, in order to achieve transaction class-based QoS while minimizing costs of the infrastructure. We also present some results showing the effectiveness of our architecture, which has been evaluated on top of Future Grid IaaS Cloud using Red Hat Infinispan in-memory data grid and the TPC-C benchmark.","","CD-ROM:978-0-7695-5168-5; Electronic:978-1-4799-4952-6; POD:978-1-4799-4951-9","10.1109/NCCA.2014.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786762","In-memory transactional data grids;cloud computing;machine learning;neural networks;performance optimization;quality of service","Cloud computing;Data models;Memory management;Neural networks;Servers;Time factors","cloud computing;grid computing;learning (artificial intelligence);neural nets","Grid IaaS Cloud;Red Hat Infinispan in-memory data grid;TPC-C benchmark;cloud infrastructure;elastic architecture;elastic computing platform;in-memory transactional data grid;machine learning;neural network-based architecture;pay-as-you-go resource pricing model;run-time automatic sizing;transaction class-based QoS","","3","","20","","","5-7 Feb. 2014","","IEEE","IEEE Conference Publications"
"Text Simplification Tools: Using Machine Learning to Discover Features that Identify Difficult Text","D. Kauchak; O. Mouradi; C. Pentoney; G. Leroy","Middlebury Coll., Middlebury, VT, USA","2014 47th Hawaii International Conference on System Sciences","20140310","2014","","","2616","2625","Although providing understandable information is a critical component in healthcare, few tools exist to help clinicians identify difficult sections in text. We systematically examine sixteen features for predicting the difficulty of health texts using six different machine learning algorithms. Three represent new features not previously examined: medical concept density, specificity (calculated using word-level depth in MeSH); and ambiguity (calculated using the number of UMLS Metathesaurus concepts associated with a word). We examine these features for a binary prediction task on 118,000 simple and difficult sentences from a sentence-aligned corpus. Using all features, random forests is the most accurate with 84% accuracy. Model analysis of the six models and a complementary ablation study shows that the specificity and ambiguity features are the strongest predictors (24% combined impact on accuracy). Notably, a training size study showed that even with a 1% sample (1,062 sentences) an accuracy of 80% can be achieved.","1530-1605;15301605","Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6","10.1109/HICSS.2014.330","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758930","machine learning;text readability;text simplification","Electronic publishing;Encyclopedias;Feature extraction;Internet;Readability metrics;Unified modeling language","health care;learning (artificial intelligence);medical computing;natural language processing;text analysis","MeSH;UMLS metathesaurus concepts;binary prediction task;health texts;healthcare;machine learning;medical concept ambiguity;medical concept density;medical concept specificity;random forests;sentence-aligned corpus;text identification;text simplification tools;word-level depth","","0","","52","","","6-9 Jan. 2014","","IEEE","IEEE Conference Publications"
"Resolution of overlapping fluorescence spectra using the kernel learning machine","L. Gao; S. Ren","Dept. of Chem., Inner Mongolia Univ., Hohhot, China","2013 Ninth International Conference on Natural Computation (ICNC)","20140519","2013","","","60","64","A novel method based on combination of highly sensitive spectrofluorimetry and flexible the machine learning techniques was proposed for the simultaneous spectrofluorimetric determination of α-naphthol, β-naphthylamine and carbazole with overlapping peaks. This method addresses multivariate calibration based on the least square support vector machines (LS-SVM) regression to provide a powerful model for machine learning and data mining. The LS-SVM technique has the advantages to offer the capability of learning a high dimensional feature with fewer training data, and to decrease the computational complexity by only requiring to solve a set of linear equations instead of a quadratic programming problem. Experimental results showed the LS-SVM method to be successful for simultaneous multicomponent determination even where severe overlap of spectra was present. The relative standard errors of prediction (RSEP) obtained for total components using LS-SVM and PLS were compared. It is found that the LS-SVM method is better than the conventional PLS methods.","2157-9555;21579555","Electronic:978-1-4673-4714-3; POD:978-1-4673-4712-9","10.1109/ICNC.2013.6817944","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6817944","least square support vector machines;overlapping fluorescence spectra;polycyclic aromatic hydrocarbon;the kernel learning machine","Equations;Fluorescence;Kernel;Mathematical model;Standards;Support vector machines;Training","chemical engineering computing;computational complexity;data mining;fluorescence;learning (artificial intelligence);linear algebra;regression analysis;spectroscopy computing;support vector machines","α-naphthol;β-naphthylamine;LS-SVM technique;RSEP;carbazole;data mining;decrease computational complexity;highly sensitive spectrofluorimetry;kernel learning machine;least square support vector machines regression;linear equations;machine learning;multivariate calibration;overlapping fluorescence spectra resolution;overlapping peaks;relative standard errors-of-prediction;simultaneous multicomponent determination;simultaneous spectrofluorimetric determination","","0","","17","","","23-25 July 2013","","IEEE","IEEE Conference Publications"
"Fault Detection in Wireless Sensor Networks: A Machine Learning Approach","E. U. Warriach; K. Tei","Dept. of Math. & Comput. Sci., Univ. of Groningen, Groningen, Netherlands","2013 IEEE 16th International Conference on Computational Science and Engineering","20140306","2013","","","758","765","Wireless Sensor Network (WSN) deployment experiences show that collected data is prone to be faulty. Faults are due to internal and external influences, such as calibration, low battery, environmental interference and sensor aging. However, only few solutions exist to deal with faulty sensory data in WSN. We develop a statistical approach to detect and identify faults in a WSN. In particular, we focus on the identification and classification of data and system fault types as it is essential to perform accurate recovery actions. Our method uses Hidden Markov Models (HMMs) to capture the fault-free dynamics of an environment and dynamics of faulty data. It then performs a structural analysis of these HMMs to determine the type of data and system faults affecting sensor measurements. The approach is validated using real data obtained from over one month of samples from motes deployed in an actual living lab.","","Electronic:978-0-7695-5096-1; POD:978-1-4799-4897-0","10.1109/CSE.2013.116","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755296","","Fault detection;Fault diagnosis;Hidden Markov models;Humidity;Temperature measurement;Temperature sensors;Wireless sensor networks","fault diagnosis;hidden Markov models;learning (artificial intelligence);telecommunication computing;wireless sensor networks","HMM;WSN;fault detection;fault-free dynamics;hidden Markov model;machine learning;sensor measurement;statistical approach;wireless sensor network","","3","","14","","","3-5 Dec. 2013","","IEEE","IEEE Conference Publications"
"Energy-Efficient Time-of-Flight Estimation in the Presence of Outliers: A Machine Learning Approach","A. Apartsin; L. N. Cooper; N. Intrator","Blavatnik Sch. of Comput. Sci., Tel-Aviv Univ., Tel-Aviv, Israel","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20140422","2014","7","4","1306","1313","The time-of-flight (ToF) estimation problem is common in sonar, ultrasound, radar, and other remote sensing applications. The conventional ToF maximum-likelihood estimator (MLE) exhibits a rapid deterioration in the accuracy when the signal-to-noise ratio (SNR) falls below a certain threshold. This threshold effect emerges mostly due to appearance of outliers associated with the side lobes in the autocorrelation function of a narrowband source signal. In our previous work, we have introduced a bank of unmatched filters and biased ToF estimators derived using these filters. These biased estimators form a feature vector for training a classifier which, subsequently, is used for reducing the bias and the variance parts induced by outliers in the mean-square error (MSE) of the MLE. In this paper, we extend the above method by introducing an adaptive scheme for controlling the number of measurements (pulses) required to achieve a desired accuracy. We show that using the information provided by a classifier, it is possible to achieve the estimation error of the MLE but by using significantly less number of pulses and thus energy on average.","1939-1404;19391404","","10.1109/JSTARS.2013.2295324","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6701351","Biosonar;fusion of estimates;sonar;threshold effect;time-of-flight (ToF) estimation","Correlation;Maximum likelihood estimation;Measurement uncertainty;Pulse measurements;Remote sensing;Signal to noise ratio","geophysical techniques;learning (artificial intelligence);mean square error methods;remote sensing","biased ToF estimators;energy-efficient time-of-flight estimation;estimation error;machine learning approach;mean-square error method;radar applications;remote sensing applications;sonar applications;ultrasound applications","","1","","33","","20140102","April 2014","","IEEE","IEEE Journals & Magazines"
"Machine learning-based multi-channel evaluation pooling strategy for image quality assessment","A. Hu; R. Zhang; D. Yin; W. Hu","Dept. of Electron. Eng. & Inf. Sci., Univ. of Sci. & Technol. of China, Hefei, China","2013 IEEE International Conference on Image Processing","20140213","2013","","","427","430","Multi-channel peculiarity is one of the most widely accepted human visual system (HVS) models for perceptual image quality assessment (IQA). Otherwise than extensive studies of channel decomposition and intra-channel distortion measure, relatively scant research effort has been devoted to develop efficient multichannel evaluation pooling strategies. In this paper, we review and address the limitations of the conventional pooling models based on HVS sensitivities-weighted average. Instead, we explore the utilization of machine learning for this pooling problem, since machine learning can establish an optimal and generalized mapping that models the highly complex relationship between the multi-channel distortion evaluations and the perceived image quality. Experiments based on available subjective IQA databases demonstrate the rationality, reliability and robustness of our proposed scheme.","1522-4880;15224880","Electronic:978-1-4799-2341-0; POD:978-1-4799-2342-7; USB:978-1-4799-2340-3","10.1109/ICIP.2013.6738088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738088","Evaluation pooling;Image quality assessment (IQA);Machine learning;Multichannel model","Databases;Image quality;Sensitivity;Training;Visualization","image processing;learning (artificial intelligence)","HVS model;HVS sensitivity-weighted average;IQA database;channel decomposition;generalized mapping;highly-complex relationship;human visual system model;image quality assessment;intrachannel distortion measure;machine learning;multichannel distortion evaluation;multichannel evaluation pooling strategy;multichannel peculiarity;optimal mapping;perceived image quality;perceptual IQA;perceptual image quality assessment","","0","","22","","","15-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Machine Learning Techniques for LV Wall Motion Classification Based on Spatio-temporal Profiles from Cardiac Cine MRI","J. Mantilla; M. Garreau; J. J. Bellanger; J. L. Paredes","INSERM, Rennes, France","2013 12th International Conference on Machine Learning and Applications","20140410","2013","1","","167","172","In this paper, we propose an automated method to classify normal/abnormal wall motion in Left Ventricle (LV) function in cardiac cine-Magnetic Resonance Imaging (MRI). Without the need of pre-processing and by exploiting all the images of a cardiac cycle, spatio-temporal profiles are extracted from a subset of diametrical lines crossing opposites segments of the ventricular cavity. Two machine learning techniques are adapted and tested. The first one, is based on classical Support Vector Machines (SVM) and the second one that is proposed is based on dictionary learning (DL), adapted for classification in a supervised learning fashion. The experiments are evaluated based on features extracted from gray levels of the spatio-temporal profile as well as their representations in other basis such as Fourier and Wavelet domains under the assumption that the data may be sparse in one of those domains. The best classification performance has been obtained with a three-level db4 2-Dimensional wavelet transform using Fisher Discriminative Dictionary Learning as technique of classification.","","CD-ROM:978-1-4799-4154-4; Electronic:978-0-7695-5144-9; POD:978-1-4799-4155-1","10.1109/ICMLA.2013.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784606","Dictionary Learning;Left Ventricle motion;Support Vector Machine;cardiac cine-Magnetic Resonance Imaging","Dictionaries;Image segmentation;Kernel;Support vector machines;Training;Vectors;Wavelet domain","Fourier transforms;biomedical MRI;cardiology;feature extraction;image classification;learning (artificial intelligence);medical image processing;support vector machines;wavelet transforms","Fisher discriminative dictionary learning;LV wall motion classification;SVM;abnormal wall motion;cardiac cine MRI;cardiac cine-magnetic resonance imaging;classical support vector machines;diametrical lines;feature extraction;left ventricle function;machine learning techniques;normal wall motion;spatio-temporal profiles;supervised learning fashion;three-level db4 2-dimensional wavelet transform;ventricular cavity","","1","","22","","","4-7 Dec. 2013","","IEEE","IEEE Conference Publications"
"Application for video analysis based on machine learning and computer vision algorithms","V. Pavlov; V. Khryashchev; E. Pavlov; L. Shmaglit","Yaroslavl State Univ., Yaroslavl, Russia","14th Conference of Open Innovation Association FRUCT","20140213","2013","","","90","100","An application for video data analysis based on computer vision methods is presented. The proposed system consists of five consecutive stages: face detection, face tracking, gender recognition, age classification and statistics analysis. AdaBoost classifier is utilized for face detection. A modification of Lucas and Kanade algorithm is introduced on the stage of tracking. Novel gender and age classifiers based on adaptive features and support vector machines are proposed. All the stages are united into a single system of audience analysis. The proposed software complex can find its applications in different areas, from digital signage and video surveillance to automatic systems of accident prevention and intelligent human-computer interfaces.","2305-7254;23057254","Electronic:978-1-4799-4977-9; POD:978-1-4799-2090-7","10.1109/FRUCT.2013.6737950","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737950","face recognition;gender and age estimation;machine learning;video analysis","Algorithm design and analysis;Classification algorithms;Computer vision;Estimation;Face;Optical filters;Support vector machines","computer vision;face recognition;image classification;learning (artificial intelligence);statistical analysis;video signal processing","AdaBoost classifier;Kanade algorithm;Lucas algorithm;accident prevention;adaptive features;age classification;age classifiers;audience analysis;automatic systems;computer vision algorithms;computer vision methods;digital signage;face detection;face tracking;gender recognition;intelligent human-computer interfaces;machine learning;software complex;statistics analysis;support vector machines;video data analysis;video surveillance","","2","","23","","","11-15 Nov. 2013","","IEEE","IEEE Conference Publications"
"Sparse Bayesian Extreme Learning Machine for Multi-classification","J. Luo; C. M. Vong; P. K. Wong","Department of Computer and Information Science, University of Macau, Taipa, Macao","IEEE Transactions on Neural Networks and Learning Systems","20140311","2014","25","4","836","843","Extreme learning machine (ELM) has become a popular topic in machine learning in recent years. ELM is a new kind of single-hidden layer feedforward neural network with an extremely low computational cost. ELM, however, has two evident drawbacks: 1) the output weights solved by Moore-Penrose generalized inverse is a least squares minimization issue, which easily suffers from overfitting and 2) the accuracy of ELM is drastically sensitive to the number of hidden neurons so that a large model is usually generated. This brief presents a sparse Bayesian approach for learning the output weights of ELM in classification. The new model, called Sparse Bayesian ELM (SBELM), can resolve these two drawbacks by estimating the marginal likelihood of network outputs and automatically pruning most of the redundant hidden neurons during learning phase, which results in an accurate and compact model. The proposed SBELM is evaluated on wide types of benchmark classification problems, which verifies that the accuracy of SBELM model is relatively insensitive to the number of hidden neurons; and hence a much more compact model is always produced as compared with other state-of-the-art neural network classifiers.","2162-237X;2162237X","","10.1109/TNNLS.2013.2281839","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615928","Bayesian learning;classification;extreme learning machine (ELM);sparsity","Accuracy;Bayes methods;Computational modeling;Couplings;Neurons;Support vector machines;Training","belief networks;feedforward neural nets;learning (artificial intelligence);minimisation;pattern classification","ELM;Moore-Penrose generalized inverse;benchmark classification problem;hidden neurons;least squares minimization;machine learning;marginal likelihood estimation;redundant hidden neurons;single-hidden layer feedforward neural network;sparse Bayesian extreme learning machine","0","43","","25","","20130930","April 2014","","IEEE","IEEE Journals & Magazines"
"Dynamic Feature Selection for Machine-Learning Based Concurrency Regulation in STM","D. Rughetti; P. D. Sanzo; B. Ciciani; F. Quaglia","","2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","20140414","2014","","","68","75","In this paper we explore machine-learning approaches for dynamically selecting the well suited amount of concurrent threads in applications relying on Software Transactional Memory (STM). Specifically, we present a solution that dynamically shrinks or enlarges the set of input features to be exploited by the machine-learner. This allows for tuning the concurrency level while also minimizing the overhead for input-features sampling, given that the cardinality of the input-feature set is always tuned to the minimum value that still guarantees reliability of workload characterization. We also present a fully heedged implementation of our proposal within the TinySTM open source framework, and provide the results of an experimental study relying on the STAMP benchmark suite, which show significant reduction of the response time with respect to proposals based on static feature selection.","1066-6192;10666192","Electronic:978-1-4799-2729-6; POD:978-1-4799-2731-9","10.1109/PDP.2014.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787254","Concurrency;Performance Models;Performance Optimization;Software Transactional Memory","Artificial neural networks;Benchmark testing;Concurrent computing;Correlation;Instruction sets;Proposals;Reliability","concurrency control;feature selection;learning (artificial intelligence)","STAMP benchmark suite;STM;TinySTM open source framework;concurrency level;concurrent threads;dynamic feature selection;input-features sampling;machine-learning based concurrency regulation;software transactional memory;static feature selection","","1","","14","","","12-14 Feb. 2014","","IEEE","IEEE Conference Publications"
"An intelligent maintenance based on machine learning approach for wireless and mobile systems","A. Chohra; F. Di Giandomenico; S. Porcarelli; A. Bondavalli","Images Signals and Intelligent Systems Laboratory, Paris-East University, Avenue Pierre Point, 77127, Lieusaint, France","Proceedings of the International Conference on Wireless Information Networks and Systems","20140206","2011","","","115","118","To enhance wireless and mobile system dependability, audit operations are necessary, to periodically check the database consistency and recover in case of data corruption. Consequently, how to tune the database audit parameters and which operation order and frequency to apply becomes important aspects, to optimize performance and satisfy a certain degree of Quality of Service, over system life-cycle. The aim of this work is then to suggest an intelligent maintenance system based on reinforcement Q-Learning approach, built of a given audit operation set and an audit manager, in order to maximize the performance (performability and unreliability). For this purpose, a methodology, based on deterministic and stochastic Petri nets, to model and analyze the dependability attributes of different scheduled audit strategies is first developed. Afterwards, an intelligent (reinforcement Q-Learning) software agent approach is developed for planning and learning to derive optimal maintenance policies adaptively dealing with the highly dynamic evolution of the environmental conditions. This intelligent approach, is then implemented with feedforward artificial neural networks under the supervised gradient back-propagation learning to guarantee the success even with large state spaces, exploits intelligent behaviors traits (learning, adaptation, generalization, and robustness) to derive optimal actions in different system states in order to achieve an intelligent maintenance system.","","Electronic:978-989-8425-73-7; POD:978-1-4673-0205-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733057","Analysis and Decision-Making;Artificial Neural Networks;Complex and Uncertain Systems;Database Audit Behaviors;Deterministic and Stochastic Petri Nets;Intelligent Software Agent;Optimal Maintenance Policies;Reinforcement Q-Learning and Supervised Gradient Back-Propagation Learning Paradigms;Wireless and Mobile Communication Systems","","","","","0","","7","","","18-21 July 2011","","IEEE","IEEE Conference Publications"
"Machine Learning in Prediction of Stock Market Indicators Based on Historical Data and Data from Twitter Sentiment Analysis","A. Porshnev; I. Redkin; A. Shevchenko","Higher Sch. of Econ., Nat. Res. Univ., Nizhny Novgorod, Russia","2013 IEEE 13th International Conference on Data Mining Workshops","20140306","2013","","","440","444","Development of linguistic technologies and penetration of social media provide powerful possibilities to investigate users' moods and psychological states of people. In this paper we discussed possibility to improve accuracy of stock market indicators predictions by using data about psychological states of Twitter users. For analysis of psychological states we used lexicon-based approach, which allow us to evaluate presence of eight basic emotions in more than 755 million tweets. The application of Support Vectors Machine and Neural Networks algorithms to predict DJIA and S&P500 indicators are discussed.","2375-9232;23759232","Electronic:978-1-4799-3142-2; POD:978-1-4799-3144-6","10.1109/ICDMW.2013.111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6753954","Neural Networks;Support Vectors Machine;Twitter;mood;prediction;psychological states;stock market indicators","Accuracy;Algorithm design and analysis;Dictionaries;Prediction algorithms;Stock markets;Support vector machines;Twitter","Internet;data handling;learning (artificial intelligence);neural nets;social networking (online);stock markets","Twitter sentiment analysis;Twitter users;historical data;lexicon based approach;linguistic technologies;machine learning;market indicators;neural networks algorithms;psychological states;social media;stock market indicators;support vectors machine","","4","","15","","","7-10 Dec. 2013","","IEEE","IEEE Conference Publications"
"Towards Model-Driven Engineering for Big Data Analytics -- An Exploratory Analysis of Domain-Specific Languages for Machine Learning","D. Breuker","ERCIS, Univ. of Muenster, Muenster, Germany","2014 47th Hawaii International Conference on System Sciences","20140310","2014","","","758","767","Graphical models and general purpose inference algorithms are powerful tools for moving from imperative towards declarative specification of machine learning problems. Although graphical models define the principle information necessary to adapt inference algorithms to specific probabilistic models, entirely model-driven development is not yet possible. However, generating executable code from graphical models could have several advantages. It could reduce the skills necessary to implement probabilistic models and may speed up development processes. Both advantages address pressing industry needs. They come along with increased supply of data scientist labor, the demand of which cannot be fulfilled at the moment. To explore the opportunities of model-driven big data analytics, I review the main modeling languages used in machine learning as well as inference algorithms and corresponding software implementations. Gaps hampering direct code generation from graphical models are identified and closed by proposing an initial conceptualization of a domain-specific modeling language.","1530-1605;15301605","Electronic:978-1-4799-2504-9; POD:978-1-4799-2505-6","10.1109/HICSS.2014.101","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6758697","Graphical Models;Machine Learning;Model-driven Engineering","Adaptation models;Computational modeling;Data models;Graphical models;Inference algorithms;Random variables;Unified modeling language","Big Data;computer graphics;data analysis;inference mechanisms;learning (artificial intelligence);program compilers;specification languages","big data analytics;direct code generation;domain-specific languages;domain-specific modeling language;general purpose inference algorithms;graphical models;machine learning problems;model-driven development;model-driven engineering;modeling languages;probabilistic models","","1","","37","","","6-9 Jan. 2014","","IEEE","IEEE Conference Publications"
"Nondegenerate Piecewise Linear Systems: A Finite Newton Algorithm and Applications in Machine Learning","X. T. Yuan; S. Yan","Department of Statistics, Rutgers University, NJ 08854, U.S.A., and Department of Electrical and Computer Engineering, National University of Singapore, 117583, Singapore xyuan@stat.rutgers.edu","Neural Computation","20140519","2012","24","4","1047","1084","We investigate Newton-type optimization methods for solving piecewise linear systems (PLSs) with nondegenerate coefficient matrix. Such systems arise, for example, from the numerical solution of linear complementarity problem, which is useful to model several learning and optimization problems. In this letter, we propose an effective damped Newton method, PLS-DN, to find the exact (up to machine precision) solution of nondegenerate PLSs. PLS-DN exhibits provable semiiterative property, that is, the algorithm converges globally to the exact solution in a finite number of iterations. The rate of convergence is shown to be at least linear before termination. We emphasize the applications of our method in modeling, from a novel perspective of PLSs, some statistical learning problems such as box-constrained least squares, elitist Lasso (Kowalski & Torreesani, <xref ref-type=""bibr"" rid=""B26"">2008</xref>), and support vector machines (Cortes & Vapnik, <xref ref-type=""bibr"" rid=""B13"">1995</xref>). Numerical results on synthetic and benchmark data sets are presented to demonstrate the effectiveness and efficiency of PLS-DN on these problems.","0899-7667;08997667","","10.1162/NECO_a_00241","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797397","","","","","","0","","","","","April 2012","","MIT Press","MIT Press Journals"
"Predicting an Orchestral Conductor's Baton Movements Using Machine Learning","D. G. Dansereau; N. Brock; J. R. Cooperstock","*Australian Centre for Field Robotics, School of Aerospace, Mechanical and Mechatronic Engineering, Rose Street Building J04, The University of Sydney 2006 NSW Sydney, Australia. d.dansereau@acfr.usyd.edu.au","Computer Music Journal","20140519","2013","37","2","28","45","Telematic musical performance, in which performers at two or more sites collaborate via networked audio and video, suffers significantly from latency. In the extreme case, performers at all sites slow to match their delayed counterparts, resulting in a steadily decreasing tempo. Introducing video of a conductor does not immediately solve the problem, as conductor video is also subjected to network latencies. This article lays the groundwork for an alternative approach to mitigating the effects of latency in distributed orchestral performances, based on generation of a predicted version of the conductor's baton trajectory. The prediction step is the most fundamental problem in this scheme, for which we propose the use of conventional machine learning techniques. Specifically, we demonstrate a particle filter and an extended Kalman filter that each track the location of the baton's tip and predict it multiple beats into the future; we compare these with a conventional feature-based method. We also describe a generic two-part framework that prescribes the incorporation of rehearsal data into a probabilistic model, which is then adapted during live performance. Finally, we suggest a framework and experimental methodology for establishing perceptually based metrics for predicted baton paths. Note that the perceptual efficacy of the presented methods requires experimental confirmation beyond the scope of this article.","0148-9267;01489267","","10.1162/COMJ_a_00173","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6792514","","","","","","0","","","","","June 2013","","MIT Press","MIT Press Journals"
"Assessing Intervention Timing in Computer-Based Education Using Machine Learning Algorithms","A. J. Stimpson; M. L. Cummings","Dept. of Aeronaut. & Astronaut., Massachusetts Inst. of Technol., Cambridge, MA, USA","IEEE Access","20140205","2014","2","","78","87","The use of computer-based and online education systems has made new data available that can describe the temporal and process-level progression of learning. To date, machine learning research has not considered the impacts of these properties on the machine learning prediction task in educational settings. Machine learning algorithms may have applications in supporting targeted intervention approaches. The goals of this paper are to: 1) determine the impact of process-level information on machine learning prediction results and 2) establish the effect of type of machine learning algorithm used on prediction results. Data were collected from a university level course in human factors engineering (n=35), which included both traditional classroom assessment and computer-based assessment methods. A set of common regression and classification algorithms were applied to the data to predict final course score. The overall prediction accuracy as well as the chronological progression of prediction accuracy was analyzed for each algorithm. Simple machine learning algorithms (linear regression, logistic regression) had comparable performance with more complex methods (support vector machines, artificial neural networks). Process-level information was not useful in post-hoc predictions, but contributed significantly to allowing for accurate predictions to be made earlier in the course. Process level information provides useful prediction features for development of targeted intervention techniques, as it allows more accurate predictions to be made earlier in the course. For small course data sets, the prediction accuracy and simplicity of linear regression and logistic regression make these methods preferable to more complex algorithms.","2169-3536;21693536","","10.1109/ACCESS.2014.2303071","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730683","Machine learning;decision support systems;educational technology;statistical learning;training","Artificial neural networks;Linear regression;Machine learning;Machine learning algorithms;Measurement;Prediction algorithms;Predictive models","computer based training;data analysis;decision support systems;educational courses;learning (artificial intelligence);pattern classification;regression analysis","chronological prediction accuracy progression;classification algorithms;computer-based assessment method;computer-based education systems;data collection;decision support systems;educational technology;final course score prediction;human factors engineering;intervention timing assessment;linear regression;logistic regression;machine learning algorithms;machine learning prediction results;online education systems;process-level learning progression;statistical learning;temporal learning progression;traditional classroom assessment method;university level course","","3","","15","","20140131","2014","","IEEE","IEEE Journals & Magazines"
"MLI: An API for Distributed Machine Learning","E. R. Sparks; A. Talwalkar; V. Smith; J. Kottalam; X. Pan; J. Gonzalez; M. J. Franklin; M. I. Jordan; T. Kraska","","2013 IEEE 13th International Conference on Data Mining","20140203","2013","","","1187","1192","MLI is an Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.","1550-4786;15504786","Electronic:978-0-7695-5108-1; POD:978-1-4799-1328-2","10.1109/ICDM.2013.158","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729619","distributed computing;machine learning;programming interface","Computational modeling;Logistics;MATLAB;Mathematical model;Sparks;Vectors","application program interfaces;distributed algorithms;learning (artificial intelligence)","API;MLI;application programming interface;data-centric computing;distributed algorithm;distributed machine learning;high-performance algorithm","","23","","28","","","7-10 Dec. 2013","","IEEE","IEEE Conference Publications"
"Energy efficient in-memory machine learning for data intensive image-processing by non-volatile domain-wall memory","H. Yu; Y. Wang; S. Chen; W. Fei; C. Weng; J. Zhao; Z. Wei","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)","20140220","2014","","","191","196","Image processing in conventional logic-memory I/O-integrated systems will incur significant communication congestion at memory I/Os for excessive big image data at exa-scale. This paper explores an in-memory machine learning on neural network architecture by utilizing the newly introduced domain-wall nanowire, called DW-NN. We show that all operations involved in machine learning on neural network can be mapped to a logic-in-memory architecture by non-volatile domain-wall nanowire. Domain-wall nanowire based logic is customized for in machine learning within image data storage. As such, both neural network training and processing can be performed locally within the memory. The experimental results show that system throughput in DW-NN is improved by 11.6x and the energy efficiency is improved by 92x when compared to conventional image processing system.","2153-6961;21536961","Electronic:978-1-4799-2816-3; POD:978-1-4799-2817-0; USB:978-1-4799-2815-6","10.1109/ASPDAC.2014.6742888","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6742888","","Adders;Energy efficiency;Nanoscale devices;Nonvolatile memory;Table lookup;Vectors","image processing;learning (artificial intelligence);logic circuits;neural nets;random-access storage","communication congestion;conventional logic-memory I-O-integrated systems;data intensive image-processing;domain-wall nanowire-based logic;energy-efficient in-memory machine learning;image data storage;logic-in-memory architecture;neural network architecture;neural network training;nonvolatile domain-wall memory;nonvolatile domain-wall nanowire","","3","","26","","","20-23 Jan. 2014","","IEEE","IEEE Conference Publications"
"Prototype Classification: Insights from Machine Learning","A. B. A. Graf; O. Bousquet; G. Rätsch; B. Schölkopf","Max Planck Institute for Biological Cybernetics, 72076 T&#252;bingen, Germany, and New York University, Center for Neural Science, New York, NY 10003, U.S.A. arnulf.graf@nyu.edu","Neural Computation","20140519","2009","21","1","272","300","We shed light on the discrimination between patterns belonging to two different classes by casting this decoding problem into a generalized prototype framework. The discrimination process is then separated into two stages: a projection stage that reduces the dimensionality of the data by projecting it on a line and a threshold stage where the distributions of the projected patterns of both classes are separated. For this, we extend the popular mean-of-class prototype classification using algorithms from machine learning that satisfy a set of invariance properties. We report a simple yet general approach to express different types of linear classification algorithms in an identical and easy-to-visualize formal framework using generalized prototypes where these prototypes are used to express the normal vector and offset of the hyperplane. We investigate non-margin classifiers such as the classical prototype classifier, the Fisher classifier, and the relevance vector machine. We then study hard and soft margin classifiers such as the support vector machine and a boosted version of the prototype classifier. Subsequently, we relate mean-of-class prototype classification to other classification algorithms by showing that the prototype classifier is a limit of any soft margin classifier and that boosting a prototype classifier yields the support vector machine. While giving novel insights into classification per se by presenting a common and unified formalism, our generalized prototype framework also provides an efficient visualization and a principled comparison of machine learning classification.","0899-7667;08997667","","10.1162/neco.2009.01-07-443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6796437","","","","","","0","","","","","Jan. 2009","","MIT Press","MIT Press Journals"
"Machine learning applied to human learning","K. Chandnani; D. Chavan; P. Desai; D. R. Kalbande","Dept. of Comput. Eng., Univ. of Mumbai, Mumbai, India","2013 Annual IEEE India Conference (INDICON)","20140130","2013","","","1","6","In order to memorize information, humans use various methods: mnemonics, visualization and relating the information to already known information. Despite these efforts, memorizing information is a difficult task for humans. On the other hand, a computer can easily memorize information. Using this ability of computers, our research tries to make memorizing word meanings easier for humans. Humans need to be reminded of the information at intervals (that need not be equally spaced) in order to register the information in the memory. Keeping this in mind, we use vocabulary building as the area of this research. Vocabulary building can be contextualized as in reading a book or de-contextualized as in learning word meanings from a list. We focus on de-contextualized vocabulary building, which is used in vocabulary based tests. In order to achieve the goal of building the de-contextualized vocabulary for a student, we need to assess the student at intervals of time, and using the information thus obtained, maximize the probability that the student will be successful in his/her next assessment. Thus, the aim is to find the assessment intervals and the number of times the word and its meaning have to be reiterated, using the data from the previous assessments, so that the word and its meaning get registered in the student's memory. Also, learning is a cognitive process, thus, adaptability for each student comes into the picture. In order to account for adaptability we use machine learning. The result of each assessment of the student is recorded in order to determine the probability of success in the next assessment. The research thus tries to solve the difficulties involving memory faced by humans in learning new vocabulary: be it in a native or non-native language.","2325-940X;2325940X","CD-ROM:978-1-4799-2274-1; Electronic:978-1-4799-2275-8; POD:978-1-4799-2276-5","10.1109/INDCON.2013.6725908","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6725908","","Buildings;Computers;Educational institutions;Logistics;Testing;Training data;Vocabulary","learning (artificial intelligence);vocabulary","cognitive process;decontextualized vocabulary building;human learning;machine learning;memorizing information;memory;mnemonics;nonnative language;probability;visualization;vocabulary based tests;word meanings","","0","","7","","","13-15 Dec. 2013","","IEEE","IEEE Conference Publications"
"Educational courseware evaluation using Machine Learning techniques","S. Singh; S. P. Lal","School of Computing, Information & Mathematical Science, University of the South Pacific, Suva, Fiji Islands","2013 IEEE Conference on e-Learning, e-Management and e-Services","20140210","2013","","","73","78","With the introduction of massive open online courses (MOOCs) and other web-based learning management systems (LMS), there is a greater need to develop methods for exploring the unique types of data that come from the educational context. This paper highlights the advantage of using Machine Learning (ML) as an e-planning tool to enhance learning and improve courseware development. Researchers generally consider student evaluation survey on courses to be highly reliable and at least moderately valid on courseware evaluation. However, low response rate, retaliation, grades and comparison with past instructors sometimes affects the reliability of the result. ML algorithms has been deployed in this paper to intelligently examine the interaction log data from the LMS to obtain a predictive map that permits mapping the online interaction behaviour of students with their course outcome. These predictive relationships are then investigated and ranked using various ML algorithms to evaluate and validate the various learning tools and activities, and their effectiveness within the course.","","Electronic:978-1-4799-1574-3; POD:978-1-4799-1575-0","10.1109/IC3e.2013.6735969","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735969","Artificial Intelligence;attribute ranking;e-learning;machine learning;online development","Conferences;Courseware;Discussion forums;Educational institutions;Electronic learning;Least squares approximations;Prediction algorithms","Internet;courseware;educational courses;human computer interaction;learning (artificial intelligence)","LMS;ML;MOOC;Web-based learning management systems;courseware development;e-planning tool;educational courseware evaluation;interaction log data;machine learning technique;massive open online courses;student evaluation survey;student online interaction behaviour","","0","","22","","","2-4 Dec. 2013","","IEEE","IEEE Conference Publications"
"Learning Fingerprint Orientation Fields Using Continuous Restricted Boltzmann Machines","M. Sahasrabudhe; A. M. Namboodiri","Centre for Visual Inf. Technol., IIIT Hyderabad, Hyderabad, India","2013 2nd IAPR Asian Conference on Pattern Recognition","20140327","2013","","","351","355","We aim to learn local orientation field patterns in fingerprints and correct distorted field patterns in noisy fingerprint images. This is formulated as a learning problem and achieved using two continuous restricted Boltzmann machines. The learnt orientation fields are then used in conjunction with traditional Gabor based algorithms for fingerprint enhancement. Orientation fields extracted by gradient-based methods are local, and do not consider neighboring orientations. If some amount of noise is present in a fingerprint, then these methods perform poorly when enhancing the image, affecting fingerprint matching. This paper presents a method to correct the resulting noisy regions over patches of the fingerprint by training two continuous restricted Boltzmann machines. The continuous RBMs are trained with clean fingerprint images and applied to overlapping patches of the input fingerprint. Experimental results show that one can successfully restore patches of noisy fingerprint images.","0730-6512;07306512","Electronic:978-1-4799-2190-4; POD:978-1-4799-2191-1","10.1109/ACPR.2013.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778339","Biometrics;Continuous Restricted Boltzmann Machines;Fingerprint Enhancement;Gabor Filters;Machine Learning","Algorithm design and analysis;Backpropagation;Databases;Neurons;Noise;Noise measurement;Training","Boltzmann machines;Gabor filters;fingerprint identification;gradient methods;image enhancement;image matching;image restoration;learning (artificial intelligence)","Gabor based algorithms;RBM;clean fingerprint images;continuous restricted Boltzmann machines;fingerprint enhancement;fingerprint matching;fingerprint orientation field learning;gradient-based methods;input fingerprint overlapping patches;local orientation field pattern learning;noisy fingerprint image patch restoration","","3","","15","","","5-8 Nov. 2013","","IEEE","IEEE Conference Publications"
"Estimating the wake losses in large wind farms: A machine learning approach","F. Japar; S. Mathew; B. Narayanaswamy; Chee Ming Lim; J. Hazra","Univ. Brunei Darussalam, Gadong, Brunei","ISGT 2014","20140519","2014","","","1","5","Estimating the wake losses in a wind farm is critical in the short term forecast of wind power, following the Numerical Weather Prediction (NWP) approach. Understanding the intensity of the wakes and the nature of its propagation within the wind farm still remains a challenge to scientist, engineers and utility operators. In this paper, five different machine learning methods are used to estimate the power deficit experienced by wind turbines due to the wake losses. Production data from the Horns Rev offshore wind farm, Denmark, have been used for the study. The methods used are linear regression, linear regression with feature engineering, nonlinear regression, Artificial Neural Networks (ANN) and Support Vector Regression (SVR). Power developed by individual turbines located at different positions within the farm were computed based on the above methods and compared with the actual power measurements. With the respective Variance Normalized Root Mean Square Error (VNRMSE) of 0.21 and 0.22, models based on ANN and SVR could estimate the wind farm wake effects at an acceptable accuracy level. The study shows that suitable machine learning methods can effectively be used in estimating the power deficits due to wake effects experienced in large wind farms.","","Electronic:978-1-4799-3653-3; POD:978-1-4799-3654-0; USB:978-1-4799-3652-6","10.1109/ISGT.2014.6816427","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6816427","Artificial neural networks;Machine learning;Regression analysis;Support vector machines;Wind farms","Artificial neural networks;Linear regression;Wind farms;Wind power generation;Wind speed;Wind turbines","learning (artificial intelligence);load forecasting;mean square error methods;neural nets;power engineering computing;regression analysis;support vector machines;wakes;wind turbines","ANN;Denmark;Horns Rev offshore wind farm;SVR;VNRMSE;artificial neural networks;linear regression;machine learning approach;machine learning methods;nonlinear regression;numerical weather prediction approach;power measurements;production data;support vector regression;variance normalized root mean square error;wake losses;wind power forecasting;wind turbines","","1","","21","","","19-22 Feb. 2014","","IEEE","IEEE Conference Publications"
"Machine learning of forecasting long-term economic crisis in Indonesia","S. Sa'adah; T. H. Liong; Adiwijaya","","2013 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20140313","2013","","","261","266","Oil (energy) is huge influence of economic Indonesia. Since many sectors from Industries until individual need it. In fact, Indonesia is a country with high density population. Because of that, the necessity of oil must be meet amount of inhabitant in Indonesia. If government failed to answer the demand of oil, then Indonesia will be face economic crisis for long-term. So that, the forecast of it still need to be concerned. Furthermore, data about population and oil import were used to forecast economic condition. Shape of it had been done by machine learning. The result shows that the growth of population has influence of oil needed. Because when the population increases exponentially then the necessity of energy (oil) consumption followed. It roots of economic crisis in long-term. It can be proved from the accuracy result in training around 98%, while 90% in testing. By mean of that, Indonesia should concern more about population aging on economic growth refer to availability of oil. In other perspective, machine either show that the model forecast still find error. Error was caused by the use of few data; beside the aspect of economic is complex and chaos area.","","Electronic:978-1-4799-4692-1; POD:978-1-4799-2489-9","10.1109/ICACSIS.2013.6761586","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761586","long term economic crisis;machine learning;oil import;population","Economics;Equations;Mathematical model;Sociology;Statistics;Testing;Training","economic forecasting;learning (artificial intelligence);petroleum;supply and demand","Indonesia;economic aspect;economic condition;economic growth;energy consumption;long-term economic crisis forecasting;machine learning;oil demand;oil energy;oil import;oil necessity;population aging","","0","","14","","","28-29 Sept. 2013","","IEEE","IEEE Conference Publications"
"Pedestrian detection using heuristic statistics and machine learning","Chia-Chen Li; Pei-Chen Wu; Chang Hong Lin","Dept. of Electron. & Comput. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan","2013 9th International Conference on Information, Communications & Signal Processing","20140407","2013","","","1","5","Pedestrian detection is an important research field in advanced driver assistance system (ADAS). This paper puts forward a pedestrian detection framework based on both heuristic statistics and machine learning. First, a restriction of region of interest (ROI) is set on the captured image. Second, the template matching coarsely detects candidate pedestrians by using a set of template images, the edge image of the current frame, and the difference image from previous and current frames. Next, the histogram analysis again roughly filters out the candidate pedestrians. Finally, Histogram of Oriented Gradients (HOG) combined with library support vector machine (LIBSVM) is used to verify those candidate pedestrians. The experimental results show that the proposed method can run in real-time, where the false negative rate is 1.43%, and the false positive rate is 0.16%.","","Electronic:978-1-4799-0434-1; POD:978-1-4799-0432-7","10.1109/ICICS.2013.6782960","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6782960","histogram analysis;histogram of oriented gradients;pedestrian detection;template matching","Accidents;Cameras;Histograms;Image edge detection;Roads;Training data;Vehicles","edge detection;learning (artificial intelligence);object detection;pedestrians;support vector machines","ADAS;HOG;LIBSVM;ROI;advanced driver assistance system;edge image;heuristic statistics;histogram analysis;histogram of oriented gradients;library support vector machine;machine learning;pedestrian detection;region of interest;template images;template matching","","0","","18","","","10-13 Dec. 2013","","IEEE","IEEE Conference Publications"
"An efficient flow-based botnet detection using supervised machine learning","M. Stevanovic; J. M. Pedersen","Dept. of Electron. Syst., Aalborg Univ., Aalborg, Denmark","2014 International Conference on Computing, Networking and Communications (ICNC)","20140410","2014","","","797","801","Botnet detection represents one of the most crucial prerequisites of successful botnet neutralization. This paper explores how accurate and timely detection can be achieved by using supervised machine learning as the tool of inferring about malicious botnet traffic. In order to do so, the paper introduces a novel flow-based detection system that relies on supervised machine learning for identifying botnet network traffic. For use in the system we consider eight highly regarded machine learning algorithms, indicating the best performing one. Furthermore, the paper evaluates how much traffic needs to be observed per flow in order to capture the patterns of malicious traffic. The proposed system has been tested through the series of experiments using traffic traces originating from two well-known P2P botnets and diverse non-malicious applications. The results of experiments indicate that the system is able to accurately and timely detect botnet traffic using purely flow-based traffic analysis and supervised machine learning. Additionally, the results show that in order to achieve accurate detection traffic flows need to be monitored for only a limited time period and number of packets per flow. This indicates a strong potential of using the proposed approach within a future on-line detection framework.","","Electronic:978-1-4799-2358-8; POD:978-1-4799-2360-1; USB:978-1-4799-2359-5","10.1109/ICCNC.2014.6785439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785439","Botnet;Botnet detection;Machine learning;Traffic analysis;Traffic classification","Accuracy;Bayes methods;Feature extraction;Protocols;Support vector machines;Training;Vegetation","computer network security;invasive software;learning (artificial intelligence);peer-to-peer computing;telecommunication traffic","P2P botnets;botnet neutralization;flow-based botnet detection;flow-based traffic analysis;malicious botnet network traffic identification;nonmalicious applications;packet flow;supervised machine learning","","8","","7","","","3-6 Feb. 2014","","IEEE","IEEE Conference Publications"
"Fingerprint Matching Using Rotational Invariant Image Based Descriptor and Machine Learning Techniques","R. Kumar; P. Chandra; M. Hanmandlu","Sch. of Inf. & Comm. Technol., GGSIP Univ., New Delhi, India","2013 6th International Conference on Emerging Trends in Engineering and Technology","20140327","2013","","","13","18","The reliability of fingerprint matching system is highly depends on the perfect alignment algorithm and a suitable matching techniques, which assign a label to the input fingerprint image. In this paper, we propose a rotation invariant fingerprint descriptor and a improved generalization performance classifier. The proposed new descriptor is represented by a histogram of local directional pattern (LDP) computed from extracted region of interest (ROI) of fingerprint images. For fingerprint matching, we propose a single hidden layer neural network (SLFN), which combines a powerful extreme learning machine (ELM) and a well generalized resilient propagation (RPROP) algorithm. The proposed fingerprint matching system comprises the following steps: fingerprint pre-processing/enhancement, ROI extraction, invariant LDP feature extraction, and matching using proposed hybrid classifier. The experimental result shows that the matching accuracy of the proposed system is improved as compare to ELM for lower values of hidden nodes, and other distance based matching approaches proposed in the literature.","2157-0477;21570477","CD-ROM:978-1-4799-2560-5; Electronic:978-1-4799-2561-2; POD:978-1-4799-2562-9","10.1109/ICETET.2013.4","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754764","ELM;SLFN;fingerprint matching;hybrid RP ELM;local directional pattern;region of interest (ROI);training algorithm","Classification algorithms;Feature extraction;Fingerprint recognition;Image matching;Machine learning algorithms;Training;Vectors","feature extraction;fingerprint identification;image classification;image enhancement;image matching;image segmentation;learning (artificial intelligence);neural nets","ELM;LDP histogram;ROI extraction;RPROP algorithm;SLFN;alignment algorithm;distance-based matching approaches;extreme learning machine;fingerprint enhancement;fingerprint matching system;fingerprint preprocessing;generalized resilient propagation algorithm;hidden node values;hybrid classifier;improved generalization performance classifier;input fingerprint image label assignment;invariant LDP feature extraction;local directional pattern histogram;machine learning techniques;matching accuracy improvement;region-of-interest extraction;rotational invariant fingerprint image-based descriptor;single-hidden layer neural network","","1","","31","","","16-18 Dec. 2013","","IEEE","IEEE Conference Publications"
"Relaxed context-aware machine learning midddleware (RCAMM) for Android","J. Punjabi; G. Taneja; S. Parkhi; N. Giri","Comput. Eng., V.E.S. Inst. of Technol., Mumbai, India","2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)","20140220","2013","","","92","97","Context Aware Computing is a promising approach of developing mobile applications that provide experiences and services in a manner that is fine-tuned based on the user's preferences. Applications such as Google Now, Apple Siri learn the User's activities from context related information and subsequently provide suggestions to the users in real-time. However, in almost all cases, application developers have to develop the same set of mechanisms to consume the context information and storing it in an appropriate form rather than focusing on the parts of the application that consume the context information. This approach results in the repetition of the same task and multiple copies of data. This paper presents our work detailing the development of a middleware that handles context information collection and its storage. The work provides a framework that allows the developers to easily implement context aware applications that consume the services provided by the middleware. Applications will only have to react to context data (past and present) while the middleware takes care of everything else such as the background service for context information collection and storage, thus reducing the redundancy, increasing adaptability and flexibility, and simultaneously supporting developers in rapid prototyping of context-aware applications. Thus the paper presents our work towards building sustainable Android Framework which follows the principle of Reformat, Reduce, Regenerate, Reuse and Repurpose.","","CD-ROM:978-1-4799-2177-5; Electronic:978-1-4799-2178-2; POD:978-1-4799-4960-1","10.1109/RAICS.2013.6745453","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6745453","Android;Context;context-awareness;framework;machine-learning;middleware","Androids;Context;Context-aware services;Humanoid robots;Middleware;Object oriented modeling;Runtime","Android (operating system);learning (artificial intelligence);middleware;mobile computing","Android framework;Apple Siri;Google Now;RCAMM;context aware computing;context information collection;mobile applications;reduce principle;reformat principle;regenerate principle;relaxed context-aware machine learning middleware;repurpose principle;reuse principle;user preferences","","1","","15","","","19-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"A comparative analysis of data sets using Machine Learning techniques","C. B. Abhilash; K. Rohitaksha; S. Biradar","Comput. Sci. & Eng., JSS Acad. of Tech. Educ., Bangalore, India","2014 IEEE International Advance Computing Conference (IACC)","20140327","2014","","","24","29","Machine Learning techniques are most widely used in the field of clustering of data. The K-means algorithm is one which is widely used algorithm for clustering of data sets and is easy to understand and simulate on different datasets. In our paper work we have used K-means algorithm for clustering of yeast dataset and iris datasets, in which clustering resulted in less accuracy with more number of iterations. We are simulating an improved version in K- means algorithm for clustering of these datasets, the Improved K-means algorithm use the technique of minimum spanning tree. An undirected graph is generated for all the input data points and then shortest distance is calculated which intern results in better accuracy and also with less number of iterations. Both algorithms have been simulated using java programming language; the results obtained from both algorithms are been compared and analyzed. Algorithms have been run for several times under different clustering groups and the analysis results showed that the Improved K- means algorithm has provided a better performance as compared to K-means algorithm; also Improved K-means algorithm showed that, as the number of cluster values increases the accuracy of the algorithm also increases. Also we have inferred from the results that at a particular value of K (cluster groups) the accuracy of Improved K-means algorithm is optimal.","","CD-ROM:978-1-4799-2571-1; Electronic:978-1-4799-2572-8; POD:978-1-4799-2573-5","10.1109/IAdCC.2014.6779289","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779289","Improved K-Means;K-Means;MST;Yeast dataset;iris dataset","Accuracy;Algorithm design and analysis;Bioinformatics;Clustering algorithms;Genomics;Iris","Java;data analysis;learning (artificial intelligence);pattern clustering;trees (mathematics)","Java programming language;clustering groups;data clustering;data sets comparative analysis;improved K-means algorithm;machine learning techniques;minimum spanning tree technique;undirected graph;yeast dataset","","0","","12","","","21-22 Feb. 2014","","IEEE","IEEE Conference Publications"
"A Machine Learning Evaluation of an Artificial Immune System","M. Glickman; J. Balthrop; S. Forrest","Department of Computer Science, University of New Mexico, Albuquerque, NM 87131-1386, USA, <email xlink:href="mailto:glickman@cs.unm.edu" xlink:type="simple">glickman@cs.unm.edu</email>","Evolutionary Computation","20140519","2005","13","2","179","212","ARTIS is an artificial immune system framework which contains several adaptive mechanisms. LISYS is a version of ARTIS specialized for the problem of network intrusion detection. The adaptive mechanisms of LISYS are characterized in terms of their machine-learning counterparts, and a series of experiments is described, each of which isolates a different mechanism of LISYS and studies its contribution to the system's overall performance. The experiments were conducted on a new data set, which is more recent and realistic than earlier data sets. The network intrusion detection problem is challenging because it requires one-class learning in an on-line setting with concept drift. The experiments confirm earlier experimental results with LISYS, and they study in detail how LISYS achieves success on the new data set.","1063-6560;10636560","","10.1162/1063656054088503","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6789163","Anomaly detection;artificial immune systems;computer security;immune system;machine learning;network intrusion detection","","","","","3","","","","","June 2005","","MIT Press","MIT Press Journals"
"Ventricular Fibrillation and Tachycardia Classification Using a Machine Learning Approach","Q. Li; C. Rajagopalan; G. D. Clifford","Inst. of Biomed. Eng., Shandong Univ., Jinan, China","IEEE Transactions on Biomedical Engineering","20140516","2014","61","6","1607","1613","Correct detection and classification of ventricular fibrillation (VF) and rapid ventricular tachycardia (VT) is of pivotal importance for an automatic external defibrillator and patient monitoring. In this paper, a VF/VT classification algorithm using a machine learning method, a support vector machine, is proposed. A total of 14 metrics were extracted from a specific window length of the electrocardiogram (ECG). A genetic algorithm was then used to select the optimal variable combinations. Three annotated public domain ECG databases (the American Heart Association Database, the Creighton University Ventricular Tachyarrhythmia Database, and the MIT-BIH Malignant Ventricular Arrhythmia Database) were used as training, test, and validation datasets. Different window sizes, varying from 1 to 10 s were tested. An accuracy (Ac) of 98.1%, sensitivity (Se) of 98.4%, and specificity (Sp) of 98.0% were obtained on the in-sample training data with 5 s-window size and two selected metrics. On the out-of-sample validation data, an Ac of 96.3% ± 3.4%, Se of 96.2% ± 2.7%, and Sp of 96.2% ± 4.6% were obtained by fivefold cross validation. The results surpass those of current reported methods.","0018-9294;00189294","","10.1109/TBME.2013.2275000","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570512","Machine learning;public domain electrocardiogram (ECG) database;support vector machine (SVM);ventricular fibrillation (VF) detection","Accuracy;Databases;Electrocardiography;Genetic algorithms;Measurement;Support vector machines;Training","electrocardiography;genetic algorithms;learning (artificial intelligence);medical signal detection;medical signal processing;patient monitoring;signal classification;support vector machines","American Heart Association Database;Creighton University Ventricular Tachyarrhythmia Database;MIT-BIH Malignant Ventricular Arrhythmia Database;VF-VT classification algorithm;annotated public domain ECG databases;electrocardiogram;external defibrillator;genetic algorithm;in-sample training data;machine learning approach;patient monitoring;rapid ventricular tachycardia classification;specific window length;support vector machine;ventricular fibrillation classification","0","34","","23","","20130726","June 2014","","IEEE","IEEE Journals & Magazines"
"Machine Learning of Jazz Grammars","J. Gillick; K. Tang; R. M. Keller","*Annkissam, One Broadway, 14th Floor, Cambridge, Massachusetts 02459, USA. jrgillick@wesleyan.edu","Computer Music Journal","20140519","2010","34","3","56","66","","0148-9267;01489267","","10.1162/COMJ_a_00006","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6792369","","","","","","0","","","","","Sept. 2010","","MIT Press","MIT Press Journals"
"A machine learning based method for optimal journal classification","S. Iqbal; M. Shaheen; Fazl-e-basit","Dept. of Compter Sci., Nat. Univ. of Comput. & Emerging Scinece, Peshawar, Pakistan","8th International Conference for Internet Technology and Secured Transactions (ICITST-2013)","20140303","2013","","","259","264","We present a hypothetical and realistic examination and exploration of a number of bibliometric indicators of journal performance. In this paper, the indicators we have focused upon are Eigenfactor indicator, Impact factor, audience factor and Article influence weight indicator. Our focus is to find the missing parameters and some limitations that have not been conducted in previous algorithms. To find the influential parameters and to propose a new journal performance factor, that ranked a journal in best accepted manner. For classification and verification purpose we use a machine learning classification technique (Bayesian classification). It is one of the most common learning algorithms in machine learning classification. Using bayesain classification, we classify several journals according to our proposed methods and compare results with the previous methods.","","Electronic:978-1-908320-20-9; POD:978-1-4799-3962-6","10.1109/ICITST.2013.6750202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750202","Article Influence;Eigenfactor;Impact Factor;Journal ranking;Prestige of Journal","Bibliometrics;Data mining;Databases;Editorials;Equations;Internet;Mathematical model","Bayes methods;electronic publishing;information analysis;learning (artificial intelligence);pattern classification","Bayesian classification;Eigenfactor indicator;audience factor;bibliometric indicators;impact factor;journal performance;journal performance factor;machine learning based method;machine learning classification technique;optimal journal classification","","0","","40","","","9-12 Dec. 2013","","IEEE","IEEE Conference Publications"
"Automated Shmoo data analysis: A machine learning approach","W. Wang","Intel Corp., Santa Clara, CA, USA","Fifteenth International Symposium on Quality Electronic Design","20140407","2014","","","212","218","In silicon testing, a Shmoo plot is commonly used to give us an insight into the silicon manufacturing development health. Shmoo plots and other silicon characterization data has high value, however, analysis of them is a time-consuming work. This paper establishes a machine learning based model to improve and automate the procedure in silicon data analysis for HVM test content development. Our experiment shows that the supervised learning model has good accuracy on VMIN estimation across various kinds of Shmoo issues (crack/sprinkle/ceiling). The accuracy attained is greatly improved over previous tools. The framework can be easily integrated into any automated tester software and would save time to market during first silicon characterization. Additionally, the methodology discussed in this work can be extended to the HVM test flow for silicon behavior.","1948-3287;19483287","CD-ROM:978-1-4799-3945-9; Electronic:978-1-4799-3946-6; POD:978-1-4799-3947-3","10.1109/ISQED.2014.6783327","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6783327","HVM;Machine Learning;Shmoo experiment;Silicon Testing","Accuracy;Algorithm design and analysis;Classification algorithms;Decision trees;Prediction algorithms;Silicon;Training","electronic engineering computing;elemental semiconductors;learning (artificial intelligence);semiconductor device manufacture;semiconductor device testing;silicon","HVM test content development;HVM test flow;Shmoo plot;VMIN estimation;automated Shmoo data analysis;automated tester software;machine learning approach;silicon characterization;silicon characterization data;silicon data analysis;silicon manufacturing development health;silicon testing;supervised learning model","","1","","27","","","3-5 March 2014","","IEEE","IEEE Conference Publications"
"A Novel Machine Learning Approach Toward Quality Assessment of Sensor Data","A. Rahman; D. V. Smith; G. Timms","Comput. Inf., CSIRO, Hobart, TAS, Australia","IEEE Sensors Journal","20140214","2014","14","4","1035","1047","A novel machine learning approach to assess the quality of sensor data using an ensemble classification framework is presented in this paper. The quality of sensor data is indicated by discrete quality flags that indicate the level of uncertainty associated with a sensor reading. Depending on the domain and the problem under consideration, the level of uncertainty is different and thus unsupervised methods like outlier detection fails to match the expectation. The quality flags are normally assigned by domain experts. Considering the volume of sensor data, manual assignment is a laborious task and subject to human error. Given a representative set of labelled data, a supervised classification approach is thus a feasible alternative. The nature of sensor data, however, poses some challenges to the classification task. Data of dubious quality exists in such data sets with very small frequency leading to the class imbalance problem. We thus adopt a cluster oriented sampling approach to address the imbalance issue. In addition, it is beneficial to train multiple classifiers to improve the overall classification accuracy. We thus produce multiple under-sampled training sets using cluster oriented sampling and train base classifiers on each of them. Decisions produced by the base classifiers are fused into a single decision using majority voting. We have evaluated the proposed ensemble classification framework by assessing the quality of marine sensor data obtained from sensors situated at Sullivans Cove, Hobart, Australia. Experimental results reveal that the proposed framework agrees with expert judgement with high accuracy and achieves superior classification performance than other state-of-the-art approaches.","1530-437X;1530437X","","10.1109/JSEN.2013.2291855","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671378","Sensors;class balancing;ensemble classifier;quality assessment of sensor data;time series classification","Australia;Biosensors;Conductivity;Quality assessment;Temperature sensors;Training","learning (artificial intelligence);pattern classification;sampling methods;sensors","cluster oriented sampling approach;ensemble classification framework;machine learning approach;marine sensor;multiple classifiers;multiple under-sampled training sets;quality assessment;sensor data;train base classifiers","","3","","42","","20131120","April 2014","","IEEE","IEEE Journals & Magazines"
"A machine learning approach to predict future power demand in real-time for a battery operated car","S. Pradhan; J. Roychaudhury","Embedded Syst. Lab., Central Mech. Eng. Res. Inst., Durgapur, India","2014 International Conference on the IMpact of E-Technology on US (IMPETUS)","20140320","2014","","","49","56","For any battery-employed system, it is essential for the battery management system to correctly predict the present operational condition of the battery. The fail safe operation of a safety critical system like battery operated car or any other lifesaving systems are heavily depend upon earlier prediction of battery life. SOC or State-of-Charge estimation is one of the well-known method to predict the runtime of a battery. Various approaches are adapted by automotive society to correctly predict the runtime or the SOC of a battery like Kalman filter, UKF and many others. This paper proposes a new approach, the method of regression to predict the future power demand of a car while running on the road. The aim is to identify that, the battery will support the run of the car in next 10 seconds or not. The runtime prediction of a battery, not only depends upon the starting SOC but also depends upon other factors like battery health and road profile imposed. To overcome this type of difficulties the self-corrective regression model is proposed and implemented. Experiments performed on different road profiles, validate demanded power by the car in up-coming 10 seconds of its run. The major problem of SoC estimation is to determine initial SoC of a battery. Extensive experiments needed to calculate the initial SoC and which may also vary with the life of the battery. The novelty of this work shows, the method to predict the future power demand by updating its model parameters and without any initial SoC calculation. Model parameters are updated by the introducing new current and voltage sample in the model.","","Electronic:978-93-329-0264-0; POD:978-1-4799-2603-9","10.1109/IMPETUS.2014.6775877","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6775877","Data Driven Prognostic Model;End of Discharge (EoD);State Of Charge (SOC)","Batteries;Data models;Mathematical model;Polynomials;Roads;System-on-chip","Kalman filters;battery chargers;battery management systems;battery powered vehicles;learning (artificial intelligence);power engineering computing;regression analysis;secondary cells","Kalman filter;SOC estimation;UKF;battery health;battery life prediction;battery management system;battery operated car;battery runtime prediction;battery-employed system;current sample;future power demand prediction;lifesaving systems;machine learning approach;regression method;road profile;safety critical system;self-corrective regression model;state-of-charge estimation;voltage sample","","0","","9","","","10-11 Jan. 2014","","IEEE","IEEE Conference Publications"
"Contrastive divergence learning for the Restricted Boltzmann Machine","J. W. Liu; G. H. Chi; X. L. Luo","Dept. of Autom., China Univ. of Pet., Beijing, China","2013 Ninth International Conference on Natural Computation (ICNC)","20140519","2013","","","18","22","The Deep Belief Network (DBN) recently introduced by Hinton is a kind of deep architectures which have been applied with success in many machine learning tasks. The DBN is based on Restricted Boltzmann Machine (RBM), which is a particular energy-based model. In this paper, we lay more emphasis on the modeling process and learning algorithm of the RBM. Furthermore, we design two kinds of experiments to prove the efficiency of the algorithm based on synthetic dataset and real dataset. The reconstruction data experiments are aimed at proving the convergence of the learning algorithm. The classification experiments are designed to testify the efficiency of the trained models. The result shows that contrastive divergence learning is an effective training algorithm for the RBM model.","2157-9555;21579555","Electronic:978-1-4673-4714-3; POD:978-1-4673-4712-9","10.1109/ICNC.2013.6817936","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6817936","contrastive divergence learning;markov chain;restricted boltzmann machine","Approximation methods;Computational modeling;Data models;Markov processes;Mathematical model;Training;Vectors","Boltzmann machines;belief networks;learning (artificial intelligence);pattern classification","DBN;RBM;classification experiments;contrastive divergence learning;deep belief network;energy-based model;learning algorithm;machine learning tasks;real dataset;reconstruction data experiments;restricted Boltzmann machine;synthetic dataset","","0","","14","","","23-25 July 2013","","IEEE","IEEE Conference Publications"
"Grasp Recognition for Uncalibrated Data Gloves: A Machine Learning Approach","G. Heumer; H. B. Amor; B. Jung","","Presence","20140519","2008","17","2","121","142","This paper presents a comparison of various machine learning methods applied to the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove. Instead, raw sensor readings are used as input features that are directly mapped to different categories of hand shapes. An experiment was carried out in which test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was comprehensively analyzed using numerous classification techniques provided in an open-source machine learning toolbox. Evaluated machine learning methods are composed of (a) 38 classifiers including different types of function learners, decision trees, rule-based learners, Bayes nets, and lazy learners; (b) data preprocessing using principal component analysis (PCA) with varying degrees of dimensionality reduction; and (c) five meta-learning algorithms under various configurations where selection of suitable base classifier combinations was informed by the results of the foregoing classifier evaluation. Classification performance was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably good to highly reliable recognition of grasp types can be achieved—depending on whether or not the glove user is among those training the classifier—even with uncalibrated data gloves. (2) We identify the best performing classification methods for the recognition of various grasp types. To conclude,- cumbersome calibration processes before productive usage of data gloves can be spared in many situations.","1054-7460;10547460","","10.1162/pres.17.2.121","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6797627","","","","","","3","","","","","April 1 2008","","MIT Press","MIT Press Journals"
