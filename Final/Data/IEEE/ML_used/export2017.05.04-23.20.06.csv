"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7918816,7918949,7915505,7916151,7916583,7916978,7916596,7916993,7917369,7915088,7915033,7912076,7912627,7910598,7913149,7912036,7912315,7906512,7908958,7906492,7792374,7906813,7899657,7906930,7882711,7904295,7904310,7812651,7899798,7899003,7899628,7899957,7905606,7817807,7552467,7899577,7902083,7899421,7897278,7897279,7897295,7897282,7898513,7894091,7894020,7894539,7894238,7893155,7889638,7891834,7890925,7892568,7892598,7891792,7892954,7891989,7890601,7890884,7890812,7891546,7891134,7890191,7888195,7889679,7887916,7889581,7889104,7889879,7875410,7885853,7867770,7887648,7886650,7885715,7886051,7172462,7885706,7865914,7501537,7882012,7882607,7882011,7883736,7883220,7883377,7881510,7881397,7881632,7881497,7881307,7881333,7881739,7880337,7880972,7882673,7877759,7877124,7878317,7877439,7878645",2017/05/04 23:20:06
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Data mining in cloud usage data with Matlab's statistics and machine learning toolbox","M. Miškuf; P. Michalik; I. Zolotová","Department of Cybernetics and Artificial Intelligence, Faculty of Electrical Engineering and Informatics, Technical University of Ko&#x0161;ice, Slovak Republic","2017 IEEE 15th International Symposium on Applied Machine Intelligence and Informatics (SAMI)","20170320","2017","","","000377","000382","This paper focuses on use of Matlab for data mining. There is wide range of data mining software where free or cheaper solutions offer similar possibilities. We wanted to try Matlab for these purposes. Our data consists of parameters, which describes cloud usage at IT company that offers cloud services. We used phases from the CRISP-DM methodology in our work. We built clustering and classification models that use functions of the Statistics and Machine Learning Toolbox. In the conclusion we summarize our outcomes, weather Matlab is appropriate to data analysis based on conducted experiments.","","Electronic:978-1-5090-5655-2; POD:978-1-5090-5656-9; USB:978-1-5090-5654-5","10.1109/SAMI.2017.7880337","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880337","CRISP-DM;Matlab;data mining;modeling","Companies;Data analysis;Data mining;Data models;Databases;Decision trees;MATLAB","cloud computing;data mining;learning (artificial intelligence);mathematics computing;pattern classification;pattern clustering;statistics","CRISP-DM methodology;Matlab statistics;classification models;cloud services;cloud usage data;clustering models;data mining software;machine learning toolbox","","","","","","","26-28 Jan. 2017","","IEEE","IEEE Conference Publications"
"A Machine Learning Framework for Performance Coverage Analysis of Proxy Applications","T. Z. Islam; J. J. Thiagarajan; A. Bhatele; M. Schulz; T. Gamblin","Center for Appl. Sci. Comput., Lawrence Livermore Nat. Lab., Livermore, CA, USA","SC '16: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","20170316","2016","","","538","549","Proxy applications are written to represent subsets of performance behaviors of larger, and more complex applications that often have distribution restrictions. They enable easy evaluation of these behaviors across systems, e.g., for procurement or co-design purposes. However, the intended correlation between the performance behaviors of proxy applications and their parent codes is often based solely on the developer's intuition. In this paper, we present novel machine learning techniques to methodically quantify the coverage of performance behaviors of parent codes by their proxy applications. We have developed a framework, VERITAS, to answer these questions in the context of on-node performance: (a) which hardware resources are covered by a proxy application and how well, and (b) which resources are important, but not covered. We present our techniques in the context of two benchmarks, STREAM and DGEMM, and two production applications, OpenMC and CMTnek, and their respective proxy applications.","","Electronic:978-1-4673-8815-3; POD:978-1-4673-8816-0","10.1109/SC.2016.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877124","Machine learning;Performance analysis;Scalability;Unsupervised learning","Computer architecture;Correlation;Hardware;Loss measurement;Production;Radiation detectors","Monte Carlo methods;learning (artificial intelligence);parallel processing","CMTnek production application;DGEMM benchmark;OpenMC production application;STREAM benchmark;VERITAS;machine learning;performance coverage analysis;proxy applications","","","","","","","13-18 Nov. 2016","","IEEE","IEEE Conference Publications"
"Using machine learning for black-box autoscaling","M. Wajahat; A. Gandhi; A. Karve; A. Kochut","Stony Brook University, United States of America","2016 Seventh International Green and Sustainable Computing Conference (IGSC)","20170406","2016","","","1","8","Autoscaling is the practice of automatically adding or removing resources for an application deployment to meet performance targets in response to changing workload conditions. However, existing autoscaling approaches typically require expert application and system knowledge to minimize resource costs and performance target violations, thus limiting their applicability. We present MLscale, an application-agnostic, machine learning based autoscaler that is composed of: (i) a neural network based online (black-box) performance modeler, and (ii) a regression based metrics predictor to estimate post-scaling application and system metrics. Implementation results for diverse applications across several traces highlight MLscale's application-agnostic behavior and show that MLscale (i) reduces resource costs by about 50% compared to the optimal static policy, (ii) is within 15% of the cost of the optimal dynamic policy, and (iii) provides similar cost-performance tradeoffs, without requiring any tuning, when compared to carefully tuned threshold-based policies.","","Electronic:978-1-5090-5117-5; POD:978-1-5090-5118-2","10.1109/IGCC.2016.7892598","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892598","","Load modeling;Measurement;Neural networks;Predictive models;Time factors;Training;Training data","learning (artificial intelligence);neural nets;regression analysis","MLscale;application-agnostic based autoscaler;black-box autoscaling;machine learning;neural network;performance target violations;performance targets;regression based metrics predictor;resource costs","","","","","","","7-9 Nov. 2016","","IEEE","IEEE Conference Publications"
"Incoming data prediction in smart home environment with HMM-based machine learning","K. Zaouali; M. L. Ammari; R. Bouallegue; I. Sahloul; A. Chouaieb","Innov'COM, ENIT, University of Tunis El Manar, Tunis, Tunisia","2016 International Symposium on Signal, Image, Video and Communications (ISIVC)","20170412","2016","","","384","389","The Internet of things is characterized by a high level of heterogeneity between its diversified systems ranging from entertainment to automation process. A smart home application is intrinsically dynamic in the sense that it makes up a time series, whose behavior may change over time. The challenge of the incoming data prediction in a smart home is to analyze the energy consumption of each appliance and to notify the risks to remotely control the installed wireless sensor network. This paper proposes a new methodology of data mining in order to predict energy consumption, environment parameters and moving (presence) cases. We present a prediction model based on a hidden-Markov model based for the smart home environment. This model is used as a classification machine learning but it has never used for the incoming data prediction in a smart home. Using a real “Smart Life” database, we demonstrate the validity of our methodology in the scenarios of smart homes incoming data prediction. The proposed prediction technique is tested and proves that there is a high amount of reliability on the considered model.","","Electronic:978-1-5090-3611-0; POD:978-1-5090-3612-7","10.1109/ISIVC.2016.7894020","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894020","Data mining;Hidden-Markov model;Smart home;Wireless sensor network","","data mining;hidden Markov models;home automation;learning (artificial intelligence);wireless sensor networks","Internet of things;Smart Life database;classification machine learning;data mining;data prediction;energy consumption;environment parameters;hidden-Markov model;smart home application;wireless sensor network","","","","","","","21-23 Nov. 2016","","IEEE","IEEE Conference Publications"
"SoC Speed Binning Using Machine Learning and On-Chip Slack Sensors","M. Sadi; S. Kannan; L. Winemberg; M. Tehranipoor","Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20170419","2017","36","5","842","854","Speed binning of system-on-chips (SoCs) using conventional <inline-formula> <tex-math notation=""LaTeX"">$F_{mathrm {max}}$ </tex-math></inline-formula> test requires application of complex functional test patterns. Functional workload-based speed binning techniques incur high test-cost in terms of long test-time and complexity in functional test generation, and require high-end automatic test equipment. In this paper, we propose a novel speed binning flow that uses path timing slacks, extracted with robust digital embedded sensor IPs, of selected critical/near-critical paths. We apply machine learning techniques to model a predictor considering the extracted slacks and the <inline-formula> <tex-math notation=""LaTeX"">$F_{mathrm {max}}$ </tex-math></inline-formula> values from a set of randomly tested die during wafer sort. The trained predictor is used to obtain the <inline-formula> <tex-math notation=""LaTeX"">$F_{mathrm {max}}$ </tex-math></inline-formula> for the remaining chips. The proposed flow has been demonstrated in an SoC benchmark circuit at 28 nm technology. For sufficient number of training samples, <inline-formula> <tex-math notation=""LaTeX"">$F_{mathrm {max}}$ </tex-math></inline-formula> is correctly predicted for 99% of the prediction samples.","0278-0070;02780070","","10.1109/TCAD.2016.2602806","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552467","<italic xmlns:ali=""http://www.niso.org/schemas/ali/1.0/"" xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"">F</italic>max test;machine learning;slack sensor;speed-binning","Circuit faults;Correlation;Delays;Monitoring;Sensors;System-on-chip","","","","","","","","20160825","May 2017","","IEEE","IEEE Journals & Magazines"
"Using machine learning to secure IoT systems","J. Cañedo; A. Skjellum","Auburn Cyber Research Center, Samuel Ginn College of Engineering, Auburn University, United States of America","2016 14th Annual Conference on Privacy, Security and Trust (PST)","20170424","2016","","","219","222","The Internet of Things (IoT) is a massive group of devices containing sensors or actuators connected together over wired or wireless networks. With an estimate of over 25 billion devices connected together by 2020, IoT has been rapidly growing over the past decade. During the growth, security has been identified as one of the weakest areas in IoT. When implementing security within an IoT network, there are several challenges including heterogeneity within the system as well as the quantity of devices that need to be addressed. To approach the challenges in securing IoT devices, we propose using machine learning within an IoT gateway to help secure the system. We investigate using Artificial Neural Networks in a gateway to detect anomalies in the data sent from the edge devices. We are convinced that this approach can improve the security of IoT systems.","","Electronic:978-1-5090-4379-8; POD:978-1-5090-4380-4","10.1109/PST.2016.7906930","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906930","Internet of Things;Machine Learning;Security","Biological neural networks;Delays;Internet;Logic gates;Neurons;Security;Sensors","","","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"EEG graph analysis for identification of ex-combatants: A machine learning approach","A. Quintero-Zea; M. R. Calvache; S. T. Orrego; F. Vargas-Bonilla; N. T. Orrego; J. D. López","SISTEMIC, Engineering Faculty, Universidad de Antioquia (UDEA), Calle 70 No. 52 - 21, Medell&#x00ED;n, Colombia","2016 IEEE Latin American Conference on Computational Intelligence (LA-CCI)","20170327","2016","","","1","6","Emotional processing of ex-combatants is affected by chronic exposure to violent events. For a successful reintegration into society, it is necessary to discriminate their brain responses from civilian people, as a first stage to develop treatment strategies. This paper presents a comparative analysis between a Multilayer Perceptron Neural Network and a Fuzzy C-Means classifier to differentiate ex-combatant subjects from civilian controls using graph-based functional connectivity networks calculated from scalp EEG recordings. EEG data were acquired while participants performed an adaptation of a Dual Valence Task, a well-known experimental task commonly used to analyze emotional processing. We obtained up to 80%of accuracy with both proposed algorithms. These results are comparable with other approaches reported in the literature. Together with conductual tasks, the computarized techniques proposed in this paper provide a decision support system to psychologists for determining the level of exposure to the conflict of the ex-combatants, and consequently a tool for their posterior treatment.","","Electronic:978-1-5090-5105-2; POD:978-1-5090-5106-9; USB:978-1-5090-5104-5","10.1109/LA-CCI.2016.7885706","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885706","Artificial Neural Networks;Ex-combatants;Functional Connectivity;Fuzzy Clustering;Graph Theory;Scalp EEG","Correlation;Electrodes;Electroencephalography;Feature extraction;Neural networks;Scalp;Standards","decision support systems;electroencephalography;fuzzy set theory;graph theory;learning (artificial intelligence);medical signal processing;multilayer perceptrons;network theory (graphs);signal classification","EEG graph analysis;decision support system;dual valence task;emotional processing;ex-combatant identification;fuzzy c-means classifier;graph-based functional connectivity network;machine learning;multilayer perceptron neural network","","","","","","","2-4 Nov. 2016","","IEEE","IEEE Conference Publications"
"Engineering safety in machine learning","K. R. Varshney","Mathematical Sciences and Analytics Department, IBM Thomas J. Watson Research Center, Yorktown Heights, New York 10598","2016 Information Theory and Applications Workshop (ITA)","20170330","2016","","","1","5","Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through interpretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.","","Electronic:978-1-5090-2529-9; POD:978-1-5090-2530-5","10.1109/ITA.2016.7888195","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888195","","Learning systems;Machine learning algorithms;Risk management;Safety;Sociotechnical systems;Training;Uncertainty","learning (artificial intelligence);software engineering;statistical analysis","empirical risk minimization;engineering safety;predictive models;statistical machine learning","","","","","","","Jan. 31 2016-Feb. 5 2016","","IEEE","IEEE Conference Publications"
"Augmented ontology by handshaking with machine learning","M. Kim; H. Kang; S. Kwon; Y. Lee; K. Kim; C. S. Pyo","KSB Convergence Research Department, ETRI 218 Gajeong-ro, Yuseong-gu, Daejeon, 34129, KOREA Korea","2017 19th International Conference on Advanced Communication Technology (ICACT)","20170330","2017","","","740","743","Artificial intelligence products are already around us and will be emerging dramatically a lot in near future. Artificial intelligence is all about data analysis. When it comes to data analysis, there are two representative techniques: machine learning and semantic technology. They stand on the other side from where to begin analysis. Simply speaking, machine learning is based on the data while semantic technology relies on human domain knowledge (human learning). What if collected data are insufficient to reflect whole phenomenon? This is a limitation of machine learning. What if circumstance changes a lot as time goes by? Manual rule updating by experts is not a good solution in that circumstance. Based on these observations, we investigate two approaches and find a good solution which maximizes the advantages of both techniques and mitigates the limitations of them. This paper suggests a novel integration idea to compensate each technology with the other: that is semantic filtering. This paper includes a toy semantic modelling and a machine learning algorithm implementation to realize the proposed concept, semantic filtering.","","CD:978-89-968650-8-7; Electronic:978-89-968650-9-4; POD:978-1-5090-4892-2","10.23919/ICACT.2017.7890191","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890191","Internet of Things;data analysis;machine learning;semantic filtering;semantic technology","","data analysis;learning (artificial intelligence);ontologies (artificial intelligence);semantic networks","artificial intelligence;augmented ontology;data analysis;machine learning algorithm;semantic filtering;semantic technology;toy semantic modelling","","","","","","","19-22 Feb. 2017","","IEEE","IEEE Conference Publications"
"SimML Framework: Monte Carlo Simulation of Statistical Machine Learning Algorithms for IoT Prognostic Applications","A. R. More; K. C. Gross","Oracle Phys. Sci. Res. Center, Oracle Corp., San Diego, CA, USA","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","174","179","Advanced statistical machine learning (ML) algorithms are being developed, trained, tuned, optimized, and validated for real-time prognostics for internet-of-things (IoT) applications in the fields of manufacturing, transportation, and utilities. For such applications, we have achieved greatest prognostic success with ML algorithms from a class of pattern recognition known as nonlinear, nonparametric regression. To intercompare candidate ML algorithmics to identify the ""best"" algorithms for IoT prognostic applications, we use three quantitative performance metrics: false alarm probability (FAP), missed alarm probability (MAP), and overhead compute cost (CC) for real-time surveillance. This paper presents a comprehensive framework, SimML, for systematic parametric evaluation of statistical ML algorithmics for IoT prognostic applications. SimML evaluates quantitative FAP, MAP, and CC performance as a parametric function of input signals' degree of cross-correlation, signal-to-noise ratio, number of input signals, sampling rates for the input signals, and number of training vectors selected for training. Output from SimML is provided in the form of 3D response surfaces for the performance metrics that are essential for comparing candidate ML algorithms in precise, quantitative terms.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0040","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881333","AAKR;IoT Prognostics;MSET;Machine Learning","","Internet of Things;Monte Carlo methods;learning (artificial intelligence);real-time systems;regression analysis","3D response surfaces;CSCI-ISOT;FAP;Internet-of-things;IoT applications;MAP;ML algorithms;Monte Carlo simulation;SimML framework;advanced statistical machine learning algorithms;degree of cross-correlation;false alarm probability;missed alarm probability;nonlinear regression;nonparametric regression;overhead compute cost;pattern recognition;quantitative performance metrics;real-time prognostics;real-time surveillance;signal-to-noise ratio;systematic parametric evaluation","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automatic Equatorial GPS Amplitude Scintillation Detection Using a Machine Learning Algorithm","Y. Jiao; J. J. Hall; Y. T. Morton","Colorado State University, Fort Collins, USA","IEEE Transactions on Aerospace and Electronic Systems","20170424","2017","53","1","405","418","This paper presents a machine learning algorithm to detect ionospheric scintillation and classify scintillation events based on training data in the frequency domain. The detector input is the signal intensity. Validation using data from Ascension Island, Hong Kong, and Peru shows 91–96% accuracy of scintillation detection. Different combinations of training data, observation matrices, and learning algorithms are investigated to obtain performance measures. Testing results on data from Singapore demonstrate the general capabilities of the detector.","0018-9251;00189251","","10.1109/TAES.2017.2650758","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7812651","Automatic detector;GPS;equatorial ionospheric scintillation;support vector machine (SVM)","Detectors;Indexes;Receivers;Support vector machines;Testing;Training;Training data","","","","","","","","20170110","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Machine learning based path management for mobile devices over MPTCP","Jonghwan Chung; D. Han; J. Kim; Chong-kwon Kim","Seoul National University, South Korea","2017 IEEE International Conference on Big Data and Smart Computing (BigComp)","20170320","2017","","","206","209","Recent mobile devices are equipped with multiple network interfaces such as LTE and Wi-Fi. Transport protocols that can transfer data over multiple paths, especially MPTCP (Multipath TCP), allows the devices like smartphones and tablets to exploit both interfaces concurrently. However, in real environments, wireless devices abound and network quality changes frequently. It makes network connection affect the MPTCP performance negatively. In this paper, we propose a novel path management scheme called MPTCP-ML (MPTCP based on Machine Learning) to make MPTCP troubleshoot the problem. It manages path usage among multiple connections based on decision computed by machine learning model. For accurate capturing of path quality, we utilize various quality metrics including signal strength, data rate, TCP throughput, the number of interference APs, and RTT (Round Trip Time). We have implemented MPTCP-ML in Android and conducted experiments for various and dynamic environments. The results show that MPTCP-ML outperforms generic MPTCP, especially for mobile environments.","","Electronic:978-1-5090-3015-6; POD:978-1-5090-3016-3; USB:978-1-5090-3014-9","10.1109/BIGCOMP.2017.7881739","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881739","MPTCP;Path Management;Smartphone;Wi-Fi","Interference;Mobile communication;Performance evaluation;Smart phones;Throughput;Wireless fidelity","Long Term Evolution;learning (artificial intelligence);multipath channels;smart phones;transport protocols;wireless LAN","AP number-of-interference;LTE;MPTCP;RTT;TCP throughput;Wi-Fi;data rate;data transfer;machine learning based path management scheme;mobile devices;multipath TCP;multiple network interfaces;network connection;network quality;path quality;path usage management;round trip time;signal strength;smartphones;tablets;transport protocols;wireless devices","","","","","","","13-16 Feb. 2017","","IEEE","IEEE Conference Publications"
"A machine learning model for radar rainfall estimation based on gauge observations","Haiming Tan; V. Chandrasekar; H. Chen","Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA","2017 United States National Committee of URSI National Radio Science Meeting (USNC-URSI NRSM)","20170316","2017","","","1","2","Rainfall estimation based on radar measurements has been addressed by using parametric algorithms such as Z-R relation. However, such empirical relations may not be sufficient to capture the space-time variability of precipitation. In this paper, we introduce a DMLP-based machine learning model for rainfall estimation by using multiple layers to capture the complex abstractions of radar reflectivity at different attitude levels. The radar data collected by the Weather Surveillance Radar - 1988 Doppler (WSR-88DP) in Melbourne, Florida (i.e., KMLB radar) are used for demonstration purposes, while the rain gauge data are used for training purposes. The rainfall product derived from the DMLP model is compared against an independent rain gauge dataset, which shows excellent performance of the new machine learning based rainfall model.","","Electronic:978-1-946815-00-2; POD:978-1-5090-4817-5","10.1109/USNC-URSI-NRSM.2017.7878317","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878317","","Doppler radar;Estimation;Meteorological radar;Radar measurements;Rain;Training","geophysics computing;learning (artificial intelligence);meteorological radar;rain","DMLP-based machine learning model;gauge observations;parametric algorithms;radar rainfall estimation;radar reflectivity;space-time variability;weather surveillance radar","","","","","","","4-7 Jan. 2017","","IEEE","IEEE Conference Publications"
"Effort Estimation for Embedded Software Development Projects by Combining Machine Learning with Classification","K. Iwata; T. Nakashima; Y. Anan; N. Ishii","","2016 4th Intl Conf on Applied Computing and Information Technology/3rd Intl Conf on Computational Science/Intelligence and Applied Informatics/1st Intl Conf on Big Data, Cloud Computing, Data Science & Engineering (ACIT-CSII-BCD)","20170504","2016","","","265","270","This paper discusses the effect of classification in estimating the amount of effort (in man-days) associated with code development. Estimating the effort requirements for new software projects is especially important. As outliers are harmful to the estimation, they are excluded from many estimation models. However, such outliers can be identified in practice once the projects are completed, and so they should not be excluded during the creation of models and when estimating the required effort. This paper presents classifications for embedded software development projects using an artificial neural network (ANN) and a support vector machine. After defining the classifications, effort estimation models are created for each class using linear regression, an ANN, and a form of support vector regression. Evaluation experiments are carried out to compare the estimation accuracy of the model both with and without the classifications using 10-fold cross-validation. In addition, the Games–Howell test with one-way analysis of variance is performed to consider statistically significant evidence.","","Electronic:978-1-5090-4871-7; POD:978-1-5090-4872-4","10.1109/ACIT-CSII-BCD.2016.058","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916993","artificial neural network;embedded software;software development process improvement;support vector regression","Analysis of variance;Analytical models;Data models;Embedded software;Estimation;Support vector machines","","","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"LASSO: A feature selection technique in predictive modeling for machine learning","R. Muthukrishnan; R. Rohini","Dept. of Statistics, Bharathiar University, Coimbatore, India","2016 IEEE International Conference on Advances in Computer Applications (ICACA)","20170330","2016","","","18","20","Feature selection is one of the techniques in machine learning for selecting a subset of relevant features namely variables for the construction of models. The feature selection technique aims at removing the redundant or irrelevant features or features which are strongly correlated in the data without much loss of information. It is broadly used for making the model much easier to interpret and increase generalization by reducing the variance. Regression analysis plays a vital role in statistical modeling and in turn for performing machine learning tasks. The traditional procedures such as Ordinary Least Squares (OLS) regression, Stepwise regression and partial least squares regression are very sensitive to random errors. Many alternatives have been established in the literature during the past few decades such as Ridge regression and LASSO and its variants. This paper explores the features of the popular regression methods, OLS regression, ridge regression and the LASSO regression. The performance of these procedures has been studied in terms of model fitting and prediction accuracy using real data and simulated environment with the help of R package.","","CD:978-1-5090-3767-4; Electronic:978-1-5090-3770-4; POD:978-1-5090-3771-1; Paper:978-1-5090-3769-8; USB:978-1-5090-3768-1","10.1109/ICACA.2016.7887916","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887916","LASSO;OLS;R software;Ridge regression","Computational modeling;Computer applications;Conferences;Data models;Input variables;Predictive models;Regression analysis","feature selection;learning (artificial intelligence);regression analysis","LASSO regression analysis;OLS regression analysis;R package;feature selection technique;machine learning;model fitting;models construction;ordinary least squares regression analysis;partial least squares regression analysis;prediction accuracy;predictive modeling;ridge regression analysis;statistical modeling;stepwise regression analysis;variance reduction","","","","","","","24-24 Oct. 2016","","IEEE","IEEE Conference Publications"
"Classification of Alzheimer's disease and mild cognitive impairment: Machine learning applied to rs-fMRI brain graphs","S. Golbabaei; A. Vahid; J. Hatami; H. Soltanian-Zadeh","Department of Psychology, School of Psychology and Educational Sciences, University of Tehran, Iran","2016 23rd Iranian Conference on Biomedical Engineering and 2016 1st International Iranian Conference on Biomedical Engineering (ICBME)","20170406","2016","","","35","40","A growing number of studies use resting state functional magnetic resonance imaging (rs-fMRI) to investigate functional alterations in Alzheimer's disease (AD) and Mild cognitive impairment (MCI). In this study, we evaluate the effectiveness of graph theory and machine learning in the diagnosis of AD and MCI at the subject level. Moreover, we explore the effect of different methods of graph construction on the classification accuracy. To this end, the rs-fMRI data from 32 AD, 76 MCI, and 46 healthy elderly subjects are used along with three different connectivity measures: Pearson correlation, bend correlation, and mutual information. Both weighted and binary graphs are created for each subject at different density ranges. Based on each, various graph measures have been extracted. Finally, sequential floating forward selection and support vector machine are utilized. Our results suggest that bend correlation is more effective in the classification of AD and MCI. Moreover, weighted graphs and lower density ranges increase accuracy. Finally, bend correlation along with the weighted graph and density threshold of 10% generate the highest accuracy (93%). This study concludes that rs-fMRI and graph theory may provide a non-invasive means for the diagnosis of AD and MCI.","","Electronic:978-1-5090-3452-9; POD:978-1-5090-3453-6","10.1109/ICBME.2016.7890925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890925","Alzheimer's disease;MCI;graph theory;machine learning;network construction;resting state fMRI","Alzheimer's disease;Atmospheric measurements;Correlation;Graph theory;Mutual information;Particle measurements","biomedical MRI;brain;cognition;diseases;feature extraction;graph theory;image classification;learning (artificial intelligence);medical image processing;neurophysiology;support vector machines","Alzheimer's disease classification;Pearson correlation;bend correlation;binary graphs;functional alterations;graph construction;graph measure extraction;graph theory;machine learning;mild cognitive impairment;mutual information;resting state functional magnetic resonance imaging;rs-fMRI brain graphs;sequential floating forward selection;subject level;support vector machine;weighted graphs","","","","","","","24-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Machine learning and systems for the next frontier in formal verification","M. Pandey","Synopsys","2016 Formal Methods in Computer-Aided Design (FMCAD)","20170327","2016","","","4","4","Summary form only given. This tutorial covers basics of machine learning, systems and infrastructure considerations for performing machine learning at scale, and applications of machine learning to improve formal verification performance and usability. It starts with blackbox classifier training with gradient descent, and proceeds on to deep network training and simple convolutional neural networks. Next, it discusses how machine learning can be performed at scale, overcoming the performance and throughput limitations of traditional compute and storage systems. Finally, the tutorial describes several ways in which machine learning can be applied for improving formal tools performance and enhancing debug capabilities.","","Electronic:978-0-9835678-6-8; POD:978-1-5386-2692-4","10.1109/FMCAD.2016.7886650","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886650","","Neural networks;Throughput;Training;Tutorials;Usability","formal verification;learning (artificial intelligence);neural nets","blackbox classifier training;convolutional neural networks;deep network training;formal tools performance;formal verification;gradient descent;machine learning;usability","","","","","","","3-6 Oct. 2016","","IEEE","IEEE Conference Publications"
"Parameter Identifiability in Statistical Machine Learning: A Review","Z. Y. Ran; B. G. Hu","Chongqing Key Laboratory of Computational Intelligence, School of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, 400065, China ranzy@cqupt.edu.cn","Neural Computation","20170417","2017","29","5","1151","1203","<para>This review examines the relevance of parameter identifiability for statistical models used in machine learning. In addition to defining main concepts, we address several issues of identifiability closely related to machine learning, showing the advantages and disadvantages of state-of-the-art research and demonstrating recent progress. First, we review criteria for determining the parameter structure of models from the literature. This has three related issues: parameter identifiability, parameter redundancy, and reparameterization. Second, we review the deep influence of identifiability on various aspects of machine learning from theoretical and application viewpoints. In addition to illustrating the utility and influence of identifiability, we emphasize the interplay among identifiability theory, machine learning, mathematical statistics, information theory, optimization theory, information geometry, Riemann geometry, symbolic computation, Bayesian inference, algebraic geometry, and others. Finally, we present a new perspective together with the associated challenges.</para>","0899-7667;08997667","","10.1162/NECO_a_00947","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7902083","","","","","","","","","","","May 2017","","MIT Press","MIT Press Journals"
"GPU-Accelerated Parallel Hierarchical Extreme Learning Machine on Flink for Big Data","C. Chen; K. Li; A. Ouyang; Z. Tang; K. Li","College of Information Science and Engineering, Hunan University, Changsha 410082, China, and also with the National Supercomputing Center, Changsha 410082, China.","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","PP","99","1","14","The extreme learning machine (ELM) has become one of the most important and popular algorithms of machine learning, because of its extremely fast training speed, good generalization, and universal approximation/classification capability. The proposal of hierarchical ELM (H-ELM) extends ELM from single hidden layer feedforward networks to multilayer perceptron, greatly strengthening the applicability of ELM. Generally speaking, during training H-ELM, large-scale datasets (DSTs) are needed. Therefore, how to make use of H-ELM framework in processing big data is worth further exploration. This paper proposes a parallel H-ELM algorithm based on Flink, which is one of the in-memory cluster computing platforms, and graphics processing units (GPUs). Several optimizations are adopted to improve the performance, such as cache-based scheme, reasonable partitioning strategy, memory mapping scheme for mapping specific Java virtual machine objects to buffers. Most importantly, our proposed framework for utilizing GPUs to accelerate Flink for big data is general. This framework can be utilized to accelerate many other variants of ELM and other machine learning algorithms. To the best of our knowledge, it is the first kind of library, which combines in-memory cluster computing with GPUs to parallelize H-ELM. The experimental results have demonstrated that our proposed GPU-accelerated parallel H-ELM named as GPH-ELM can efficiently process large-scale DSTs with good performance of speedup and scalability, leveraging the computing power of both CPUs and GPUs in the cluster.","2168-2216;21682216","","10.1109/TSMC.2017.2690673","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7908958","Big data;Flink;GPGPU;deep learning (DL);hierarchical extreme learning machine (H-ELM);parallel","Acceleration;Approximation algorithms;Big Data;Clustering algorithms;Libraries;Machine learning algorithms;Training","","","","","","","","20170424","","","IEEE","IEEE Early Access Articles"
"Distributed and weighted extreme learning machine for imbalanced big data learning","Z. Wang; J. Xin; H. Yang; S. Tian; G. Yu; C. Xu; Y. Yao","Sino- Dutch Biomedical & Information Engineering School, Northeastern University, Shenyang 110169, China","Tsinghua Science and Technology","20170406","2017","22","2","160","173","The Extreme Learning Machine (ELM) and its variants are effective in many machine learning applications such as Imbalanced Learning (IL) or Big Data (BD) learning. However, they are unable to solve both imbalanced and large-volume data learning problems. This study addresses the IL problem in BD applications. The Distributed and Weighted ELM (DW-ELM) algorithm is proposed, which is based on the MapReduce framework. To confirm the feasibility of parallel computation, first, the fact that matrix multiplication operators are decomposable is illustrated. Then, to further improve the computational efficiency, an Improved DW-ELM algorithm (IDW-ELM) is developed using only one MapReduce job. The successful operations of the proposed DW-ELM and IDW-ELM algorithms are finally validated through experiments.","","","10.23919/TST.2017.7889638","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889638","weighted Extreme Learning Machine (ELM); imbalanced big data; MapReduce framework; user-definedcounter","Big Data;Computer science;Distributed databases;Machine learning algorithms;Matrix decomposition;Prediction algorithms;Training","","","","","","","","","April 2017","","TUP","TUP Journals & Magazines"
"Future black board using Internet of Things with cognitive computing: Machine learning aspects","N. N. Santhosh","ECE Department, Sri Sairam Institute Of Technology, Chennai India","2016 International Conference on Communication and Electronics Systems (ICCES)","20170330","2016","","","1","4","In an era of Digital computing, where people are more concerned in developing products to get there job done soon and also benefit them in all parameters, came Internet Of things to Solve. After Embedded Systems, there was a void left to be filled to solve automation. By the introduction of Internet, everyone started to visualize the things in a different way, Thus the Arrival of Internet of Things and now It is said to be the future for next few years. On the other side it's Cognitive Computing that's making the Big data to the next level, which is a branch Of Artificial Intelligence. In a similar way, The notes given by the teacher each and every day are taken by the students, but there is no forum yet to save those so to use them again, with help of Internet Of Things this could be possible where Sensors talk to the Chalk piece and gets the data to save. By analyzing (Pattern Recognition) the data obtained we can make a mini augmented reality system, where students can ask board the questions so the board answers, at end of the day it's the kids who will be benefited through this. Henceforth we have come to change the way you look at the Blackboard!!!","","DVD:978-1-5090-1064-6; Electronic:978-1-5090-1066-0; POD:978-1-5090-1067-7; Paper:978-1-5090-1065-3","10.1109/CESYS.2016.7889879","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889879","Artificial Intelligence;Communication module;Connected Boards and Internet Of everything;Connecting the dots;Grid Computing;Internet of things;MIMIO Smart Tech;MIT-Sketch Project;Pattern Recognition;Sensors in Future Board Framework;Speech Recognition;Word by Word Analyzing","Artificial intelligence;Hardware;Intelligent sensors;Internet of Things;Radiofrequency identification","Internet of Things;augmented reality;cognition;computer aided instruction;learning (artificial intelligence);pattern recognition","Internet Of things;augmented reality system;blackboard;chalk piece;cognitive computing;data analysis;digital computing;machine learning;pattern recognition;sensors","","","","","","","21-22 Oct. 2016","","IEEE","IEEE Conference Publications"
"A methodological framework using statistical tests for comparing machine learning based models applied to fault diagnosis in rotating machinery","F. Pacheco; M. Cerrada; R. V. Sánchez; D. Cabrera; C. Li; J. V. de Oliveira","GIDTEC-Mechanical Engineering Department, Universidad Polit&#x00E9;cnica Salesiana, Ecuador","2016 IEEE Latin American Conference on Computational Intelligence (LA-CCI)","20170327","2016","","","1","6","Selecting an adequate machine learning model, e.g. for feature selection or classification, is a very important task in developing machine learning applications. In order to perform an adequate selection, statistic tests are introduced by several approaches but some of them are hard to reproduce in different case studies due to the lack of a systematic application procedure. This work presents a methodological framework based on statistic tests, either parametric or non-parametric, to compare multiple machine learning models for solving a specific problem. The procedure first aims to detect which feature selection method is the best for each machine learning based model, and then such models are compared using the previous results. A real world problem for fault detection in rotating machinery is studied to illustrate the application of the proposed methodological framework, using the accuracy in classification as the performance measure.","","Electronic:978-1-5090-5105-2; POD:978-1-5090-5106-9; USB:978-1-5090-5104-5","10.1109/LA-CCI.2016.7885715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885715","","Algorithm design and analysis;Electronic mail;Feature extraction;Gears;Machine learning algorithms;Pattern recognition;Unified modeling language","electric machines;fault diagnosis;feature selection;learning (artificial intelligence);mechanical engineering computing;nonparametric statistics;pattern classification;statistical testing","classification;fault detection;fault diagnosis;feature selection method;machine learning application;nonparametric statistic test;performance measure;rotating machinery;statistical test","","","","","","","2-4 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Combined Prognostic Model Based on Machine Learning for Tidal Current Prediction","A. Kavousi-Fard; W. Su","Department of Electrical and Computer Engineering, University of Michigan-Dearborn, Dearborn, MI 48126 USA.","IEEE Transactions on Geoscience and Remote Sensing","","2017","PP","99","1","7","This paper proposes a univariate prognostic approach based on wavelet transform and support vector regression (SVR) to predict the tidal current speed and direction with high accuracy. The proposed model decomposes the tidal current data into some subharmonic components. The details and approximation components are later fed to several SVR models to attend the prediction process. In order to increase the robustness of the model, the idea of combined prediction is used to model each subharmonic signal by several SVRs. The median operator is further used to determine the aggregated forecast tidal current data. Due to the high reliance of SVR model on the kernel function and hyperplane parameters, a new optimization method based on the bat algorithm is used to train the SVR model. The final forecast tidal current data are constructed using an aggregation operator in the output of the SVRs. The accuracy and satisfying performance of the proposed model are examined on the practical tidal data collected from the Bay of Fundy, NS, Canada. The experimental results reveal the high capability and robustness of the proposed hybrid model for the tidal current prediction.","0196-2892;01962892","","10.1109/TGRS.2017.2659538","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882673","Aggregated prediction;support vector regression (SVR);tidal current;wavelet transform.","Discrete wavelet transforms;Kernel;Optimization;Predictive models;Training","","","","","","","","20170320","","","IEEE","IEEE Early Access Articles"
"Real-time room occupancy estimation with Bayesian machine learning using a single PIR sensor and microcontroller","C. Leech; Y. P. Raykov; E. Ozer; G. V. Merrett","ARM, Cambridge, University of Southampton, UK","2017 IEEE Sensors Applications Symposium (SAS)","20170412","2017","","","1","6","This paper presents the implementation and deployment of a compute/memory intensive non-parametric Bayesian machine learning algorithm on a microcontroller unit (MCU) to estimate room occupancy in a Smart Room using a single analogue PIR sensor. We envisage an IoT device consisting of a resource-constrained MCU, PIR sensor and a battery running the occupancy estimation algorithm and operating over days or months without recharging or replacing the battery. Both hardware-independent and hardware-dependent optimizations are performed to reduce memory footprint and yet provide acceptable real-time performance while consuming less energy. We show a significant reduction in the on-chip memory usage in the MCUs by the algorithm through optimisation of the machine learning models and of the static memory footprint and dynamic memory usage. We also show that a low-end MCU does not meet the real-time requirements of the application without causing high average power consumption. However, a moderately high-performance MCU with a higher clock frequency and hardware floating-point unit provides 19x improvement in the execution time of the algorithm, better meeting the real-time specification of the application and reducing power consumption. Further, we estimate the battery lifetime of the IoT device if it operates continuously in a Smart Room. With a typical size battery, an IoT device consisting of a Cortex-M4F MCU and PIR sensor can operate for more than a month without replacement or recharging of the battery while running the compute-intensive Bayesian machine learning algorithm.","","Electronic:978-1-5090-3202-0; POD:978-1-5090-3203-7","10.1109/SAS.2017.7894091","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894091","","Batteries;Bayes methods;Estimation;Hidden Markov models;MATLAB;Optimization;Standards","Internet of Things;belief networks;floating point arithmetic;home automation;learning (artificial intelligence);microcontrollers","IoT device;compute-intensive Bayesian machine learning algorithm;dynamic memory usage;hardware floating-point unit;hardware-dependent optimizations;machine learning models;memory footprint reduction;microcontroller;microcontroller unit;onchip memory usage;power consumption reduction;real-time room occupancy estimation;resource-constrained MCU;single analogue PIR sensor;smart room;static memory footprint","","","","","","","13-15 March 2017","","IEEE","IEEE Conference Publications"
"Sequential vs. batch machine-learning with evolutionary hyperparameter optimization for segmenting aortic dissection thrombus","C. A. Morariu; M. Thomas; J. Pauli; D. S. Dohle; K. Tsagakis","Intelligent Systems, Faculty of Engineering, University of Duisburg-Essen, Germany","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","1189","1194","While delineation of aortic aneurysms has been subject of research in several publications, this represents the first contribution to address segmentation of thrombus in case of aortic dissections. The segmentation process ensues in multiplanar reformated slices (MPRs). In 3D CTA data, thrombus hardly differs from surrounding tissue outside the aorta. Segmentation is further complicated by the high variance of adjacent structures along the aorta in thoracic and abdominal area. Therefore, we propose a combination of machine learning methods and additional features for the detection of the aortic outer wall, which includes both lumen and thrombus. The optimal path is sought in each MPR in polar space based on the result of a classifier, as well as the filter response of a phase congruency filter and a distance-based component. Hyperparameters for the classifier are inferred by employing evolutionary algorithms.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899798","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899798","","Aneurysm;Blood;Feature extraction;Image segmentation;Narrowband;Three-dimensional displays;Training","","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Real Life Machine Learning Case on Mobile Advertisement: A Set of Real-Life Machine Learning Problems and Solutions for Mobile Advertisement","S. E. Seker","Comput. Sci., Smith Coll., Northampton, MA, USA","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","520","524","This paper is an output of data science study on a real life problem. The paper starts with the problem definition and a brief introduction to the mobile advertisement for addressing the machine learning problems. Later on, some machine learning solutions are provided for each of the problems, furthermore the success of classical solution methods in the literature is also compared for the real life problems. Some problems addressed are: unbalanced data sets, parameter optimization, time slicing and history optimization and there are also some performance metrics related to the mobile advertisement problem domain. This paper mainly considers the actions generated by users and advertisement providers as a data stream and proposes a well optimized recommender algorithm based on crucial parameters. Different than most of the papers in the literature, this study is an output of a research collaboration with a real life advertisement platform.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881397","Data Science;Data Stream;Machine Learning;Mobile Advertisement","Algorithm design and analysis;GSM;History;Machine learning algorithms;Mobile communication;Optimization;Web pages","advertising data processing;learning (artificial intelligence);mobile computing;recommender systems","machine learning;mobile advertisement;recommender algorithm","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning based MoM (ML-MoM) for parasitic capacitance extractions","H. M. Yao; Y. W. Qin; L. J. Jiang","Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong","2016 IEEE Electrical Design of Advanced Packaging and Systems (EDAPS)","20170406","2016","","","171","173","This paper is a rethinking of the conventional method of moments (MoM) using the modern machine learning (ML) technology. By repositioning the MoM matrix and unknowns in an artificial neural network (ANN), the conventional linear algebra MoM solving is changed into a machine learning training process. The trained result is the solution. As an application, the parasitic capacitance extraction broadly needed by VLSI modeling is solved through the proposed new machine learning based method of moments (ML-MoM). The multiple linear regression (MLR) is employed to train the model. The computations are done on Amazon Web Service (AWS). Benchmarks demonstrated the interesting feasibility and efficiency of the proposed approach. According to our knowledge, this is the first MoM truly powered by machine learning methods. It opens enormous software and hardware resources for MoM and related algorithms that can be applied to signal integrity and power integrity simulations.","","Electronic:978-1-5090-6185-3; POD:978-1-5090-6186-0; USB:978-1-5090-6184-6","10.1109/EDAPS.2016.7893155","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893155","Artificial Neural Network;Capacitance Extraction;Machine Learning;Method of Moments","Artificial neural networks;Capacitance;Computational modeling;Machine learning algorithms;Method of moments;Software;Training","VLSI;integrated circuit modelling;learning (artificial intelligence);method of moments;neural nets;regression analysis","ANN;AWS;Amazon Web Service;ML technology;ML-MoM;MLR;MoM matrix;VLSI modeling;artificial neural network;hardware resources;linear algebra MoM solving;machine learning training process;method of moments;multiple linear regression;parasitic capacitance extractions;power integrity simulations;signal integrity;software resources","","","","","","","14-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach to identify and track learning styles in MOOCs","B. Hmedna; A. El Mezouary; O. Baz; D. Mammass","IRF-SIC Laboratory, FSA, Ibn Zohr University, Agadir - Morocco","2016 5th International Conference on Multimedia Computing and Systems (ICMCS)","20170424","2016","","","212","216","This paper is devoted to describe a preliminary draft of our approach that aims to identify and track learners' learning styles based on their behavior and actions during a MOOC then to provide them with personalized recommendations based on their learning styles. Massive Open Online Courses are attracting a debate in the research community about their influence in online education. Indeed, with their advent, we are assisting to a substantial expansion of online learning with new advantages such as: massiveness, openness, democratization of learning, etc. However, it raises particular issues related to the dropout rates and the heterogeneity of massive learner's cohorts. In this approach, we use neural networks for the identification and tracking of learner's learning styles in MOOCs so as to increase learners' engagement and satisfaction. The purpose of this paper is to examine the point of view of literature, the dropout issue and solution to integrate an adaptive recommendation system with MOOC.","","CD:978-1-5090-5145-8; Electronic:978-1-5090-5146-5; POD:978-1-5090-5147-2","10.1109/ICMCS.2016.7905606","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905606","Learning style;MOOC;Neural network;adaptation","Adaptation models;Adaptive systems;Collaboration;Feature extraction;Navigation;Neural networks;Videos","","","","","","","","","Sept. 29 2016-Oct. 1 2016","","IEEE","IEEE Conference Publications"
"An efficient machine learning approach for the detection of melanoma using dermoscopic images","Z. Waheed; A. Waheed; M. Zafar; F. Riaz","Department of Computer Engineering, Bahria University, Islamabad, Pakistan","2017 International Conference on Communication, Computing and Digital Systems (C-CODE)","20170504","2017","","","316","319","Diagnosis of dermoscopic skin lesions due to skin cancer is the most challenging task for the experienced dermatologists. In this context, dermoscopy is the non-invasive useful method for the detection of skin lesions which are not visible to naked human eye. Among different types of skin cancers, malignant melanoma is the most aggressive and deadliest form of skin cancer. Its diagnosis is crucial if not detected in early stage. This paper mainly aims to present an efficient machine learning approach for the detection of melanoma from dermoscopic images. It detects melanomic skin lesions based upon their discriminating properties. In first step of proposed method, different types of color and texture features are extracted from dermoscopic images based on distinguished structures and varying intensities of melanomic lesions. In second step, extracted features are fed to the classifier to classify melanoma out of dermoscopic images. Paper also focuses on the role of color and texture features in the context of detection of melanomas. Proposed method is tested on publicly available PH2 dataset in terms of accuracy, sensitivity, specificity and Area under ROC curve (AUC). It is observed that good results are achieved using extracted features, hence proving the validity of the proposed system.","","CD:978-1-5090-4447-4; Electronic:978-1-5090-4448-1; POD:978-1-5090-4449-8","10.1109/C-CODE.2017.7918949","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918949","color and texture features extraction;machine learning approach;melanoma detection;role of color and texture features","","","","","","","","","","8-9 March 2017","","IEEE","IEEE Conference Publications"
"Using Azure Machine Learning for Estimating Indoor Locations","S. Hayakawa; H. Hayashi","Fac. of Sci. & Technol., Sophia Univ., Tokyo, Japan","2017 International Conference on Platform Technology and Service (PlatCon)","20170323","2017","","","1","4","Indoor systems cannot obtain a precise estimate of the location, due to unstable signals. In this paper, we use realistic wireless data from the IEEE International Conference on Data Mining (ICDM) dataset and Azure Machine Learning Studio to perform Bagging (also called bootstrap aggregating). By using the machine leaning technique in the Azure Machine Learning Studio, we can obtain more than 69 percent precision in identifying the correct area among 247 areas with only 505 training data. This result is equivalent to the second place entry in the IEEE ICDM Data Mining Contest. We show that this can achieve a highly accurate location estimation.","","Electronic:978-1-5090-5140-3; POD:978-1-5090-5141-0","10.1109/PlatCon.2017.7883736","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883736","","Bagging;Conferences;Data mining;Decision trees;Estimation;Training data;Wireless communication","indoor navigation;learning (artificial intelligence);radionavigation;telecommunication computing","Azure Machine Learning Studio;Bagging;IEEE ICDM Data Mining Contest;IEEE International Conference on Data Mining dataset;bootstrap aggregating;indoor location estimation;training data;wireless data","","","","","","","13-15 Feb. 2017","","IEEE","IEEE Conference Publications"
"In-Memory Computation of a Machine-Learning Classifier in a Standard 6T SRAM Array","J. Zhang; Z. Wang; N. Verma","Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Journal of Solid-State Circuits","20170329","2017","52","4","915","924","This paper presents a machine-learning classifier where computations are performed in a standard 6T SRAM array, which stores the machine-learning model. Peripheral circuits implement mixed-signal weak classifiers via columns of the SRAM, and a training algorithm enables a strong classifier through boosting and also overcomes circuit nonidealities, by combining multiple columns. A prototype 128 × 128 SRAM array, implemented in a 130-nm CMOS process, demonstrates ten-way classification of MNIST images (using image-pixel features downsampled from 28 × 28 = 784 to 9 × 9 = 81, which yields a baseline accuracy of 90%). In SRAM mode (bit-cell read/write), the prototype operates up to 300 MHz, and in classify mode, it operates at 50 MHz, generating a classification every cycle. With accuracy equivalent to a discrete SRAM/digital-MAC system, the system achieves ten-way classification at an energy of 630 pJ per decision, 113× lower than a discrete system with standard training algorithm and 13× lower than a discrete system with the proposed training algorithm.","0018-9200;00189200","","10.1109/JSSC.2016.2642198","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875410","Analog computation;image detection;in-memory computation","Boosting;Computational modeling;Computer architecture;Machine learning algorithms;Random access memory;Standards;Training","CMOS memory circuits;SRAM chips;learning (artificial intelligence);pattern classification","CMOS process;MNIST images;circuit nonidealities;digital-MAC system;discrete system;frequency 50 MHz;image-pixel feature;in-memory computation;machine-learning classifier;mixed-signal weak classifier;peripheral circuit;size 130 nm;standard 6T SRAM array;training algorithm","","","","","","20170310","April 2017","","IEEE","IEEE Journals & Magazines"
"Prediction of Stock Market performance by using machine learning techniques","K. Raza","FEST, Iqra University Karachi, Pakistan","2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT)","20170504","2017","","","1","1","One decision in Stock Market can make huge impact on an investor's life. The stock market is a complex system and often covered in mystery, it is therefore, very difficult to analyze all the impacting factors before making a decision. In this research, we have tried to design a stock market prediction model which is based on different factors. The model was built to predict performance of KSE-100 index. The prediction model predicts market as positive or negative with the help of different attributes. These factors include price fluctuation of fuel, commodity, foreign exchange, interest rate, general public sentiment, related NEWS and Auto-Regressive Integrated Moving Average (ARIMA) and Simple Moving Average (SMA) predicted values with help of historical data of the market. The techniques used for prediction include four different versions of Artificial Neural Network (ANN) including Single Layer Perceptron (SLP), Multi-layer Perceptron (MLP), Radial Basis Function (RBF) and Deep Belief Network (DBN). Other techniques include Support Vector Machine (SVM), Decision Tree and Naïve Bayes. All these techniques were compared to find the best predicting model. The results showed that MLP performed best and predicted the market with accuracy of 77%. Each factor was studied independently to find out its association with market performance. The change in Petrol prices showed the strongest association with market performance. The results suggested that behavior of market can be predicted using machine learning techniques.","","Electronic:978-1-5090-3310-2; POD:978-1-5090-3311-9","10.1109/ICIEECT.2017.7916583","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916583","","Artificial neural networks;Complex systems;Fluctuations;Indexes;Predictive models;Stock markets;Support vector machines","","","","","","","","","5-7 April 2017","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach for Tracking and Predicting Student Performance in Degree Programs","J. Xu; K. H. Moon; M. van der Schaar","","IEEE Journal of Selected Topics in Signal Processing","","2017","PP","99","1","1","Accurately predicting students’ future performance based on their ongoing academic records is crucial for effectively carrying out necessary pedagogical interventions to ensure students’ on-time and satisfactory graduation. Although there is a rich literature on predicting student performance when solving problems or studying for courses using data-driven approaches, predicting student performance in completing degrees (e.g. college programs) is much less studied and faces new challenges: (1) Students differ tremendously in terms of backgrounds and selected courses; (2) Courses are not equally informative for making accurate predictions; (3) Students’ evolving progress needs to be incorporated into the prediction. In this paper, we develop a novel machine learning method for predicting student performance in degree programs that is able to address these key challenges. The proposed method has two major features. First, a bilayered structure comprising of multiple base predictors and a cascade of ensemble predictors is developed for making predictions based on students’ evolving performance states. Second, a data-driven approach based on latent factor models and probabilistic matrix factorization is proposed to discover course relevance, which is important for constructing efficient base predictors. Through extensive simulations on an undergraduate student dataset collected over three years at UCLA, we show that the proposed method achieves superior performance to benchmark approaches.","1932-4553;19324553","","10.1109/JSTSP.2017.2692560","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894238","Student performance prediction;data-driven course clustering;personalized education","Complexity theory;Education;Machine learning algorithms;Prediction algorithms;Probabilistic logic;Recommender systems;Signal processing algorithms","","","","","","","","20170407","","","IEEE","IEEE Early Access Articles"
"Application of machine learning techniques to sentiment analysis","A. P. Jain; P. Dandannavar","Computer Science and Engineering, Mtech, Gogte Institute of Technology, Belgaum, India","2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","20170427","2016","","","628","632","Today, we live in a ‘data age’. Due to rapid increase in the amount of user-generated data on social media platforms like Twitter, several opportunities and new open doors have been prompted for organizations that endeavour hard to keep a track on customer reviews and opinions about their products. Twitter is a huge fast emergent micro-blogging social networking platform for users to express their views about politics, products sports etc. These views are useful for businesses, government and individuals. Hence, tweets can be used as a valuable source for mining public's opinion. Sentiment analysis is a process of automatically identifying whether a user-generated text expresses positive, negative or neutral opinion about an entity (i.e. product, people, topic, event etc). The objective of this paper is to give step-by-step detail about the process of sentiment analysis on twitter data using machine learning. This paper also provides details of proposed approach for sentiment analysis. This work proposes a Text analysis framework for twitter data using Apache spark and hence is more flexible, fast and scalable. Naïve Bayes and Decision trees machine learning algorithms are used for sentiment analysis in the proposed framework.","","Electronic:978-1-5090-2399-8; POD:978-1-5090-2400-1; USB:978-1-5090-2398-1","10.1109/ICATCCT.2016.7912076","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912076","Natural Language Processing;machine learning;sentiment analysis;twitter","Data mining;Feature extraction;Machine learning algorithms;Sentiment analysis;Training;Twitter","","","","","","","","","21-23 July 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Paradigms for Next-Generation Wireless Networks","C. Jiang; H. Zhang; Y. Ren; Z. Han; K. C. Chen; L. Hanzo","","IEEE Wireless Communications","20170424","2017","24","2","98","105","Next-generation wireless networks are expected to support extremely high data rates and radically new applications, which require a new wireless radio technology paradigm. The challenge is that of assisting the radio in intelligent adaptive learning and decision making, so that the diverse requirements of next-generation wireless networks can be satisfied. Machine learning is one of the most promising artificial intelligence tools, conceived to support smart radio terminals. Future smart 5G mobile terminals are expected to autonomously access the most meritorious spectral bands with the aid of sophisticated spectral efficiency learning and inference, in order to control the transmission power, while relying on energy efficiency learning/inference and simultaneously adjusting the transmission protocols with the aid of quality of service learning/inference. Hence we briefly review the rudimentary concepts of machine learning and propose their employment in the compelling applications of 5G networks, including cognitive radios, massive MIMOs, femto/small cells, heterogeneous networks, smart grid, energy harvesting, device-todevice communications, and so on. Our goal is to assist the readers in refining the motivation, problem formulation, and methodology of powerful machine learning algorithms in the context of future networks in order to tap into hitherto unexplored applications and services.","1536-1284;15361284","","10.1109/MWC.2016.1500356WC","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792374","","5G mobile communication;Bayes methods;Clustering algorithms;Hidden Markov models;MIMO;Machine learning algorithms;Support vector machines","","","","","","","","20161220","April 2017","","IEEE","IEEE Journals & Magazines"
"Machine learning framework incorporating expert knowledge in tissue image annotation","F. Kromp; I. Ambros; T. Weiss; D. Bogen; H. Dodig; M. Berneder; T. Gerber; S. Taschner-Mandl; P. Ambros; A. Hanbury","Children's Cancer Research Institute, Tumor Biology Lab, Zimmermannplatz 10, 1090 Vienna, Austria","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","343","348","The annotation of cellular nuclei in images of tissue sections is a time consuming but crucial task in quantitative microscopy. We present a machine learning framework incorporating expert knowledge enabling biologists to annotate a large number of nuclear images in a reasonable time. The proposed system is designed to generate three successive levels of annotation, each presenting more details until single nuclei are annotated. Moreover, the output of each level is used to update the model of the next level, increasing the performance speed of the system. A crucial task is the separation of aggregated nuclei. This task is modeled as an Integer Linear Program (ILP), based on the output of an ensemble segmentation and solved by using a genetic algorithm. To incorporate user input at runtime, we use a modified version of an Online Random Forest (ORF). The proposed system was tested by biologists annotating images of ganglioneuroma tissue sections. Results show that the time biologists need to annotate an image is considerably reduced after the system has been trained.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899657","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899657","image annotation;machine learning;online training","Algorithm design and analysis;Biological tissues;Image segmentation;Machine learning algorithms;Morphology;Prediction algorithms","","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Cardiotocography Analysis Using Conjunction of Machine Learning Algorithms","A. Batra; A. Chandra; V. Matoria","Math. & Comput., Delhi Technol. Univ., New Delhi, India","2017 International Conference on Machine Vision and Information Technology (CMVIT)","20170316","2017","","","1","6","Fetal distress is a primary factor for cesarean section in obstetrics and gynecology. If there is a lack of oxygen for the fetus in the uterus, then there is always a risk of deteriorating health which could lead to death as well. Cardiotocography (CTG) the most popular technique to observe fetal health. Fetalheart rate (FHR) is an essential index to identify occurences of fetal distress. This study makes use of the following to evaluate fetal distress: 1) Decision tree (DT) 2) Support Vector Machines (SVM) 3) Random Forest 4) Neural Networks 5) Gradient Boosting We present in this paper a new algorithm which improves the accuracy to 99.25% which is higher than what was obtained inprevious research.","","Electronic:978-1-5090-4993-6; POD:978-1-5090-4994-3","10.1109/CMVIT.2017.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7878645","Cardiotocography;Machine Learning;Neural Networks;Random Forest;SVM","Boosting;Cardiography;Decision trees;Fetal heart rate;Histograms;Neural networks;Support vector machines","cardiology;decision trees;learning (artificial intelligence);medical computing;neural nets;obstetrics;support vector machines","CTG;DT;FHR;SVM;cardiotocography analysis;cesarean section;decision tree;fetal distress;fetal health;fetal heart rate;gradient boosting;gynecology;machine learning algorithm conjunction;neural networks;obstetrics;random forest;support vector machines","","","","","","","17-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Statistical and Machine Learning approach in forex prediction based on empirical data","S. W. Sidehabi; Indrabayu; S. Tandungan","Department of Electrical Engineering, Politeknik ATI Makassar, Indonesia","2016 International Conference on Computational Intelligence and Cybernetics","20170406","2016","","","63","68","This study proposed a new insight in comparing common methods used in predicting based on data series i.e statistical method and machine learning. The corresponding techniques are use in predicting Forex (Foreign Exchange) rates. The Statistical method used in this paper is Adaptive Spline Threshold Autoregression (ASTAR), while for machine learning, Support Vector Machine (SVM) and hybrid form of Genetic Algorithm-Neural Network (GA-NN) are chosen. The comparison among the three methods accurate rate is measured in root mean squared error (RMSE). It is found that ASTAR and GA-NN method has advantages depend on the period time intervals.","","CD:978-1-5090-5546-3; Electronic:978-1-5090-5548-7; POD:978-1-5090-5549-4","10.1109/CyberneticsCom.2016.7892568","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892568","ASTAR;GA-NN;RMSE;SVM;forex;prediction","Adaptation models;Artificial neural networks;Data models;Genetic algorithms;Predictive models;Sociology;Support vector machines","autoregressive processes;foreign exchange trading;genetic algorithms;learning (artificial intelligence);mean square error methods;neural nets;splines (mathematics);statistical analysis;support vector machines","ASTAR;GA-NN;RMSE;SVM;adaptive spline threshold autoregression;data series;empirical data;foreign exchange;forex prediction;genetic algorithm-neural network;machine learning approach;root mean squared error;statistical approach;support vector machine","","","","","","","22-24 Nov. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Detecting Pronominal Anaphora Ambiguity in NL Requirements","R. Sharma; N. Sharma; K. K. Biswas","","2016 4th Intl Conf on Applied Computing and Information Technology/3rd Intl Conf on Computational Science/Intelligence and Applied Informatics/1st Intl Conf on Big Data, Cloud Computing, Data Science & Engineering (ACIT-CSII-BCD)","20170504","2016","","","177","182","Automated or semi-automated analysis of requirements specification documents, expressed in Natural Language (NL), has always been desirable. An important precursor to this goal is the identification and correction of potentially ambiguous requirements statements. Pronominal Anaphora ambiguity is one such type of pragmatic or referential ambiguity in NL requirements, which needs attention. However, identification of such ambiguous requirements statements is a challenging task since the count of such statements is relatively lower. We present a solution to this challenge by considering the task as that of a classification problem to classify ambiguous requirements statements having pronominal anaphora ambiguity from a corpus of potentially ambiguous requirements statements with pronominal anaphora ambiguity. We show how a classifier can be trained in semi-supervised manner to detect such instances of pronominal anaphoric ambiguous requirements statements. Our study indicates a recall of 95% with Bayesian network classification algorithm.","","Electronic:978-1-5090-4871-7; POD:978-1-5090-4872-4","10.1109/ACIT-CSII-BCD.2016.043","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916978","ambiguity;anaphora ambiguity;machine learning;requirements analysis","Bayes methods;Business;Classification algorithms;Pragmatics;Software;Training;Vegetation","","","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"CogNitive 5G networks: Comprehensive operator use cases with machine learning for management operations","I. G. Ben Yahia; J. Bendriss; A. Samba; P. Dooze","&#x201C;SDM, Cognitive Orchestration, Management and resilience&#x201D; department, Orange Labs Networks. Ch&#x00E2;tillon, France","2017 20th Conference on Innovations in Clouds, Internet and Networks (ICIN)","20170417","2017","","","252","259","This paper presents a novel cognitive management architecture developed within the H2020 CogNet project to manage 5G networks. We also present the instantiation of this architecture for two Operator use cases, namely `SLA enforcement' and `Mobile Quality Predictor'. The SLA enforcement use case tackles the SLA management with machine learning techniques, precisely, LSTM (Long Short Term Memory). The second use case, Mobile Quality Predictor, proposes a framework using machine learning to enable an accurate bandwidth prediction for each mobile subscriber in real-time. A problem statement, stakeholders, an instantiation of the cognitive management architecture, a related work as well as an evaluation results are presented for each use case.","","Electronic:978-1-5090-3672-1; POD:978-1-5090-3673-8","10.1109/ICIN.2017.7899421","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899421","5G management;Autonomic management;Long Short Term Memory;NFV;QoS;Random Forest algorithm;SDN","5G mobile communication;Computer architecture;Engines;Monitoring;Real-time systems;Software","5G mobile communication;cognitive radio;learning (artificial intelligence);telecommunication computing;telecommunication network management","LSTM;SLA enforcement;SLA management;bandwidth prediction;cognitive 5G networks;cognitive management architecture;comprehensive operator use cases;long short term memory;machine learning techniques;management operations;mobile quality predictor","","","","","","","7-9 March 2017","","IEEE","IEEE Conference Publications"
"Diagnosis of diabetic retinopathy using machine learning classification algorithm","K. Bhatia; S. Arora; R. Tomar","University of Petroleum and Energy Studies, Dehradun, India","2016 2nd International Conference on Next Generation Computing Technologies (NGCT)","20170316","2016","","","347","351","Diabetic Retinopathy is human eye disease which causes damage to retina of eye and it may eventually lead to complete blindness. Detection of diabetic retinopathy in early stage is essential to avoid complete blindness. Many physical tests like visual acuity test, pupil dilation, optical coherence tomography can be used to detect diabetic retinopathy but are time consuming and affects patients as well. This review paper focuses on decision about the presence of disease by applying ensemble of machine learning classifying algorithms on features extracted from output of different retinal image processing algorithms, like diameter of optic disk, lesion specific (microaneurysms, exudates), image level (pre-screening, AM/FM, quality assessment). Decision making for predicting the presence of diabetic retinopathy was performed using alternating decision tree, adaBoost, Naive Bayes, Random Forest and SVM.","","DVD:978-1-5090-3256-3; Electronic:978-1-5090-3257-0; POD:978-1-5090-3258-7","10.1109/NGCT.2016.7877439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877439","Diabetic retinopathy;Naive Bayes;Random Forest;SVM;adaBoost;decision tree;ensemble learning;exudate;machine learning","Classification algorithms;Diabetes;Feature extraction;Optical imaging;Retina;Retinopathy;Support vector machines","Bayes methods;biomedical optical imaging;decision trees;diseases;eye;feature extraction;image classification;learning (artificial intelligence);medical image processing;optical tomography;support vector machines","Naive Bayes;SVM;adaBoost;alternating decision tree;complete blindness avoidance;decision making;diabetic retinopathy diagnosis;exudates;eye retina damage;feature extraction;human eye disease;image level;lesion specific;machine learning classification algorithm;microaneurysms;optic disk diameter;optical coherence tomography;pupil dilation;quality assessment;random forest;retinal image processing algorithms;visual acuity test","","","","","","","14-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","J. Kremer; K. Stensbo-Smidt; F. Gieseke; K. S. Pedersen; C. Igel","University of Copenhagen","IEEE Intelligent Systems","20170327","2017","32","2","16","22","Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night, and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular, highly efficient machine learning and image analysis algorithms. But scalability isn't the only challenge: astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. The authors argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. They focus here on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications.","1541-1672;15411672","","10.1109/MIS.2017.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887648","astronomy;big data;computer vision;intelligent systems;machine learning","Astronomy;Big data;Computer vision;Extrasolar planets;Extraterrestrial measurements;Image analysis;Machine learning;Telescopes","Big Data;astronomical image processing;learning (artificial intelligence)","Big Data;aperture telescopes;astronomy;astrophysics;computer science research;cosmology;data analysis;digital cameras;image analysis;image analysis algorithms;machine learning","","","","","","","Mar.-Apr. 2017","","IEEE","IEEE Journals & Magazines"
"Prognostics of damage growth in composite materials using machine learning techniques","H. Liu; S. Liu; Z. Liu; N. Mrad; H. Dong","School of Automation, China University of Geosciences, Hubei, Wuhan, China","2017 IEEE International Conference on Industrial Technology (ICIT)","20170504","2017","","","1042","1047","Composite materials have been adopted and become critical in aerospace industry. However, due to the fatigue under continuous loading, the uncertain in structural integrity still remains an unsolved problem. The assessment of structural damage in composite laminates can be achieved by damage location, classification, and quantification. The growth trend of delamination area is one of the most important factors. In order to predict the delamination size efficiently and accurately, this paper proposes a prognostic method based on machine learning techniques. Prediction models, including linear model, support vector machines, and random forests were investigated. An optimal solution was identified by comparing the test results of different models. In this study, the length of the path across delamination area was selected as the objective value to train the models. The path length measurements augmented the training data sets and avoid the overfitting problem for the models. Moreover, the path length can be used to measure the size of delamination area. The interrogation frequency collected on several composite coupons was adopted as an input variable for the predict model. Experimental results demonstrate the effectiveness of the proposed method.","","Electronic:978-1-5090-5320-9; POD:978-1-5090-5321-6; USB:978-1-5090-5319-3","10.1109/ICIT.2017.7915505","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915505","Composite material;Damage prognostics;Lamb wave propagation;Machine learning","Aircraft;Delamination;Inspection;Predictive models;Support vector machines;Training;Vegetation","","","","","","","","","22-25 March 2017","","IEEE","IEEE Conference Publications"
"Parallelization of Machine Learning Applied to Call Graphs of Binaries for Malware Detection","R. Searles; L. Xu; W. Killian; T. Vanderbruggen; T. Forren; J. Howe; Z. Pearson; C. Shannon; J. Simmons; J. Cavazos","","2017 25th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)","20170427","2017","","","69","77","Malicious applications have become increasingly numerous. This demands adaptive, learning-based techniques for constructing malware detection engines, instead of the traditional manual-based strategies. Prior work in learning-based malware detection engines primarily focuses on dynamic trace analysis and byte-level n-grams. Our approach in this paper differs in that we use compiler intermediate representations, i.e., the callgraph representation of binaries. Using graph-based program representations for learning provides structure of the program, which can be used to learn more advanced patterns. We use the Shortest Path Graph Kernel (SPGK) to identify similarities between call graphs extracted from binaries. The output similarity matrix is fed into a Support Vector Machine (SVM) algorithm to construct highly-accurate models to predict whether a binary is malicious or not. However, SPGK is computationally expensive due to the size of the input graphs. Therefore, we evaluate different parallelization methods for CPUs and GPUs to speed up this kernel, allowing us to continuously construct up-to-date models in a timely manner. Our hybrid implementation, which leverages both CPU and GPU, yields the best performance, achieving up to a 14.2x improvement over our already optimized OpenMP version. We compared our generated graph-based models to previously state-of-the-art feature vector 2-gram and 3-gram models on a dataset consisting of over 22,000 binaries. We show that our classification accuracy using graphs is over 19% higher than either n-gram model and gives a false positive rate (FPR) of less than 0.1%. We are also able to consider large call graphs and dataset sizes because of the reduced execution time of our parallelized SPGK implementation.","","Electronic:978-1-5090-6058-0; POD:978-1-5090-6059-7","10.1109/PDP.2017.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912627","Binary Analysis;Call Graphs;Compiler Techniques;Heterogeneous Computing;Malware;Parallelization","Engines;Feature extraction;Graphics processing units;Kernel;Machine learning algorithms;Malware;Support vector machines","","","","","","","","","6-8 March 2017","","IEEE","IEEE Conference Publications"
"Units and structure of automated “smart” house control system using machine learning algorithms","A. Kazarian; V. Teslyuk; I. Tsmots; M. Mashevska","ACS Department, Lviv Polytechnic National University, UKRAINE, Lviv, 12 S. Bandery str.","2017 14th International Conference The Experience of Designing and Application of CAD Systems in Microelectronics (CADSM)","20170504","2017","","","364","366","In the article developed the structure of the automated system for house devices control using machine learning algorithms. The main element of the proposed structure is responsible for setting up automated devices parameters, according to the data from sensors in the home, based on decision making using artificial neural network.","","Electronic:978-1-5090-5045-1; POD:978-1-5090-5046-8","10.1109/CADSM.2017.7916151","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916151","“smart” house;artificial neural network;control system structure;machine learning","Artificial neural networks;Decision making;Sensor systems;Smart homes;Temperature sensors;Training","","","","","","","","","21-25 Feb. 2017","","IEEE","IEEE Conference Publications"
"Recognition of fruits using hybrid features and machine learning","D. Shukla; A. Desai","Comp. Science and Engineering Department, Institute of Technology, Nirma University, Ahmedabad, India","2016 International Conference on Computing, Analytics and Security Trends (CAST)","20170501","2016","","","572","577","Recognition of fruits automatically using machine vision is considered as challenging task as fruits exist in various colors, sizes, shapes and textures. Additionally, when images are acquired of them, variation is introduced due to imaging conditions also. In this paper we have recognized nine different classes of fruits. Fruit image dataset are obtained from web as well as certain images are acquired by using mobile phone camera. These images are pre-processed to subtract the background and extract the blob representing fruit. For representing fruits and capturing their visual characteristics, combination of color, shape and texture features are used. These feature dataset is further passed to two different classifiers; multiclass SVM and KNN. The experimental results obtained are used to draw various conclusions. The best accuracy obtained by us in the study is 91.3% with KNN (K=2), classifier whereas with multiclass SVM (one-versus-all), the best accuracy obtained is 86.96%.","","Electronic:978-1-5090-1338-8; POD:978-1-5090-1339-5","10.1109/CAST.2016.7915033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915033","Color Coherence vector;Fruit recognition;Gray Level Co-occurrence Matrix;LAB;Local Binary Pattern;SVM","Coherence;Feature extraction;Histograms;Image color analysis;Shape;Support vector machines;Training","","","","","","","","","19-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"Dynamic Mapping of Road Conditions Using Smartphone Sensors and Machine Learning Techniques","S. M. A. Gawad; A. El Mougy; M. A. El-Meligy","Fac. of Media Eng. & Technol., German Univ. in Cairo, Cairo, Egypt","2016 IEEE 84th Vehicular Technology Conference (VTC-Fall)","20170320","2016","","","1","5","Road surface conditions can cause serious traffic accidents, often with tragic consequences. Thus, an efficient system for mapping road anomalies can significantly promote the safety of drivers and pedestrians. This paper proposes a novel road anomaly mapping system that is able to detect a wide variety of conditions with high accuracy. The smartphone's accelerometer and GPS sensors are used for detection to minimize infrastructure costs. In addition, to ensure the system is adaptive to different road conditions, pattern recognition techniques are used to automatically calculate the detection threshold. Furthermore, to compensate for GPS inaccuracies, reinforcement learning based on a proposed reward system is used to maximize confidence in the detected anomalies. The reward system is also able to forget anomalies that have been fixed. Moreover, the system is implemented in a distributed way between the smartphone and a cloud server to minimize cellular bandwidth usage, while still retaining the accuracy advantages of a centralized cloud. Live tests have been conducted to evaluate the performance of the system and the results show it is accurate under different driving conditions.","","Electronic:978-1-5090-1701-0; POD:978-1-5090-1702-7","10.1109/VTCFall.2016.7880972","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880972","","Acceleration;Accelerometers;Global Positioning System;Roads;Sensors;Servers;Vehicles","Global Positioning System;accelerometers;cloud computing;learning (artificial intelligence);pattern recognition;road safety;smart phones;terrain mapping","GPS sensors;cellular bandwidth usage;cloud server;driver safety;dynamic mapping;machine learning techniques;pattern recognition techniques;pedestrian safety;reinforcement learning;road anomaly mapping system;road surface condition;smartphone sensors;smartphones accelerometer;traffic accidents","","","","","","","18-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Estimation of New York Heart Association class in heart failure patients based on machine learning techniques","E. E. Tripoliti; T. G. Papadopoulos; G. S. Karanasiou; F. G. Kalatzis; A. Bechlioulis; Y. Goletsis; K. K. Naka; D. I. Fotiadis","Department of Biomedical Research, Institute of Molecular Biology and Biotechnology, FORTH, GR 45110, Ioannina, Greece","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","421","424","The aim of this work is to present an automated method for the early identification of New York Heart Association (NYHA) class change in patients with heart failure using classification techniques. The proposed method consists of three main steps: a) data processing, b) feature selection, and c) classification. The estimation of the severity of heart failure in terms of NYHA class is addressed as two, three and, for the first time, as four class classification problem. Eleven classifiers are employed and combined with resampling techniques. The proposed method is evaluated on a dataset of 378 patients, through a 10-fold-cross-validation approach. The highest detection accuracy is 97, 87 and 67% for the two, three and the four class classification problem, respectively.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897295","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897295","","Classification algorithms;Estimation;Hafnium;Heart;Medical diagnostic imaging;Radio frequency;Support vector machines","","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine for Effective Clinical Diagnosis and Treatment","S. Yang; R. Wei; J. Guo; L. Xu","Faculty of Science and Technology, University of Macau, Taipa, China","IEEE Access","20170327","2017","5","","3529","3546","Clinical practice calls for reliable diagnosis and optimized treatment. However, human errors in health care remain a severe issue even in industrialized countries. The application of clinical decision support systems (CDSS) casts light on this problem. However, given the great improvement in CDSS over the past several years, challenges to their wide-scale application are still present, including: 1) decision making of CDSS is complicated by the complexity of the data regarding human physiology and pathology, which could render the whole process more time-consuming by loading big data related to patients; and 2) information incompatibility among different health information systems (HIS) makes CDSS an information island, i.e., additional input work on patient information might be required, which would further increase the burden on clinicians. One popular strategy is the integration of CDSS in HIS to directly read electronic health records (EHRs) for analysis. However, gathering data from EHRs could constitute another problem, because EHR document standards are not unified. In addition, HIS could use different default clinical terminologies to define input data, which could cause additional misinterpretation. Several proposals have been published thus far to allow CDSS access to EHRs via the redefinition of data terminologies according to the standards used by the recipients of the data flow, but they mostly aim at specific versions of CDSS guidelines. This paper views these problems in a different way. Compared with conventional approaches, we suggest more fundamental changes; specifically, uniform and updatable clinical terminology and document syntax should be used by EHRs, HIS, and their integrated CDSS. Facilitated data exchange will increase the overall data loading efficacy, enabling CDSS to read more information for analysis at a given time. Furthermore, a proposed CDSS should be based on self-learning, which dynamically updates a kn- wledge model according to the data-stream-based upcoming data set. The experiment results show that our system increases the accuracy of the diagnosis and treatment strategy designs.","2169-3536;21693536","","10.1109/ACCESS.2017.2672975","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865914","Big data;case-based reasoning;clinical diagnosis;data stream mining;decision tree;disease detection;electronic health record;medical record;semantic integration","Clinical diagnosis;Cognition;Data mining;Decision trees;Diseases;Medical diagnostic imaging","Big Data;decision support systems;document handling;electronic health records;health care;inference mechanisms;learning (artificial intelligence);medical diagnostic computing;patient treatment","Big Data;CDSS;EHR document standards;clinical decision support systems;clinical diagnosis;clinical documents;clinical treatment;data exchange;data loading efficacy;data-stream-based upcoming data set;document syntax;electronic health records;health care;health information systems;inference engine;knowledge model;machine learning;semantic inference;updatable clinical terminology","","","","","","20170301","2017","","IEEE","IEEE Journals & Magazines"
"Comparison of Machine Learning Approaches for Prediction of Advanced Liver Fibrosis in Chronic Hepatitis C Patients","S. Hashem; G. Esmat; W. Elakel; S. Habashy; S. Abdel Raouf; M. Elhefnawi; M. Eladawy; M. Elhefnawi","","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2017","PP","99","1","1","Background/Aim: Using machine learning approaches as non-invasive methods have been used recently as an alternative method in staging chronic liver diseases for avoiding the drawbacks of biopsy. This study aims to evaluate different machine learning techniques in prediction of advanced fibrosis by combining the serum bio-markers and clinical information to develop the classification models. Methods: A prospective cohort of 39,567 patients with chronic hepatitis C was divided into two sets – one categorized as mild to moderate fibrosis (F0-F2), and the other categorized as advanced fibrosis (F3-F4) according to METAVIR score. Decision tree, genetic algorithm, particle swarm optimization, and multilinear regression models for advanced fibrosis risk prediction were developed. Receiver operating characteristic curve analysis was performed to evaluate the performance of the proposed models. Results: Age, platelet count, AST, and albumin were found to be statistically significant to advanced fibrosis. The machine learning algorithms under study were able to predict advanced fibrosis in patients with HCC with AUROC ranging between 0.73 and 0.76 and accuracy between 66.3% and 84.4%. Conclusions: Machine-learning approaches could be used as alternative methods in prediction of the risk of advanced liver fibrosis due to chronic hepatitis C.","1545-5963;15455963","","10.1109/TCBB.2017.2690848","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891989","Decision Learning Tree;Hepatitis C virus;Liver fibrosis prediction;Machine Learning Algorithm;Particle Swarm Optimization;Serum marker","Biopsy;Decision trees;Diseases;Genetic algorithms;Liver;Particle swarm optimization;Sociology","","","","","","","","20170404","","","IEEE","IEEE Early Access Articles"
"Enrichment of Machine Learning Based Activity Classification in Smart Homes Using Ensemble Learning","B. Agarwal; A. Chakravorty; T. Wiktorski; C. Rong","Dept. of Comput. & Electr. Eng., Univ. of Stavanger, Stavanger, Norway","2016 IEEE/ACM 9th International Conference on Utility and Cloud Computing (UCC)","20170320","2016","","","196","201","Data streams from various Internet-Of-Things (IOT) enabled sensors in smart homes provide an opportunity to develop predictive models to offer actionable insights in form of preventive care to its residence. This becomes particularly relevant for Aging-In-Place (AIP) solutions for the care of the elderly. Over the last decade, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated to furnish homes with a variety of IOT enabled sensors to record daily activities of individuals. Machine Learning on such streams allows for detection of patterns and prediction of activities which enables preventive care. Behavior patterns that lead to preventive care constitute a series of activities. Accurate labeling of activities is an extremely time-consuming process and the resulting labels are often noisy and error prone. In this paper, we analyze the classification accuracy of various activities within a home using machine learning models. We present that the use of an ensemble model that combines multiple learning models allows to obtain better classification of activities than any of the constituent learning algorithms.","","Electronic:978-1-4503-4616-0; POD:978-1-5090-4467-2","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881632","aging in place (AIP);ensemble learning;internet of things (IOT);machine learning;smart homes","Activity recognition;Feature extraction;Mathematical model;Sensor phenomena and characterization;Smart homes;Vegetation","Internet of Things;geriatrics;home computing;learning (artificial intelligence);pattern classification;sensors","AIP;IOT;Internet of Things;activity classification;aging in place;data streams;ensemble learning;machine learning;sensors;smart homes","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Atrial fibrillation detection with multiparametric RR interval feature and machine learning technique","S. Islam; N. Ammour; N. Alajlan","Department of Computer Science College of Computer and Information Sciences King Saud University, Kingdom of Saudi Arabia","2017 International Conference on Informatics, Health & Technology (ICIHT)","20170424","2017","","","1","5","Automatic screening of atrial fibrillation (AF) in the out-of-clinic environment is potentially an effective method for early detection of this life-threatening arrhythmia which is often paroxysmal and asymptomatic. Different technologies such as modified blood pressure monitor, single lead ECG-based finger-probe, and smartphone using plethysmogram signal have been emerging for this purpose. All these technologies use irregularity of RR interval (RRI) as a feature for AF detection. For real-time applications scalar feature is extracted from RRI signal and classified with a threshold. In this work, we have introduced multi-parametric RRI feature yielding a multidimensional feature vector. We used machine learning technique to learn the optimal decision boundary. The proposed method was tested with a publicly available landmark database. Initial experiments show promising AF detection performances comparable to those of state-of-the-art methods. Development and implementation of such a method in existing screen devices such a smartphone could be important for prevention of AFrelated risk of stroke, dementia, and death.","","Electronic:978-1-4673-8765-1; POD:978-1-4673-8766-8","10.1109/ICIHT.2017.7899003","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899003","RR interval feature;atrial fibrillation;automatic screening;decision boundary;support vector machine","Databases;Feature extraction;Rail to rail inputs;Real-time systems;Rhythm;Support vector machines;Training","","","","","","","","","21-23 Feb. 2017","","IEEE","IEEE Conference Publications"
"Predicting adherence of patients with HF through machine learning techniques","G. S. Karanasiou; E. E. Tripoliti; T. G. Papadopoulos; F. G. Kalatzis; Y. Goletsis; K. K. Naka; A. Bechlioulis; A. Errachid; D. I. Fotiadis","Institute of Molecular Biology and Biotechnology, Greece","Healthcare Technology Letters","20170330","2016","3","3","165","170","Heart failure (HF) is a chronic disease characterised by poor quality of life, recurrent hospitalisation and high mortality. Adherence of patient to treatment suggested by the experts has been proven a significant deterrent of the above-mentioned serious consequences. However, the non-adherence rates are significantly high; a fact that highlights the importance of predicting the adherence of the patient and enabling experts to adjust accordingly patient monitoring and management. The aim of this work is to predict the adherence of patients with HF, through the application of machine learning techniques. Specifically, it aims to classify a patient not only as medication adherent or not, but also as adherent or not in terms of medication, nutrition and physical activity (global adherent). Two classification problems are addressed: (i) if the patient is global adherent or not and (ii) if the patient is medication adherent or not. About 11 classification algorithms are employed and combined with feature selection and resampling techniques. The classifiers are evaluated on a dataset of 90 patients. The patients are characterised as medication and global adherent, based on clinician estimation. The highest detection accuracy is 82 and 91% for the first and the second classification problem, respectively.","2053-3713;20533713","","10.1049/htl.2016.0041","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889104","","","cardiology;diseases;learning (artificial intelligence);patient monitoring;patient treatment","chronic disease;heart failure;machine learning techniques;medication;nutrition;patient adherence prediction;patient monitoring;physical activity","","","","","","","9 2016","","IET","IET Journals & Magazines"
"Indexing facial attractiveness and well beings using machine learning","G. Choudhary; T. K. Gandhi","Department of Electrical Engineering, IIT Delhi, New Delhi, India","2016 IEEE Region 10 Humanitarian Technology Conference (R10-HTC)","20170424","2016","","","1","6","A challenge is indexing the facial beauty by a machine as same evaluated by human beings. A question arises: Can beauty be learnt by machines? Every individual have different concept of facial beauty. Somebody can be attracted by someone but might not be by another person. In recent past, many psychologists, neurologists and other scientists have done tremendous work in this area. This work presents a study on the facial attractiveness in a machine learning context. Various techniques applied on SCUT-FBP facial images dataset for learning the facial attractiveness. From the results, we showed that facial beauty is a universal concept that a machine can learn. A model is designed that learns from the facial images with their attractiveness ratings and produced human like evaluation of attractiveness ratings. We extracted features from images and normalized all the features value. After that we proposed some techniques of machine learning like Support Vector Machine (SVM), k-Nearest Neighbor (KNN), Decision tree and Artificial Neural Network (ANN). An accuracy of 80% was obtained using KNN and 86% was obtained using ANN for multiclass classification. Accuracies of 61% and 62% were reported by SVM using linear kernel and RBF kernel respectively. These accuracies were obtained for multiclass classification. We also evaluated accuracy for binary class to reduce the effect of non-uniformity of data.","","Electronic:978-1-5090-4177-0; POD:978-1-5090-4178-7","10.1109/R10-HTC.2016.7906813","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906813","Artificial Neural Network(ANN);Decision Tree;Facial Attractiveness;Facial Beauty;Support Vector Machine (SVM);k-Nearest Neighbor (KNN)","Artificial neural networks;Classification algorithms;Decision trees;Image color analysis;Kernel;Skin;Support vector machines","","","","","","","","","21-23 Dec. 2016","","IEEE","IEEE Conference Publications"
"HazeEst: Machine Learning Based Metropolitan Air Pollution Estimation from Fixed and Mobile Sensors","K. Hu; A. Rahman; H. Bhrugubanda; V. Sivaraman","","IEEE Sensors Journal","","2017","PP","99","1","1","Metropolitan air pollution is a growing concern in both developing and developed countries. Fixed-station monitors, typically operated by governments, offer accurate but sparse data, and are increasingly being augmented by lower-fidelity but denser measurements taken by mobile sensors carried by concerned citizens and researchers. In this paper we introduce HazeEst – a machine learning model that combines sparse fixedstation data with dense mobile sensor data to estimate the air pollution surface for any given hour on any given day in Sydney. We assess our system using seven regression models and ten-fold cross validation. The results show that estimation accuracy of SVR (Support Vector Regression) is similar to DTR (Decision Tree Regression) and RFR (Random Forest Regression), and higher than XGB (Extreme Gradient Boosting), MLP (Multi- Layer Perceptrons), LR (Linear Regression) and ABR (Adaptive Boosting Regression). The air pollution estimates from our models are validated via field trials, and results show that SVR not only yields high spatial resolution estimates that correspond well with the pollution surface obtained from fixed and mobile sensor monitoring systems, but also indicates boundaries of polluted area better than other regression models. Our results can be visualized using a web-based application customized for metropolitan Sydney. We believe that the continuous estimates provided by our system can better inform air pollution exposure and its impact on human health.","1530-437X;1530437X","","10.1109/JSEN.2017.2690975","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892954","Air pollution monitoring;machine learning;support vector regression;web application;wireless sensor network","Air pollution;Atmospheric modeling;Data models;Mobile communication;Monitoring;Regression tree analysis;Sensors","","","","","","","","20170405","","","IEEE","IEEE Early Access Articles"
"Structured Latent Label Consistent Dictionary Learning for Salient Machine Faults Representation-Based Robust Classification","Z. Zhang; W. Jiang; F. Li; M. Zhao; B. Li; L. Zhang","School of Computer Science and Technology & Joint International Research Laboratory of Machine Learning and Neuromorphic Computing, Soochow University, Suzhou, China","IEEE Transactions on Industrial Informatics","20170419","2017","13","2","644","656","This paper investigates the salient machine faults representation-based classification issue by dictionary learning. A novel structured latent label consistent dictionary learning (LLC-DL) model is proposed for joint discriminative salient representation and classification. Our LLC-DL deals with the tasks by solving one objective function that aims to minimize the structured reconstruction error, structured discriminative sparse-code error and classification error simultaneously. Also, LLC-DL decomposes given signals into a sparse reconstruction part over structured latent weighted discriminative dictionary, a salient feature extraction part and an error part fitting noise. Specifically, the dictionary is learnt atom by atom, where each dictionary atom is learnt with a latent vector that reduces the disturbance between interclass atoms. The structured coding coefficients are calculated via minimizing the reconstruction error and discriminative sparse code error simultaneously. The salient representations are learnt by embedding signals onto a projection and a robust linear classifier is then trained over the learned salient features directly so that features can be ensured to be optimal for classification, where robust <italic>l</italic><sub>2</sub> <italic><sub>,</sub></italic><sub>1</sub>-norm imposed on the classifier can make the prediction results more accurate. By including a salient feature extraction term, the classification approach of LLC-DL is very efficient, since there is no need to involve an extra time-consuming sparse reconstruction process with the well-trained dictionary for each test signal. Extensive simulations versify the effectiveness of our algorithm.","1551-3203;15513203","","10.1109/TII.2017.2653184","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817807","Classification;salient machine faults representation;structured latent label consistent dictionary learning (LLC-DL)","Classification algorithms;Dictionaries;Encoding;Feature extraction;Informatics;Linear programming;Robustness","","","","","","","","20170116","April 2017","","IEEE","IEEE Journals & Magazines"
"Machine Learning approach to dissimilarity computation: Iris matching","N. Aginako; J. M. Martínez-Otzeta; I. Rodriguez; E. Lazkano; B. Sierra","Applied Mathematics Department, University of the Basque Country, Donostia-San Sebastin, Spain","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","170","175","This paper presents a novel approach for iris dissimilarity computation based on Machine Learning paradigms and Computer Vision transformations. Based on the training dataset given by the MICHE II Challenge organizers, a set of classifiers has been constructed and tested, aiming at classifying a single image.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899628","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899628","","Computer vision;Image edge detection;Iris recognition;Pattern recognition;Training;Vegetation","","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Voltage Stability Prediction Using Active Machine Learning","V. Malbasa; C. Zheng; P. C. Chen; T. Popovic; M. Kezunovic","University of Novi Sad, Novi Sad, Serbia (e-mail: vmalbasa@uns.ac.rs)","IEEE Transactions on Smart Grid","","2017","PP","99","1","1","An active machine learning technique for monitoring the voltage stability in transmission systems is presented. It has been shown that machine learning algorithms may be used to supplement the traditional simulation approach, but they suffer from the difficulties of online machine learning model update and offline training data preparation. We propose an active learning solution to enhance existing machine learning applications by actively interacting with the online prediction and offline training process. The technique identifies operating points where machine learning predictions based on power system measurements contradict with actual system conditions. By creating the training set around the identified operating points, it is possible to improve the capability of machine learning tools to predict future power system states. The technique also accelerates the offline training process by reducing the amount of simulations on a detailed power system model around operating points where correct predictions are made. Experiments show a significant advantage in relation to the training time, prediction time and number of measurements that need to be queried to achieve high prediction accuracy.","1949-3053;19493053","","10.1109/TSG.2017.2693394","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898513","Machine learning;active learning;data mining;power system analysis;power system planning;power system stability;power transmission;prediction methods;smart grids;synchrophasors","Computational modeling;Data models;Power system stability;Predictive models;Stability analysis;Training;Voltage measurement","","","","","","","","20170412","","","IEEE","IEEE Early Access Articles"
"Disease Prediction by Machine Learning over Big Data from Healthcare Communities","M. Chen; Y. Hao; K. Hwang; L. Wang; L. Wang","","IEEE Access","","2017","PP","99","1","1","With big data growth in biomedical and healthcare communities, accurate analysis of medical data benefits early disease detection, patient care and community services. However, the analysis accuracy is reduced when the quality of medical data is incomplete. Moreover, different regions exhibit unique characteristics of certain regional diseases, which may weaken the prediction of disease outbreaks. In this paper, we streamline machine learning algorithms for effective prediction of chronic disease outbreak in disease-frequent communities.We experiment the modified prediction models over real-life hospital data collected from central China in 2013-2015. To overcome the difficulty of incomplete data, we use a latent factor model to reconstruct the missing data. We experiment on a regional chronic disease of cerebral infarction. We propose a new convolutional neural network based multimodal disease risk prediction (CNN-MDRP) algorithm using structured and unstructured data from hospital. To the best of our knowledge, none of the existing work focused on both data types in the area of medical big data analytics. Compared to several typical prediction algorithms, the prediction accuracy of our proposed algorithm reaches 94.8% with a convergence speed which is faster than that of the CNN-based unimodal disease risk prediction (CNN-UDRP) algorithm.","2169-3536;21693536","","10.1109/ACCESS.2017.2694446","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912315","Big data analytics;Healthcare;Machine Learning","Big Data;Data models;Diseases;Hospitals;Machine learning algorithms;Prediction algorithms","","","","","","","","20170426","","","IEEE","IEEE Early Access Articles"
"Exposing and modeling underlying mechanisms in ALS with machine learning","J. Gordon; B. Lerner","Dept. of Industrial Engineering & Management, Ben-Gurion University of the Negev, Beer-Sheva, Israel 84105","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","2168","2173","We develop methodologies and apply machine-learning algorithms to a database of ALS patients to expose and model underlying mechanisms and relations in the disease. We view the disease state as an ordinal variable (with values between 4 for normal function and 0 for complete loss of function), and show that ordinal classification applied to the data has an advantage over classification that does not utilize the ordinal nature of the domain. To identify important physiological and lab test variables in relation to patient functionality, we rank variables with a decision tree that predicts future disease state using current and past variable instantiations. In addition, we cluster data of patient functionalities in performing daily tasks into higher level groupings of body segments and show how certain variables relate more concretely to certain groupings than to others, thus reducing the dimensionality of the disease state representation in a natural manner that was found to be medically interpretable. Finally, we learn Bayesian networks to detect predictors within the Markov blanket of the disease-state variable and to expose relations among the predictors and with the disease state, as well as to identify value combinations of the predictors that distinguish severe and mild patients.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7899957","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899957","","Clinical trials;Databases;Diseases;Prediction algorithms;Predictive models;Radio frequency;Vegetation","","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Interactive intelligent agents with creative minds: Experiments with mobile robots in cooperating tasks by using machine learning","M. A. Qayum; N. Nahar; N. A. Siddique; Z. M. Saifullah","Klipsch School of Electrical and Computer Engineering, New Mexico State University, Las Cruces, USA","2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","20170403","2017","","","1","6","In this paper, we present an intelligent system where agents can co-ordinate creative tasks through machine learning and cooperation. For machine learning, we used commonly used pattern recognition algorithm - Principal Component Analysis (PCA). Based on recognition, we plan a task that is performed by multiple intelligent agents. In our case, task is to draw a pattern or perform a creative art by agents. The task action is divided into three phases: obtaining a design, composing a mathematical model and and performing the task by agents. In case of agents co-ordination, various feedback techniques using wireless sensors and on-board sensors are used. As for proof of concept (POC), a flower pattern is detected, which is painted on a canvas by using mobile robots. Also, person's identity and mood is detected and then a creative art is performed by mobile robots to improve the mood.","","Electronic:978-1-5090-6004-7; POD:978-1-5090-6005-4","10.1109/ICIVPR.2017.7890884","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890884","Intelligent Agent;Intelligent Transportation System;Principal Component Analysis;Robot Operating Systems (RoS);Robotics","Art;Databases;Face;Principal component analysis;Sensors;Service robots","art;learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;principal component analysis;wireless sensor networks","PCA;POC;cooperating tasks;creative art;creative minds;feedback techniques;flower pattern;interactive intelligent agents;machine learning;mobile robots;on-board sensors;principal component analysis;proof of concept;wireless sensors","","","","","","","13-14 Feb. 2017","","IEEE","IEEE Conference Publications"
"Anomaly-based NIDS: A review of machine learning methods on malware detection","R. Z. A. Mohd; M. F. Zuhairi; A. Z. A. Shadil; Hassan Dao","Malaysian Institute of Information Technology, University of Kuala Lumpur, Malaysia","2016 International Conference on Information and Communication Technology (ICICTM)","20170403","2016","","","266","270","The increasing amount of network traffic threat may originates from various sources, that can led to a higher probability for an organization to be exposed to intruder. Security mechanism such as Intrusion Detection System (IDS) is significant to alleviate such issue. Despite the ability of IDS to detect, some of the anomaly traffic may not be effectively detected. As such, it is vital the IDS algorithm to be reliable and can provide high detection accuracy, reducing as much as possible threats from the network. Nonetheless, every security mechanism has its weaknesses that can be exploited by intruders. Many research works exists, that attempts to address the issue using various methods. This paper discusses a hybrid approach to network IDS, which can minimize the malicious traffic in the network by using machine learning. The paper also provides a review of the available methods to further improve Anomaly-based Network Intrusion Detection System.","","Electronic:978-1-5090-0412-6; POD:978-1-5090-0413-3","10.1109/ICICTM.2016.7890812","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890812","Anomaly-based NIDS;Intrusion Detection System;intrusion;network security","Classification algorithms;Databases;Filtering algorithms;Monitoring;Nails;Sociology;Statistics","computer network security;invasive software;learning (artificial intelligence)","IDS algorithm;anomaly-based NIDS;anomaly-based network intrusion detection system;machine learning methods;malware detection;network traffic threat","","","","","","","16-17 May 2016","","IEEE","IEEE Conference Publications"
"Feature selection to detect botnets using machine learning algorithms","F. V. Alejandre; N. C. Cortés; E. A. Anaya","Instituto Polit&#x00E9;cnico Nacional, Centro de Investigaci&#x00F3;n en Computaci&#x00F3;n, Laboratory of Cybersecurity, M&#x00E9;xico, D.F.","2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)","20170406","2017","","","1","7","In this paper, a novel method to do feature selection to detect botnets at their phase of Command and Control (C&C) is presented. A major problem is that researchers have proposed features based on their expertise, but there is no a method to evaluate these features since some of these features could get a lower detection rate than other. To this aim, we find the feature set based on connections of botnets at their phase of C&C, that maximizes the detection rate of these botnets. A Genetic Algorithm (GA) was used to select the set of features that gives the highest detection rate. We used the machine learning algorithm C4.5, this algorithm did the classification between connections belonging or not to a botnet. The datasets used in this paper were extracted from the repositories ISOT and ISCX. Some tests were done to get the best parameters in a GA and the algorithm C4.5. We also performed experiments in order to obtain the best set of features for each botnet analyzed (specific), and for each type of botnet (general) too. The results are shown at the end of the paper, in which a considerable reduction of features and a higher detection rate than the related work presented were obtained.","","Electronic:978-1-5090-3621-9; POD:978-1-5090-3622-6","10.1109/CONIELECOMP.2017.7891834","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891834","Botnet;Feature Selection;Machine Learning;Malware Detection","Feature extraction;Genetic algorithms;Machine learning algorithms;Proposals;Servers;Testing;Training","command and control systems;feature selection;genetic algorithms;invasive software;learning (artificial intelligence);pattern classification","C&C;C4.5 algorithm;ISCX repositories;ISOT repositories;botnet detection;command and control;feature evaluation;feature selection;genetic algorithm;machine learning","","","","","","","22-24 Feb. 2017","","IEEE","IEEE Conference Publications"
"Application of Machine Learning algorithms for betterment in education system","R. R. Halde","L & T Infotech, Mumbai, India","2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)","20170316","2016","","","1110","1114","Machine learning is the process which converts the information into intelligent actions. This paper presents a literature review on application of different Machine Learning algorithms on huge amount of data collected by the academic institutes. Predictive analytics using the machine learning algorithms has become a new tool of this modern era, as it assists academic institutions in improving the retention and success rate of students and to get overview of performance before the examination to reduce the risk of failure. The main aim of the paper is to describe the various ways in which the machine learning is used in educational institutes and how institutes can get prediction of students' performance and the important features that are needed to be considered while making prediction for different things. In addition to this the study also compares the prediction given by different machine learning algorithms. Lastly the paper concludes that the prediction of the students' performance can be made more precise and accurate by considering the learning style of students, their motivation and interest, concentration level, family background, personality type, information processing ability and the way they attempt the exams.","","Electronic:978-1-5090-2080-5; POD:978-1-5090-2081-2","10.1109/ICACDOT.2016.7877759","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877759","Classification;Machine Learning;Performance Prediction;Predictive analytics;Regression","Decision trees;Education;Linear regression;Machine learning algorithms;Neural networks;Prediction algorithms;Predictive models","behavioural sciences computing;educational computing;educational institutions;learning (artificial intelligence)","academic institutions;concentration level;education system;educational institutes;failure risk reduction;family background;information processing ability;learning style;machine learning algorithms;motivation-and-interest;personality type;predictive analytics","","","","","","","9-10 Sept. 2016","","IEEE","IEEE Conference Publications"
"Error Tolerance Analysis of Deep Learning Hardware Using a Restricted Boltzmann Machine Toward Low-Power Memory Implementation","T. Marukame; K. Ueyoshi; T. Asai; M. Motomura; A. Schmid; M. Suzuki; Y. Higashi; Y. Mitani","Microelectronic Systems Laboratory, Advanced LSI Technology Laboratory, Swiss Federal Institute of Technology (EPFL), Toshiba Corporation, Lausanne, Kawasaki, SwitzerlandJapan","IEEE Transactions on Circuits and Systems II: Express Briefs","20170324","2017","64","4","462","466","Remarkable hardware robustness of deep learning (DL) is revealed by error injection analyses performed using a custom hardware model implementing parallelized restricted Boltzmann machines (RBMs). RBMs in deep belief networks demonstrate robustness against memory errors during and after learning. Fine-tuning significantly affects the recovery of accuracy for static errors injected to the structural data of RBMs. The memory error tolerance is observable using our hardware networks with fine-graded memory distribution, resulting in reliable DL hardware with low-voltage driven memory suitable to low-power applications.","1549-7747;15497747","","10.1109/TCSII.2016.2585675","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501537","Deep learning (DL);fault tolerance;low power;restricted Boltzmann machines (RBMs);static random access memory (SRAM)","Circuit faults;Computer architecture;Data models;Field programmable gate arrays;Hardware;Machine learning;Robustness","Boltzmann machines;belief networks;error analysis;learning (artificial intelligence);low-power electronics;neural chips;storage management chips","RBMs;custom hardware model;deep belief networks;deep learning hardware;error injection analysis;fine-graded memory distribution;low-power memory implementation;low-voltage driven memory;memory error tolerance analysis;parallelized restricted Boltzmann machines;structural data","","","","","","20160628","April 2017","","IEEE","IEEE Journals & Magazines"
"Machine Learning Algorithms for Natural Language Semantics and Cognitive Computing","E. Khan","Dept. of Comput. Sci., MUM (Maharishi U. of Manage.), Fairfield, IA, USA","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","1146","1151","We present elegant machine learning algorithms to efficiently learn natural language semantics (MLANLP), thus enabling much better Natural Language Computing (NLC) and Cognitive Computing (CC). Our algorithms use human brain-like learning approach and achieve very good generalization on natural language (mainly text) data. Existing machine learning algorithms performs well on numerical data and cannot easily learn semantics of natural language. Such algorithms, however, can address well some specific problems of natural language, like Name Entity Recognition where data can be easily represented by numbers and semantics between words (name and entity) are simple. Besides, the generalization capabilities of existing machine learning algorithms are limited, especially for complex data. The generalization capability for learning semantics of natural language should be very good to ensure reliable NLC and CC. Our MLANLP has good generalization capability, and can also derive new semantics and knowledge, very much needed for NLC and CC.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0217","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881510","Artificial Intelligence;Cognitive Computing;Machine Learning;Natural Language Computing;Semantic Engine","Algorithm design and analysis;Classification algorithms;Computer science;Engines;Machine learning algorithms;Natural languages;Semantics","learning (artificial intelligence);natural language processing;text analysis","CC;MLANLP;NLC;cognitive computing;generalization capability;human brain-like learning approach;learning semantics;machine learning algorithms;name entity recognition;natural language data;natural language semantics","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Predictive maintenance applications for machine learning","B. Cline; R. S. Niculescu; D. Huffman; B. Deckel","Parametric Technology Corporation","2017 Annual Reliability and Maintainability Symposium (RAMS)","20170330","2017","","","1","7","Machine Learning provides a complementary approach to maintenance planning by analyzing significant data sets of individual machine performance and environment variables, identifying failure signatures and profiles, and providing an actionable prediction of failure for individual parts.","","Electronic:978-1-5090-5284-4; POD:978-1-5090-5285-1","10.1109/RAM.2017.7889679","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889679","Machine Learning;Predicted Failure Analysis;Predictive Maintenance","Analytical models;Connectors;Data models;Inspection;Predictive maintenance;Predictive models","failure analysis;learning (artificial intelligence);maintenance engineering;planning;production engineering computing;reliability","environmental variable;failure signature;machine learning;machine performance;maintenance planning;predictive maintenance;reliability","","","","","","","23-26 Jan. 2017","","IEEE","IEEE Conference Publications"
"Machine learning algorithms for process analytical technology","N. O'Mahony; T. Murphy; K. Panduru; D. Riordan; J. Walsh","IMAR Technology Gateway, Institute of Technology Tralee, Ireland","2016 World Congress on Industrial Control Systems Security (WCICSS)","20170323","2016","","","1","7","Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.","","Electronic:978-1-9083-2063-6; POD:978-1-5090-2544-2; USB:978-1-9083-2064-3","10.1109/WCICSS.2016.7882607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882607","Adaptive Neuro-Fuzzy Inference Systems;Genetic Algorithms;Machine Learning;Neural Networks;System Modelling and Parameter Estimation","Adaptation models;Genetic algorithms;Image color analysis;Industries;Machine learning algorithms;Neural networks;Process control","chemical engineering computing;data analysis;fuzzy neural nets;fuzzy reasoning;genetic algorithms;globalisation;learning (artificial intelligence);polymerisation;process control;process monitoring","MATLAB software;MSE;PAT demands;adaptive neuro-fuzzy inference systems;critical control parameter monitoring;genetic algorithms;globalisation;industrial applications;industrial process industry;integrated process control;machine learning algorithms;neural networks;polymerisation process;process analytical technologies;process modelling;real-time data analytics;sensor data;sensor measurements","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Toward Emotion-Aware Computing: A Loop Selection Approach Based on Machine Learning for Speculative Multithreading","B. Liu; J. He; Y. Geng; L. Huang; S. Li","College of Information Engineering, Northwest A&#x0026;F University, Yangling, China","IEEE Access","20170424","2017","5","","3675","3686","Emotion-aware computing can recognize, interpret, process, and simulate human affects. These programs in this area are compute-intensive applications, so they need to be executed in parallel. Loops usually have regular structures and programs spend significant amounts of time executing them, and thus loops are ideal candidates for exploiting the parallelism of sequential programs. However, it is difficult to decide which set of loops should be parallelized to improve program performance. The existing research is one-size-fits-all strategy and cannot guarantee to select profitable loops to be parallelized. This paper proposes a novel loop selection approach based on machine learning (ML-based) for selecting the profitable loops and paralleling them on multi-core by speculative multithreading (SpMT). It includes establishing sufficient training examples, building and applying prediction model to select profitable loops for speculative parallelization. Using the ML-based loop selection approach, an unseen emotion-aware sequential program can obtain a stable, much higher speedup than the one-size-fits-all approach. On Prophet, which is a generic SpMT processor to evaluate the performance of multithreaded programs, the novel loop selection approach is evaluated and reaches an average speedup of 1.87 on a 4-core processor. Experiment results show that the ML-based approach can obtain a significant increase in speedup, and Olden benchmarks deliver a better performance improvement of 6.70% on a 4-core than the one-size-fits-all approach.","2169-3536;21693536","","10.1109/ACCESS.2017.2684129","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882711","Emotion-aware computing;loop selection;machine learning;speculative multithreading","Benchmark testing;Computers;Instruction sets;Multithreading;Predictive models;Training","","","","","","","","20170320","2017","","IEEE","IEEE Journals & Magazines"
"Dataset Coverage for Testing Machine Learning Computer Programs","S. Nakajima; H. N. Bui","Nat. Inst. of Inf., Tokyo, Japan","2016 23rd Asia-Pacific Software Engineering Conference (APSEC)","20170403","2016","","","297","304","Machine learning programs are non-testable, and thus testing with pseudo oracles is recommended. Although metamorphic testing is effective for testing with pseudo oracles, identifying metamorphic properties has been mostly ad hoc. This paper proposes a systematic method to derive a set of metamorphic properties for machine learning classifiers, support vector machines. The proposal includes a new notion of test coverage for the machine learning programs; this test coverage provides a clear guideline for conducting a series of metamorphic testing.","1530-1362;15301362","Electronic:978-1-5090-5575-3; POD:978-1-5090-5576-0","10.1109/APSEC.2016.049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890601","Metamorphic Testing;Pseudo Oracles;Support Vector Machines;Test Coverage","Computers;Guidelines;Software testing;Support vector machines;Systematics;Training","learning (artificial intelligence);program testing;support vector machines","dataset coverage;machine learning classifiers;machine learning computer program testing;metamorphic testing;support vector machines","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Comparison of Text Sentiment Analysis Based on Machine Learning","X. Zhang; X. Zheng","","2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)","20170424","2016","","","230","233","Sentiment analysis is a technology with great practical value, it can solve the phenomenon of network comment information disorderly to a certain extent, and accurate positioning of user information required. Currently for Chinese sentiment analysis research is relatively small, including a variety of supervised learning method of classification result and the text feature representation methods and feature selection mechanism and other factors impact on the classification performance is an urgent problem. In this paper, we taken the verb, adjectives and adverbs as text features, used TF-IDF to calculate weight of words. Then we adopted the SVM and ELM with kernels to analyze the text emotion tendentiousness. The experimental results show that ELM with kernels can be obtained a better classification result in a relatively short period of time than SVM.","","Electronic:978-1-5090-4152-7; POD:978-1-5090-4153-4","10.1109/ISPDC.2016.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904295","Extreme learning machine;Machine learning;Sentiment analysis;Support Vector Machine","Analytical models;Kernel;Neural networks;Sentiment analysis;Supervised learning;Support vector machines;Training","","","","","","","","","8-10 July 2016","","IEEE","IEEE Conference Publications"
"Applying machine learning methods to predict hand hygiene compliance characteristics","P. Zhang; J. White; D. Schmidt; T. Dennis","Institute for Software Integrated Systems, Vanderbilt University, Nashville, TN, USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","353","356","Increasing hospital re-admission rates due to Hospital Acquired Infections (HAIs) are a concern at many healthcare facilities. To prevent the spread of HAIs, caregivers should comply with hand hygiene guidelines, which require reliable and timely hand hygiene compliance monitoring systems. The current standard practice of monitoring compliance involves the direct observation of caregivers' hand cleaning as they enter or exit a patient room by a trained observer, which can be time-consuming, resource-intensive, and subject to bias. To alleviate tedious manual effort and reduce errors, this paper describes how we applied machine learning to study the characteristics of compliance that can later be used to (1) assist direct observation by deciding when and where to station manual auditors and (2) improve compliance by providing just-in-time alerts or recommending training materials to non-compliant staff. The paper analyzes location and handwashing station activation data from a 30-bed intensive care unit study and uses machine learning to assess if location, time-based factors, or other behavior data can determine what characteristics are predictive of handwashing non-compliance events. The results of this study show that a care provider's entry compliance is highly indicative of the same provider's exit compliance. Moreover, compliance of the most recent patient room visit can also predict entry compliance of a provider's current patient room visit.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897278","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897278","","Guidelines;Hospitals;Manuals;Measurement;Monitoring;Standards;Training","","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Feasibility of Supervised Machine Learning for Cloud Security","D. Bhamare; T. Salman; M. Samaka; A. Erbad; R. Jain","Qatar Univ., Doha, Qatar","2016 International Conference on Information Science and Security (ICISS)","20170327","2016","","","1","5","Cloud computing is gaining significant attention, however, security is the biggest hurdle in its wide acceptance. Users of cloud services are under constant fear of data loss, security threats and availability issues. Recently, learning-based methods for security applications are gaining popularity in the literature with the advents in machine learning techniques. However, the major challenge in these methods is obtaining real-time and unbiased datasets. Many datasets are internal and cannot be shared due to privacy issues or may lack certain statistical characteristics. As a result of this, researchers prefer to generate datasets for training and testing purpose in the simulated or closed experimental environments which may lack comprehensiveness. Machine learning models trained with such a single dataset generally result in a semantic gap between results and their application. There is a dearth of research work which demonstrates the effectiveness of these models across multiple datasets obtained in different environments. We argue that it is necessary to test the robustness of the machine learning models, especially in diversified operating conditions, which are prevalent in cloud scenarios. In this work, we use the UNSW dataset to train the supervised machine learning models. We then test these models with ISOT dataset. We present our results and argue that more research in the field of machine learning is still required for its applicability to the cloud security.","","Electronic:978-1-5090-5493-0; POD:978-1-5090-5494-7","10.1109/ICISSEC.2016.7885853","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885853","","Cloud computing;Machine learning algorithms;Robustness;Security;Support vector machines;Testing;Training","cloud computing;learning (artificial intelligence);security of data","ISOT dataset;UNSW dataset;cloud computing;cloud security;cloud services;learning-based methods;machine learning techniques;supervised machine learning","","","","","","","19-22 Dec. 2016","","IEEE","IEEE Conference Publications"
"Imbalance Learning Machine Based Power System Short-Term Voltage Stability Assessment","L. Zhu; C. Lu; Z. Y. Dong; C. Hong","","IEEE Transactions on Industrial Informatics","","2017","PP","99","1","1","In terms of machine learning based power system dy-namic stability assessment, it is feasible to collect learning data from massive synchrophasor measurements in practice. However, the fact that instability events rarely occur would lead to a chal-lenging class imbalance problem. Besides, short-term feature ex-traction from scarce instability seems extremely difficult for con-ventional learning machines. Faced with such a dilemma, this pa-per develops a systematic imbalance learning machine for online short-term voltage stability assessment. A powerful time series shapelet (discriminative subsequence) classification method is em-bedded into the machine for sequential transient feature mining. A forecasting-based nonlinear synthetic minority oversampling technique is proposed to mitigate the distortion of class distribu-tion. Cost-sensitive learning is employed to intensify bias towards those scarce yet valuable unstable cases. Furthermore, an incre-mental learning strategy is put forward for online monitoring, con-tributing to adaptability and reliability enhancement along with time. Simulation results on the Nordic test system illustrate the high performance of the proposed learning machine and of the as-sessment scheme.","1551-3203;15513203","","10.1109/TII.2017.2696534","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906492","Class imbalance;cost-sensitive;incremental learning;shapelets;short-term voltage stability.","Feature extraction;Power system dynamics;Power system stability;Stability criteria;Thermal stability;Voltage measurement","","","","","","","","20170424","","","IEEE","IEEE Early Access Articles"
"Relational machine learning author disambiguation","E. Bastrakova; R. Ledesma; J. Millan; F. Rico; D. Zighed","Data Mining and Knowledge Management - Universite Lumiere Lyon II, Lyon France","2016 IEEE Artificial Intelligence and Natural Language Conference (AINL)","20170406","2016","","","1","7","Author disambiguation is an open issue in the world of academic digital libraries. As many problems arise when trying to identify if two different signatures are from the same author and than group them, this issue has become more relevant inside the scientific community. This paper illustrates a workflow that aims to solve this issue. By using the best of a relational database engine and data mining techniques implemented in R, we have implemented a workflow that correctly disambiguates different instances of an author's name present in academic publications retrieved from the Internet. To evaluate he results we perform a two=step-validation process inside the workflow, validating if two articles were written by the same author, and if so, validating the authors grouped together as unique disambiguated author. With the validations performed, the workflow implemented allows the process of identifying and disambiguating any new author.","","Electronic:978-952-68397-8-3; POD:978-1-5090-4954-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891792","","","Internet;academic libraries;data mining;digital libraries;learning (artificial intelligence);publishing;relational databases","Internet;R;academic digital libraries;academic publications;data mining techniques;relational database engine;relational machine learning author disambiguation;scientific community;two step-validation process","","","","","","","10-12 Nov. 2016","","IEEE","IEEE Conference Publications"
"Conceptual design of proactive SONs based on the Big Data framework for 5G cellular networks: A novel Machine Learning perspective facilitating a shift in the SON paradigm","B. Keshavamurthy; M. Ashraf","Department of Electronics and Communication, BMS College of Engineering, Bangalore, India","2016 International Conference System Modeling & Advancement in Research Trends (SMART)","20170412","2016","","","298","304","Self-Organizing Networks (SONs) are being researched extensively in the existing 3G and 4G landscape primarily to facilitate a convenient yet cost-effective approach in the configuration, optimization and troubleshooting of networks. However, the existing SONs will be no match for the operational complexity of the envisioned 5G networks. The promise of 5G revolves around the premise of infinite capacity and zero latency. 5G networks are set for commercial availability by 2020 and these networks will be part of a flexible and dynamic telecom ecosystem supporting cross-domain integration and multi-RAT environments. Network Densification, Network Function Virtualization, Flexible spectrum allocation, E2E security and Massive MTC are a few of the features promised by 5G networks. With this assortment of numerous technologies, it is obvious that SONs have to evolve beyond the existing reactive paradigm without which the maintenance and management of 5G networks would prove to be a herculean task. Hence, we propose a novel Proactive SON methodology based on the Big Data framework to enable the shift in the SON paradigm. In this article we present a comprehensive Big-Data based SON framework involving innovative Machine Learning techniques which would cater to scalability and programmability of 5G networks with respect to availability, reliability, speed, capacity, security and latency.","","Electronic:978-1-5090-3543-4; POD:978-1-5090-3544-1","10.1109/SYSMART.2016.7894539","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894539","Bitmaps;COD;Handover;Heuristic Analysis;KPIs;LTE;MTC;Regression;Root Cause Analysis;SONs;UMTS","3GPP;5G mobile communication;Automation;Big Data;Cellular networks;Complexity theory;Maintenance engineering","5G mobile communication;Big Data;cellular radio;learning (artificial intelligence);radio access networks;telecommunication computing;telecommunication network management;telecommunication network reliability;telecommunication security;virtualisation","3G landscape;4G landscape;5G cellular networks;Big-Data based SON framework;E2E security;conceptual proactive SON design;cross-domain integration;flexible dynamic telecom ecosystem;flexible spectrum allocation;machine learning;massive MTC;multiRAT environments;network densification;network function virtualization;self-organizing networks","","","","","","","25-27 Nov. 2016","","IEEE","IEEE Conference Publications"
"Retinal vessel segmentation under pathological conditions using supervised machine learning","P. Rani; Priyadarshini N.; Rajkumar E. R.; K. Rajamani","Division of Biomedical Engineering, School of Bio Sciences and Technology, VIT University, Vellore, India","2016 International Conference on Systems in Medicine and Biology (ICSMB)","20170501","2016","","","62","66","In this paper we present an automated blood vessel segmentation system algorithm for the retinal images under pathological conditions like Diabetic Retinopathy (DR) using matched filters and supervised classification techniques. Matched filter has been extensively used in the enhancement and segmentation of the retinal blood vessels due to the cross sectional similarity of the vessels to the Gaussian profile. However in addition to the vessel edges the non vessel edges also gives a strong response to the matched filter leading to false detection. Based on the structural and spatial differences between the segmented vessels and the non vessels components, we propose a classification technique using machine learning approach to mask out the false detection due to non vessel structures. The proposed method shows an increased accuracy than the state of the art matched filter techniques especially in the case of vessel segmentation from pathologically affected retinal images.","","Electronic:978-1-4673-7666-2; POD:978-1-4673-7667-9; USB:978-1-4673-7665-5","10.1109/ICSMB.2016.7915088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915088","","Blood vessels;Databases;Image segmentation;Kernel;Pathology;Retina;Support vector machines","","","","","","","","","4-7 Jan. 2016","","IEEE","IEEE Conference Publications"
"Improving Recognition of Antimicrobial Peptides and Target Selectivity through Machine Learning and Genetic Programming","D. Veltri; U. Kamath; A. Shehu","School of Systems Biology, George Mason University, Fairfax, VA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20170327","2017","14","2","300","313","Growing bacterial resistance to antibiotics is spurring research on utilizing naturally-occurring antimicrobial peptides (AMPs) as templates for novel drug design. While experimentalists mainly focus on systematic point mutations to measure the effect on antibacterial activity, the computational community seeks to understand what determines such activity in a machine learning setting. The latter seeks to identify the biological signals or features that govern activity. In this paper, we advance research in this direction through a novel method that constructs and selects complex sequence-based features which capture information about distal patterns within a peptide. Comparative analysis with state-of-the-art methods in AMP recognition reveals our method is not only among the top performers, but it also provides transparent summarizations of antibacterial activity at the sequence level. Moreover, this paper demonstrates for the first time the capability not only to recognize that a peptide is an AMP or not but also to predict its target selectivity based on models of activity against only Gram-positive, only Gram-negative, or both types of bacteria. The work described in this paper is a step forward in computational research seeking to facilitate AMP design or modification in the wet laboratory.","1545-5963;15455963","","10.1109/TCBB.2015.2462364","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172462","Antimicrobial peptide recognition;Gram-negative;Gram-positive;evolutionary algorithms;evolutionary computing;feature construction;feature selection;genetic programming;machine learning","Computational biology;IEEE transactions;Microorganisms;Peptides;Standards;Support vector machines;Training","antibacterial activity;genetics;learning (artificial intelligence);medical computing;microorganisms;molecular biophysics","antibiotics;antimicrobial peptide recognition;bacterial resistance;biological signals;complex sequence-based features;computational research;drug design;genetic programming;machine learning;naturally-occurring antimicrobial peptides;target selectivity;wet laboratory","","0","","","","20150729","March-April 1 2017","","IEEE","IEEE Journals & Magazines"
"Machine Learning Application to Predict the Risk of Coronary Artery Atherosclerosis","S. Nikan; F. Gwadry-Sridhar; M. Bauer","Dept. of Comput. Sci., Univ. of Western Ontario, London, ON, Canada","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","34","39","Coronary artery disease is the leading cause of death in the world. In this research, we propose an algorithm based on the machine learning techniques to predict the risk of coronary artery atherosclerosis. A ridge expectation maximization imputation (REMI) technique is proposed to estimate the missing values in the atherosclerosis databases. A conditional likelihood maximization method is used to remove irrelevant attributes and reduce the size of feature space and thus improve the speed of the learning. The STULONG and UCI databases are used to evaluate the proposed algorithm. The performance of heart disease prediction for two classification models is analyzed and compared to previous work. Experimental results show the improved accuracy percentage of risk prediction of our proposed method compared to other works. The effect of missing value imputation on the prediction performance is also evaluated and the proposed REMI approach performs significantly better than conventional techniques.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881307","atherosclerosis;conditional likelihood;machine learning;ridge expectation maximization","Arteries;Atherosclerosis;Databases;Solid modeling;Support vector machines;Training","blood vessels;cardiology;diseases;expectation-maximisation algorithm;learning (artificial intelligence);medical computing;pattern classification","REMI technique;STULONG databases;UCI databases;atherosclerosis databases;classification models;conditional likelihood maximization method;coronary artery atherosclerosis;coronary artery disease;heart disease prediction;machine learning;ridge expectation maximization imputation;risk prediction","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Classification and development of tool for heart diseases (MRI images) using machine learning","L. Sharma; G. Gupta; V. Jaiswal","Faculty of Engineering and Technology, Shoolini University, Solan, India","2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)","20170427","2016","","","219","224","Heart diseases are one of the major killers worldwide. Early detection of heart disease such as Global Hypokinesia can reduce this global burden. Computational method has potential to predict disease in early stages automatically and especially helpful in resources limited countries. Computational method to predict global hypokinesia based on confirms cases of global hypokinesia through MRI was developed. Almost all feature extraction method was used on MRI images and model was generated on merged and different images separately. High accuracy of model independent test set justified our approaches and reliability of model. The newly developed was implemented in python and available for open use.","","Electronic:978-1-5090-3669-1; POD:978-1-5090-3670-7","10.1109/PDGC.2016.7913149","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913149","Global hypokenesia;MRI;cellprofiler;cp-charm;feature extraction;prediction","Data models;Decision support systems;Diseases;Feature extraction;Heart;Magnetic resonance imaging;Standards","","","","","","","","","22-24 Dec. 2016","","IEEE","IEEE Conference Publications"
"Named Entity Recognition using Machine learning techniques for Telugu language","M. H. Khanam; M. A. Khudhus; M. S. P. Babu","Dept. of Computer Science and Engineering, Sri Venkateswara University College of Engineering, Tirupati, India","2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20170323","2016","","","940","944","In this paper, we depict hybrid approach, i.e., combination of rule based approach and machine learning techniques, i.e Conditional Random Fields (CRF) for Named Entity Recognition (NER). The main objective of Named Entity Recognition is to categorize all Named Entities (NE) in a document into predefined classes like Person name, Location name, Organization name. This paper first outlines the Named Entity Recognizer using rule based approach. In this approach we prepared Gazette lists for names of persons, locations and organizations, some suffix and prefix features and dictionary consist of 200000 words to recognize the category of names entities. Further, we used Machine learning technique, i.e., CRF in order to improve the accuracy of the system.","","Electronic:978-1-4673-9904-3; POD:978-1-4673-9905-0","10.1109/ICSESS.2016.7883220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883220","Conditional Random Field (CRF);Human Computer Interaction (HCI);Named Entity Recognition (NER);Natural Language Processing (NLP);Rule Based Approach","Algorithm design and analysis;Context","knowledge based systems;learning (artificial intelligence);natural language processing","CRF;Gazette lists;Telugu language;conditional random fields;machine learning techniques;named entity recognition;rule based approach","","","","","","","26-28 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning models for material selection: Framework for predicting flatwise compressive strength using ANN","Antony P. J.; Jnanesh N. A.; Prajna M. R.","Department of Computer Science and Engineering, KVG College of Engineering, Sullia, D.K, 574327, India","2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","20170427","2016","","","424","427","The Polyurethane foam cores (PUR) are synthesized by combining reactive chemicals namely Isocyanates and Polyols in appropriate proportions. The paper presents the physical and ANN approach of forming the PUR foam cores with change in chemical compositions. Five varied proportions with five densities of foams were synthesized and tested. The paper reports the physical properties in comparison with flatwise compression strength. It is observed that the physical test data is in absolute compatibility with the ANN results. The regression values are in good agreement with the 50∶50 chemical compositions and higher density foams are observed to be brittle and lower density foams were flexible and elastic.","","Electronic:978-1-5090-2399-8; POD:978-1-5090-2400-1; USB:978-1-5090-2398-1","10.1109/ICATCCT.2016.7912036","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912036","Artificial Neural Network;Flatwise compression strength;Polyurethane foam cores","Artificial neural networks;Chemicals;Mechanical factors;Metals;Predictive models;Sandwich structures","","","","","","","","","21-23 July 2016","","IEEE","IEEE Conference Publications"
"Integrating machine learning in embedded sensor systems for Internet-of-Things applications","J. Lee; M. Stanley; A. Spanias; C. Tepedelenlioglu","School of ECEE, SenSIP Center, Arizona State University, United States of America","2016 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)","20170327","2016","","","290","294","Interpreting sensor data in Internet-of-Things applications is a challenging problem particularly in embedded systems. We consider sensor data analytics where machine learning algorithms can be fully implemented on an embedded processor/sensor board. We develop an efficient real-time realization of a Gaussian mixture model (GMM) for execution on the NXP FRDM-K64F embedded sensor board. We demonstrate the design of a customized program and data structure that generates real-time sensor features, and we show details and training/classification results for select IoT applications. The integrated hardware/software system enables real-time data analytics and continuous training and re-training of the machine learning (ML) algorithm. The real-time ML platform can accommodate several applications with lower sensor data traffic.","","Electronic:978-1-5090-5844-0; POD:978-1-5090-5845-7","10.1109/ISSPIT.2016.7886051","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886051","Internet-of-Things;condition monitoring;embedded machine learning;sensor data analytics","","Gaussian processes;Internet of Things;data analysis;data structures;embedded systems;intelligent sensors;learning (artificial intelligence);mixture models","GMM;Gaussian mixture model;Internet-of-Things;NXP FRDM-K64F embedded sensor board;customized program design;data structure;embedded processor-sensor board;embedded sensor systems;integrated hardware-software system;machine learning algorithms;real-time ML platform;real-time data analytics;real-time sensor feature generation;sensor data analytics;sensor data traffic","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"DEVS execution acceleration with machine learning","H. Saadawi; G. Wainer; G. Pliego","Government of Canada, Ottawa, ON, Canada","2016 Symposium on Theory of Modeling and Simulation (TMS-DEVS)","20170504","2016","","","1","6","Discrete Event System Specification DEVS separates modeling and simulation execution. Simulation execution is done within a runtime environment that is often called a DEVS simulator. This separation creates an opportunity to incorporate smart algorithms in the simulator to optimize simulation execution. In this paper, we propose incorporating some predictive machine learning algorithms into the DEVS simulator that can cut simulation execution times significantly for many simulation applications without compromising the simulation accuracy. In this paper, we introduce a specific learning mechanism that can be embedded into the DEVS simulator to incrementally build a predictive model that learns from past simulations. We further look into issues related to the predictive model selection, its prediction accuracy, its effect on the overall simulation performance, and when to switch between the predictive model and the simulation during an execution.","","Electronic:978-1-5108-2321-1; POD:978-1-5386-1856-1","10.23919/TMS.2016.7918816","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918816","Discrete Event System Specification DEVS;Machine Learning;Regression Models","","","","","","","","","","3-6 April 2016","","IEEE","IEEE Conference Publications"
"Extracting Conceptual Interoperability Constraints from API Documentation Using Machine Learning","H. Abukwaik; M. Abujayyab; S. R. Humayoun; D. Rombach","Univ. of Kaiserslautern, Kaiserslautern, Germany","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","701","703","Successfully using a software web-service/platform API requires satisfying its conceptual interoperability constraints that are stated within its shared documentation. However, manual and unguided analysis of text in API documents is a tedious and time consuming task. In this work, we present our empirical-based methodology of using machine learning techniques for automatically identifying conceptual interoperability constraints from natural language text. We also show some initial promising results of our research.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883377","API documentation;Interoperability;conceptual constraints;empirical study;machine learning","Documentation;Interoperability;Natural languages;Niobium;Software;Software engineering;Support vector machines","Web services;application program interfaces;learning (artificial intelligence);natural language processing;open systems;system documentation;text analysis","API documentation;API documents;conceptual interoperability constraints;machine learning;natural language text;software Web-service API;text analysis","","","","","","","14-22 May 2016","","IEEE","IEEE Conference Publications"
"Automated histologic grading from free-text pathology reports using graph-of-words features and machine learning","H. J. Yoon; L. Roberts; G. Tourassi","Health Data Science Institute, Oak Ridge National Laboratory, Oak Ridge, TN 37831 USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","369","372","Traditional n-gram feature representation of freetext documents often fails to capture word ordering and semantics, thus compromising text comprehension. Graph-of-words, a new text representation approach based on graph analytics, is a superior method overcoming the limitations by modeling word co-occurrence. In this study, we present a novel application of graph-of-words text description for automated extraction of histologic grade from unstructured pathology reports. Using 10-fold cross-validation tests, the proposed approach resulted in substantially higher macro and micro-F1 scores with undirected graph-of-words features, compared to traditional bi-gram text features. Our feasibility study demonstrated that graph-of-words is a highly efficient method of text comprehension for information extraction from free-text clinical documents.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897282","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897282","","Breast;Cancer;Feature extraction;Information retrieval;Natural languages;Pathology;Unified modeling language","","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"A Multi-Label Learning Method Using Affinity Propagation and Support Vector Machine","J. J. Li; F. Alzami; Y. J. Gong; Z. Yu","School of Computer Science, South China Normal University, Guangzhou, China","IEEE Access","20170327","2017","5","","2955","2966","Multi-label learning plays a critical role in the areas of data mining, multimedia, and machine learning. Although many multi-label approaches have been proposed, few of them have considered to de-emphasize the effect of noisy features in the learning process. To address this issue, this paper designs a new method named representative multi-label learning algorithm. Instead of considering all features, the proposed algorithm focuses only on the representative ones, via incorporating an affinity propagation algorithm, kernel formulation, and a multi-label support vector machine into the learning framework. Specifically, it first adopts an affinity propagation algorithm to select a set of representative features and capture the relationships among features. Then, the algorithm constructs the representative kernel functions to measure the similarity between data instances. Finally, a multi-label support vector machine is applied to solve the learning problem. Based on the representative multi-label learning algorithm, we further design a representative multi-label learning ensemble framework to improve the accuracy, stableness, and robustness. Experimental results show that the proposed algorithm works well on most of the datasets and outperforms the compared multi-label learning approaches.","2169-3536;21693536","","10.1109/ACCESS.2017.2676761","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867770","Affinity propagation;classifier ensemble;multi-label learning;support vector machine","Algorithm design and analysis;Bioinformatics;Data mining;Kernel;Noise measurement;Prediction algorithms;Support vector machines","data mining;learning (artificial intelligence);multimedia systems;support vector machines","affinity propagation;data mining;kernel formulation;learning problem solution;machine learning;multilabel support vector machine;representative kernel functions;representative multilabel learning algorithm","","","","","","20170301","2017","","IEEE","IEEE Journals & Magazines"
"Glycaemic index prediction: A pilot study of data linkage challenges and the application of machine learning","J. Li; O. Arandjelovic","University of St Andrews, United Kingdom","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","357","360","The glycaemic index (GI) is widely used to characterize the effect that a food has on blood glucose which is of major importance to diabetic individuals as well as the general population at large. At present, its applicability is severely limited by the labour involved in its measurement and the lack of understanding about how different foods interact to produce the GI of the meal comprising them. In this pilot study we examine if readily available biochemical properties of foods can be used to predict their GI, thus opening possibilities for practicable use of the GI in the management of blood glucose in everyday life. We also examine practical challenges in the cross-linking of food information sources collected by different organizations, and highlight the need for the development of a universal standard which would facilitate automatic and error free data integration.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897279","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897279","","Blood;Diabetes;Indexes;Insulin;Open area test sites;Organizations;Sugar","biochemistry;bioinformatics;diseases;food technology;learning (artificial intelligence);molecular biophysics","biochemical properties;blood glucose;data linkage challenges;diabetic individuals;error free data integration;food;glycaemic index prediction;machine learning","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"Applications of Machine learning to document classification and clustering","M. S. Shaikh","Habib University, Karachi, Pakistan","2017 International Conference on Innovations in Electrical Engineering and Computational Technologies (ICIEECT)","20170504","2017","","","1","1","Recently, Machines learning techniques have had great successes in areas such as spam filtering, recommender systems, human speech recognition systems, handwriting recognition, news articles clustering, etc.","","Electronic:978-1-5090-3310-2; POD:978-1-5090-3311-9","10.1109/ICIEECT.2017.7916596","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916596","","Handwriting recognition;Recommender systems;Semantics;Speech recognition","","","","","","","","","5-7 April 2017","","IEEE","IEEE Conference Publications"
"Machine learning for embedded devices software analysis via hardware platform emulation","A. S. Lovyannikov; S. V. Zapechnikov","Department of Cryptology and Cybersecurity, National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Moscow, Russian Federation","2017 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)","20170427","2017","","","489","492","Nowadays commercial-off-the-shelf (COTS) embedded devices are widely used in many security-critical systems like nuclear stations and traffic control systems. Most of these devices has proprietary hardware and software (frequently called firmware) with little documentation available. Another common feature is the use of “binary blob” firmware, where hardware specific and high level layers can't be easily separated and, therefore are forced to be analyzed together. All these facts make firmware analysis quite a challenging task. In this paper, we'll suggest an approach for the firmware analysis with poorly documented hardware platform emulation. The advantages of this approach are almost full control under firmware state, achieving easy to scale fuzzing and manual bug hunting facilitation. For the purpose of successful realization, an identification of communication between hardware components (e.g. communication between main SoC and Bluetooth SoC) should be done. To address this issue, we suggest the use of machine learning, which, because of its nature, enables construction of algorithms that can learn from and make predictions on data.","","Electronic:978-1-5090-4865-6; POD:978-1-5090-4866-3","10.1109/EIConRus.2017.7910598","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7910598","embedded devices;firmware analysis;hardware emulation;machine learning","Analytical models;Embedded systems;Emulation;Hardware;Information security;Microprogramming","","","","","","","","","1-3 Feb. 2017","","IEEE","IEEE Conference Publications"
"Preliminary Results of Applying Machine Learning Algorithms to Android Malware Detection","M. Leeds; T. Atkison","Comput. Sci. Dept., Univ. of Alabama, Tuscaloosa, AL, USA","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","1070","1073","As the use of mobile devices continues to increase, so does the need for sophisticated malware detection algorithms. The preliminary research presented in this paper focuses on examining permission requests made by Android apps as a means for detecting malware. By using a machine learning algorithm, we are able to differentiate between benign and malicious apps. The model presented achieved a classification accuracy between 75% and 80% for our dataset and the best combination of parameters. Future work will seek to improve the model by expanding the training dataset, taking more features into account, and exploring other machine learning algorithms.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0204","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881497","Android;Machine Learning;Malware Detection","Androids;Humanoid robots;Machine learning algorithms;Malware;Smart phones;Training","invasive software;learning (artificial intelligence);mobile computing","Android apps;Android malware detection;benign apps;classification accuracy;machine learning;malicious apps;mobile devices;permission requests","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Protein Function Detection Based on Machine Learning: Survey and Possible Solutions","K. L. X. Zheng; C. Rong; Y. Yu; R. Chen","","2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)","20170424","2016","","","327","333","With the completion of the Human Genome Project, proteomics research has become one of the most important topics in the fields of life science and natural science. The project determined that proteins participate in life activities mainly in the form of complexes. At present, research on protein-protein interaction networks (PPINs) have mainly focused on detecting protein complexes or function modules. This problem has been transformed into a recognizable dense subgraph problem in a PPIN diagram.The situation in PPIN research in recent years is introduced in this study, including commonly used databases, traditional detection algorithms, recent solutions, and the application of the swarm intelligence algorithms in this field. We then propose a detection scheme based on particle swarm optimization (PSO) and gene ontology knowledge. This scheme combines PSO and biological gene ontology knowledge to identify complexes from PPINs. Simultaneously, network topology knowledge improves the detection accuracy of the protein module.","","Electronic:978-1-5090-4152-7; POD:978-1-5090-4153-4","10.1109/ISPDC.2016.78","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904310","Machine Learning;Particle Swarm Optimization;Protein Function Module;Protein–Protein Interaction Networks","Algorithm design and analysis;Clustering algorithms;Databases;Machine learning algorithms;Particle swarm optimization;Partitioning algorithms;Proteins","","","","","","","","","8-10 July 2016","","IEEE","IEEE Conference Publications"
"Using machine learning to design a flexible LOC counter","M. Ochodek; M. Staron; D. Bargowski; W. Meding; R. Hebig","Poznan University of Technology, Poland","2017 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)","20170323","2017","","","14","20","The results of counting the size of programs in terms of Lines-of-Code (LOC) depends on the rules used for counting (i.e. definition of which lines should be counted). In the majority of the measurement tools, the rules are statically coded in the tool and the users of the measurement tools do not know which lines were counted and which were not. The goal of our research is to investigate how to use machine learning to teach a measurement tool which lines should be counted and which should not. Our interest is to identify which parameters of the learning algorithm can be used to classify lines to be counted. Our research is based on the design science research methodology where we construct a measurement tool based on machine learning and evaluate it based on open source programs. As a training set, we use industry professionals to classify which lines should be counted. The results show that classifying the lines as to be counted or not has an average accuracy varying between 0.90 and 0.99 measured as Matthew's Correlation Coefficient and between 95% and nearly 100% measured as the percentage of correctly classified lines. Based on the results we conclude that using machine learning algorithms as the core of modern measurement instruments has a large potential and should be explored further.","","Electronic:978-1-5090-6597-4; POD:978-1-5090-6598-1","10.1109/MALTESQUE.2017.7882011","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882011","","Computer languages;Design methodology;Instruments;Machine learning algorithms;Prediction algorithms;Radiation detectors;Training","learning (artificial intelligence);pattern classification;software metrics;source code (software)","design science research;flexible LOC counter design;line classification;lines-of-code;machine learning;measurement instruments;measurement tool;open source programs;programs size counting","","","","","","","21-21 Feb. 2017","","IEEE","IEEE Conference Publications"
"Tensor Decomposition for Signal Processing and Machine Learning","N. Sidiropoulos; L. De Lathauwer; X. Fu; K. Huang; E. Papalexakis; C. Faloutsos","ECE Department, University of Minnesota, Minneapolis, USA.(email:nikos@umn.edu)","IEEE Transactions on Signal Processing","","2017","PP","99","1","1","Tensors or multi-way arrays are functions of three or more indices (i; j; k; ) – similar to matrices (two-way arrays), which are functions of two indices (r; c) for (row,column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.","1053-587X;1053587X","","10.1109/TSP.2017.2690524","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891546","Cramer-Rao bound;Gauss-Newton;NP-hard problems;Tensor decomposition;Tucker model;alternating direction method of multipliers;alternating optimization;canonical polyadic decomposition (CPD);classification;collaborative filtering;communications;gradient descent;harmonic retrieval;higher-order singular value decomposition (HOSVD);mixture modeling;multilinear singular value decomposition (MLSVD);parallel factor analysis (PARAFAC);rank;source separation;speech separation;stochastic gradient;subspace learning;tensor factorization;topic modeling;uniqueness","Matrix decomposition;Optimization;Signal processing;Signal processing algorithms;Tensile stress;Tutorials","","","","","","","","20170403","","","IEEE","IEEE Early Access Articles"
"Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection","A. Demontis; M. Melis; B. Biggio; D. Maiorca; D. Arp; K. Rieck; I. Corona; G. Giacinto; F. Roli","","IEEE Transactions on Dependable and Secure Computing","","2017","PP","99","1","1","To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.","1545-5971;15455971","","10.1109/TDSC.2017.2700270","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917369","Android Malware Detection;Computer Security;Secure Machine Learning;Static Analysis","Algorithm design and analysis;Androids;Feature extraction;Humanoid robots;Malware;Security;Tools","","","","","","","","20170502","","","IEEE","IEEE Early Access Articles"
"Machine learning for finding bugs: An initial report","T. Chappelly; C. Cifuentes; P. Krishnan; S. Gevay","Queensland University of Technology, Brisbane, Australia","2017 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE)","20170323","2017","","","21","26","Static program analysis is a technique to analyse code without executing it, and can be used to find bugs in source code. Many open source and commercial tools have been developed in this space over the past 20 years. Scalability and precision are of importance for the deployment of static code analysis tools - numerous false positives and slow runtime both make the tool hard to be used by development, where integration into a nightly build is the standard goal. This requires one to identify a suitable abstraction for the static analysis which is typically a manual process and can be expensive. In this paper we report our findings on using machine learning techniques to detect defects in C programs. We use three offthe- shelf machine learning techniques and use a large corpus of programs available for use in both the training and evaluation of the results. We compare the results produced by the machine learning technique against the Parfait static program analysis tool used internally at Oracle by thousands of developers. While on the surface the initial results were encouraging, further investigation suggests that the machine learning techniques we used are not suitable replacements for static program analysis tools due to low precision of the results. This could be due to a variety of reasons including not using domain knowledge such as the semantics of the programming language and lack of suitable data used in the training process.","","Electronic:978-1-5090-6597-4; POD:978-1-5090-6598-1","10.1109/MALTESQUE.2017.7882012","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882012","C Programs;Machine Learning;Static Analysis","Benchmark testing;Complexity theory;Computer bugs;Data models;Feature extraction;Training","C language;learning (artificial intelligence);program debugging;program diagnostics;software tools;source code (software)","C program defect detection;Oracle;Parfait static program analysis tool;bug finding;commercial tools;machine learning;machine learning technique;open source tools;programming language semantics;source code bugs;static analysis;static code analysis tools;static program analysis;static program analysis tools","","","","","","","21-21 Feb. 2017","","IEEE","IEEE Conference Publications"
"On-Device Mobile Phone Security Exploits Machine Learning","N. Islam; S. Das; Y. Chen","Qualcomm","IEEE Pervasive Computing","20170331","2017","16","2","92","96","The authors present a novel approach to protecting mobile devices from malware that might leak private information or exploit vulnerabilities. The approach, which can also keep devices from connecting to malicious access points, uses learning techniques to statically analyze apps, analyze the behavior of apps at runtime, and monitor the way devices associate with Wi-Fi access points.","1536-1268;15361268","","10.1109/MPRV.2017.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891134","hackers;malware;mobile;networking;pervasive computing;security","Computer hacking;Computer security;Feature extraction;Malware;Mobile handsets;Monitoring;Runtime","learning (artificial intelligence);mobile computing;security of data;smart phones;wireless LAN","Wi-Fi access points;leak private information;learning techniques;machine learning;malicious access points;malware;mobile devices;on-device mobile phone security","","","","","","","April-June 2017","","IEEE","IEEE Journals & Magazines"
"Machine Learning with Big Data: Challenges and Approaches","A. L'Heureux; K. Grolinger; H. F. ElYamany; M. Capretz","","IEEE Access","","2017","PP","99","1","1","The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision-making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era and thus are based upon multiple assumptions, such as the dataset fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause-effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: Volume, Velocity, Variety or Veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this study provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.","2169-3536;21693536","","10.1109/ACCESS.2017.2696365","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906512","Big Data Vs;Big Data, , , , , , .;Data Analysis;Data Analytics;Deep Learning;Distributed Computing;Machine Learning;Neural Networks","Algorithm design and analysis;Big Data;Classification algorithms;Data analysis;Data mining;Machine learning algorithms;Support vector machines","","","","","","","","20170424","","","IEEE","IEEE Early Access Articles"
"Design and implementation of low-level machine learning API and API server","D. H. Kim; Kyoung Seok Na; Jae Min Lee; Jung Bin Park; Jun Young Lim; J. D. Kim","Department of Computer Science and Engineering, Pusan National University, South Korea","2017 International Conference on Information Networking (ICOIN)","20170417","2017","","","644","648","The purpose of this paper is to understand various problems related to Computer Science and find solutions from optimized machine learning algorithm by realizing machine learning API and API server. The representative machine-learning algorithm, TensorFlow, need to express algorithm from the stage of nodes and edges while IBM Watson only uses functions in completed form. Those are the problems to be solved in this paper; therefore, we suggest low-level API, which is in the middle stage between TensorFlow and IBM Watson and realize the low-level API. The machine learning API is realized in machine learning of artificial neural network and stochastic shunt, which will be applicable in other areas.","","Electronic:978-1-5090-5124-3; POD:978-1-5090-5125-0","10.1109/ICOIN.2017.7899577","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899577","API;Machine Learning;TensorFlow;Watson","Artificial neural networks;Clustering algorithms;Connectors;Genetic algorithms;Learning (artificial intelligence);Machine learning algorithms;Servers","","","","","","","","","11-13 Jan. 2017","","IEEE","IEEE Conference Publications"
"A Review on Sarcasm Detection from Machine-Learning Perspective","S. G. Wicana; T. Y. İbisoglu; U. Yavanoglu","Dept. of Comput. Eng., Gazi Univ., Ankara, Turkey","2017 IEEE 11th International Conference on Semantic Computing (ICSC)","20170330","2017","","","469","476","In this paper, we want to review one of the challenging problems for the opinion mining task, which is sarcasm detection. To be able to do that, many researchers tried to explore such properties in sarcasm like theories of sarcasm, syntactical properties, psycholinguistic of sarcasm, lexical feature, semantic properties, etc. Studies done in the last 15 years not only made progress in semantic features, but also show increasing amount of method of analysis using a machine-learning approach to process data. Because of this reason, this paper will try to explain current mostly used method to detect sarcasm. Lastly, we will present a result of our finding, which might help other researchers to gain a better result in the future.","","Electronic:978-1-5090-4284-5; POD:978-1-5090-4285-2","10.1109/ICSC.2017.74","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889581","machine learning;natural language processing;opinion mining;review;sarcasm;semantic analysis","Classification algorithms;Context;Logistics;Semantics;Supervised learning;Support vector machines;Twitter","data mining;learning (artificial intelligence);sentiment analysis","machine learning;natural language processing;opinion mining;sarcasm detection","","","","","","","Jan. 30 2017-Feb. 1 2017","","IEEE","IEEE Conference Publications"
