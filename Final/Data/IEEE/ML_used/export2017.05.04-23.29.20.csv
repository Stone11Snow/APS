"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7346645,7344638,7102732,7348282,7347128,7346934,7239545,7340664,7344002,7340027,7340725,7343981,7340607,7344435,7342672,7345436,7341896,7341753,7338643,7336882,7339243,7336197,7338456,7338115,7338513,7336192,7336422,7047917,7123640,7330558,7333450,7335264,7335095,6868988,7326109,7324057,7326122,7324345,7325843,7324087,7323306,7326186,7322761,7326065,7123635,7128727,6963442,7317390,7319948,7318335,7319945,7317397,7317946,7319932,7319088,7319351,7319381,7320432,7318626,7109855,6868201,7300421,7314218,7312059,7313309,7314299,7314188,7313204,7313415,7313563,7313192,7307663,7307821,7307576,7307574,7311387,7306745,7301027,7300948,7300871,7301416,7302476,7306985,7306762,7306935,7300302,7300286,7300319,7269275,7024132,7299098,7298020,7299106,7047737,7294923,7293887,7293488,7294264,7294164,7293972",2017/05/04 23:29:20
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A Machine Learning Approach for Self-Diagnosing Multiprocessors Systems under the Generalized Comparison Model","M. Elhadef","Coll. of Eng., Abu Dhabi Univ., Abu Dhabi, United Arab Emirates","2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops","20151026","2014","","","417","424","Support Vector Machines (SVMs) have been successfully applied to pattern recognition, regression, and classification. Because of their good performance and their mathematical foundations, SVMs are gaining popularity in solving various diagnosis problems. In this paper, we introduce a novel approach using a SVMs to solve the system-level fault diagnosis problem under the generalized comparison model (GCM). The GCM assumes that a set of tasks is assigned to pairs of nodes and their outcomes are compared by neighboring nodes. Given that comparisons are performed by the nodes themselves, faulty nodes can incorrectly claim that fault-free nodes are faulty or that faulty ones are fault-free. The collections of all matches and mismatches, i.e., The comparison outcomes, among the nodes are used to identify the set of permanently faulty nodes. First, we show how SVMs can be adapted to the GCM-based diagnosis problem. Then, from the results of an extensive simulation study we show that the new diagnosis approach succeeded in identifying all faulty nodes in the faults situations considered under t-diagnosable systems. The simulations demonstrate that the SVM-based diagnosis approach remarkably identified all faulty nodes, with a diagnosis correctness of 100% and with very low diagnosis latencies, providing hence an effective solution to the system-level self-diagnosis problem.","","CD-ROM:978-1-4799-7645-4; Electronic:978-1-4799-7646-1; POD:978-1-4799-7647-8","10.1109/UIC-ATC-ScalCom.2014.5","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306985","Fault tolerance;Generalized comparison model;Machine learning;Support vector machines;System-level self-diagnosis","Adaptation models;Conferences;Fault diagnosis;Kernel;Support vector machines;Testing;Training","fault diagnosis;learning (artificial intelligence);multiprocessing systems;support vector machines","GCM-based diagnosis problem;SVM;generalized comparison model;machine learning approach;self-diagnosing multiprocessors systems;support vector machines;system-level fault diagnosis problem","","","","17","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Machine learning based detection of compensatory balance responses to lateral perturbation using wearable sensors","M. Nouredanesh; J. Tung","Dept. of Mech. & Mechatron. Eng., Univ. of Waterloo, Waterloo, ON, Canada","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","20151207","2015","","","1","4","Loss of balance is prevalent in the older population and also in people who have mobility impairment. The primary aim of the present paper is to develop an efficient classifier to automatically distinguish compensatory balance responses (or near-falls) from regular stepping patterns. In this study, 5 young, healthy subjects were perturbed by lateral pushes while walking and the compensatory reactions were recorded by three wearable inertial measurement units (IMUs). Time domain features of these signals were extracted and reduced, using different dimension reduction methods, i.e., PCA, SPCA and KSPCA. The performance of k-nearest neighbor (k-NN) and support vector machines (SVMs) classification methods for detection of compensatory balance responses is investigated. The results of this study advances wearable measurement methods to accurately and reliably monitor gait balance control behavior in at-home settings (unsupervised conditions), over long periods of time (i.e., weeks, months). Building on the current study, subsequent research will examine ambulatory data to identify balance recovery processes for clinical assessment of fall risk.","","Electronic:978-1-4799-7234-0; POD:978-1-4799-7235-7; USB:978-1-4799-7233-3","10.1109/BioCAS.2015.7348282","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348282","Compensatory stepping;IMUs;machine learning techniqies;wearable sensors","Acceleration;Feature extraction;Legged locomotion;Principal component analysis;Sternum;Support vector machines;Wearable sensors","biomedical equipment;gait analysis;learning (artificial intelligence);mechanoception;medical signal detection;medical signal processing;signal classification;support vector machines","KSPCA;compensatory balance responses;dimension reduction methods;gait balance control behavior;k-nearest neighbor classification method;lateral perturbation;machine learning-based detection;mobility impairment;support vector machine classification method;unsupervised conditions;wearable inertial measurement units;wearable sensors","","1","","6","","","22-24 Oct. 2015","","IEEE","IEEE Conference Publications"
"Securing virtual execution environments through machine learning-based intrusion detection","F. Azmandian; D. R. Kaeli; J. G. Dy; J. A. Aslam","Northeastern University, ECE Department, Boston, MA, USA","2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)","20151112","2015","","","1","6","Virtualization has gained tremendous traction as the go-to computing technology due to many advantages it offers such as server consolidation, increased reliability and availability, and enhanced security through isolation of virtual machines. Within a virtual machine itself, securing workloads against cyber attacks becomes an increasingly critical task. In this paper, we present the application of machine learning and anomaly detection to automatically detect malicious attacks on typical server workloads running on virtual machines. An integral aspect of the work is finding the right set of features that can be used to distinguish normal from malicious activity.","1551-2541;15512541","Electronic:978-1-4673-7454-5; POD:978-1-4673-7455-2","10.1109/MLSP.2015.7324345","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324345","Anomaly Detection;Cyber-Security;Machine Learning;Virtualization","Feature extraction;Home appliances;Intrusion detection;Machine learning algorithms;Malware;Servers;Virtual machining","learning (artificial intelligence);security of data;virtual machines;virtualisation","anomaly detection;automatic malicious attack detection;cyber attacks;machine learning-based intrusion detection;server availability;server consolidation;server reliability;virtual execution environment security;virtual machine isolation;virtualization","","","","7","","","17-20 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine learning for the activation of contraflows during hurricane evacuation","J. W. Burris; R. Shrestha; B. Gautam; B. Bista","Department of Computer Science and Industrial Technology, Southeastern Louisiana University Hammond, LA, USA","2015 IEEE Global Humanitarian Technology Conference (GHTC)","20151203","2015","","","254","258","Contraflows are a critical part of an emergency evacuation plan. In most cases, a contraflow lane reversal will double the capacity of key evacuation routes. The Contraflow plan for the evacuation of southeast Louisiana during a hurricane threat uses a typical schedule for the activation of contraflows based on the predicted time of landfall. This work will apply machine learning techniques using real-time traffic data to schedule the activation of contraflows. Optimizing the Contraflow plan should increase the effectiveness of the evacuation plan by increasing the flow of evacuation traffic based on demand and retaining the availability of incoming traffic until contraflow lanes are needed. These techniques could be applied to other locations, including those without an existing evacuation plan.","","Electronic:978-1-4673-6561-1; POD:978-1-4673-6562-8; USB:978-1-4673-6560-4","10.1109/GHTC.2015.7343981","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343981","disaster management;machine learning;traffic control","Algorithm design and analysis;Classification algorithms;Delays;Hurricanes;Machine learning algorithms;Prediction algorithms;Supervised learning","emergency management;learning (artificial intelligence);road traffic;storms","contraflow activation scheduling;contraflow lane reversal;contraflow plan;emergency evacuation plan;evacuation routes;hurricane evacuation;hurricane threat;landfall;machine learning techniques;real-time traffic data;southeast Louisiana","","","","9","","","8-11 Oct. 2015","","IEEE","IEEE Conference Publications"
"Implementation of machine learning for classifying prosthesis type through conventional gait analysis","R. LeMoyne; T. Mastroianni; A. Hessel; K. Nishikawa","Department of Biological Sciences, Northern Arizona University, Flagstaff, 86011-5640 USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","202","205","Current forecasts imply a significant increase in the quantity of lower limb amputations. Synergizing the capabilities of a conventional gait analysis system and machine learning facilitates the capacity to classify disparate types of transtibial prostheses. Automated classification of prosthesis type may eventually advance rehabilitative acuity for selecting an appropriate prosthesis for a given aspect of the rehabilitation process. The presented research utilized a force plate as a conventional gait analysis device to acquire a feature set for two types of prosthesis: passive Solid Ankle Cushioned Heel (SACH) and the iWalk BiOM powered prosthesis. The feature set consists of both temporal and kinetic data with respect to the force plate signal during stance. Intuitively a passive prosthesis and powered prosthesis generate distinctively different force plate recordings. A support vector machine, which is type of machine learning application, achieves 100% classification between a passive prosthesis and powered prosthesis regarding the feature set derived from force plate recordings.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318335","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318335","Force Plate;Gait Analysis;Machine Learning;Powered Prosthesis;Support Vector Machine","Brakes;Context;Force;Kinetic theory;Legged locomotion;Prosthetics;Support vector machines","gait analysis;learning (artificial intelligence);prosthetics","Solid Ankle Cushioned Heel;conventional gait analysis;iWalk BiOM powered prosthesis;lower limb amputations;machine learning;prosthesis type classification;rehabilitative acuity;transtibial prostheses","","1","","19","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning","C. L. P. Chen; C. Y. Zhang; L. Chen; M. Gan","Department of Computer and Infor-mation Science, Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Fuzzy Systems","20151125","2015","23","6","2163","2173","In recent years, deep learning caves out a research wave in machine learning. With outstanding performance, more and more applications of deep learning in pattern recognition, image recognition, speech recognition, and video processing have been developed. Restricted Boltzmann machine (RBM) plays an important role in current deep learning techniques, as most of existing deep networks are based on or related to it. For regular RBM, the relationships between visible units and hidden units are restricted to be constants. This restriction will certainly downgrade the representation capability of the RBM. To avoid this flaw and enhance deep learning capability, the fuzzy restricted Boltzmann machine (FRBM) and its learning algorithm are proposed in this paper, in which the parameters governing the model are replaced by fuzzy numbers. This way, the original RBM becomes a special case in the FRBM, when there is no fuzziness in the FRBM model. In the process of learning FRBM, the fuzzy free energy function is defuzzified before the probability is defined. The experimental results based on bar-and-stripe benchmark inpainting and MNIST handwritten digits classification problems show that the representation capability of FRBM model is significantly better than the traditional RBM. Additionally, the FRBM also reveals better robustness property compared with RBM when the training data are contaminated by noises.","1063-6706;10636706","","10.1109/TFUZZ.2015.2406889","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047917","Deep learning;Fuzzy deep networks;Fuzzy restricted Boltzmann machine;Image classification;Image inpainting;RBM;fuzzy deep networks;fuzzy restricted Boltzmann machine;image classification;image inpainting;restricted Boltzmann machine (RBM)","Approximation methods;Linear programming;Markov processes;Optimization;Probability distribution;Robustness;Training","Boltzmann machines;learning (artificial intelligence);pattern classification;probability","FRBM model;MNIST handwritten digits classification problem;bar-and-stripe benchmark inpainting;deep learning capability;deep learning enhancement;deep learning technique;deep network;fuzziness;fuzzy free energy function;fuzzy number;fuzzy restricted Boltzmann machine;image recognition;learning algorithm;machine learning;pattern recognition;probability;representation capability;research wave;robustness property;speech recognition;training data;video processing","","8","","37","","20150224","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"Slow release drug dissolution profile prediction in pharmaceutical manufacturing: A multivariate and machine learning approach","G. A. Susto; S. McLoone","University of Padova, Italy","2015 IEEE International Conference on Automation Science and Engineering (CASE)","20151008","2015","","","1218","1223","Slow release drugs must be manufactured to meet target specifications with respect to dissolution curve profiles. In this paper we consider the problem of identifying the drivers of dissolution curve variability of a drug from historical manufacturing data. Several data sources are considered: raw material parameters, coating data, loss on drying and pellet size statistics. The methodology employed is to develop predictive models using LASSO, a powerful machine learning algorithm for regression with high-dimensional datasets. LASSO provides sparse solutions facilitating the identification of the most important causes of variability in the drug fabrication process. The proposed methodology is illustrated using manufacturing data for a slow release drug.","2161-8070;21618070","Electronic:978-1-4673-8183-3; POD:978-1-4673-8184-0","10.1109/CoASE.2015.7294264","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294264","Dissolution profile;LASSO;Machine Learning;Manufacturing;Monte Carlo Cross-Validation;Root Cause Analysis;System Identification;Variable Selection","Analytical models;Coatings;Computational modeling;Data models;Drugs;Predictive models;Training","coatings;dissolving;drugs;drying;identification;learning (artificial intelligence);manufacturing data processing;production engineering computing;raw materials;regression analysis","LASSO;coating data;data sources;dissolution curve variability;drug fabrication process;drying statistics;high-dimensional datasets;historical manufacturing data;machine learning algorithm;machine learning approach;pellet size statistics;pharmaceutical manufacturing;predictive models;raw material parameters;slow release drug dissolution profile prediction","","","","18","","","24-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Petuum: A New Platform for Distributed Machine Learning on Big Data","E. P. Xing; Q. Ho; W. Dai; J. K. Kim; J. Wei; S. Lee; X. Zheng; P. Xie; A. Kumar; Y. Yu","School of Computer Science, Carnegie Mellon University","IEEE Transactions on Big Data","20151204","2015","1","2","49","67","What is a systematic way to efficiently apply a wide spectrum of advanced ML programs to industrial scale problems, using Big Models (up to 100 s of billions of parameters) on Big Data (up to terabytes or petabytes)? Modern parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized graph-based execution that relies on graph representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of ML programs at scale. We propose a general-purpose framework, Petuum, that systematically addresses data- and model-parallel challenges in large-scale ML, by observing that many ML programs are fundamentally optimization-centric and admit error-tolerant, iterative-convergent algorithmic solutions. This presents unique opportunities for an integrative system design, such as bounded-error network synchronization and dynamic scheduling based on ML program structure. We demonstrate the efficacy of these system designs versus well-known implementations of modern ML algorithms, showing that Petuum allows ML programs to run in much less time and at considerably larger model sizes, even on modestly-sized compute clusters.","","","10.1109/TBDATA.2015.2472014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7239545","Big Data;Big Model;Data-Parallelism;Distributed Systems;Machine Learning;Machine learning;Model-Parallelism;Theory;big data;big model;data-parallelism;distributed systems;model-parallelism;theory","Big data;Computational modeling;Convergence;Data models;Mathematical model;Servers;Synchronization","Big Data;learning (artificial intelligence);parallel programming;scheduling","Big Data;Big Models;MapReduce;Petuum platform;bounded-error network synchronization;bulk-synchronous processing paradigm;data-parallel method;distributed machine learning;dynamic scheduling;error-tolerant-iterative-convergent algorithmic solutions;fine-grained operations;general-purpose framework;graph representations;graph-based execution;integrative system design;large-scale ML;model-parallel method;optimization-centric programs;parallelization strategies;pull systems;universal platform","","4","","38","","20150903","June 1 2015","","IEEE","IEEE Journals & Magazines"
"An automated segmentation of brain MRI for detection of normal tissues using improved machine learning approach","M. Y. Bhanumurthy; K. Anne","Dept. of ECE, Vasireddy Venkatadri Institute of Technology, Guntur - 522508. A.P, India","2015 International Conference on Advanced Computing and Communication Systems","20151112","2015","","","1","6","Due to an increased need for efficient and objective evaluation of large amounts of data, MRI-based medical image analysis is gaining attention in recent times. The goal is to simplify an image into something that is more meaningful and making it easier to analyze. The aim of medical image segmentation in brain MRI is to separate the region of interest from the background after denoising and skull removal. Accurate segmentation of normal and abnormal tissues is still a challenge for researchers. In this paper, we propose a fully automated segmentation of normal tissues viz., white matter (WM), gray matter (GM) and cerebro spinal fluid (CSF) from brain MRI using an improved machine learning approach that uses Neuro-fuzzy as classifier. The segmentation is carried out using gradient method and orthogonal polynomial transform. The performance of our method is assessed with metrics such as false positive rate (FPR), false negative rate (FNR), specificity, sensitivity and accuracy. Also, the entire procedure is developed as a graphical user interface (GUI) which results in automated classification and segmentation.","","Electronic:978-1-4799-6438-3; POD:978-1-4799-6439-0","10.1109/ICACCS.2015.7324087","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324087","GUI;Gradient Method;Machine Learning;Neuro-Fuzzy classifier;Normal Tissues;Orthogonal Polynomial Transform;Segmentation","Accuracy;Biomedical imaging;Communication systems;Feature extraction;Graphical user interfaces;Image segmentation;Magnetic resonance imaging","biological tissues;biomedical MRI;brain;fuzzy neural nets;image classification;image denoising;image segmentation;learning (artificial intelligence);medical image processing;object detection;transforms","FNR;FPR;GUI;MRI-based medical image analysis;abnormal tissues;automated classification;brain MRI;cerebro spinal fluid;classifier;denoising;false negative rate;false positive rate;fully automated segmentation;gradient method;graphical user interface;gray matter;machine learning approach;medical image segmentation;neuro-fuzzy;normal tissues detection;orthogonal polynomial transform;skull removal;white matter","","","","16","","","5-7 Jan. 2015","","IEEE","IEEE Conference Publications"
"Automated microscopy and machine learning for expert-level malaria field diagnosis","C. B. Delahunt; C. Mehanian; L. Hu; S. K. McGuire; C. R. Champlin; M. P. Horning; B. K. Wilson; C. M. Thompon","Global Good/Intellectual Ventures Laboratory, Bellevue, Washington, USA, University of Washington, Seattle, Washington, USA","2015 IEEE Global Humanitarian Technology Conference (GHTC)","20151203","2015","","","393","399","The optical microscope is one of the most widely used tools for diagnosing infectious diseases in the developing world. Due to its reliance on trained microscopists, field microscopy often suffers from poor sensitivity, specificity, and reproducibility. The goal of this work, called the Autoscope, is a low-cost automated digital microscope coupled with a set of computer vision and classification algorithms, which can accurately diagnose of a variety of infectious diseases, targeting use-cases in the developing world. Our initial target is malaria, because of the high difficulty of the task and because manual microscopy is currently a central but highly imperfect tool for malaria work in the field. In addition to diagnosis, the algorithm performs species identification and quantitation of parasite load, parameters which are critical in many field applications but which are not effectively determined by rapid diagnostic tests (RDTs). We have built a hardware prototype which can scan approximately 0.1 μL of blood volume in a standard Giemsa-stained thick smear blood slide in approximately 20 minutes. We have also developed a comprehensive machine learning framework, leveraging computer vision and machine learning techniques including support vector machines (SVMs) and convolutional neural networks (CNNs). The Autoscope has undergone successful initial field testing for malaria diagnosis in Thailand.","","Electronic:978-1-4673-6561-1; POD:978-1-4673-6562-8; USB:978-1-4673-6560-4","10.1109/GHTC.2015.7344002","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344002","computer vision;computer-aided diagnosis;machine learning;malaria;microscopy","Blood;Computer vision;Diseases;Image color analysis;Microscopy;Optical microscopy;Support vector machines","biomedical optical imaging;blood;diseases;image classification;learning (artificial intelligence);medical expert systems;medical image processing;neural nets;optical microscopy;patient diagnosis;support vector machines","Autoscope;SVM;blood volume;classification algorithms;comprehensive machine learning framework;computer vision;convolutional neural networks;expert-level malaria field diagnosis;hardware prototype;infectious disease diagnosis;leveraging computer vision;low-cost automated digital microscope;machine learning techniques;optical microscope;parasite load;rapid diagnostic tests;species identification;species quantitation;standard Giemsastained thick smear blood slide;support vector machines","","","","17","","","8-11 Oct. 2015","","IEEE","IEEE Conference Publications"
"Online prediction of glucose concentration in type 1 diabetes using extreme learning machines","E. I. Georga; V. C. Protopappas; D. Polyzos; D. I. Fotiadis","Unit of Medical Technology and Intelligent Information Systems, Materials Science and Engineering Department, University of Ioannina, GR 45110 Greece","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","3262","3265","We propose an online machine-learning solution to the problem of nonlinear glucose time series prediction in type 1 diabetes. Recently, extreme learning machine (ELM) has been proposed for training single hidden layer feed-forward neural networks. The high accuracy and fast learning speed of ELM drive us to investigate its applicability to the glucose prediction problem. Given that diabetes self-monitoring data are received sequentially, we focus on online sequential ELM (OS-ELM) and online sequential ELM kernels (KOS-ELM). A multivariate feature set is utilized concerning subcutaneous glucose, insulin therapy, carbohydrates intake and physical activity. The dataset comes from the continuous multi-day recordings of 15 type 1 patients in free-living conditions. Assuming stationarity and evaluating the performance of the proposed method by 10-fold cross- validation, KOS-ELM were found to perform better than OS-ELM in terms of prediction error, temporal gain and regularity of predictions for a 30-min prediction horizon.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319088","","Diabetes;Insulin;Kernel;Predictive models;Sugar;Time series analysis;Yttrium","diseases;feedforward neural nets;learning (artificial intelligence);sugar;time series","carbohydrates intake;extreme learning machines;glucose concentration;hidden layer feed forward neural networks;insulin therapy;online prediction;online sequential ELM kernels;physical activity;self monitoring data;time series;type 1 diabetes","","","","21","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Machine learning techniques in optical communication","D. Zibar; M. Piels; R. Jones; C. G. Schaeffer","DTU Fotonik, Technical University of Denmark, Build. 343, DK-2800, Denmark","2015 European Conference on Optical Communication (ECOC)","20151203","2015","","","1","3","Techniques from the machine learning community are reviewed and employed for laser characterization, signal detection in the presence of nonlinear phase noise, and nonlinearity mitigation. Bayesian filtering and expectation maximization are employed within nonlinear state-space framework for parameter tracking.","","Electronic:978-8-4608-1741-3; POD:978-1-4673-7950-2","10.1109/ECOC.2015.7341896","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341896","","Bayes methods;Nonlinear optics;Optical noise;Optical polarization;Optical signal processing;Optical variables measurement;Phase noise","expectation-maximisation algorithm;lasers;learning (artificial intelligence);nonlinear optics;optical communication;optical filters;optical noise;optical signal detection;phase noise","Bayesian filtering;expectation maximization;laser characterization;machine learning techniques;nonlinear phase noise;nonlinear state-space framework;optical communication;parameter tracking;signal detection","","2","","7","","","Sept. 27 2015-Oct. 1 2015","","IEEE","IEEE Conference Publications"
"A seizure-detection IC employing machine learning to overcome data-conversion and analog-processing non-idealities","J. Zhang; L. Huang; Z. Wang; N. Verma","Princeton University, Princeton, NJ 08544, USA","2015 IEEE Custom Integrated Circuits Conference (CICC)","20151130","2015","","","1","4","This paper presents a seizure-detection system wherein the accuracy required of the analog frontend is substantially relaxed. Typically, readout of electroencephalogram (EEG) signals would dominate the energy of such a system, due to the precision (noise, linearity) requirements. The presented system performs data conversion and analog multiplication for EEG feature extraction via simple circuits to demonstrate that feature errors can be overcome by appropriate retraining of a classification model, using a machine-learning algorithm. This precludes the need to design a high-precision frontend. The prototype, in 32nm CMOS, results in features whose RMS error normalized to their ideal values is 1.16 (i.e. errors are larger than ideal values). An ideal implementation of the seizure detector exhibits sensitivity, latency, false alarms of 5/5, 2.0 sec., 8, respectively. The feature errors degrade this to 5/5, 3.6 sec., 443, causing high false alarms; but retraining of the classification model restores this to 5/5, 3.4 sec., 4.","","Electronic:978-1-4799-8682-8; POD:978-1-4799-8683-5; USB:978-1-4799-8681-1","10.1109/CICC.2015.7338456","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338456","Machine learning;analog processing circuits;electroencephalography;epilepsy;error compensation;system-on-chip","Decision support systems","CMOS integrated circuits;analogue-digital conversion;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing;readout electronics","CMOS integrated circuit;EEG feature extraction;analog multiplication;analog processing nonidealities;data conversion nonidealities;electroencephalogram signal readout;machine learning algorithm;seizure detection integrated circuit;size 32 nm","","2","","6","","","28-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"A Multi-dimensional Comparison of Toolkits for Machine Learning with Big Data","A. N. Richter; T. M. Khoshgoftaar; S. Landset; T. Hasanin","Florida Atlantic Univ., Boca Raton, FL, USA","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","1","8","Big data is a big business, and effective modeling of this data is key. This paper provides a comprehensive multidimensional analysis of various open source tools for machine learning with big data. An evaluation standard is proposed along with detailed comparisons of the frameworks discussed, with regard to algorithm availability, scalability, speed, and more. The major tools profiled are Mahout, MLlib, H2O, and SAMOA, along with the big data processing engines they utilize, including Hadoop MapReduce, Apache Spark, and Apache Storm. There is not yet one framework that ""does it all"", but this paper provides insight into each tool's strengths and weaknesses along with guidance on tool choice for specific needs.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300948","Hadoop;Mahout;Spark;big data;data mining;machine learning","Big data;Clustering algorithms;Data models;Engines;Machine learning algorithms;Sparks;Water","Big Data;data analysis;learning (artificial intelligence)","Apache Spark;Apache Storm;Big Data;H2O tool;Hadoop MapReduce;MLlib tool;Mahout tool;SAMOA tool;evaluation standard;machine learning;multidimensional analysis;toolkit multidimensional comparison","","2","","36","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Exploiting Intrinsic Variability of Filamentary Resistive Memory for Extreme Learning Machine Architectures","M. Suri; V. Parmar","Department of Electrical Engineering, Indian Institute of Technology, New Delhi, India","IEEE Transactions on Nanotechnology","20151109","2015","14","6","963","968","In this paper, we show for the first time how unavoidable device variability of emerging nonvolatile resistive memory devices can be exploited to design efficient low-power, low-footprint extreme learning machine (ELM) architectures. In particular, we utilize the uncontrollable off-state resistance (Roff/HRS) spreads, of nanoscale filamentary-resistive memory devices, to realize random input weights and random hidden neuron biases; a characteristic requirement of ELM. We propose a novel RRAM-ELM architecture. To validate our approach, experimental data from different filamentary-resistive switching devices (CBRAM, OXRAM) are used for full-network simulations. Learning capability of our RRAM-ELM architecture is illustrated with the help of two real-world applications: 1) diabetes diagnosis test (classification) and 2) SinC curve fitting (regression).","1536-125X;1536125X","","10.1109/TNANO.2015.2441112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123635","Brain-Inspired;CBRAM;Cognitive Computing;Extreme Learning;Machine;Machine Learning;Neuromorphic;OXRAM;RRAM;Resistive Memory;brain-inspired;extreme learning machine;machine learning;neuromorphic;resistive memory;stochastic computing","Machine learning;Memory architecture;Nanoscale devices;Neuromorphic engineering;Random access memory;Stochastic systems","learning (artificial intelligence);memory architecture;random-access storage","CBRAM;OXRAM;RRAM-ELM architecture;SinC curve fitting;diabetes diagnosis test;extreme learning machine architectures;filamentary-resistive switching devices;full-network simulations;intrinsic variability;nanoscale filamentary-resistive memory devices;nonvolatile resistive memory devices;random hidden neuron biases;random input weights;unavoidable device variability;uncontrollable off-state resistance spreads","","8","","25","","20150615","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"Machine learning approach for shaft crack detection through acoustical emission signals","J. Wu; X. Li; S. Xu; M. J. Er; L. Wei; W. F. Lu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, 639798","2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)","20151026","2015","","","1","7","A research approach of crack detection of rotating shafts based on acoustic emission (AE) signals and machine learning is proposed in this paper. The relationship between crack intensity and domain features are investigated, and the features which could well indicate the crack condition are selected for modelling and crack prediction. Multiple Linear Regression (MLR), Artificial Neural Networks (ANN) and Adaptive Neural-Fuzzy Inference System (ANFIS) methods are used to establish the predictive correlation models by using selected features. A case study is carried out to emulate online working conditions of rotating shafts by using 10 normal shafts with 0.8mm - 8mm crack intensities. It is proved that AE signals can be used for earlier crack intensity detection, for example 0.8mm - 2.4 mm cracks can be fully detected according to experimental results in this study. Different modelling methods are also compared and discussed. Results show that ANFIS is a good choice in terms of overall predictive accuracy for earlier crack detection and prediction.","1946-0740;19460740","Electronic:978-1-4673-7929-8; POD:978-1-4673-7930-4; USB:978-1-4673-7928-1","10.1109/ETFA.2015.7301416","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301416","Acoustic Emission Techniques;Machine Learning;Shaft Crack Detection","Accuracy;Artificial neural networks;Mathematical model;Noise;Predictive models;Sensors;Shafts","acoustic emission testing;crack detection;learning (artificial intelligence);neural nets;regression analysis;shafts","ANFIS;acoustical emission signal;adaptive neural-fuzzy inference system method;artificial neural networks;machine learning approach;multiple linear regression;rotating shafts;shaft crack detection","","","","15","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Classification of older adults with/without a fall history using machine learning methods","L. Zhang; O. Ma; J. M. Fabre; R. H. Wood; S. U. Garcia; K. M. Ivey; E. D. McCann","New Mexico State University, Las Cruces, 88003 USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","6760","6763","Falling is a serious problem in an aged society such that assessment of the risk of falls for individuals is imperative for the research and practice of falls prevention. This paper introduces an application of several machine learning methods for training a classifier which is capable of classifying individual older adults into a high risk group and a low risk group (distinguished by whether or not the members of the group have a recent history of falls). Using a 3D motion capture system, significant gait features related to falls risk are extracted. By training these features, classification hypotheses are obtained based on machine learning techniques (K Nearest-neighbour, Naive Bayes, Logistic Regression, Neural Network, and Support Vector Machine). Training and test accuracies with sensitivity and specificity of each of these techniques are assessed. The feature adjustment and tuning of the machine learning algorithms are discussed. The outcome of the study will benefit the prediction and prevention of falls.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319945","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319945","","Accuracy;History;Joints;Kinematics;Logistics;Support vector machines;Training","Bayes methods;feature extraction;gait analysis;geriatrics;learning (artificial intelligence);medical computing;neural nets;pattern classification;regression analysis;support vector machines","3D motion capture system;K nearest-neighbour classification;Naive Bayes classification;aged society;fall history;fall prevention;fall risk assessment;gait feature extraction;logistic regression;machine learning method;neural network;older adult classification;support vector machine","","","","14","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Closed-Loop Restoration Approach to Blurry Images Based on Machine Learning and Feedback Optimization","S. Yousaf; S. Qin","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Image Processing","20151103","2015","24","12","5928","5941","Blind image deconvolution (BID) aims to remove or reduce the degradations that have occurred during the acquisition or processing. It is a challenging ill-posed problem due to a lack of enough information in degraded image for unambiguous recovery of both point spread function (PSF) and clear image. Although recently many powerful algorithms appeared; however, it is still an active research area due to the diversity of degraded images as well as degradations. Closed-loop control systems are characterized with their powerful ability to stabilize the behavior response and overcome external disturbances by designing an effective feedback optimization. In this paper, we employed feedback control to enhance the stability of BID by driving the current estimation quality of PSF to the desired level without manually selecting restoration parameters and using an effective combination of machine learning with feedback optimization. The foremost challenge when designing a feedback structure is to construct or choose a suitable performance metric as a controlled index and a feedback information. Our proposed quality metric is based on the blur assessment of deconvolved patches to identify the best PSF and computing its relative quality. The Kalman filter-based extremum seeking approach is employed to find the optimum value of controlled variable. To find better restoration parameters, learning algorithms, such as multilayer perceptron and bagged decision trees, are used to estimate the generic PSF support size instead of trial and error methods. The problem is modeled as a combination of pattern classification and regression using multiple training features, including noise metrics, blur metrics, and low-level statistics. Multi-objective genetic algorithm is used to find key patches from multiple saliency maps which enhance performance and save extra computation by avoiding ineffectual regions of the image. The proposed scheme is shown to outperform corresponding o- en-loop schemes, which often fails or needs many assumptions regarding images and thus resulting in sub-optimal results.","1057-7149;10577149","","10.1109/TIP.2015.2492825","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300421","Feedback optimization;bagged decision trees;blind deconvolution;blue metric;blur metric;closed-loop;feedback optimization;image quality;learning","Deconvolution;Estimation;Image edge detection;Image restoration;Kernel;Measurement;Optimization","Kalman filters;deconvolution;image restoration;learning (artificial intelligence)","BID;Kalman filter-based extremum seeking approach;PSF;blind image deconvolution;blurry images;closed-loop control systems;closed-loop restoration approach;feedback optimization;machine learning;pattern classification;point spread function","","1","","72","","20151019","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"Comparative study of metroplex airspace and procedures using machine learning to discover flight track anomalies","B. Matthews; D. Nielsen; J. Schade; K. Chan; M. Kiniry","SGT Inc. NASA Ames Research Center, Moffett Field, CA, USA","2015 IEEE/AIAA 34th Digital Avionics Systems Conference (DASC)","20151029","2015","","","2G4-1","2G4-15","The National Airspace (NAS) is constantly changing and adapting to new and complex challenges, and as a result Next Generation Air Transportation System (NextGen) will need to address these important aspects. These challenges range from increased traffic flow, to reducing environmental impact, to routing efficiency, all while maintaining high safety. The FAA has recently been involved in a number of large scale Metroplex redesigns across the country to enable controllers and pilots to implement more efficient Performance-based Navigation (PBN) procedures on a regional basis. The Houston Metroplex project is one of the first to implement such a large scale change in the NAS, and it significantly changed the traffic flows into the Houston Terminal Radar Approach Control Facility (TRACON) airspace for the two major airports: Houston's George Bush Intercontinental Airport (IAH) and Houston's William P. Hobby International Airport (HOU). This paper addresses an anomaly detection approach that has previously been used to detect operationally significant anomalous flights on approach to Denver International, Newark International, LaGuardia International, and John F. Kennedy International airports. The same method is applied to radar track surveillance data to identify anomalies in the airspace before and after the Metroplex procedure change at Houston. The study covers flights traversing through Houston TRACON (I90) and landing at IAH and HOU over a period of 2 years before and after the significant procedure change. Anomalies identified before and after the procedure change were characterized by their safety risk and operational efficiency to determine whether the types of anomalies that were discovered from before continued to exist or if they were eliminated after the procedure change or if new types of anomalies began to appear.","2155-7195;21557195","Electronic:978-1-4799-8940-9; POD:978-1-4799-8941-6","10.1109/DASC.2015.7311387","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311387","","Airports;FAA;Kernel;Monitoring;NASA;Safety;Trajectory","aerospace computing;air safety;air traffic control;airborne radar;aircraft navigation;airports;learning (artificial intelligence);radar tracking;search radar","Denver International airports;FAA;HOU;Houston George Bush Intercontinental Airport;Houston Metroplex project;Houston William P. Hobby International Airport;Houston terminal radar approach control facility airspace;IAH;John F. Kennedy International airports;LaGuardia International airports;Metroplex airspace;NAS;Newark International airports;NextGen;PBN;TRACON;anomaly detection approach;environmental impact reduction;flight track anomaly discovery;large scale Metroplex redesigns;machine learning;national airspace;next generation air transportation system;performance-based navigation procedures;radar track surveillance data;routing efficiency;traffic flow","","","","12","","","13-17 Sept. 2015","","IEEE","IEEE Conference Publications"
"A Digital Liquid State Machine With Biologically Inspired Learning and Its Application to Speech Recognition","Y. Zhang; P. Li; Y. Jin; Y. Choe","Department of Electrical and Computer Engineering, Texas A&M University, College Station, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","20151016","2015","26","11","2635","2649","This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks.","2162-237X;2162237X","","10.1109/TNNLS.2015.2388544","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024132","Hardware implementation;liquid-state machine (LSM);speech recognition;spike-based learning;spike-based learning.","Biological neural networks;Hidden Markov models;Neurons;Reservoirs;Speech;Speech recognition","VLSI;backpropagation;finite state machines;hidden Markov models;neural nets;speech recognition","Markov-model-based recognizer;Sphinx-4;TI46 speech corpus;VLSI-based machine learning;backpropagation-based learning;bioinspired digital LSM;bioinspired spike-based learning algorithm;data storage;digital liquid state machine;isolated word recognition;parallel VLSI implementation;postsynaptic neuron;presynaptic neuron;speech recognition;spiking neural network model;synaptic weight resolution;very-large-scale- integration-based machine learning","1","7","","54","","20150127","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"Predicting Purchase Decisions Based on Spatio-Temporal Functional MRI Features Using Machine Learning","Y. Wang; V. Chattaraman; H. Kim; G. Deshpande","AU MRI Research Center, Department of Electrical and Computer Engineering, Auburn University, Auburn, AL, USA","IEEE Transactions on Autonomous Mental Development","20151104","2015","7","3","248","255","Machine learning algorithms allow us to directly predict brain states based on functional magnetic resonance imaging (fMRI) data. In this study, we demonstrate the application of this framework to neuromarketing by predicting purchase decisions from spatio-temporal fMRI data. A sample of 24 subjects were shown product images and asked to make decisions of whether to buy them or not while undergoing fMRI scanning. Eight brain regions which were significantly activated during decision-making were identified using a general linear model. Time series were extracted from these regions and input into a recursive cluster elimination based support vector machine (RCE-SVM) for predicting purchase decisions. This method iteratively eliminates features which are unimportant until only the most discriminative features giving maximum accuracy are obtained. We were able to predict purchase decisions with 71% accuracy, which is higher than previously reported. In addition, we found that the most discriminative features were in signals from medial and superior frontal cortices. Therefore, this approach provides a reliable framework for using fMRI data to predict purchase-related decision-making as well as infer its neural correlates.","1943-0604;19430604","","10.1109/TAMD.2015.2434733","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109855","Decision-making;functional magnetic resonance imaging (fMRI);machine learning;neuromarketing;pattern classification;support vector machine","Accuracy;Feature extraction;Magnetic resonance imaging;Prediction algorithms;Support vector machines;Testing;Time series analysis","biomedical MRI;consumer behaviour;decision making;feature selection;iterative methods;learning (artificial intelligence);medical image processing;purchasing;recursive estimation;support vector machines;time series","RCE-SVM;brain region;brain state prediction;decision making;discriminative features;fMRI scanning;functional magnetic resonance imaging;general linear model;iterative feature elimination;machine learning algorithm;medial frontal cortex;neural correlation;neuromarketing;product image;purchase decision prediction;recursive cluster elimination based support vector machine;spatio-temporal fMRI data;spatio-temporal functional MRI features;superior frontal cortex;time series extraction","","1","","41","","20150518","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Machine learning methods for credibility assessment of interviewees based on posturographic data","S. K. Saripalle; S. Vemulapalli; G. W. King; J. K. Burgoon; R. Derakhshani","Department of Electrical and Computer Engineering at University of Missouri - Kansas City, 64110, USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","6708","6711","This paper discusses the advantages of using posturographic signals from force plates for non-invasive credibility assessment. The contributions of our work are two fold: first, the proposed method is highly efficient and non invasive. Second, feasibility for creating an autonomous credibility assessment system using machine-learning algorithms is studied. This study employs an interview paradigm that includes subjects responding with truthful and deceptive intent while their center of pressure (COP) signal is being recorded. Classification models utilizing sets of COP features for deceptive responses are derived and best accuracy of 93.5% for test interval is reported.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319932","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319932","","Accuracy;Feature extraction;Force;Kernel;Polynomials;Sensitivity;Support vector machines","learning (artificial intelligence);pose estimation;signal classification","COP feature sets;COP signal;autonomous credibility assessment system;center of pressure signal;classification models;interview paradigm;machine learning methods;machine-learning algorithms;noninvasive credibility assessment;posturographic data;posturographic signals","","","","27","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"On the usefulness of machine learning techniques in collaborative anomaly detection","S. Senel-Kleine; J. Bouché; M. Kappes","Faculty of Computer Science and Engineering, Frankfurt University of Applied Sciences, Frankfurt am Main, Germany","2015 Internet Technologies and Applications (ITA)","20151105","2015","","","213","218","Due to the increase in network attacks, anomaly detection has gained importance. In this paper, we present and investigate the idea of institutions cooperating for performing anomaly detection, i.e. institutions jointly analyzing their network traffic, in order to identify malicious attacks, using classification-based machine learning techniques. We compare the results of such a collaborative analysis with a single analysis. Moreover, as institutions might not be willing to share confidential data, we analyze the benefits of a collaborative approach if some parts of the traffic are being anonymized. While, intuitively, having more data at hand should lead to improved detection rates, our results indicate that a federated analysis using standard classification-based methods improves detection rates only slightly. Moreover, when using anonymized data, the obtained detection rates of a joint data analysis further deteriorate such that the analysis of individual traffic is more useful. Thus, our research indicates that the classical classification based machine learning approaches for anomaly detection must be adapted and improved in order to leverage the advantage of having data from various sources.","","Electronic:978-1-4673-9557-1; POD:978-1-4799-8037-6","10.1109/ITechA.2015.7317397","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317397","classification-based;collaborative anomaly detection;machine-learning","Accuracy;Hardware;Random access memory;Software;Virtualization","computer network security;learning (artificial intelligence);pattern classification;telecommunication traffic","anonymized data;anonymized traffic;classification-based machine learning techniques;collaborative analysis;collaborative anomaly detection;confidential data sharing;detection rate improvement;federated analysis;joint data analysis;malicious attack identification;network attacks;network traffic analysis;standard classification-based methods","","","","20","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine learning approach to segment Saccharomyces cerevisiae yeast cells","M. Tleis; F. Verbeek","Section Imaging and BioInformatics, LIACS, Leiden University, Leiden, The Netherlands","2015 International Conference on Advances in Biomedical Engineering (ICABME)","20151112","2015","","","278","281","In biological studies, Saccharomyces cerevisiae yeast cells are used to study the behaviour of proteins. This is a time consuming and not completely objective process. Hence, image analysis platforms are developed to address these problems and to offer analysis per cell as well. The segmentation algorithms implemented in such platforms can segment the healthy cells, along with artefacts such as debris and dead cells that exist in the cultured medium. The novel idea in this work is to apply a machine learning approach to train the segmentation system in order to classify the healthy cell objects from the other objects. Such approach is based on the analysis of a set of relevant individual cell features extracted from the microscope images of yeast cells. These features include texture measurements and wavelet-based texture measurements, as well as moment invariant features. Those features were introduced to describe the intensity and morphology characteristics in a more sophisticated way. A set of classification systems, data sampling techniques, data normalization schemes and feature selection algorithms were tested and evaluated to build a classification model in order to be used within the segmentation module. The study picks the simple logistic classification model as the best approach to classify our dataset of 1380 cells. This system increases the performance level in our image and data analysis modules, improve the segmentation and consequently the analysis of the measurement results. This leads to a better pattern recognition system as well.","2377-5688;23775688","Electronic:978-1-4673-6516-1; POD:978-1-4673-6517-8","10.1109/ICABME.2015.7323306","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323306","","Accuracy;Biomedical measurement;Image segmentation;Logistics;Machine learning algorithms;Vegetation","biological techniques;biology computing;cellular biophysics;feature extraction;image classification;image segmentation;image texture;learning (artificial intelligence);microorganisms;optical microscopy;wavelet transforms","S. cerevisiae cell segmentation;Saccharomyces cerevisiae;data analysis module;data normalization schemes;data sampling techniques;feature selection algorithms;image analysis module;logistic classification model;machine learning approach;moment invariant features;pattern recognition system;segmentation system training;wavelet based texture measurements;yeast cell microscope images;yeast cells","","","","19","","","16-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"A stable multi-scale kernel for topological machine learning","J. Reininghaus; S. Huber; U. Bauer; R. Kwitt","IST Austria","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","4741","4748","Topological data analysis offers a rich source of valuable information to study vision problems. Yet, so far we lack a theoretically sound connection to popular kernel-based learning techniques, such as kernel SVMs or kernel PCA. In this work, we establish such a connection by designing a multi-scale kernel for persistence diagrams, a stable summary representation of topological features in data. We show that this kernel is positive definite and prove its stability with respect to the 1-Wasserstein distance. Experiments on two benchmark datasets for 3D shape classification/retrieval and texture recognition show considerable performance gains of the proposed method compared to an alternative approach that is based on the recently introduced persistence landscapes.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7299106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299106","","Yttrium","image classification;image retrieval;image texture;learning (artificial intelligence);shape recognition","1-Wasserstein distance;3D shape classification;3D shape retrieval;multiscale kernel;persistence diagrams;texture recognition;topological machine learning","","5","","28","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Nonlinear decision boundary created by a machine learning-based classifier to mitigate nonlinear phase noise","D. Wang; M. Zhang; Z. Li; Y. Cui; J. Liu; Y. Yang; H. Wang","State Key Laboratory of Information Photonics and Optical Communications, Beijing University of Posts and Telecommunications, Beijing 100876, China","2015 European Conference on Optical Communication (ECOC)","20151203","2015","","","1","3","A machine learning-based classifier, namely SVM, is introduced to create the nonlinear decision boundary in M-ary PSK-based coherent optical system to mitigate NLPN. The maximum transmission distance and LPRD tolerance are improved by 480 km and 3.3 dBm for 8PSK.","","Electronic:978-8-4608-1741-3; POD:978-1-4673-7950-2","10.1109/ECOC.2015.7341753","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341753","","Binary phase shift keying;Bit error rate;Maximum likelihood estimation;Optical fiber communication;Optical fibers;Support vector machines","learning (artificial intelligence);optical fibre communication;optical modulation;pattern classification;phase noise;phase shift keying;support vector machines;telecommunication computing","LPRD tolerance;M-ary PSK-based coherent optical system;SVM;machine learning-based classifier;maximum transmission distance;nonlinear decision boundary;nonlinear phase noise","","3","","6","","","Sept. 27 2015-Oct. 1 2015","","IEEE","IEEE Conference Publications"
"A study of power distribution system fault classification with machine learning techniques","N. S. Coleman; C. Schegan; K. N. Miu","Department of Electrical and Computer Engineering, Drexel University, Philadelphia, USA","2015 North American Power Symposium (NAPS)","20151123","2015","","","1","6","Power system protection includes the process of identifying and correcting faults (failures) before fault currents cause damage to utility equipment or customer property. In distribution systems, where the number of measurements is increasing, there is an opportunity to improve fault classification techniques. This work presents a study in fault classification using machine learning techniques and quarter-cycle fault signatures. Separate voltage- and current-based feature vectors are defined using multi-resolution analysis and input to a two-stage classifier. The classifier was trained and tested on experimental fault data collected in Drexel University's Reconfigurable Distribution Automation and Control (RDAC) software/hardware laboratory. Results show: (1) non-linear, and even non-contiguous decision regions on a “fault plane”, using a phase voltage-based feature, and (2) an accurate classifier for determining the grounding status of multi-phase faults, using a neutral current-based feature.","","Electronic:978-1-4673-7389-0; POD:978-1-4673-7390-6; USB:978-1-4673-7388-3","10.1109/NAPS.2015.7335264","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335264","fault classification;machine learning;power distribution systems;power system protection","Circuit faults;Classification algorithms;Discrete wavelet transforms;Fault diagnosis;Grounding;Support vector machines","earthing;fault diagnosis;learning (artificial intelligence);power distribution faults;power distribution protection;power system analysis computing","Drexel University Reconfigurable Distribution Automation and Control;RDAC hardware laboratory;RDAC software laboratory;current-based feature vectors;customer property;fault correction process;fault currents;fault identification process;fault plane;grounding status determination;machine learning techniques;neutral current-based feature;noncontiguous decision regions;nonlinear decision regions;phase voltage-based feature;power distribution system fault classification;power system protection;quarter-cycle fault signatures;utility equipment;voltage-based feature vectors","","","","14","","","4-6 Oct. 2015","","IEEE","IEEE Conference Publications"
"Cardamom grading — A solution through machine learning techniques","R. M. Jose; K. S. S. Krishnan","Department of Computer Science and Engineering, Mar Baselios College Of Engineering and Technology, Trivandrum, Kerala, India","2015 Global Conference on Communication Technologies (GCCT)","20151203","2015","","","303","306","Cardamom grading is a vital agricultural product processing industry in Kerala which always seek for better methods of mechanization. Grading of cardamom is completely based on its external features namely weight, color, pod size, pod surface finishing and blacks and dots. There are certain standards like AGMARK set for different grades of cardamom. Machine learning techniques like k-NN, Decision tree etc can be used as a platform to do the due purpose of grading. Ensemble of two or more such algorithms can further enhance the grading efficiency and precision.","","CD-ROM:978-1-4799-8552-4; Electronic:978-1-4799-8553-1; POD:978-1-4799-8554-8","10.1109/GCCT.2015.7342672","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342672","cardamom;grading;image processing;k-NN","Finishing;Fuzzy logic;Image color analysis;Machine learning algorithms;Shape;Sorting","agricultural products;decision trees;feature extraction;learning (artificial intelligence)","AGMARK standards;Kerala;agricultural product processing industry;blacks;cardamom grading;color feature;decision tree;dots;external features;grading efficiency enhancement;grading precision enhancement;k-NN;machine learning techniques;pod size feature;pod surface finishing;weight feature","","","","11","","","23-24 April 2015","","IEEE","IEEE Conference Publications"
"Web Application Vulnerability Prediction Using Hybrid Program Analysis and Machine Learning","L. K. Shar; L. C. Briand; H. B. K. Tan","Interdisciplinary Centre for ICT Security, Reliability and Trust, University of Luxembourg, Luxembourg","IEEE Transactions on Dependable and Secure Computing","20151109","2015","12","6","688","707","Due to limited time and resources, web software engineers need support in identifying vulnerable code. A practical approach to predicting vulnerable code would enable them to prioritize security auditing efforts. In this paper, we propose using a set of hybrid (static+dynamic) code attributes that characterize input validation and input sanitization code patterns and are expected to be significant indicators of web application vulnerabilities. Because static and dynamic program analyses complement each other, both techniques are used to extract the proposed attributes in an accurate and scalable way. Current vulnerability prediction techniques rely on the availability of data labeled with vulnerability information for training. For many real world applications, past vulnerability data is often not available or at least not complete. Hence, to address both situations where labeled past data is fully available or not, we apply both supervised and semi-supervised learning when building vulnerability predictors based on hybrid code attributes. Given that semi-supervised learning is entirely unexplored in this domain, we describe how to use this learning scheme effectively for vulnerability prediction. We performed empirical case studies on seven open source projects where we built and evaluated supervised and semi-supervised models. When cross validated with fully available labeled data, the supervised models achieve an average of 77 percent recall and 5 percent probability of false alarm for predicting SQL injection, cross site scripting, remote code execution and file inclusion vulnerabilities. With a low amount of labeled data, when compared to the supervised model, the semi-supervised model showed an average improvement of 24 percent higher recall and 3 percent lower probability of false alarm, thus suggesting semi-supervised learning may be a preferable solution for many real world applications where vulnerability data is missing.","1545-5971;15455971","","10.1109/TDSC.2014.2373377","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963442","Vulnerability prediction;empirical study;input validation and sanitization;program analysis;security measures","Computer security;Data models;HTML;Predictive models;Semisupervised learning;Servers;Software protection","Internet;learning (artificial intelligence);program diagnostics;security of data","SQL injection;Web application vulnerability prediction;cross site scripting;dynamic program analyses;false alarm probability;file inclusion vulnerabilities;hybrid program analysis;hybrid static+dynamic code attributes;input sanitization code patterns;input validation code patterns;machine learning;open source projects;remote code execution;security auditing;semisupervised learning;static program analyses;vulnerability prediction techniques;vulnerability predictors;vulnerable code identification;vulnerable code prediction","","1","","51","","20141120","Nov.-Dec. 1 2015","","IEEE","IEEE Journals & Magazines"
"Automated change detection in satellite images using machine learning algorithms for Delhi, India","A. Bhatt; S. K. Ghosh; A. Kumar","Indian Institute of Technology Roorkee, Roorkee, India","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1678","1681","In remote sensing, change detection is used in land use and cover analysis, forest or vegetation assessment and, flood monitoring. Although manual change detection is an option, the time required for it can be prohibitive. It is also highly subjective depending on the expertise of the analyst. Hence, the need for automated methods for such analysis tasks have emerged and thus gives rise to unsupervised machine learning algorithm. This paper analyzes the effectiveness of the three types of unsupervised Machine learning algorithms (MLAs) for change detection to detect the change in some of the dominant classes in an urban area, such as, vegetation, built-up and water bodies. Landsat 5 TM and Landsat 8 OLI imageries have been selected for a part of New Delhi and its nearby area. In this study, three indices namely Normalized Difference Built-up Index (NDBI), Modified Normalized Difference Water Index (MNDWI) and Modified Soil Adjusted Vegetation Index (MSAVI<sub>2</sub>) have been generated from the Landsat data. Three algorithms, namely, K-Means, FCM and EM have been used, since these represent three different concepts in machine learning category i.e. partition based, fuzzy and probability based respectively. The same have been implemented in MATLAB for identifying different type of land cover over a period of 1998-2011. Considering both the intra- and inter-cluster distances, silhouette coefficients have been used for evaluation of cluster quality. The change is quantified in terms of percentage that depends upon the outcome of clustering and the number of pixel grouped in each class i.e. urban, vegetation and water.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326109","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326109","Change detection;EM;FCM;K-means;Machine learning;silhouette coefficient;urban remote sensing","Classification algorithms;Clustering algorithms;Earth;Machine learning algorithms;Remote sensing;Satellites;Vegetation mapping","fuzzy logic;geophysical image processing;land cover;land use;learning (artificial intelligence);mathematics computing;probability;terrain mapping;vegetation mapping","AD 1998 to 2011;Delhi;EM;FCM;India;Landsat 5 TM imagery;Landsat 8 OLI imagery;MATLAB;MNDWI;MSAVI;NDBI;automated change detection;cluster quality;flood monitoring;forest assessment;fuzzy based machine learning;intercluster distance;intracluster distance;land cover analysis;land use analysis;manual change detection;modified normalized difference water index;modified soil adjusted vegetation index;normalized difference built-up index;partition based machine learning;probability based machine learning;remote sensing;satellite image;silhouette coefficients;unsupervised machine learning algorithm;vegetation assessment","","","","13","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"A Semi-Supervised Machine Learning Method for Chinese Patent Effect Annotation","X. Chen; N. Deng","Sch. of Inf. & Safety Eng., Zhongnan Univ. of Econ. & Law, Wuhan, China","2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","20151029","2015","","","243","250","Patents are public and scientific literatures protected by the law, and their abstracts highly contain valuable information. Patent's semantic annotation can effectively protect intellectual property rights and promote corporations' scientific research innovation. Currently, automatic patent annotation mainly uses supervised machine learning algorithms, which is required abundant expensive labeled patent data. Due to lack of enough labeled Chinese patent data, this paper adopts a semi-supervised machine learning method named co-training, which starts from a little labeled data. This method cooperates keyword extraction with list extraction, and incrementally annotates functional clauses in patent abstract. Experiment results indicate this method can gradually improve the recall without sacrificing too much precision.","","Electronic:978-1-4673-9200-6; POD:978-1-4673-9201-3","10.1109/CyberC.2015.99","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307821","Semantic annotation;co-training;information extraction;patent mining","Data mining;Dictionaries;Industries;Patents;Semantics;Technological innovation","learning (artificial intelligence);patents","Chinese patent effect annotation;automatic patent annotation;co-training;functional clauses annotation;intellectual property rights protection;keyword extraction;labeled Chinese patent data;list extraction;patent abstract;patent semantic annotation;public literatures;scientific literatures;scientific research innovation;semisupervised machine learning method","","","","20","","","17-19 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning and Mass Estimation Methods for Ground-Based Aircraft Climb Prediction","R. Alligier; D. Gianazza; N. Durand","Lab. de Math. Appl., Inf. et Autom. pour l'Aerien, Ecole Nat. de l'Aviation Civile, Toulouse, France","IEEE Transactions on Intelligent Transportation Systems","20151124","2015","16","6","3138","3149","In this paper, we apply machine learning methods to improve the aircraft climb prediction in the context of ground-based applications. Mass is a key parameter for climb prediction. As it is considered a competitive parameter by many airlines, it is currently not available to ground-based trajectory predictors. Consequently, most predictors today use a reference mass that may be different from the actual aircraft mass. In previous papers, we have introduced a least squares method to estimate the mass from past trajectory points, using the physical model of the aircraft. Another mass estimation method, based on an adaptive mechanism, has also been proposed by Schultz et al. We now introduce a new approach, in which the mass is considered the response variable of a prediction model that is learned from a set of example trajectories. This machine learning approach is compared with the results obtained when using the base of aircraft data (BADA) reference mass or the two state-of-the-art mass estimation methods. In these experiments, nine different aircraft types are considered. When compared with the baseline method (respectively, the mass estimation methods), the Machine Learning approach reduces the RMSE (Root Mean Square Error) on the predicted altitude by at least 58% (resp. 27%) when assuming the speed profile to be known, and by at least 29% (resp. 17%) when using the BADA speed profile except for the aircraft types E145 and F100. For these types, the observed speed profile is far from the BADA speed profile.","1524-9050;15249050","","10.1109/TITS.2015.2437452","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123640","Aircraft trajectory prediction;base of aircraft data (BADA);machine learning;mass estimation","Aircraft;Machine learning;Predictive models;Trajectory","","","","0","","39","","20150615","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"Feature selection and recognition of electroencephalogram signals: An extreme learning machine and genetic algorithm-based approach","Q. Lin; J. B. Huang; J. Zhong; S. D. Lin; Y. Xue","School of Information Engineering, Guangdong Medical College, Dongguan, China, 523808","2015 International Conference on Machine Learning and Cybernetics (ICMLC)","20151203","2015","2","","499","504","The effective recognition approach of the electroencephalogram (EEG) signals can significantly boost the performance and the development of the EEG-based diagnosis and treatment. A new approach which combines the Extreme Learning Machine (ELM) with the Genetic algorithm (GA) is proposed in this paper. In the proposed approach, the ELM is used both as the final classifier and the fitness function for the GA to select the optimal feature subset from the initial features extracted through time-frequency (TF) analysis. The GA is adopted as the complementary input optimization mechanism to improve the performance of the ELM. To testify the performance of the proposed approach, experiments were simulated using the real-world EEG signals of 2003 International BCI Competition dataset. The recognition results have proved the effectiveness of the proposed approach.","","CD-ROM:978-1-4673-7220-6; Electronic:978-1-4673-7221-3; POD:978-1-4673-7222-0","10.1109/ICMLC.2015.7340607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340607","Artificial Neural Network (ANN);Brain-Computer Interface (BCI);Electroencephalogram (EEG);Extreme Learning Machine (ELM);Genetic algorithm (GA)","","brain-computer interfaces;electroencephalography;feature selection;genetic algorithms;learning (artificial intelligence);medical signal processing;patient diagnosis;patient treatment;time-frequency analysis","2003 International BCI competition dataset;EEG signal;EEG treatment;EEG-based diagnosis;ELM;TF analysis;complementary input optimization mechanism;electroencephalogram signal recognition;extreme learning machine;feature extraction;feature selection;genetic algorithm-based approach;optimal feature subset;recognition approach;time-frequency analysis","","","","16","","","12-15 July 2015","","IEEE","IEEE Conference Publications"
"Automated frequency selection for machine-learning based EH/EW prediction from S-Parameters","N. Ambasana; D. Gope; B. Mutnury; G. Anand","Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India","2015 IEEE 24th Electrical Performance of Electronic Packaging and Systems (EPEPS)","20151207","2015","","","53","56","In the field of High Speed SerDes (HSS) channel analysis and design, the most widely accepted metrics for gauging signal integrity are Time Domain (TD) metrics: Bit Error Rate (BER), Eye-Height (EH) and Eye-Width (EW). With increasing bit-rates, TD simulations are getting compute-time intensive especially as the BER criterion is getting lower. Learning based mapping of Frequency Domain (FD) S-Parameter data to EH/EW in TD provides a fast alternative solution for thorough design-space exploration. A key challenge in this mapping procedure is the identification of the optimal frequency points in the S-Parameter data that are used for training the learning network. This paper outlines a methodology to identify the minimal set of critical frequency points using a Fast Correlation Based Feature (FCBF) selection algorithm. This technique is applied for prediction of EH/EW for a PCIe Gen 3 interface and the prediction accuracy is quantified.","","Electronic:978-1-5090-0040-1; Paper:978-1-5090-0038-8; USB:978-1-5090-0039-5","10.1109/EPEPS.2015.7347128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347128","ANN;Eye-Height;Feature Selection;Insertion Loss;PCIe;Signal Integrity;Total FEXT","Artificial neural networks;Bit error rate;Correlation;Measurement;Predictive models;Scattering parameters;Training","S-parameters;error statistics;frequency-domain analysis;learning (artificial intelligence);peripheral interfaces;time-domain analysis","BER;EH/EW prediction;FCBF selection algorithm;HSS channel analysis;PCIe Gen 3 interface;S-parameter;automated frequency selection;bit error rate;eye-height;eye-width;fast correlation based feature;frequency domain;high speed SerDes channel analysis;learning based mapping;machine learning;optimal frequency point;signal integrity;time domain metric","","1","","8","","","25-28 Oct. 2015","","IEEE","IEEE Conference Publications"
"Diabetic retinal exudates detection using machine learning techniques","P. R. Asha; S. Karpagavalli","PSGR Krishnammal College for Women Coimbatore, India","2015 International Conference on Advanced Computing and Communication Systems","20151112","2015","","","1","5","Diabetic Retinopathy (DR) is an eye filled illness caused by the complication of polygenic disease and that is to be detected accurately for timely treatment. As polygenic disease progresses, the vision of a patient could begin to deteriorate and leads to blindness. In this proposed work, the presence or absence of retinal exudates are detected using machine learning (ML) techniques. To detect the presence of exudates features like Mean, Standard deviation, Centroid and Edge Strength are extracted from Luv color space after segmenting the Retinal image. A total of 100 images were used, out of which 80 images were used for training and 20 images were used for testing. The classification task carried out with classifiers like Naive bayes (NB), Multilayer Perceptron (MLP) and Extreme Learning Machine (ELM). Experimental results shows that the model built using Extreme Learning Machine outperforms other two models and effectively detects the presence of exudates in retinal images.","","Electronic:978-1-4799-6438-3; POD:978-1-4799-6439-0","10.1109/ICACCS.2015.7324057","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324057","color space;extreme learning machine;fuzzy C-means;histogram specification","Accuracy;Diabetes;Feature extraction;Image color analysis;Image segmentation;Neurons;Retina","Bayes methods;feature extraction;image classification;image colour analysis;image segmentation;learning (artificial intelligence);medical image processing;multilayer perceptrons;retinal recognition","ELM;Luv color space;MLP;diabetic retinal exudate detection;diabetic retinopathy;extreme learning machine;eye filled illness;feature extraction;image classification task;machine learning techniques;multilayer perceptron;naive Bayes;polygenic disease complication;retinal image segmentation","","","","20","","","5-7 Jan. 2015","","IEEE","IEEE Conference Publications"
"Comparison of machine learning algotithms for leaf area index retrieval from time series MODIS data","T. Wang; Z. Xiao; Z. Liu","State Key Laboratory of Remote Sensing Science, School of Geography, Beijing Normal University. Beijing, China, 100875","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1729","1732","Temporally continuous and high quality leaf area index (LAI) products are urgently needed for crop growth monitoring, yield estimation and other research fields. However, most of the methods used to retrieve LAI just use a single phase satellite observational data to estimate LAI. Because of the impact of clouds and aerosols, the LAI products generated by these methods are temporally discontinuous. In this study, performance of three machine learning algorithms for parameter estimation using time series data is evaluated. The three machine learning algorithms are back-propagation neutral network (BPNN), general regression neutral networks (GRNNs) and multivariate adaptive regression splines (MARS). The results show that these machine learning algorithms have a good performance in time series LAI retrieval and GRNNs outperform the other algorithms.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326122","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326122","MODIS;Machine learning;comparison;leaf area index;time series","Indexes;MODIS;Machine learning algorithms;Mars;Reflectivity;Time series analysis;Training","aerosols;agricultural engineering;backpropagation;clouds;crops;geophysics computing;learning (artificial intelligence);neural nets;regression analysis;remote sensing;splines (mathematics);time series","MACHINE LEARNING ALGOTITHMS;TIME SERIES MODIS DATA;aerosols;back-propagation neutral network;clouds impact;crop growth monitoring;general regression neutral networks;high quality leaf area index RETRIEVAL;multivariate adaptive regression splines;parameter estimation;research fields;single phase satellite observational data;yield estimation","","","","6","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Maximizing Hardware Prefetch Effectiveness with Machine Learning","S. Rahman; M. Burtscher; Z. Zong; A. Qasem","Dept. of Comput. Sci., Texas State Univ., San Marcos, TX, USA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","20151130","2015","","","383","389","Modern processors are equipped with multiple hardware prefetchers, each of which targets a distinct level in the memory hierarchy and employs a separate prefetching algorithm. However, different programs require different subsets of these prefetchers to maximize their performance. Turning on all available prefetchers rarely yields the best performance and, in some cases, prefetching even hurts performance. This paper studies the effect of hardware prefetching on multithreaded code and presents a machine-learning technique to predict the optimal combination of prefetchers for a given application. This technique is based on program characterization and utilizes hardware performance events in conjunction with a pruning algorithm to obtain a concise and expressive feature set. The resulting feature set is used in three different learning models. All necessary steps are implemented in a framework that reaches, on average, 96% of the best possible prefetcher speedup. The framework is built from open-source tools, making it easy to extend and port to other architectures.","","Electronic:978-1-4799-8937-9; POD:978-1-4799-8938-6","10.1109/HPCC-CSS-ICESS.2015.175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336192","","Algorithms;Hardware;Optimization;Prefetching;Testing;Training","learning (artificial intelligence);multi-threading;performance evaluation;public domain software;storage management","feature set;hardware performance events;hardware prefetch effectiveness maximization;learning models;machine learning;memory hierarchy;multiple hardware prefetchers;multithreaded code;open-source tools;optimal prefetcher prediction;performance maximization;prefetcher speedup;program characterization;pruning algorithm","","1","","14","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Classification for Safety-Critical Car-Cyclist Scenarios Using Machine Learning","I. Cara; E. d. Gelder","TNO, Helmond, Netherlands","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","1995","2000","The number of fatal car-cyclist accidents is increasing. Advanced Driver Assistance Systems (ADAS) can improve the safety of cyclists, but they need to be tested with realistic safety-critical car-cyclist scenarios. In order to store only relevant scenarios, an online classification algorithm is needed. We demonstrate that machine learning techniques can be used to detect and classify those scenarios based on their trajectory data. A dataset consisting of 99 realistic car-cyclist scenarios is gathered using an instrumented vehicle. We achieved a classification accuracy of the gathered data of 87.9%. The execution time of only 45.8 us shows that the algorithm is suitable for online purposes.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.323","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313415","","Accuracy;Classification algorithms;Hidden Markov models;Machine learning algorithms;Support vector machines;Trajectory;Vehicles","automobiles;bicycles;feature extraction;intelligent transportation systems;learning (artificial intelligence);pattern classification;road accidents","ADAS;advanced driver assistance system;car-cyclist accident;classification algorithm;machine learning;safety-critical car-cyclist scenario","","","","27","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"Automated File-Based Quality Control: A Machine-Learning Approach","M. De Geyter; N. Vercammen; D. Deschrijver; T. Dhaene; P. Demeester; B. Vermeulen","VRT-medialab, Gaston Crommenlaan 10 (Bus 101), B-9050 Ghent","The 2011 Annual Technical Conference & Exhibition","20151019","2011","","","1","6","In recent years, broadcasters successfully introduced file-based workflows to improve production efficiency. However, they are increasingly dealing with a proliferation of file formats, and many of them still have large archives that need to be digitized for reuse. To guarantee trouble-free workflows and long-term preservation in this quickly evolving digital domain, it is essential that media files adhere to well-described, established standards. Furthermore, their audiovisual quality should be up to broadcast level. A variety of content analysis tools checking container and encoding formats, as well as audiovisual quality, are available but often hard to configure, and frequently provide difficult-to-interpret results. In this research, a learning algorithm takes into account the results of several sources of content analysis to perform a reliable automatic interpretation, which is communicated as a traffic light decision to an operator who can then take further action if necessary. Thus, valuable time and money can be saved.","","Paper:978-1-61482-940-9","10.5594/M001058","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7269275","automation;file-based workflows;machine-learning;quality control","","","","","","","1","","","25-27 Oct. 2011","","SMPTE","SMPTE Conference Publications"
"FPGA based nonlinear Support Vector Machine training using an ensemble learning","M. B. Rabieah; C. S. Bouganis","Department of Electrical and Electronic Engineering, Imperial College London, UK","2015 25th International Conference on Field Programmable Logic and Applications (FPL)","20151008","2015","","","1","4","Support Vector Machines (SVMs) are powerful supervised learning methods in machine learning. However, their applicability to large problems has been limited due to the time consuming training stage whose computational cost scales quadratically with the number of examples. In this work, a complete FPGA-based system for nonlinear SVM training using ensemble learning is presented. The proposed framework builds on the FPGA architecture and utilizes a cascaded multi-precision training flow, exploits the heterogeneity within the training problem by tuning the number representation used, and supports ensemble training tuned to each internal memory structure so to address very large datasets. Its performance evaluation shows that the proposed system achieves more than an order of magnitude better results compared to state-of-the-art CPU and GPU-based implementations.","1946-147X;1946147X","Electronic:978-0-9934-2800-5; POD:978-1-4673-8123-9","10.1109/FPL.2015.7293972","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293972","","Acceleration;Computer architecture;Field programmable gate arrays;Hardware;Kernel;Support vector machines;Training","field programmable gate arrays;learning (artificial intelligence);performance evaluation;support vector machines","CPU-based implementation;FPGA architecture;FPGA based nonlinear support vector machine training;FPGA-based system;GPU-based implementation;cascaded multiprecision training flow;computational cost;ensemble learning;ensemble training;internal memory structure;machine learning;nonlinear SVM training;number representation tuning;performance evaluation;supervised learning methods;very large datasets","","1","","15","","","2-4 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning for Achieving Self-* Properties and Seamless Execution of Applications in the Cloud","P. Di Sanzo; A. Pellegrini; D. R. Avresky","DIAG, Univ. of Rome, Rome, Italy","2015 IEEE Fourth Symposium on Network Cloud Computing and Applications (NCCA)","20151203","2015","","","51","58","Software anomalies are recognized as a major problem affecting the performance and availability of many computer systems. Accumulation of anomalies of different nature, such as memory leaks and unterminated threads, may lead the system to both fail or work with suboptimal performance levels. This problem particularly affects web servers, where hosted applications are typically intended to continuously run, thus incrementing the probability, therefore the associated effects, of accumulation of anomalies. Given the unpredictability of occurrence of anomalies, continuous system monitoring would be required to detect possible system failures and/or excessive performance degradation in order to timely start some recovering procedure. In this paper, we present a Machine Learning-based framework for proactive management of client-server applications in the cloud. Through optimized Machine Learning models and continually measuring system features, the framework predicts the remaining time to the occurrence of some unexpected event (system failure, service level agreement violation, etc.) of a virtual machine hosting a server instance of the application. The framework is able to manage virtual machines in the presence of different types anomalies and with different anomaly occurrence patterns. We show the effectiveness of the proposed solution by presenting results of a set of experiments we carried out in the context of a real world-inspired scenario.","","Electronic:978-1-4673-7741-6; POD:978-1-4673-7742-3","10.1109/NCCA.2015.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340027","Cloud Computing;Machine Learning;Rejuvenation;Software Aging","Cloud computing;Computer crashes;Monitoring;Predictive models;Servers;System performance","client-server systems;cloud computing;contracts;file servers;learning (artificial intelligence);system monitoring;system recovery;virtual machines","Web server;anomaly occurrence pattern;client-server application;cloud application;machine learning;service level agreement violation;software anomaly;system failure;system monitoring;virtual machine","","3","","18","","","11-12 June 2015","","IEEE","IEEE Conference Publications"
"Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare","M. Mozaffari-Kermani; S. Sur-Kolay; A. Raghunathan; N. K. Jha","Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA","IEEE Journal of Biomedical and Health Informatics","20151103","2015","19","6","1893","1905","Machine learning is being used in a wide range of application domains to discover patterns in large datasets. Increasingly, the results of machine learning drive critical decisions in applications related to healthcare and biomedicine. Such health-related applications are often sensitive, and thus, any security breach would be catastrophic. Naturally, the integrity of the results computed by machine learning is of great importance. Recent research has shown that some machine-learning algorithms can be compromised by augmenting their training datasets with malicious data, leading to a new class of attacks called poisoning attacks. Hindrance of a diagnosis may have life-threatening consequences and could cause distrust. On the other hand, not only may a false diagnosis prompt users to distrust the machine-learning algorithm and even abandon the entire system but also such a false positive classification may cause patient distress. In this paper, we present a systematic, algorithm-independent approach for mounting poisoning attacks across a wide range of machine-learning algorithms and healthcare datasets. The proposed attack procedure generates input data, which, when added to the training set, can either cause the results of machine learning to have targeted errors (e.g., increase the likelihood of classification into a specific class), or simply introduce arbitrary errors (incorrect classification). These attacks may be applied to both fixed and evolving datasets. They can be applied even when only statistics of the training dataset are available or, in some cases, even without access to the training dataset, although at a lower efficacy. We establish the effectiveness of the proposed attacks using a suite of six machine-learning algorithms and five healthcare datasets. Finally, we present countermeasures against the proposed generic attacks that are based on tracking and detecting deviations in various accuracy metrics, and benchmark their effectiveness.","2168-2194;21682194","","10.1109/JBHI.2014.2344095","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868201","Healthcare;machine learning;poisoning attacks;security","Machine learning algorithms;Malware;Security;Training","health care;learning (artificial intelligence);medical computing;pattern classification;security of data","application domains;arbitrary errors;biomedicine;critical decisions;false diagnosis prompt users;false positive classification;health-related applications;healthcare;life-threatening consequences;machine-learning algorithms;malicious data;patient distress;security breach;systematic poisoning attacks;targeted errors;training datasets","","0","","47","","20140730","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"Machine learning approach for quality assessment and prediction in large software organizations","R. Rana; M. Staron","Department of Computer Science and Engineering, Chalmers &#x007C; University of Gothenburg, G&#x00F6;teborg, Sweden","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20151130","2015","","","1098","1101","The importance of software in everyday products and services has been on constant rise and so is the complexity of software. In face of this rising complexity and our dependence on software - measuring, maintaining and increasing software quality is of critical importance. Software metrics provide a quantitative means to measure and thus control various attributes of software systems. In the paradigm of machine learning, software quality prediction can be cast as a classification or concept learning problem. In this paper we provide a general framework for applying machine learning approaches for assessment and prediction of software quality in large software organizations. Using ISO 15939 measurement information model we show how different software metrics can be used to build software quality model which can be used for quality assessment and prediction that satisfies the information need of these organizations with respect to quality. We also document how machine learning approaches can be effectively used for such evaluation.","2327-0586;23270586","CD-ROM:978-1-4799-8351-3; Electronic:978-1-4799-8353-7; POD:978-1-4799-8354-4","10.1109/ICSESS.2015.7339243","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339243","","Adaptation models;IEC Standards;ISO Standards;Organizations;Software measurement;Software quality","learning (artificial intelligence);software metrics;software quality","ISO 15939 measurement information model;classification problem;concept learning problem;machine learning approach;quality assessment;quality prediction;software complexity;software metrics;software organizations;software quality;software system atttribute","","","","21","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"New hierarchical approach for microaneurysms detection with matched filter and machine learning","J. Wu; J. Xin; L. Hong; J. You; N. Zheng","Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, 710049, China","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","4322","4325","Microaneurysms are regarded as the first signs of diabetic retinopathy (DR), but the microaneurysms are not clear in the color retinal images, and many researches were studied to detect and locate these lesions. In this paper, a new hierarchical computing-aided diagnosis approach is proposed for the microaneurysms detection by using the multi-scale and multi-orientation sum of matched filter (MMMF) and machine learning, where 37 dimensional features are extracted from each candidate. Furthermore, several classifiers such as the k-nearest neighbor (kNN), local linear discrimination analysis (LLDA) and support vector machine (SVM) are modified to distinguish the true microaneurysms from the false ones, which is a typical unbalanced classification problem. The effectiveness of the proposed method is verified through the training set of a publicly available database, and the experiment results show that the proposed method has better detection performance including the receiver operating characteristic (ROC) curve and the free-response receiver operating characteristic (FROC) curve. Moreover, the proposed method with 37 dimensional features outperforms that with other features and has a sensitivity from 1/8 to 8 with the average of all seven points being 0.286 tested on the same database.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319351","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319351","","Feature extraction;Image color analysis;Lesions;Retina;Sensitivity;Support vector machines;Training","biomedical optical imaging;blood vessels;diseases;eye;feature extraction;image classification;learning (artificial intelligence);medical image processing;sensitivity analysis;support vector machines","DR;FROC;LLDA;MMMF;SVM;color retinal images;detection performance;diabetic retinopathy;feature extraction;free-response receiver operating characteristic curve;hierarchical computing-aided diagnosis;k-nearest neighbor;kNN;local linear discrimination analysis;machine learning;microaneurysm detection;multiscale and multiorientation sum of matched filter;support vector machine;training set;true microaneurysms;unbalanced classification problem","","","","9","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Evaluation of Machine Learning Frameworks on Bank Marketing and Higgs Datasets","B. M. Shashidhara; S. Jain; V. D. Rao; N. Patil; G. S. Raghavendra","Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Surathkal, India","2015 Second International Conference on Advances in Computing and Communication Engineering","20151026","2015","","","551","555","Big data is an emerging field with different datasets of various sizes are being analyzed for potential applications. In parallel, many frameworks are being introduced where these datasets can be fed into machine learning algorithms. Though some experiments have been done to compare different machine learning algorithms on different data, these experiments have not been tested out on different platforms. Our research aims to compare two selected machine learning algorithms on data sets of different sizes deployed on different platforms like Weka, Scikit-Learn and Apache Spark. They are evaluated based on Training time, Accuracy and Root mean squared error. This comparison helps us to decide what platform is best suited to work while applying computationally expensive selected machine learning algorithms on a particular size of data. Experiments suggested that Scikit-Learn would be optimal on data which can fit into memory. While working with huge, data Apache Spark would be optimal as it performs parallel computations by distributing the data over a cluster. Hence this study concludes that spark platform which has growing support for parallel implementation of machine learning algorithms could be optimal to analyze big data.","","Electronic:978-1-4799-1734-1; POD:978-1-4799-1735-8","10.1109/ICACCE.2015.31","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306745","Apache Spark;Big Data;Distributed Computing;Machine Learning Algorithms;Parallel Execution;Scikit-Learn;Weka","Accuracy;Algorithm design and analysis;Big data;Machine learning algorithms;Sparks;Support vector machines;Training","Big Data;data analysis;learning (artificial intelligence)","Apache Spark platform;Big Data analysis;Higgs datasets;Scikit-Learn platform;Weka platform;bank marketing datasets;machine learning frameworks","","","","16","","","1-2 May 2015","","IEEE","IEEE Conference Publications"
"Collective I/O Tuning Using Analytical and Machine Learning Models","F. Isaila; P. Balaprakash; S. M. Wild; D. Kimpe; R. Latham; R. Ross; P. Hovland","","2015 IEEE International Conference on Cluster Computing","20151029","2015","","","128","137","The optimization of parallel I/O has become challenging because of the increasing storage hierarchy, performance variability of shared storage systems, and the number of factors in the hardware and software stacks that impact performance. In this paper, we perform an in-depth study of the complexity involved in I/O autotuning and performance modeling, including the architecture, software stack, and noise. We propose a novel hybrid model combining analytical models for communication and storage operations and black-box models for the performance of the individual operations. The experimental results show that the hybrid approach performs significantly better and shows a higher robustness to noise than state-of-the-art machine learning approaches, at the cost of a higher modeling complexity.","1552-5244;15525244","Electronic:978-1-4673-6598-7; POD:978-1-4673-6599-4","10.1109/CLUSTER.2015.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307576","I/O performance modeling;model-based tuning;statistical and analytical performance models","Analytical models;Computational modeling;Computer architecture;Interference;Optimization;Predictive models;Tuning","learning (artificial intelligence);optimisation;parallel processing;shared memory systems","I/O autotuning;analytical models;architecture;black-box models;collective I/O tuning;communication operations;hardware stacks;hybrid model;machine learning models;optimization;parallel I/O;performance modeling;performance variability;shared storage systems;software stacks;storage hierarchy;storage operations","","1","","18","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine learning with the internet of virtual things","G. Bovet; A. Ridi; J. Hennebert","LTCI, Telecom ParisTech, 46 Rue Barrault, 75013, France","2015 International Conference on Protocol Engineering (ICPE) and International Conference on New Technologies of Distributed Systems (NTDS)","20151008","2015","","","1","8","Peripheral devices working in the context of the Internet of Things, specifically sensors, produce large amounts of data that can be used to infer knowledge. In this area, machine learning technologies are increasingly used to establish versatile models. In this article, we present a new architecture capable of running machine learning algorithms in a sensor network. This approach has advantages in terms of confidentiality and energy efficiency-related data transfer. First, we argue that some types of machine learning algorithms are consistent with this approach, particularly those based on the use of generative algorithms. Subsequently we detail our proposed architecture based on Internet of Things and Web of Things paradigms facilitating the integration in sensor networks. The convergence of generative models and Web Objects leads to the concept of virtual sensors exposing high-level knowledge using data from various sensors. Finally, we demonstrate the feasibility and performance of our proposal using a real scenario.","2162-1896;21621896","Electronic:978-1-4673-9265-5; POD:978-1-4673-9266-2","10.1109/NOTERE.2015.7293488","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293488","","Gold;Hidden Markov models;Java;Machine learning algorithms;Protocols;Sensors;XML","Internet;Internet of Things;learning (artificial intelligence)","Internet of virtual Things;Web objects;Web of Things;energy efficiency-related data transfer;generative algorithms;machine learning technology;peripheral devices;virtual sensor network","","","","19","","","22-24 July 2015","","IEEE","IEEE Conference Publications"
"A comparative study of different machine learning methods for electricity prices forecasting of an electricity market","E. Foruzan; S. D. Scott; J. Lin","","2015 North American Power Symposium (NAPS)","20151123","2015","","","1","6","Generally, it is difficult to accurately forecast electricity prices because they are unpredictable. Yet, accurate price forecasting is expected to provide crucial information, needed by power producers and consumers to bid strategically, thereby decreasing their risks and increasing their profits in the electricity market. In this paper, two models using artificial neural networks (ANN) and support vector machines (SVM) were developed for electricity price forecasting. In addition, ant colony optimization (ACO) was used to reduce the feature space and give the best attribute subset for ANN model. Using ACO for feature selection significantly reduced the training time for ANN-based electricity price forecasting model while the results were almost as accurate as those from ANN model.","","Electronic:978-1-4673-7389-0; POD:978-1-4673-7390-6; USB:978-1-4673-7388-3","10.1109/NAPS.2015.7335095","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335095","Ant Colony Optimization;Artificial Neural Networks;Electricity price forecasting;Machine Learning;Support Vector Machines","Artificial neural networks;Electricity supply industry;Forecasting;Mathematical model;Predictive models;Support vector machines","ant colony optimisation;economic forecasting;feature extraction;learning (artificial intelligence);neural nets;power engineering computing;power markets;support vector machines","ACO;ANN model;SVM;ant colony optimization;artificial neural networks;electricity market;electricity price forecasting model;feature selection;feature space reduction;machine learning methods;support vector machines;training time reduction","","1","","16","","","4-6 Oct. 2015","","IEEE","IEEE Conference Publications"
"Analysis of multimodality brain images using machine learning techniques","Kavitha S.; Thyagharajan K. K.","Dept. of CSE at SSN College of Engineering, Chennai - 603 110, India","2015 International Conference on Communications and Signal Processing (ICCSP)","20151112","2015","","","1482","1486","In the recent era, due to the technological growth and requirement, various modern medical imaging equipments are developed with different imaging principles. Analyzing these images manually in different dimensions has been proven critical for physicians, biologists and radiologists to seek answers for diagnosis problems. Presently problems exists at each level of imaging across different imaging modalities/scales, registration, fusion, image analysis, pattern recognition, image mining, visualization, reconstruction and informatics methods. This paper is focused on multimodality brain images and its analysis at different stages of process such as fusion, classification and understanding using machine learning techniques. The importance of fusion is illustrated using the result of classification and the need of understanding technique is introduced for further research.","","CD-ROM:978-1-4799-8080-2; Electronic:978-1-4799-8081-9; POD:978-1-4799-8082-6","10.1109/ICCSP.2015.7322761","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322761","Classification;Fusion;Pulse Coupled Neural Network;Support Vector Machine","Magnetic resonance imaging;Medical diagnostic imaging;Positron emission tomography;Semantics;Single photon emission computed tomography;Visualization","brain;image classification;image fusion;image reconstruction;learning (artificial intelligence);medical image processing","diagnosis problems;image analysis;image classification;image fusion;image mining;image reconstruction;image registration;imaging principles;informatics methods;machine learning techniques;medical imaging equipments;multimodality brain images;pattern recognition","","","","21","","","2-4 April 2015","","IEEE","IEEE Conference Publications"
"Simple modifications on heuristic rule generation and rule evaluation in Michigan-style fuzzy genetics-based machine learning","Y. Nojima; K. Watanabe; H. Ishibuchi","Department of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Sakai, 599-8531, Japan","2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","20151130","2015","","","1","8","Fuzzy genetics-based machine learning (FGBML) is one of the representative approaches to obtain a set of fuzzy if-then rules by evolutionary computation. A number of FGBML methods have been proposed so far. Among them, Michigan-style approaches are popular thanks to thier lower computational cost than Pittsburgh approaches. In this study, we introduce two simple modifications for our Michigan-style FGBML. One is related to heuristic rule generation. In the original FGBML, each rule in an initial population is generated from a randomly-selected training pattern in a heuristic manner. The heuristic rule generation also performs during evolution where each rule is generated from a misclassified pattern. As its modification, we propose the use of multiple patterns to generate each fuzzy if-then rule. The other is related to the fitness calculation. In the original FGBML, the fitness of each rule is calculated as the number of correctly classified training patterns, while the number of misclassified patterns is ignored. As its modification, we incorporate a penalty term into the fitness function. Through computational experiments using 20 benchmark data sets, we examine the effects of these two modifications on the search ability of our Michigan-style FGBML.","","Electronic:978-1-4673-7428-6; POD:978-1-4673-7429-3","10.1109/FUZZ-IEEE.2015.7338115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338115","Evolutionary fuzzy systems;fitness calculation;fuzzy genetics-based machine learning;heuristic rule generation","Fuzzy sets;Fuzzy systems;Genetics;Probabilistic logic;Sociology;Statistics;Training","fuzzy set theory;genetic algorithms;knowledge acquisition;learning (artificial intelligence);pattern classification","Michigan-style FGBML;Michigan-style fuzzy genetics-based machine learning;correctly classified training patterns;evolutionary computation;fuzzy if-then rules;heuristic rule generation;misclassified patterns;rule evaluation","","1","","20","","","2-5 Aug. 2015","","IEEE","IEEE Conference Publications"
"Advance flood detection and notification system based on sensor technology and machine learning algorithm","M. Khalaf; A. J. Hussain; D. Al-Jumeily; P. Fergus; I. O. Idowu","Applied Computing Research Group, School of Computing and Mathematical Sciences Liverpool John Moores University Byrom Street, L3 3AF, UK","2015 International Conference on Systems, Signals and Image Processing (IWSSIP)","20151102","2015","","","105","108","Floods are common natural disasters that cause severe devastation of any country. They are commonly caused by precipitation and runoff of rivers, particularly during periods of excessively high rainy season. Due to global warming issues and extreme environmental effects, flood has become a serious problem to the extent of bringing about negative impact to the mankind and infrastructure. To date, sensor network technology has been used in many areas including water level fluctuation. However, efficient flood monitoring and real time notification system still a crucial part because Information Technology enabled applications have not been employed in this sector in a broad way. This research presents a description of an alert generating system for flood detection with a focus on determining the current water level using sensors technology. The system then provides notification message about water level sensitivity via Global Communication and Mobile System modem to particular authorise person. Besides the Short Message Service, the system instantaneously uploads and broadcast information through web base public network. Machine-learning algorithms were conducted to perform the classification process. Four experiments were carried out to classify flood data from normal and at risk condition in which 99.5% classification accuracy was achieved using Random Forest algorithm. Classification using Bagging, Decision Tree and HyperPipes algorithms achieved accuracy of 97.7 %, 94.6% and 89.8 %, respectively.","2157-8672;21578672","CD-ROM:978-1-4673-8352-3; Electronic:978-1-4673-8353-0; POD:978-1-4673-8354-7","10.1109/IWSSIP.2015.7314188","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314188","Flood detection system;Global communication and mobile system;Machine learning algorithm;Realtime data;Sensor network;Short message service","Accuracy;Bagging;Classification algorithms;Decision trees;Floods;Machine learning algorithms;Monitoring","floods;hydrological techniques;rain;rivers","Global Communication modem;Mobile System modem;Short Message Service;advance flood detection;extreme environmental effects;flood data;global warming issues;machine learning algorithm;notification system;precipitation;rainy season;random forest algorithm;river runoff;sensor network technology;water level fluctuation;water level sensitivity","","","","7","","","10-12 Sept. 2015","","IEEE","IEEE Conference Publications"
"Identification of IT Incidents for Improved Risk Analysis by Using Machine Learning","S. M. Sulaman; K. Weyns; M. Höst","Dept. of Comput. Sci., Lund Univ., Lund, Sweden","2015 41st Euromicro Conference on Software Engineering and Advanced Applications","20151026","2015","","","369","373","Today almost every system or service is dependent on IT systems, and failure of these systems have serious and negative effects on the society. IT incidents are critical for the society as they can stop the function of critical systems and services. Therefore, it is important to analyze these systems for potential risks before becoming dependent on them. Moreover, in a software engineering context risk analysis is an important activity for the development and operation of safe software intensive systems. However, the increased complexity and size of software-intensive systems put additional requirements on the effectiveness of the risk analysis process. This means that the risk analysis process needs to be improved and it is believed that this can be done by having an overview of already occurred IT incidents. This study investigates how difficult it is to find relevant risks from available sources and the effort required to set up such a system. It also investigates the accuracy of the found risks. In this study 58% of texts that potentially can contain information about IT incidents were correctly identified from an experiment dataset by using the presented method. It is concluded that the identifying texts about IT incidents with automated methods like the one presented in this study is possible, but it requires some effort to set up.","1089-6503;10896503","Electronic:978-1-4673-7585-6; POD:978-1-4673-7586-3","10.1109/SEAA.2015.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7302476","IT-incident;machine learning;risk analysis;text classification","Accuracy;Cleaning;Machine learning algorithms;Risk analysis;Software engineering;Training;Yttrium","identification;learning (artificial intelligence);risk analysis;software engineering","IT incident identification;machine learning;risk analysis;software engineering;software-intensive system;system failure","","","","16","","","26-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction","W. Shi; Y. Zhu; J. Zhang; X. Tao; G. Sheng; Y. Lian; G. Wang; Y. Chen","Sch. of Electron. Inf. & Electr. Eng., Shanghai Jiao Tong Univ., Shanghai, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","20151130","2015","","","417","422","Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.","","Electronic:978-1-4799-8937-9; POD:978-1-4799-8938-6","10.1109/HPCC-CSS-ICESS.2015.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336197","machine learning;missing data prediction;power transformer;support vector machine (SVM)","Data mining;Data models;Feature extraction;Power grids;Predictive models;Support vector machines;Training","Big Data;computerised monitoring;data mining;interpolation;learning (artificial intelligence);mean square error methods;neural nets;power engineering computing;power grids;power system measurement;power transformers;standardisation;support vector machines","Big data technique;OR-MLF;OSVM;RSVM;data mining model;machine learning framework;mean square error method;missing data prediction;neural network method;optimizing support vector machine;power grid monitoring data quality;power transformer;refining SVM","","2","","22","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Implementation of a smartphone wireless accelerometer platform for establishing deep brain stimulation treatment efficacy of essential tremor with machine learning","R. LeMoyne; N. Tomycz; T. Mastroianni; C. McCandless; M. Cozza; D. Peduto","Department of Biological Sciences, Northern Arizona University, Flagstaff, 86011-5640 USA","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","6772","6775","Essential tremor (ET) is a highly prevalent movement disorder. Patients with ET exhibit a complex progressive and disabling tremor, and medical management often fails. Deep brain stimulation (DBS) has been successfully applied to this disorder, however there has been no quantifiable way to measure tremor severity or treatment efficacy in this patient population. The quantified amelioration of kinetic tremor via DBS is herein demonstrated through the application of a smartphone (iPhone) as a wireless accelerometer platform. The recorded acceleration signal can be obtained at a setting of the subject's convenience and conveyed by wireless transmission through the Internet for post-processing anywhere in the world. Further post-processing of the acceleration signal can be classified through a machine learning application, such as the support vector machine. Preliminary application of deep brain stimulation with a smartphone for acquisition of a feature set and machine learning for classification has been successfully applied. The support vector machine achieved 100% classification between deep brain stimulation in `on' and `off' mode based on the recording of an accelerometer signal through a smartphone as a wireless accelerometer platform.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319948","Accelerometer;Deep Brain Stimulation;Essential Tremor;Feedback;Machine Learning;Smartphone;Support Vector Machine;Wireless Accelerometer;iPhone","Acceleration;Accelerometers;Brain stimulation;Diseases;Support vector machines;Wireless communication;Wireless sensor networks","Internet;accelerometers;diseases;patient treatment;smart phones","Internet;deep brain stimulation treatment efficacy;essential tremor;iPhone;machine learning;medical management;movement disorder;smartphone;tremor severity;wireless accelerometer platform","","3","","20","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Predicting individual thermal comfort using machine learning algorithms","A. A. Farhan; K. Pattipati; B. Wang; P. Luh","Dept. of Comput. Sci. & Eng., Univ. of Connecticut, Storrs, CT, USA","2015 IEEE International Conference on Automation Science and Engineering (CASE)","20151008","2015","","","708","713","Human thermal sensation in an environment may be delayed, which may lead to life threatening conditions, such as hypothermia and hyperthermia. This is especially true for senior citizens, as aging alters the thermal perception in humans. We envision a decision support system that predicts human thermal comfort in real-time using various environmental conditions as well psychological and physiological features, and suggest corresponding actions, which can significantly improve overall thermal comfort and health of individuals, especially senior citizens. The key to realize this vision is an accurate thermal comfort model. We propose a novel machine learning based approach to learn an individual's thermal comfort model. This approach identifies the best set of features, and then learns a classifier that takes a feature vector as input and outputs a corresponding thermal sensation class (i.e. “feeling cold”, “neutral” and “feeling warm”). Evaluation using a large-scale publicly available data demonstrates that when using Support Vector Machines (SVM) classifiers, the accuracy of our approach is 76.7%, over two times higher than that of the widely adopted Fanger's model (which only achieves accuracy of 35.4%). In addition, our study indicates that two factors, a person's age and outdoor temperature that are not included in Fanger's model, play an important role in thermal comfort, which is a finding interesting in its own right.","2161-8070;21618070","Electronic:978-1-4673-8183-3; POD:978-1-4673-8184-0","10.1109/CoASE.2015.7294164","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294164","","Accuracy;Adaptation models;Machine learning algorithms;Support vector machines;Temperature sensors","human factors;learning (artificial intelligence);pattern classification;physiology;psychology;support vector machines","SVM classifiers;classifier learning;decision support system;environmental conditions;feature vector;feeling-cold sensation;feeling-warm sensation;human thermal sensation;large-scale publicly available data;machine learning algorithm;machine learning based approach;neutral sensation;outdoor temperature;person age;physiological features;psychological features;senior citizens;support vector machine classifiers;thermal comfort prediction;thermal perception","","","","22","","","24-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Replacing radiative transfer models by surrogate approximations through machine learning","J. Verrelst; J. P. Rivera; J. Gómez-Dans; G. Camps-Valls; J. Moreno","Image Processing Lab (IPL), University of Valencia, Spain","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","633","636","Physically-based radiative transfer models (RTMs) help in understanding the processes occurring on the Earth's surface and their interactions with vegetation and atmosphere. However, advanced RTMs can take a long computational time, which makes them unfeasible in many real applications. To overcome this problem, it has been proposed to substitute RTMs through so-called emulators. Emulators are statistical models that approximate the functioning of RTMs. They are advantageous in real practice because of the computational efficiency and excellent accuracy and flexibility for extrapolation. We here present an `Emulator toolbox' that enables analyzing three multi-output machine learning regression algorithms (MO-MLRAs) on their ability to approximate an RTM. As a proof of concept, a case study on emulating sun-induced fluorescence (SIF) is presented. The toolbox is foreseen to open new opportunities in the use of advanced RTMs, in which both consistent physical assumptions and data-driven machine learning algorithms live together.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7325843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325843","ARTMO;Emulator;FLEX;Fluorescence;multi-output regression algorithms","Accuracy;Approximation methods;Biological system modeling;Computational modeling;Fluorescence;Table lookup;Training","geophysical techniques;learning (artificial intelligence);radiative transfer","Earth surface;Emulator toolbox;computational efficiency;data-driven machine learning algorithms;multioutput machine learning regression algorithms;physically-based radiative transfer models;statistical models;sun-induced fluorescence;surrogate approximations","","","","5","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Comparison of Machine Learning Techniques for Vehicle Classification Using Road Side Sensors","D. Kleyko; R. Hostettler; W. Birk; E. Osipov","Dept. of Comput. Sci. Electr. & Space Eng., Lulea Univ. of Technol., Lulea, Sweden","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","572","577","The main contribution of this paper is a comparison of different machine learning algorithms for vehicle classification according to the ""Nordic system for intelligent classification of vehicles"" standard using measurements of road surface vibrations and magnetic field disturbances caused by vehicles. The algorithms considered are logistic regression, neural networks, and support vector machines. They are evaluated on a large dataset, consisting of 3074 samples and hence, a good estimate of the actual classification rate is obtained. The results show that for the considered classification problem logistic regression is the best choice with an overall classification rate of 93.4%.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.100","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313192","","Accelerometers;Magnetometers;Roads;Sensors;Standards;Support vector machines;Vehicles","intelligent transportation systems;learning (artificial intelligence);magnetic field measurement;neural nets;regression analysis;road vehicles;signal classification;support vector machines;traffic engineering computing;vibration measurement","Nordic system for intelligent classification of vehicles standard;logistic regression;machine learning algorithm;magnetic field disturbance;neural networks;road side sensors;road surface vibration measurement;support vector machines","","2","","14","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"Evaluating team behaviors constructed with human-guided machine learning","I. V. Karpov; L. M. Johnson; R. Miikkulainen","Dept. of Computer Science, The University of Texas at Austin, 2317 Speedway, 2.302, Austin, TX, 78712 USA","2015 IEEE Conference on Computational Intelligence and Games (CIG)","20151105","2015","","","292","298","Machine learning games such as NERO incorporate adaptive methods such as neuroevolution as an integral part of the gameplay by allowing the player to train teams of autonomous agents for effective behavior in challenging open-ended tasks. However, rigorously evaluating such human-guided machine learning methods and the resulting teams of agent policies can be challenging and is thus rarely done. This paper presents the results and analysis of a large scale online tournament between participants who evolved team agent behaviors and submitted them to be compared with others. An analysis of the teams submitted for the tournament indicates a complex, non-transitive fitness landscape, multiple successful strategies and training approaches, and performance above hand-constructed and random baselines. The tournament and analysis presented provide a practical way to study and improve human-guided machine learning methods and the resulting NPC team behaviors, potentially leading to better games and better game design tools in the future.","2325-4270;23254270","Electronic:978-1-4799-8622-4; POD:978-1-4799-8623-1","10.1109/CIG.2015.7317946","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317946","","Artificial intelligence;Correlation;Games;Neural networks;Stability criteria;Standards;Training","computer games;learning (artificial intelligence);neural nets","NERD;NPC team behaviors;adaptive methods;agent policies;autonomous agents;game design tools;gameplay;hand-constructed baselines;human-guided machine learning;large scale online tournament;multiple successful strategies;neuroevolution;nontransitive fitness landscape;open-ended tasks;random baselines;team agent behaviors;team behavior evaluation;transitive fitness landscape","","","","11","","","Aug. 31 2015-Sept. 2 2015","","IEEE","IEEE Conference Publications"
"An Energy-Efficient Nonvolatile In-Memory Computing Architecture for Extreme Learning Machine by Domain-Wall Nanowire Devices","Y. Wang; H. Yu; L. Ni; G. B. Huang; M. Yan; C. Weng; W. Yang; J. Zhao","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Nanotechnology","20151109","2015","14","6","998","1012","The data-oriented applications have introduced increased demands on memory capacity and bandwidth, which raises the need to rethink the architecture of the current computing platforms. The logic-in-memory architecture is highly promising as future logic-memory integration paradigm for high throughput data-driven applications. From memory technology aspect, as one recently introduced nonvolatile memory device, domain-wall nanowire (or race-track) not only shows potential as future power efficient memory, but also computing capacity by its unique physics of spintronics. This paper explores a novel distributed in-memory computing architecture where most logic functions are executed within the memory, which significantly alleviates the bandwidth congestion issue and improves the energy efficiency. The proposed distributed in-memory computing architecture is purely built by domain-wall nanowire, i.e., both memory and logic are implemented by domain-wall nanowire devices. As a case study, neural network-based image resolution enhancement algorithm, called DW-NN, is examined within the proposed architecture. We show that all operations involved in machine learning on neural network can be mapped to a logic-in-memory architecture by nonvolatile domain-wall nanowire. Domain-wall nanowire-based logic is customized for in machine learning within image data storage. As such, both neural network training and processing can be performed locally within the memory. The experimental results show that the domain-wall memory can reduce 92% leakage power and 16% dynamic power compared to main memory implemented by DRAM; and domain-wall logic can reduce 31% both dynamic and 65% leakage power under the similar performance compared to CMOS transistor-based logic. And system throughput in DW-NN is improved by 11.6x and the energy efficiency is improved by 56x when compared to conventional image processing system.","1536-125X;1536125X","","10.1109/TNANO.2015.2447531","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7128727","Domain wall;extreme learning machine;in-memory computing;nonvolatile memory","Magnetic tunneling;Memory architecture;Nanoscale devices;Nanowires;Nonvolatile memory;Random access memory","electric domain walls;learning (artificial intelligence);nanowires;neural nets;random-access storage","DW-NN;bandwidth congestion issue;computing capacity;data-oriented applications;distributed in-memory computing architecture;domain-wall nanowire devices;domain-wall nanowire-based logic;energy efficiency;future logic-memory integration paradigm;image data storage;logic functions;logic-in-memory architecture;machine learning;neural network training;neural network-based image resolution enhancement algorithm;nonvolatile domain-wall nanowire;nonvolatile memory device","","10","","45","","20150619","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"A machine learning pipeline for multiple sclerosis course detection from clinical scales and patient reported outcomes","S. Fiorini; A. Verri; A. Tacchino; M. Ponzio; G. Brichetto; A. Barla","DIBRIS, University of Genoa, 16146, Italy","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","4443","4446","In this work we present a machine learning pipeline for the detection of multiple sclerosis course from a collection of inexpensive and non-invasive measures such as clinical scales and patient-reported outcomes. The proposed analysis is conducted on a dataset coming from a clinical study comprising 457 patients affected by multiple sclerosis. The 91 collected variables describe patients mobility, fatigue, cognitive performance, emotional status, bladder continence and quality of life. A preliminary data exploration phase suggests that the group of patients diagnosed as Relapsing-Remitting can be isolated from other clinical courses. Supervised learning algorithms are then applied to perform feature selection and course classification. Our results confirm that clinical scales and patient-reported outcomes can be used to classify Relapsing-Remitting patients.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319381","","Accuracy;Algorithm design and analysis;Bladder;Correlation;Multiple sclerosis;Pipelines","diseases;health care;learning (artificial intelligence);patient diagnosis","clinical scales;machine learning pipeline;multiple sclerosis course;multiple sclerosis course detection;patient reported outcomes;supervised learning algorithm","","","","15","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"AMSR2 soil moisture downscaling using multisensor products through machine learning approach","S. Park; J. Im; S. Park; J. Rhee","School of Urban Environmental Engineering, Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1984","1987","Soil moisture is important to understand the interaction between the land and the atmosphere, and has an influence on hydrological and agricultural processes such as drought and crop yield. In-situ measurements at stations have been used to monitor soil moisture. However, data measured in the field are point-based and difficult to represent spatial distribution of soil moisture. Remote sensing techniques using microwave sensors provide spatially continuous soil moisture. The spatial resolution of remotely sensed soil moisture based on typical passive microwave sensors is coarse (e.g., tens of kilometers), which is inadequate for local or regional scale studies. In this study, AMSR2 soil moisture was downscaled to 1km using MODIS products that are closely related to soil moisture through statistical ordinary least squares (OLS) and random forest (RF) machine learning approaches. RF (r<sup>2</sup>=0.96, rmse=0.06) outperformed OLS (r<sup>2</sup>=0.47, rmse=0.16) in modeling soil moisture possibly because RF is much flexible through randomization and adopts an ensemble approach. Both approaches identified T·V (i.e., multiplication between land surface temperature and normalized difference vegetation index) and evapotranspiration. AMSR2 soil moisture produced from the VUA-NASA algorithm appeared overestimated at high elevation areas because the characteristics of ground data for validation and correction used in the algorithm were different from those in our study area. In future study, AMSR2 soil moisture based on the JAXA algorithm will be evaluated with additional input variables including land cover, elevation and precipitation.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326186","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326186","AMSR2;MODIS;Soil moisture;downscaling;machine learning","MODIS;Microwave radiometry;Radio frequency;Remote sensing;Soil measurements;Soil moisture;Spatial resolution","evaporation;learning (artificial intelligence);least squares approximations;moisture;remote sensing;soil;transpiration","AMSR2 soil moisture downscaling;JAXA algorithm;MODIS product;RF machine learning approach;VUA-NASA algorithm;agricultural process;crop yield;drought;evapotranspiration;hydrological process;land cover;land surface temperature;multisensor product;normalized difference vegetation index;passive microwave sensor;precipitation;random forest;remote sensing technique;soil moisture modeling;soil moisture monitoring;soil moisture spatial distribution;statistical ordinary least square","","","","13","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Pruned search: A machine learning based meta-heuristic approach for constrained continuous optimization","R. Liu; A. Agrawal; W. k. Liao; A. Choudhary; Z. Chen","EECS Department, Northwestern University, Evanston, IL USA","2015 Eighth International Conference on Contemporary Computing (IC3)","20151207","2015","","","13","18","Searching for solutions that optimize a continuous function can be difficult due to the infinite search space, and can be further complicated by the high dimensionality in the number of variables and complexity in the structure of constraints. Both deterministic and stochastic methods have been presented in the literature with a purpose of exploiting the search space and avoiding local optima as much as possible. In this research, we develop a machine learning framework aiming to `prune' the search effort of both types of optimization techniques by developing meta-heuristics, attempting to knowledgeably reordering the search space and reducing the search region. Numerical examples demonstrate that this approach can effectively find the global optimal solutions and significantly reduce the computational time for seven benchmark problems with variable dimensions of 100, 500 and 1000, compared to Genetic Algorithms.","","CD-ROM:978-1-4673-7946-5; Electronic:978-1-4673-7948-9; POD:978-1-4673-7949-6","10.1109/IC3.2015.7346645","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346645","complexity reduction;constrained optimization;machine learning;meta-heuristics","Approximation methods;Complexity theory;Computational geometry;Data mining;Linear programming;Optimization;Search problems","genetic algorithms;heuristic programming;learning (artificial intelligence);mathematics computing;search problems;stochastic processes","computational time;constrained continuous optimization;continuous function;deterministic methods;genetic algorithms;infinite search space;local optima;machine learning based metaheuristic approach;optimization techniques;pruned search;search space;stochastic methods","","","","18","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Indoor positioning by distributed machine-learning based data analytics on smart gateway network","Yuehua Cai; S. K. Rai; Hao Yu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798","2015 International Conference on Indoor Positioning and Indoor Navigation (IPIN)","20151207","2015","","","1","8","Real-time data analysis on sensor nodes is challenging due to limited computing resources. A changing environment where received signal strength (RSSI) varies with time makes it more complex to update position predictors for real-time indoor positioning. Based on the distributed collection and analytics of RSSI values in a gateway network, a time-efficient workload-based (WL) distributed support vector machine (WL-DSVM) algorithm is introduced in this paper to perform the indoor positioning. Experimental results show that with 5 distributed sensor nodes running in parallel, the proposed WL-DSVM can achieve a performance improvement in run time up to 3.2× with a stable positioning accuracy.","","Electronic:978-1-4673-8402-5; POD:978-1-4673-8403-2","10.1109/IPIN.2015.7346934","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346934","distributed support vector machine;indoor positioning;real-time RSSI analytics","Data analysis;IEEE 802.11 Standard;Logic gates;Prediction algorithms;Real-time systems;Support vector machines;Training","building management systems;data analysis;distributed sensors;indoor navigation;learning (artificial intelligence);support vector machines","RSSI;WL distributed support vector machine algorithm;WL-DSVM algorithm;data analytics;distributed collection;distributed machine-learning;distributed sensor nodes;indoor positioning;position predictors;real-time data analysis;received signal strength;smart gateway network;workload-based distributed support vector machine algorithm","","","","19","","","13-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"The failure analysis of extreme learning machine on big data and the counter measure","P. Z. Zhang; S. X. Zhao; X. Z. Wang","College of Mathematics and Information Science, Hebei University, Baoding 071002, China","2015 International Conference on Machine Learning and Cybernetics (ICMLC)","20151203","2015","2","","849","853","Extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) was known for its extremely fast learning speed while maintaining acceptable generalization. Unfortunately, the failure of ELM on big data occurs frequently. The course is, the main computation of ELM focus on the calculation of generalized inverse of hidden layer output matrix, which depends on singular value decomposition (SVD) and has very low efficiency especially on high order matrix. In view of this high calculation complexity directly courses the failure of ELM on big data, normal equation extreme learning machine is proposed, which use the normal equation to reduce the size of the matrix equation and overcome the failure. The experiments on benchmarks show that the new proposed model has better performance than the ELM, so as to have more potential for large scale data learning.","","CD-ROM:978-1-4673-7220-6; Electronic:978-1-4673-7221-3; POD:978-1-4673-7222-0","10.1109/ICMLC.2015.7340664","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340664","Extreme learning machine;Generalized inverse;Normal equation;SVD","","Big Data;failure analysis;feedforward neural nets;learning (artificial intelligence);matrix algebra;singular value decomposition","Big Data;ELM;SLFN;SVD;counter measure;extreme learning machine;failure analysis;hidden layer output matrix;high calculation complexity;high order matrix;large scale data learning;matrix equation;normal equation;single-hidden layer feedforward neural networks;singular value decomposition","","","","15","","","12-15 July 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach for Big Data in Oil and Gas Pipelines","A. Mohamed; M. S. Hamdi; S. Tahar","Inf. Syst. Dept., ABMMC, Doha, Qatar","2015 3rd International Conference on Future Internet of Things and Cloud","20151026","2015","","","585","590","Experienced pipeline operators utilize Magnetic Flux Leakage (MFL) sensors to probe oil and gas pipelines for the purpose of localizing and sizing different defect types. A large number of sensors is usually used to cover the targeted pipelines. The sensors are equally distributed around the circumference of the pipeline, and every three millimeters the sensors measure MFL signals. Thus, the collected raw data is so big that it makes the pipeline probing process difficult, exhausting and error-prone. Machine learning approaches such as neural networks have made it possible to effectively manage the complexity pertaining to big data and learn their intrinsic properties. We concentrate, in this work, on the applicability of artificial neural networks in defect depth estimation and present a detailed study of various network architectures. Discriminant features, which characterize different defect depth patterns, are first obtained from the raw data. Neural networks are then trained using these features. The Levenberg-Marquardt back-propagation learning algorithm is adopted in the training process, during which the weight and bias parameters of the networks are tuned to optimize their performances. Compared with the performance of pipeline inspection techniques reported by service providers such as GE and ROSEN, the results obtained using the method we proposed are promising. For instance, within ±10% error-tolerance range, the proposed approach yields an estimation accuracy at 86%, compared to only 80% reported by GE, and within ±15% error-tolerance range, it yields an estimation accuracy at 89% compared to 80% reported by ROSEN.","","Electronic:978-1-4673-8103-1; POD:978-1-4673-8104-8","10.1109/FiCloud.2015.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300871","big data;machine learning;magnetic flux leakage;neural networks;pipeline inspection","Accuracy;Estimation;Feature extraction;Neural networks;Neurons;Pipelines;Sensors","Big Data;backpropagation;condition monitoring;magnetic flux;magnetic leakage;magnetic sensors;neural net architecture;pipelines;production engineering computing","Big Data;Levenberg-Marquardt backpropagation learning algorithm;MFL sensors;MFL signal measurement;artificial neural networks;defect depth estimation;defect depth patterns;discriminant features;gas pipelines;machine learning approach;magnetic flux leakage sensors;neural network architectures;neural network training process;oil pipelines;pipeline inspection techniques;pipeline probing process;service providers","","1","","19","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Automated classification of author's sentiments in citation using machine learning techniques: A preliminary study","I. C. Kim; G. R. Thoma","Lister Hill National Center for Biomedical Communications, National Library of Medicine, 8600 Rockville Pike, Bethesda, MD 20894","2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)","20151019","2015","","","1","7","Scientific papers generally include citations to external sources such as journal articles, books, or Web links to refer to works that are related in an important way to the research. The reason for the citation appears within the sentences surrounding the citation tag in the body text, and represents the relationship between the citation and cited works as supportive, contrastive, corrective, etc. This could be an important clue for researchers seeking relevant previous work or approaches for a certain research purpose. We propose to develop an automated method to identify the citing author's sentiments toward the cited external sources expressed in citation sentences using machine-learning techniques and linguistic cues. As a preliminary study, this paper presents a support vector machine (SVM)-based text categorization technique to classify the author's sentiments specifically toward Comment-on (CON) articles. CON, a MEDLINE citation field, indicates previously published articles commented on by authors of a given article expressing possibly complimentary or contradictory opinions. An SVM with a radial basis kernel function (RBF) is implemented, and Input feature vectors for the SVM are created based on n-grams word statistics representing the distribution of words in CON sentences. Experiments conducted on a set of CON sentences collected from 414 different online biomedical journal titles show that the SVM with a RBF yields the best result for an input feature vector combining uni-gram and bi-gram word statistics.","","Electronic:978-1-4799-6926-5; POD:978-1-4799-6927-2","10.1109/CIBCB.2015.7300319","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300319","“Comment-on”;Citation analysis;MEDLINE;author's sentiments;n-grams word statistics;support vector machine","Accuracy;Citation analysis;Dictionaries;Kernel;Support vector machines;Text categorization;Training","biology computing;citation analysis;feature extraction;learning (artificial intelligence);radial basis function networks;support vector machines","MEDLINE citation field;SVM;Web links;author's sentiments;automated classification;bi-gram word statistics;biomedical journal titles;body text;books;citation sentences;citation tag;comment-on articles;complimentary opinions;contradictory opinions;input feature vector;input feature vectors;journal articles;machine learning techniques;n-grams word statistics;radial basis kernel function;support vector machine-based text categorization technique;uni-gram word statistics","","","","20","","","12-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Image-based object classification of defects in steel using data-driven machine learning optimization","F. Bürger; C. Buck; J. Pauli; W. Luther","Lehrstuhl Intelligente Systeme, Universit&#x00E4;t Duisburg-Essen, Bismarckstra&#x00DF;e 90, 47057, Germany","2014 International Conference on Computer Vision Theory and Applications (VISAPP)","20151012","2014","2","","143","152","In this paper we study the optimization process of an object classification task for an image-based steel quality measurement system. The goal is to distinguish hollow from solid defects inside of steel samples by using texture and shape features of reconstructed 3D objects. In order to optimize the classification results we propose a holistic machine learning framework that should automatically answer the question “How well do state-of-the-art machine learning methods work for my classification problem?” The framework consists of three layers, namely feature subset selection, feature transform and classifier which subsequently reduce the data dimensionality. A system configuration is defined by feature subset, feature transform function, classifier concept and corresponding parameters. In order to find the configuration with the highest classifier accuracies, the user only needs to provide a set of feature vectors and ground truth labels. The framework performs a totally data-driven optimization using partly heuristic grid search. We incorporate several popular machine learning concepts, such as Principal Component Analysis (PCA), Support Vector Machines (SVM) with different kernels, random trees and neural networks. We show that with our framework even non-experts can automatically generate a ready for use classifier system with a significantly higher accuracy compared to a manually arranged system.","","Electronic:978-9-8975-8133-5; POD:978-1-4799-7686-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294923","Algorithm Recommendation;Data-driven Hyper-parameter Optimization;Object Classification","Kernel;Optimization;Principal component analysis;Solids;Steel;Support vector machines;Transforms","","","","","","21","","","5-8 Jan. 2014","","IEEE","IEEE Conference Publications"
"Learning to look up: Realtime monocular gaze correction using machine learning","D. Kononenko; V. Lempitsky","Skolkovo Institute of Science and Technology (Skoltech), Novaya Street 100, Moscow Region, Russia","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","4667","4675","We revisit the well-known problem of gaze correction and present a solution based on supervised machine learning. At training time, our system observes pairs of images, where each pair contains the face of the same person with a fixed angular difference in gaze direction. It then learns to synthesize the second image of a pair from the first one. After learning, the system gets the ability to redirect the gaze of a previously unseen person by the same angular difference as in the training set. Unlike many previous solutions to gaze problem in videoconferencing, ours is purely monocular, i.e. it does not require any hardware apart from an in-built web-camera of a laptop. Being based on efficient machine learning predictors such as decision forests, the system is fast (runs in real-time on a single core of a modern laptop). In the paper, we demonstrate results on a variety of videoconferencing frames and evaluate the method quantitatively on the hold-out set of registered images. The supplementary video shows example sessions of our system at work.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7299098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299098","","Cameras;Face;Real-time systems;Training;Training data;Vegetation","gaze tracking;image registration;learning (artificial intelligence);teleconferencing;video communication","Web-camera;fixed angular difference;image pairs;realtime monocular gaze correction;registered images;supervised machine learning;training set;videoconferencing","","1","","26","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Efficient autism spectrum disorder prediction with eye movement: A machine learning framework","W. Liu; X. Yu; B. Raj; L. Yi; X. Zou; M. Li","SYSU-CMU Joint Inst. of Eng., Sun Yat-sen University, Guangzhou, China 510006","2015 International Conference on Affective Computing and Intelligent Interaction (ACII)","20151207","2015","","","649","655","We propose an autism spectrum disorder (ASD) prediction system based on machine learning techniques. Our work features the novel development and application of machine learning methods over traditional ASD evaluation protocols. Specifically, we are interested in discovering the latent patterns that possibly indicate the symptom of ASD underneath the observations of eye movement. A group of subjects (either ASD or non-ASD) are shown with a set of aligned human face images, with eye gaze locations on each image recorded sequentially. An image-level feature is then extracted from the recorded eye gaze locations on each face image. Such feature extraction process is expected to capture discriminative eye movement patterns related to ASD. In this work, we propose a variety of feature extraction methods, seeking to evaluate their prediction performance comprehensively. We further propose an ASD prediction framework in which the prediction model is learned on the labeled features. At testing stage, a test subject is also asked to view the face images with eye gaze locations recorded. The learned model predicts the image-level labels and a threshold is set to determine whether the test subject potentially has ASD or not. Despite the inherent difficulty of ASD prediction, experimental results indicates statistical significance of the predicted results, showing promising perspective of this framework.","","Electronic:978-1-4799-9953-8; POD:978-1-4799-9954-5; USB:978-1-4799-9952-1","10.1109/ACII.2015.7344638","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344638","autism spectrum disorder;bag-of-words;eye tracking;support vector machine","Dictionaries;Face;Feature extraction;Histograms;Kernel;Support vector machines;Visualization","eye;face recognition;feature extraction;gaze tracking;learning (artificial intelligence);medical computing;medical disorders;psychology","ASD evaluation protocol;ASD prediction system;ASD symptom;autism spectrum disorder prediction;discriminative eye movement pattern;eye gaze location;eye movement observation;human face images;image-level feature extraction;image-level label;latent pattern discovery;machine learning technique","","","","15","","","21-24 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine learning based social media recommendation","T. Lai; X. Zheng","College of Mathematics and Computer Science, Fuzhou University, Fuzhou 350108, China","2015 2nd IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services (ICSDM)","20151015","2015","","","28","32","In view of the problems existing in traditional recommendation algorithm of low accuracy and low efficiency, this paper presents a machine learning based social media recommendation algorithm. The algorithm is based on the traditional personalized collaborative filtering algorithm, and combines with the correlation characteristics among users in a social network. Besides, the algorithm also considers the network rating factors and upgrade its efficiency by using clustering algorithm. At last, the algorithm is realized on the Hadoop cloud platform.","","Electronic:978-1-4799-7749-9; POD:978-1-4799-7750-5","10.1109/ICSDM.2015.7298020","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298020","Clustering;Collaborative Filtering;MapReduce;Recommendation;Social Network","Algorithm design and analysis;Blogs;Clustering algorithms;Collaboration;Filtering;Machine learning algorithms;Social network services","cloud computing;collaborative filtering;learning (artificial intelligence);pattern clustering;recommender systems;social networking (online)","Hadoop cloud platform;clustering algorithm;correlation characteristics;machine learning;network rating factors;personalized collaborative filtering algorithm;social media recommendation","","","","18","","","8-10 July 2015","","IEEE","IEEE Conference Publications"
"Machine Learning for Imbalanced Datasets of Recognizing Inference in Text with Linguistic Phenomena","M. Y. Day; C. C. Tsai","Dept. of Inf. Manage., Tamkang Univ., Taipei, Taiwan","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","562","568","Recognizing inference in text (RITE) plays an important role in the answer validation modules for a Question Answering (QA) system. The problem of class imbalance has received increased attention in the machine learning community. In recent years, several attempts have been made on the linguistic phenomena analysis, however, little is known about the effects of imbalanced datasets with linguistic phenomenon in recognizing inference in text. The objective of this paper is to provide an empirical study on learning imbalanced datasets of recognizing inference in text with linguistic phenomena for a better understanding of the effects of imbalanced datasets with linguistic phenomenon in recognizing inference in text. In this paper, we proposed an analysis of imbalanced datasets of recognizing inference in text with linguistic phenomena using NTCIR 11 RITE-VAL gold standard dataset and development dataset. The experimental results suggest that the distribution of imbalanced datasets of recognizing inference in text with linguistic phenomenon could be dramatically varied on the performance of a machine learning classifier.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.99","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301027","Imbalanced Datasets;Linguistic Phenomena;Machine Learning;Recognizing Inference in Text;Textual Entailment","Accuracy;Gold;Pragmatics;Semantics;Standards;Text recognition;Yttrium","inference mechanisms;learning (artificial intelligence);linguistics;pattern classification;question answering (information retrieval);text analysis","QA system;RITE;imbalanced dataset;linguistic phenomena analysis;machine learning classifier;question answering system;recognizing inference in text","","","","29","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Analysis of XDMoD/SUPReMM Data Using Machine Learning Techniques","S. M. Gallo; J. P. White; R. L. DeLeon; T. R. Furlani; H. Ngo; A. K. Patra; M. D. Jones; J. T. Palmer; N. Simakov; J. M. Sperhac; M. Innus; T. Yearke; R. Rathsam","Center for Comput. Res., State Univ. of New York, Buffalo, NY, USA","2015 IEEE International Conference on Cluster Computing","20151029","2015","","","642","649","Machine learning techniques were applied to job accounting and performance data for application classification. Job data were accumulated using the XDMoD monitoring technology named SUPReMM, they consist of job accounting information, application information from Lariat/XALT, and job performance data from TACC_Stats. The results clearly demonstrate that community applications have characteristic signatures which can be exploited for job classification. We conclude that machine learning can assist in classifying jobs of unknown application, in characterizing the job mixture, and in harnessing the variation in node and time dependence for further analysis.","1552-5244;15525244","Electronic:978-1-4673-6598-7; POD:978-1-4673-6599-4","10.1109/CLUSTER.2015.114","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307663","HPC monitoring;Open XDMoD;SUPReMM;TACC_Stats;XDMoD;application classification;machine learning","Clocks;Data collection;Kernel;Monitoring;Quality of service;Radio frequency","data analysis;learning (artificial intelligence);pattern classification","XDMoD monitoring technology;XDMoD-SUPReMM data analysis;application classification;job accounting information;job classification;machine learning techniques","","1","","18","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"A machine learning methodology for medical imaging anonymization","E. Monteiro; C. Costa; J. L. Oliveira","Univ. of Aveiro, Portugal","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","1381","1384","Privacy protection is a major requirement for the complete success of EHR systems, becoming even more critical in collaborative scenarios, where data is shared among institutions and practitioners. While textual data can be easily de-identified, patient data in medical images implies a more elaborate approach. In this work we present a solution for sensitive word identification in medical images based on a combination of two machine-learning models, achieving a F1-score of 0.94. Three experts evaluated the system performance. They analyzed the output of the present methodology and categorized the studies in three groups: studies that had their sensitive words removed (true positive), studies with complete patient identity (false negative) and studies with mistakenly removed data (false positive). The experts were unanimous regarding the relevance of the present tool in collaborative medical environments, as it may improve the exchange of anonymized patient data between institutions.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318626","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318626","","DICOM;Image recognition;Metadata;Optical character recognition software;Pipelines;Text recognition","biomedical imaging;data privacy;learning (artificial intelligence)","collaborative medical environments;false negative;false positive;machine learning methodology;medical images;medical imaging anonymization;patient identity;sensitive word identification","","","","13","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Machine learning based multi-physical-model blending for enhancing renewable energy forecast - improvement via situation dependent error correction","S. Lu; Youngdeok Hwang; I. Khabibrakhmanov; F. J. Marianno; Xiaoyan Shao; J. Zhang; B. M. Hodge; H. F. Hamann","IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA","2015 European Control Conference (ECC)","20151123","2015","","","283","290","With increasing penetration of solar and wind energy to the total energy supply mix, the pressing need for accurate energy forecasting has become well-recognized. Here we report the development of a machine-learning based model blending approach for statistically combining multiple meteorological models for improving the accuracy of solar/wind power forecast. Importantly, we demonstrate that in addition to parameters to be predicted (such as solar irradiance and power), including additional atmospheric state parameters which collectively define weather situations as machine learning input provides further enhanced accuracy for the blended result. Functional analysis of variance shows that the error of individual model has substantial dependence on the weather situation. The machine-learning approach effectively reduces such situation dependent error thus produces more accurate results compared to conventional multi-model ensemble approaches based on simplistic equally or unequally weighted model averaging. Validation over an extended period of time results show over 30% improvement in solar irradiance/power forecast accuracy compared to forecasts based on the best individual model.","","Electronic:978-3-9524-2693-7; POD:978-1-4673-7160-5","10.1109/ECC.2015.7330558","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7330558","","Accuracy;Atmospheric modeling;Clouds;Numerical models;Predictive models;Wind forecasting","learning (artificial intelligence);load forecasting;meteorology;power engineering computing;solar power;statistical analysis;wind power","analysis of variance;atmospheric state parameter;energy forecasting;energy supply mix;machine learning based multiphysical-model blending;meteorological model;model averaging;multimodel ensemble;renewable energy forecast;situation dependent error correction;solar energy;solar irradiance/power forecast accuracy;solar/wind power forecast;substantial dependence;weather situation;wind energy","","5","","40","","","15-17 July 2015","","IEEE","IEEE Conference Publications"
"Recognizing Semantic Locations from Smartphone Log with Combined Machine Learning Techniques","H. Xu; S. B. Cho","Dept. of Comput. Sci., Yonsei Univ., Seoul, South Korea","2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops","20151026","2014","","","66","71","Smartphones, equipped with powerful processors, accelerometers, compasses and Global Positioning Systems (GPS) receivers, have favored the increase of location and context-based services over the last years. Many researchers have attempted to recognize user's semantic location by various methods. The traditional semantic location recognition methods require partitioning the location and registering all the information to construct Wi-Fi map for fine localization, which is not practical in daily life and continuous attempt for recognizing semantic location makes smartphone battery last shorter time. Even worse, they have low accuracy when recognizing the locations located near to each other. In this paper, we propose a hybrid location recognition approach. The proposed method combines k-nearest neighbor with decision tree to recognize semantic locations. It consists of moving status detection, indoor/outdoor environment check and location recognition which consists of k-nearest neighbor (kNN) and decision tree. The proposed method can be used in indoor or dense urban environments where traditional approaches fail. Finally, the proposed method is practically developed over Android smartphones and tested in terms of performance. The experiments show the usefulness of the proposed semantic location recognition method.","","CD-ROM:978-1-4799-7645-4; Electronic:978-1-4799-7646-1; POD:978-1-4799-7647-8","10.1109/UIC-ATC-ScalCom.2014.128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306935","decision tree;k nearest neighbors;location recognition;semantic location","Acceleration;Classification algorithms;Decision trees;Global Positioning System;IEEE 802.11 Standard;Magnetic fields;Semantics","decision trees;learning (artificial intelligence);mobile computing;smart phones;system monitoring","Android smartphones;decision tree;hybrid location recognition;indoor-outdoor environment check;k-nearest neighbor;kNN;machine learning;moving status detection;semantic location recognition method","","","","13","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Cluster Regularized Extreme Learning Machine for Detecting Mixed-Type Distraction in Driving","T. Liu; Y. Yang; G. B. Huang; Z. Lin; F. Klanner; C. Denk; R. H. Rasshofer","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","1323","1326","Distraction was previously studied within each dimension separately, i.e., physical, cognitive and visual. However real-world activities usually involve multiple distraction dimensions in terms of brain resources that might conflict with the driving task. This brings difficulties for classifying dimension/type of distraction even for human experts. On the other hand, many subsequent functional blocks do not utilize distraction type information. For example, a pre-collision system usually makes decision based on distraction level rather than distraction type. Therefore this study aims to detect distraction in general regardless of its type, and proposes an effective machine learning algorithm, i.e., Cluster Regularized Extreme Learning Machine (CR-ELM), to detect mixed-type distraction in driving. Compared to traditional machine learning techniques, CR-ELM is designed to handle problems with multiple clusters per class, and provides more accurate detection performance, which could be used for advanced driver assistance systems.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.217","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313309","","Error analysis;Neurons;Support vector machines;Training;Training data;Vehicles;Visualization","driver information systems;learning (artificial intelligence)","advanced driver assistance systems;brain resources;cluster regularized extreme learning machine;human experts;machine learning algorithm;mixed-type driving distraction detection;multiple distraction dimensions;pre-collision system;utilize distraction type information","","1","","15","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"Personalized learning materials for children with special needs using machine learning","L. Banik; M. Bhuiyan; A. Jahan","IIT, University of Dhaka, Centre for Research on Applied Technology, Entrepreneurship and Development, UK","2015 Internet Technologies and Applications (ITA)","20151105","2015","","","169","174","Children with neurological disorder such as autism need personalized development system for their daily activities. Technology can play a significant role. This paper introduces a research to deliver personalized learning materials for children with special needs based on diverse characteristics of children. There are four parts of the system: i) identifying level of the user by using machine learning algorithm. ii) web mining to generate multimodal learning materials from text story or learning keywords, iii) linking user preferences and IoT enabled sensor data with the result, iv) personalizing contents for users delineated with an intelligent interface. This paper explains personalization of results using machine learning algorithm.","","Electronic:978-1-4673-9557-1; POD:978-1-4799-8037-6","10.1109/ITechA.2015.7317390","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317390","children with special needs;intelligent interface;learning development;machine learning;web mining","Accuracy;Flowcharts;Nonhomogeneous media;Writing","Internet;Internet of Things;data mining;handicapped aids;learning (artificial intelligence);medical disorders;user interfaces","IoT enabled sensor data;Web mining;autism;children with special need;intelligent interface;learning keyword;machine learning algorithm;multimodal learning material;neurological disorder;personalized development system;personalized learning material;personalizing content;text story","","1","","10","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"An Improved Machine Learning Scheme for Data-Driven Fault Diagnosis of Power Grid Equipment","J. Zhang; Y. Zhu; W. Shi; G. Sheng; Y. Chen","Sch. of Electron. Inf. & Electr. Eng., Shanghai Jiao Tong Univ., Shanghai, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","20151130","2015","","","1737","1742","In recent power grid systems, data-driven approach has been taken to grid condition evaluation and classification after successful adoption of big data techniques in internet applications. However, the raw training data from single monitoring system, e.g. dissolved gas analysis (DGA), are rarely sufficient for training in the form of valid instances and the data quality can rarely meet the requirement of precise data analytics since raw data set usually contains samples with noisy data. This paper proposes a machine learning scheme (PCA_IR) to improve the accuracy of fault diagnose, which combines dimension-increment procedure based on association analysis, dimension-reduction procedure based on principal component analysis and back propagation neural network (BPNN). First, the dimension of training data is increased by adding selected data which originates from different source such as production management system (PMS) to the original data obtained by DGA. The added data would also inevitably result in more noise. Thus, we then take advantage of the PCA method to reduce the noise in the training data as well as retaining significant information for classification. Finally, the new training data yielded after PCA procedure is inputted into BPNN for classification. We test the PCA_IR scheme on fault diagnosis of power transformers in power grid system. The experimental results show that the classifiers based on our scheme achieve higher accuracy than traditional ones. Therefore, the scheme PCA_IR would be successfully deployed for fault diagnosis in power grid system.","","Electronic:978-1-4799-8937-9; POD:978-1-4799-8938-6","10.1109/HPCC-CSS-ICESS.2015.236","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336422","BPNN;PCA;PCC;transformer fault diagnosis","Accuracy;Correlation;Correlation coefficient;Fault diagnosis;Power grids;Power transformers;Principal component analysis","backpropagation;fault diagnosis;learning (artificial intelligence);neural nets;pattern classification;power engineering computing;power grids;power transformers;principal component analysis","BPNN;DGA;PCA_IR scheme;PMS;association analysis;back propagation neural network;data analytics;data quality;data-driven fault diagnosis;dimension-increment procedure;dimension-reduction procedure;fault diagnosis accuracy improvement;grid condition classification;grid condition evaluation;improved machine learning scheme;noise reduction;noisy data;power grid equipment;power transformers;principal component analysis;production management system;raw training data;training data dimension","","1","","20","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"A monitoring system to prepare machine learning data sets for earthquake prediction based on seismic-acoustic signals","A. Vahaplar; B. T. Tezel; R. Nasiboglu; E. Nasibov","Dept. of Computer Science, Dokuz Eyl&#x00FC;l University, &#x0130;zmir / T&#x00FC;rkiye","2015 9th International Conference on Application of Information and Communication Technologies (AICT)","20151130","2015","","","44","47","Estimating the location, time and magnitude of a possible earthquake has been the subject of many studies. Various methods have been tried using many input variables such as temperature changes, seismic movements, weather conditions etc. The relation between recorded seismic-acoustic data and occurring an anomalous seismic processes (ASP) has been proved in articles written by Aliev and et al. [1-4]. But it is difficult to predict the location, time and magnitude of the earthquake by using these data. In this study, it is aimed to prepare a data set/sets for prediction of an earthquake to be used in machine learning algorithms. An Earthquake-Well Signal Monitoring Software has been developed to construct these data sets. This study uses the on-line recordings of robust noise monitoring (RNM) signals of ASP from stations in Azerbaijan. An interface for analyzing the recordings and mapping them with previous earthquakes is designed.","","Electronic:978-1-4673-6856-8; POD:978-1-4673-6857-5","10.1109/ICAICT.2015.7338513","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338513","Seismic-acoustic signal analysis;anomalous seismic processes;data visualization;robust noise monitoring technology","Computer science;Databases;Earthquakes;Monitoring;Robustness;Software;Training","earthquakes;geophysical techniques;learning (artificial intelligence)","ASP;Azerbaijan;Earthquake prediction;anomalous seismic processes;machine learning algorithms;machine learning data sets;monitoring system;robust noise monitoring;seismic movements;seismic-acoustic signals;temperature changes","","","","6","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning Based Approach to <italic>de novo</italic> Sequencing of Glycans from Tandem Mass Spectrometry Spectrum","S. Kumozaki; K. Sato; Y. Sakakibara","Department of Biosciences and Informatics, Keio University, Japan","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20151207","2015","12","6","1267","1274","Recently, glycomics has been actively studied and various technologies for glycomics have been rapidly developed. Currently, tandem mass spectrometry (MS/MS) is one of the key experimental tools for identification of structures of oligosaccharides. MS/MS can observe MS/MS peaks of fragmented glycan ions including cross-ring ions resulting from internal cleavages, which provide valuable information to infer glycan structures. Thus, the aim of de novo sequencing of glycans is to find the most probable assignments of observed MS/MS peaks to glycan substructures without databases. However, there are few satisfiable algorithms for glycan de novo sequencing from MS/MS spectra. We present a machine learning based approach to de novo sequencing of glycans from MS/MS spectrum. First, we build a suitable model for the fragmentation of glycans including cross-ring ions, and implement a solver that employs Lagrangian relaxation with a dynamic programming technique. Then, to optimize scores for the algorithm, we introduce a machine learning technique called structured support vector machines that enable us to learn parameters including scores for cross-ring ions from training data, i.e., known glycan mass spectra. Furthermore, we implement additional constraints for core structures of well-known glycan types including N-linked glycans and O-linked glycans. This enables us to predict more accurate glycan structures if the glycan type of given spectra is known. Computational experiments show that our algorithm performs accurate de novo sequencing of glycans. The implementation of our algorithm and the datasets are available at http://glyfon.dna.bio.keio.ac.jp/.","1545-5963;15455963","","10.1109/TCBB.2015.2430317","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102732","Glycan structure;MS/MS spectrum;glycan structure;structured SVM","Bioinformatics;Computational biology;Dynamic programming;Glycomics;Ions;Linear programming;Sequential analysis","biology computing;dynamic programming;learning (artificial intelligence);mass spectroscopy;molecular biophysics;molecular configurations;organic compounds;support vector machines","Lagrangian relaxation;MS-MS spectra;N-linked glycans;O-linked glycans;computational experiments;core structures;cross-ring ions;dynamic programming;glycan de novo sequencing;glycan fragmentation;glycan substructures;machine learning based approach;structured support vector machines;tandem mass spectrometry spectrum","0","0","","25","","20150506","Nov.-Dec. 1 2015","","IEEE","IEEE Journals & Magazines"
"Towards a Generic Trust Management Framework Using a Machine-Learning-Based Trust Model","J. López; S. Maag","Inst. Mines Telecom, Telecom SudParis, Evry, France","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","1343","1348","Nowadays, the ever-growing capabilities in computer communication networks have entitled and encouraged developers and researchers to build collaborative applications, systems, and devices. On the one hand with increased collaboration, several advantages have been obtained, but, on the other hand, issues may arise due to untrustworthy interactions. To address these issues, many researchers have studied trust as a computer science concept. Nevertheless, one of the greatest challenges in the trust domain is the lack of a generic trust management framework that will ease and encourage existing collaborative systems to adopt such concepts. In this paper, we propose a generic trust management framework which is capable of processing different trust features as required. We propose a RESTful message exchanging architecture, and a trust model based on the solution of a multi-class classification problem using machine learning techniques, namely Support Vector Machines(SVM).","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345436","Trust framework;machine learning;trust model","Collaboration;Context;Engines;Fuzzy logic;Monitoring;Security;Support vector machines","groupware;learning (artificial intelligence);message passing;pattern classification;support vector machines;trusted computing","RESTful message exchanging architecture;SVM;collaborative applications;computer communication networks;generic trust management framework;machine learning-based trust model;multiclass classification problem;support vector machines","","1","","15","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Assessing ergonomic and postural data for pain and fatigue markers using machine learning techniques","M. M. Shein; A. Hamilton-Wright; N. Black; M. Samson; M. Lecanelier","Mathematics and Computer Science, Mount Allison University, Sackville, New Brunswick, Canada","2015 International Conference and Workshop on Computing and Communication (IEMCON)","20151203","2015","","","1","6","Ergonomic data obtained from trials with human participants at a number of workstations are evaluated in terms of whether different workstations elicit different fatigue and pain responses. Data is analyzed using a pair of simple machine-learning based classifiers in order to identify activities associated with the workstations that lead to or avoid pain and fatigue. Results indicate that information content sufficient to predict pain and fatigue is present in this data, with evidence of information increase consistent with postures held for a period of time. Additional analysis will be performed to isolate postures associated with fatigue and pain in follow-up work.","","Electronic:978-1-4799-6908-1; POD:978-1-4799-6909-8","10.1109/IEMCON.2015.7344435","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344435","classification;ergonomics;machine learning;pain prediction;posture;timeseries data","Atmospheric measurements;Back;Fatigue;Hafnium;Neck;Pain;Workstations","data mining;ergonomics;health care;learning (artificial intelligence);occupational health","ergonomic data assessment;fatigue markers;fatigue responses;human participants;information content;machine learning techniques;machine-learning based classifiers;pain markers;postural data assessment;workstations","","","","6","","","15-17 Oct. 2015","","IEEE","IEEE Conference Publications"
"Large scale thematic mapping by supervised machine learning on ‘big data’ distributed cluster computing frameworks","J. Lozano; N. Aginako; M. Quartulli; I. Olaizola; E. Zulueta; P. Iriondo","Vicomtech-IK4, Digital TV and Multimedia Services, Mikeletegi 57, 20009 Donostia, Spain","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","1504","1507","The Petabyte-scale data volumes in Earth Observation (EO) archives are not efficiently manageable with serial processes running on large isolated servers. Distributed storage and processing based on `big data' cloud computing frameworks needs to be considered as a part of the solution.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326065","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326065","","Big data;Clustering algorithms;Feature extraction;Machine learning algorithms;Prototypes;Servers;Sparks","Big Data;cloud computing;geophysics computing;learning (artificial intelligence);pattern clustering","EO;Earth observation archives;big data cloud computing frameworks;big data distributed cluster computing frameworks;large isolated servers;large scale thematic mapping;petabyte-scale data volumes;supervised machine learning","","","","5","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"A new technique for restricted Boltzmann machine learning","V. Golovko; A. Kroshchanka; V. Turchenko; S. Jankowski; D. Treadwell","Brest State Technical University, Moskowskaja 267, Brest, 224017, Belarus","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","20151203","2015","1","","182","186","Over the last decade, deep belief neural networks have been a hot topic in machine learning. Such networks can perform a deep hierarchical representation of input data. The first layer can extract low-level features, the second layer can extract high-level features and so on. In general, deep belief neural network represents many-layered perceptron and permits to overcome some limitations of conventional multilayer perceptron due to deep architecture. In this work we propose a new training technique called Reconstruction Error-Based Approach (REBA) for deep belief neural network based on restricted Boltzmann machine. In contrast to classical Hinton's training approach, which is based on a linear training rule, the proposed technique is based on a nonlinear learning rule. We demonstrate the performance of REBA technique for the MNIST dataset visualization. The main contribution of this paper is a novel view on the training of a restricted Boltzmann machine.","","CD-ROM:978-1-4673-8358-5; Electronic:978-1-4673-8361-5; POD:978-1-4673-8362-2","10.1109/IDAACS.2015.7340725","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340725","Restricted Boltzmann machine;data visualization;deep learning;machine learning","Biological neural networks;Data visualization;Feature extraction;Mathematical model;Mean square error methods;Training","Boltzmann machines;belief networks;data visualisation;feature extraction;learning (artificial intelligence);multilayer perceptrons","Hinton training approach;MNIST dataset visualization;REBA;deep architecture;deep belief neural networks;deep hierarchical representation;high-level feature extraction;low-level feature extraction;many-layered perceptron;multilayer perceptron;nonlinear learning rule;reconstruction error-based approach;restricted Boltzmann machine learning","","","","9","","","24-26 Sept. 2015","","IEEE","IEEE Conference Publications"
"A Machine-Learning Approach for Communication Prediction of Large-Scale Applications","N. Papadopoulou; G. Goumas; N. Koziris","Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece","2015 IEEE International Conference on Cluster Computing","20151029","2015","","","120","123","In this paper we present a machine-learning approach to predict the total communication time of parallel applications. Communication time is heavily dependent on a very wide set of parameters relevant to the architecture, runtime configuration and application communication profile. We focus our study on parameters that can be easily extracted from the application and the process mapping ahead of execution. To this direction we define a small set of descriptive metrics and build a simple benchmark that can sweep over the parameter space in a straightforward way. We use this benchmarking data to train a robust multiple variable regression model which serves as our communication predictor. Our experimental results show notable accuracy in predicting the communication time of two indicative application kernels on a supercomputer utilizing from a few dozen to a few thousands processing cores.","1552-5244;15525244","Electronic:978-1-4673-6598-7; POD:978-1-4673-6599-4","10.1109/CLUSTER.2015.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307574","MPI applications;communication time;large-scale systems;performance prediction","Benchmark testing;Correlation;Kernel;Mathematical model;Measurement;Predictive models;Resource management","learning (artificial intelligence);mainframes;parallel machines;regression analysis","application communication profile;benchmarking data;descriptive metrics;indicative application kernels;large-scale applications;machine-learning approach;parallel applications;parameter space;process mapping;processing cores;robust multiple variable regression model;runtime configuration;supercomputer;total communication time prediction","","","","5","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Accelerating common machine learning algorithms through GPGPU symbolic computing","M. C. Díaz; F. A. González; R. Ramos-Pollan","Mindlab Res. Group, Univ. Nac. de Colombia, Bogota, Colombia","2015 10th Computing Colombian Conference (10CCC)","20151123","2015","","","387","391","This paper evaluates the implementation of two well known machine learning algorithms, kernel k-means and logistic regression, using Graphics Processing Units (GPUs). The main goal was to do an implementation that exploited the processing power of GPU while keeping the implementation simple, easy to understand and modify. The paper presents an empirical analysis of the performance of the implementations under different execution scenarios.","","Electronic:978-1-4673-9464-2; POD:978-1-4673-9465-9","10.1109/ColumbianCC.2015.7333450","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333450","GPGPU;Kernel K-Means;Logistic Regression;Machine Learning;Theano","Algorithm design and analysis;Clustering algorithms;Graphics processing units;Kernel;Libraries;Logistics;Machine learning algorithms","graphics processing units;learning (artificial intelligence);regression analysis","GPGPU symbolic computing;graphics processing units;kernel k-means algorithm;logistic regression algorithm;machine learning algorithms","","","","","","","21-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"A framework for adoption of machine learning in industry for software defect prediction","R. Rana; M. Staron; J. Hansson; M. Nilsson; W. Meding","Computer Science & Engineering, Chalmers, University of Gothenburg, Sweden","2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA)","20151008","2014","","","383","392","Machine learning algorithms are increasingly being used in a variety of application domains including software engineering. While their practical value have been outlined, demonstrated and highlighted in number of existing studies, their adoption in industry is still not widespread. The evaluations of machine learning algorithms in literature seem to focus on few attributes and mainly on predictive accuracy. On the other hand the decision space for adoption or acceptance of machine learning algorithms in industry encompasses much more factors. Companies looking to adopt such techniques want to know where such algorithms are most useful, if the new methods are reliable and cost effective. Further questions such as how much would it cost to setup, run and maintain systems based on such techniques are currently not fully investigated in the industry or in academia leading to difficulties in assessing the business case for adoption of these techniques in industry. In this paper we argue for the need of framework for adoption of machine learning in industry. We develop a framework for factors and attributes that contribute towards the decision of adoption of machine learning techniques in industry for the purpose of software defect predictions. The framework is developed in close collaboration within industry and thus provides useful insight for industry itself, academia and suppliers of tools and services.","","Electronic:978-9-8975-8124-3; POD:978-1-4799-7691-1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293887","Adoption;Machine Learning;SDP: Software Defect Prediction;Software Defect Prediction;Software Quality Acronyms Used — ML: Machine Learning;TAM: Technology Acceptance Model;Technology Acceptance","Adaptation models;Companies;Context;Industries;Machine learning algorithms;Predictive models;Software","","","","","","25","","","29-31 Aug. 2014","","IEEE","IEEE Conference Publications"
"A Distributed Support Vector Machine Learning Over Wireless Sensor Networks","W. Kim; M. S. Stanković; K. H. Johansson; H. J. Kim","Electronics and Telecommunications Research Institute, Daejeon, Korea","IEEE Transactions on Cybernetics","20151013","2015","45","11","2599","2611","This paper is about fully-distributed support vector machine (SVM) learning over wireless sensor networks. With the concept of the geometric SVM, we propose to gossip the set of extreme points of the convex hull of local data set with neighboring nodes. It has the advantages of a simple communication mechanism and finite-time convergence to a common global solution. Furthermore, we analyze the scalability with respect to the amount of exchanged information and convergence time, with a specific emphasis on the small-world phenomenon. First, with the proposed naive convex hull algorithm, the message length remains bounded as the number of nodes increases. Second, by utilizing a small-world network, we have an opportunity to drastically improve the convergence performance with only a small increase in power consumption. These properties offer a great advantage when dealing with a large-scale network. Simulation and experimental results support the feasibility and effectiveness of the proposed gossip-based process and the analysis.","2168-2267;21682267","","10.1109/TCYB.2014.2377123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047737","Distributed learning;support vector machine (SVM);wireless sensor networks","Convergence;Kernel;Quadratic programming;Scalability;Support vector machines;Training;Wireless sensor networks","convex programming;learning (artificial intelligence);parallel processing;support vector machines;wireless sensor networks","convergence time;distributed support vector machine learning;finite-time convergence;geometric SVM learning;naive convex hull algorithm;wireless sensor network","","3","","30","","20150224","Nov. 2015","","IEEE","IEEE Journals & Magazines"
"Goals at risk? Machine learning at support of early assessment","P. Avesani; A. Perini; A. Siena; A. Susi","Fondazione Bruno Kessler, Trento-Povo, I-38123 Italy","2015 IEEE 23rd International Requirements Engineering Conference (RE)","20151105","2015","","","252","255","A relevant activity in the requirements engineering process consists in the identification, assessment and management of potential risks, which can prevent the system-to-be from meeting stakeholder needs. However, risk analysis techniques are often time- and resource- consuming activities, which may introduce in the requirements engineering process a significant overhead. To overcome this problem, we aim at supporting risk management activity in a semi-automated way, merging the capability to exploit existing risk-related information potentially present in a given organisation, with an automated ranking of the goals with respect to the level of risk the decision-maker estimates for them. In particular, this paper proposes an approach to address the general problem of risk decision-making, which combines knowledge about risks assessment techniques and Machine Learning to enable an active intervention of human evaluators in the decision process, learning from their feedback and integrating it with the organisational knowledge. The long term objective is that of improving the capacity of an organisation to be aware and to manage risks, by introducing new techniques in the field of risk management that are able to interactively and continuously extract useful knowledge from the organisation domain and from the decision-maker expertise.","1090-705X;1090705X","Electronic:978-1-4673-6905-3; POD:978-1-4673-6906-0","10.1109/RE.2015.7320432","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320432","","Approximation methods;Decision making;Open source software;Requirements engineering;Risk management;Yttrium","decision making;formal specification;knowledge acquisition;learning (artificial intelligence)","automated ranking;decision process;decision-maker;human evaluators;knowledge extraction;machine learning;organisational knowledge;potential risk assessment;potential risk identification;potential risk management;requirements engineering process;risk analysis techniques;risk-related information;risks assessment techniques","","","","9","","","24-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Driving Behavior Signals and Machine Learning: A Personalized Driver Assistance System","M. V. Martínez; I. D. Campo; J. Echanobe; K. Basterretxea","Dept. of Electr. & Electron., Univ. of the Basque Country, Leioa, Spain","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","2933","2940","The progressive integration of driver assistance systems (DAS) into vehicles in recent decades has contributed to improving the quality of the driving experience. Currently, there is a need for individualization of advanced DAS with the aim of improving safety, security and comfort of the driver. In particular, the need to adapt the vehicle to individual preferences and requirements of the driver is an important research focus. In this work, an individualized and non-intrusive monitoring system for real-time driver support is proposed. The kernel of the system is a driver identification module based on driving behavior signals and a high-performance machine learning technique. The scheme is suitable for the development of single-chip embedded systems. Moreover, most of the measurement units used in this research are nowadays available in commercial vehicles, so the deployment of the system can be performed with minimal additional cost. Experimental results using a reduced set of features are very encouraging. Identification rates greater than 75% are obtained for a working set of 11 drivers, 86% for five-driver groups, 88% for four-driver groups, and 90% for three-driver groups.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.470","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313563","ambient intelligence;classification;driver assistance systems;feature selection;machine learning;real-time system;smart car","Accelerometers;Monitoring;Real-time systems;Safety;Security;Vehicles;Wheels","behavioural sciences computing;embedded systems;learning (artificial intelligence);signal processing;traffic engineering computing","DAS;driver comfort;driver identification module;driver safety;driver security;driving behavior signals;driving experience;machine learning;personalized driver assistance system;realtime driver support;single-chip embedded systems","","1","","22","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"Identifying differentially expressed transcripts associated with prostate cancer progression using RNA-Seq and machine learning techniques","S. Singireddy; A. Alkhateeb; I. Rezaeian; L. Rueda; D. Cavallo-Medved; L. Porter","School of Computer Science, University of Windsor, 401 Sunset Avenue, Windsor, Ontario, Canada","2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)","20151019","2015","","","1","5","Background: Prostate cancer is complicated by a high level of unexplained variability in the aggressiveness of newly diagnosed disease. Given that this is one of the most prevalent cancers worldwide, finding biomarkers to effectively stratify high risk patient populations is a vital next step in improving survival rates and quality of life after treatment. Materials and Methods: In this study, we selected a dataset consisting of 106 prostate cancer samples, which represent various stages of prostate cancer and developed by RNA-Seq technology. Our objective is to identify differentially expressed transcripts associated with prostate cancer progression using pair-wise stage comparisons. Results: Using machine learning techniques, we identified 44 transcripts that are correlated to different stages of progression. Expression of an identified transcript, USP13, is reduced in stage T3 in comparison with stage T2c, a pattern also observed in breast cancer tumourigenesis. We also identified another differentially expressed transcript, PTGFR, which has also been reported to be involved in prostate cancer progression and has also been linked to breast, ovarian and renal cancers. Conclusions: The results support the use of RNA-Seq along with machine learning techniques as an essential tool in identifying potential biomarkers for prostate cancer progression. Further studies elucidating the biochemical role of identified transcripts in vitro are crucial in validating the use of these biomarkers in the prediction of disease progression and development of effective therapeutic strategies.","","Electronic:978-1-4799-6926-5; POD:978-1-4799-6927-2","10.1109/CIBCB.2015.7300302","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300302","Alternative Splicing;Classification;Feature Selection;Machine Learning;Next Generation Sequencing;Prostate Cancer","Bioinformatics;Biomarkers;Diseases;Prostate cancer;Proteins;Tumors","RNA;bioinformatics;cancer;learning (artificial intelligence);patient diagnosis","RNA-Seq;USP13;biomarkers;breast cancer;differentially expressed transcripts;disease;machine learning techniques;ovarian cancer;pair wise stage comparisons;prostate cancer progression;quality of life;renal cancer;survival rate;tumourigenesis","","","","28","","","12-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Classification with Extreme Learning Machine on GPU","T. Jeowicz; P. Gajdo; V. Uher; V. Snáel","Dept. of Comput. Sci., VSB-Tech. Univ. of Ostrava, Ostrava - Poruba, Czech Republic","2015 International Conference on Intelligent Networking and Collaborative Systems","20151102","2015","","","116","122","The general classification is a machine learning task that tries to assign the best class to a given unknown input vector based on past observations (training data). Most of developed algorithms are very time consuming for large datasets (Support Vector Machine, Deep Neural Networks, etc.). Extreme Learning Machine (ELM) is a high quality classification algorithm that gains much popularity in recent years. This paper shows that the speed of learning of this algorithm may be improved by using GPU platform. Experimental results showed that proposed approach is much faster and provides the same accuracy as the original ELM algorithm. The proposed approach runs completely on GPU platform and thus it may be effectively incorporated within other applications.","","Electronic:978-1-4673-7695-2; POD:978-1-4673-7696-9","10.1109/INCoS.2015.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312059","CUDA;Classification;Extreme learning machine;Fast;GPU;Parallel","Algorithm design and analysis;Graphics processing units;Matrix decomposition;Neural networks;Neurons;Support vector machines;Training","learning (artificial intelligence);neural nets;pattern classification;support vector machines","ELM;GPU;classification algorithm;deep neural network;extreme learning machine;general classification;machine learning task;support vector machine","","","","28","","","2-4 Sept. 2015","","IEEE","IEEE Conference Publications"
"An incremental learning approach for restricted boltzmann machines","Jongmin Yu; Jeonghwan Gwak; Sejeong Lee; Moongu Jeon","Machine Learning and Vision Laboratory, School of Information and Communication, Gwangju Institute of Science and Technology, 61005, Republic of Korea","2015 International Conference on Control, Automation and Information Sciences (ICCAIS)","20151130","2015","","","113","117","Determination of model complexity is a challenging issue to solve computer vision problems using restricted boltzmann machines (RBMs). Many algorithms for feature learning depend on cross-validation or empirical methods to optimize the number of features. In this work, we propose an learning algorithm to find the optimal model complexity for the RBMs by incrementing the hidden layer. The proposed algorithm is composed of two processes: 1) determining incrementation necessity of neurons and 2) computing the number of additional features for the increment. Specifically, the proposed algorithm uses a normalized reconstruction error in order to determine incrementation necessity and prevent unnecessary increment for the number of features during training. Our experimental results demonstrated that the proposed algorithm converges to the optimal number of features in a single layer RBMs. In the classification results, our model could outperform the non-incremental RBM.","","Electronic:978-1-4799-9892-0; POD:978-1-4799-9893-7; USB:978-1-4799-9891-3","10.1109/ICCAIS.2015.7338643","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338643","Machine learning;incremental learning;restricted boltzmann machine","Complexity theory;Computational modeling;Error analysis;Neurons;Standards;Training;Training data","Boltzmann machines;computational complexity;computer vision;convergence;feature extraction;image classification;learning (artificial intelligence)","RBM;classification;computer vision problem;convergence;feature learning;hidden layer;incremental learning approach;learning algorithm;neuron incrementation necessity;normalized reconstruction error;optimal model complexity;restricted Boltzmann machines","","","","14","","","29-31 Oct. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning and Visual Analytics for Consulting Business Decision Support","A. Cook; P. Wu; K. Mengersen","Queensland Univ. of Technol., Brisbane, QLD, Australia","2015 Big Data Visual Analytics (BDVA)","20151102","2015","","","1","2","The application of machine learning and statistical predictive models to business problems has found success in recent years due to an exponential increase in consumer data and readily available computational power. However, visualising and interpreting this data to support business decision making in the context of consulting businesses is challenging and there is scope for advancement. The accurate prediction of hours to be spent on a project (cost) ahead of time underpins the profitability of these organisations. The aim of the research is twofold: to identify suitable techniques from the fields of machine learning and statistics for internal cost prediction in a consulting business, and to develop a user interface with visual analytics displaying results from these techniques to provide interactive decision support. The data for this project was collected from a consulting business' customer relationship management(CRM) database, which contained twelve years of past consulting projects. To date, statistical linear models and machine learning decision trees have been trialed and the research will progress into random forests, neural networks, and support vector machine (SVM) models. A prototype user interface and visualisation of the results has also been developed.","","Electronic:978-1-4673-7343-2; POD:978-1-4673-7344-9","10.1109/BDVA.2015.7314299","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314299","","Business;Neural networks;Predictive models;Profitability;Support vector machines;User interfaces;Visual analytics","business data processing;consultancies;customer relationship management;data visualisation;decision support systems;decision trees;graphical user interfaces;interactive systems;learning (artificial intelligence);profitability;statistical analysis","CRM database;SVM model;business customer relationship management database;business decision making;computational power;consulting business decision support;consumer data;data interpretation;data visualisation;interactive decision support;internal cost prediction;machine learning decision trees;neural networks;organisation profitability;random forests;statistical linear models;statistical predictive models;statistics;support vector machine;user interface;visual analytics","","","","6","","","22-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"Semi-automated patient-specific scalp EEG seizure detection with unsupervised machine learning","O. Smart; M. Chen","Department of Neurosurgery, Emory University School of Medicine, Atlanta, Georgia, USA","2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)","20151019","2015","","","1","7","Epilepsy is a debilitating neurological disorder for which millions of people worldwide experience seizures. To diagnose these seizures for treatment, a patient typically stays in the hospital for days while a clinical staff must manually screen hours of electroencephalograph (EEG) data for electroencephalographic and behavioral seizures, a very time-consuming, tedious, and subjective process for humans. While decades of research have been dedicated to algorithms for alleviating this clinical procedural burden, there is still room for improvement in seizure detection techniques. Moreover, most seizure detection strategies use supervised machine learning, which requires subjective human involvement for training data. This research examines seizure detection performance for 24 patients using k-means, k-mediod, and hierarchical clustering, a Gaussian mixture model, and a hidden Markov model after using principal component analyses to reduce 45 measures to 2 measures. This work differs from past unsupervised methods, which were tested using very experimentally controlled seizure and non-seizure EEG signals: analyzing continuous EEG spanning 60 seconds pre-seizure to 60 seconds post-seizure in overlapping sliding windows rather than a single point for each seizure and non-seizure epoch. The `best' combination of classifier accuracy, sensitivity, specificity, selectivity, and rejectivity is found with k-means and k-mediod algorithms.","","Electronic:978-1-4799-6926-5; POD:978-1-4799-6927-2","10.1109/CIBCB.2015.7300286","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300286","Gaussian mixture model;electroencephalography;epilepsy;hidden Markov model;hierarchical clustering;human;k-means;k-mediod;machine learning;pattern classification;principal component analysis;seizure;unsupervised","Decision support systems","Gaussian processes;electroencephalography;hidden Markov models;medical disorders;medical signal processing;pattern clustering;principal component analysis;unsupervised learning","EEG data;Gaussian mixture model;behavioral seizure;classifier accuracy;electroencephalographic seizure;epilepsy;hidden Markov model;hierarchical clustering;k-means clustering;k-mediod clustering;neurological disorder;nonseizure EEG signal;patient-specific scalp EEG seizure detection;principal component analyses;seizure detection technique;unsupervised machine learning","","2","","33","","","12-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Applied machine learning classifiers for medical applications: Clarifying the behavioural patterns using a variety of datasets","A. J. Aljaaf; D. Al-Jumeily; A. J. Hussain; P. Fergus; M. Al-Jumaily; N. Radi","Applied Computing Research Group, Liverpool John Moores University, Byrom Street, Liverpool, L3 3AF, UK","2015 International Conference on Systems, Signals and Image Processing (IWSSIP)","20151102","2015","","","228","232","Machine-learning (ML) techniques have grown to be among the leading research topics within the health care systems and particularly for clinical decision support systems (CDSS), which are commonly used in helping physicians to make more accurate diagnosis. However, applying these techniques for CDSS is most likely would face a lack of criteria for adequate use. Therefore, a range of recent studies have focused on evaluating different machine learning classifiers with the aim of identifying the most appropriate classifier to be used for particular decision making problem-domain. The majority of these studies have used a single dataset within a certain medical-related classification domain. Nevertheless, evaluating machine-learning classifiers with one sample of data appears to be unsatisfying, perhaps it is not reflecting the classifiers capabilities or their behavioral patterns under different circumstances. In this study, five well-known supervised machine-learning classifiers were examined using five different real-world datasets with a range of attributes. The main aim was to illustrate not only the impact of the datasets volume and attributes on the evaluation, but also and more importantly, present the classifiers capabilities and shortcomings under certain conditions, which potentially provide a guidance or instructions to help health analysts and researchers to determine the most suitable classifier to address a particular medical-related decision making problem.","2157-8672;21578672","CD-ROM:978-1-4673-8352-3; Electronic:978-1-4673-8353-0; POD:978-1-4673-8354-7","10.1109/IWSSIP.2015.7314218","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314218","classification accuracy;clinical decision support system;evaluation;machine learning;sensitivity;time complexity","Accuracy;Diseases;Liver;Sensitivity;Support vector machines;Training","decision support systems;health care;learning (artificial intelligence);pattern classification","CDSS;behavioral pattern;behavioural pattern;clinical decision support system;decision making problem-domain;health analyst;health care system;machine learning classifier;machine-learning technique;medical application;medical-related classification domain;medical-related decision making;real-world dataset;supervised machine-learning classifier","","1","","21","","","10-12 Sept. 2015","","IEEE","IEEE Conference Publications"
"Is audio signal processing still useful in the era of machine learning?","E. Vincent","INRIA Nancy, France","2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)","20151130","2015","","","7","7","Summary form only given. Audio signal processing has long been the obvious approach to problems such as microphone array processing, active noise control, or speech enhancement. Yet, it is increasingly being challenged by black-box machine learning approaches based on, e.g., deep neural networks (DNN), which have already achieved superior results on certain tasks. In this talk, I will try to convince that machine learning approaches shouldn't be disregarded, but that black boxes won't solve these problems either. There is hence an opportunity for signal processing researchers to join forces with machine learning researchers and solve these problems together. I will provide examples of this multi-disciplinary approach for audio source separation and robust automatic speech recognition.","","Electronic:978-1-4799-7450-4; POD:978-1-4799-7451-1; USB:978-1-4799-7449-8","10.1109/WASPAA.2015.7336882","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336882","","Acoustics;Conferences;Laboratories;Multiple signal classification;Signal processing;Speech;Speech recognition","audio signal processing;learning (artificial intelligence);microphone arrays;neural nets;speech recognition","DNN;active noise control;audio signal processing;audio source separation;black boxes;black-box machine learning;deep neural networks;machine learning researchers;microphone array processing;robust automatic speech recognition;speech enhancement","","","","","","","18-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Cloud Computing with Machine Learning Could Help Us in the Early Diagnosis of Breast Cancer","J. A. Bhat; V. George; B. Malik","Dept. of Comput. Sci. & Eng., Christ Univ., Bangalore, India","2015 Second International Conference on Advances in Computing and Communication Engineering","20151026","2015","","","644","648","The purpose of this study is to develop tools which could help the clinicians in the primary care hospitals with the early diagnosis of breast cancer diagnosis. Breast cancer is one of the leading forms of cancer in developing countries and often gets detected at the lateral stages. The detection of cancer at later stages results not only in pain and in agony to the patients but also puts lot of financial burden on the caregivers. In this work, we are presenting the preliminary results of the project code named BCDM (Breast Cancer Diagnosis using Machine Learning) developed using Mat lab. The algorithm developed in this research cancer work based on adaptive resonance theory. In this research work, we concluded how Art 1 network will help in classification of breast. The aim of the project is to eventually run the algorithm on a cloud computer and a clinician at a primary healthcare can use the system for the early diagnosis of the patients using web based interface from anywhere in the world.","","Electronic:978-1-4799-1734-1; POD:978-1-4799-1735-8","10.1109/ICACCE.2015.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306762","Adaptive Resonance theory;Breast Cancer Diagnosis;FNA","Adaptive systems;Breast cancer;Neural networks;Neurons;Testing;Training","cancer;cloud computing;health care;learning (artificial intelligence);mathematics computing;patient diagnosis","Matlab;Web based interface;breast cancer diagnosis;cloud computing;healthcare;machine learning","","","","9","","","1-2 May 2015","","IEEE","IEEE Conference Publications"
"Adopting Machine Learning Methods to Predict Red-light Running Violations","A. Jahangiri; H. A. Rakha; T. A. Dingus","Civil & Environ. Eng. Dept., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","650","655","Statistics demonstrate that a large number of crashes occur at signalized intersections due to traffic violations, specifically red light running (RLR). In order to prevent/mitigate intersection-related crashes, these violations need to be identified before they occur, so appropriate actions can be taken. Several factors such as vehicle speed, Time to Intersection (TTI), Distance to Intersection (DTI), age, gender, etc. influence the drivers behavior when approaching intersections. However, the driver-related factors (i.e. age, gender) are more difficult to obtain in practice. On the other hand, kinetic factors (e.g. speed, acceleration) can be obtained by monitoring the movement of vehicles through video cameras installed on the infrastructure or through on-board devices installed on the vehicles. Hence, the problem of interest is to develop models to predict RLR violations using kinetic information of vehicles. A monitoring period was defined to extract data from each vehicle before reaching the intersection. Machine learning techniques, namely Support Vector Machine (SVM) and Random Forest (RF), were adopted to develop prediction models. The minimum Redundancy Maximum Relevance (mRMR) feature selection method was used to identify the most important factors for model development. To evaluate the performance of the models the K-fold cross-validation and out-of-bag (OOB) errors were used for the SVM and RF models, which contributed to high prediction accuracies of 97.9 and 93.6 percent, respectively. It was shown that other than the critical instant at which the traffic signal changes to yellow, an appropriate monitoring period with respect to the yellow onset can provide additional useful information ensuring that the driver decision occurs during that period.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313204","driver violation;machine learning;random forest;red light running;signalized intersection;support vector machine;violation prediction","Diffusion tensor imaging;Monitoring;Predictive models;Radio frequency;Support vector machines;Vehicle crash testing;Vehicles","computerised monitoring;feature selection;learning (artificial intelligence);road traffic;support vector machines;traffic engineering computing;video cameras;video signal processing","RF;RLR violation prediction;SVM;driver behavior;mRMR feature selection method;machine learning method;minimum redundancy maximum relevance;random forest;red-light running violation prediction;support vector machine;vehicle kinetic information;vehicle movement monitoring;video camera","","1","","50","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine-Learning Aided Optimal Customer Decisions for an Interactive Smart Grid","D. Li; S. K. Jayaweera","Dept. of Electr. & Comput. Eng., Univ. of New Mexico, Albuquerque, NM, USA","IEEE Systems Journal","20151120","2015","9","4","1529","1540","In this paper, a hierarchical smart grid architecture is presented. The concept of smart home is extended in two aspects: 1) from traditional households with smart devices, such as advanced metering infrastructure, to intelligent entities with instantaneous and distributive decision-making capabilities; and 2) from individual households to general customer units of possibly large scales. We then develop a hidden mode Markov decision process (HM-MDP) model for a customer real-time decision-making problem. This real-time decision-making framework can effectively be integrated with demand response schemes, which are prediction based and therefore inevitably lead to real-time power-load mismatches. With the Baum-Welch algorithm adopted to learn the nonstationary dynamics of the environment, we propose a value iteration (VI)-based exact solution algorithm for the HM-MDP problem. Unlike conventional VI, the concept of parsimonious sets is used to enable a finite representation of the optimal value function. Instead of iterating the value function in each time step, we iterate the representational parsimonious sets by using the incremental pruning algorithm. Although this exact algorithm leads to optimal policies giving maximum rewards for the smart homes, its complexity suffers from the curse of dimensionality. To obtain a low-complexity real-time algorithm that allows adaptively incorporating new observations as the environment changes, we resort to Q-learning-based approximate dynamic programming. Q-learning offers more flexibility in practice because it does not require specific starting and ending points of the scheduling period. Performance analysis of both exact and approximate algorithms, as compared with the other possible alternative decision-making strategies, is presented in simulation results.","1932-8184;19328184","","10.1109/JSYST.2014.2334637","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868988","Approximate dynamic programming (ADP);Baum–Welch algorithm;Baum???Welch algorithm;Q-learning;hidden mode Markov decision process (HM-MDP);incremental pruning (IP);smart home","Approximation algorithms;Decision making;Heuristic algorithms;Load modeling;Microgrids;Real-time systems;Smart grids","decision making;domestic appliances;dynamic programming;hidden Markov models;learning (artificial intelligence);load management;smart power grids","Baum-Welch algorithm;HM-MDP model;Q-learning-based approximate dynamic programming;VI-based exact solution algorithm;alternative decision-making strategy;demand response scheme;distributive decision-making capability;hidden mode Markov decision process model;hierarchical smart grid architecture;incremental pruning algorithm;interactive smart grid;low-complexity real-time algorithm;machine-learning aided optimal customer;optimal value function;performance analysis;power-load mismatch;smart device;smart home;value iteration-based exact solution algorithm","","2","","40","","20140731","Dec. 2015","","IEEE","IEEE Journals & Magazines"
