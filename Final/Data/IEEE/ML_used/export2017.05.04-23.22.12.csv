"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7844751,7845067,7847907,7844517,7849088,7846857,7847158,7848584,7844411,7848391,7844310,7845051,7844352,7845219,7845186,7776965,7840711,7840831,7841777,7840776,7840094,7841080,7840209,7840731,7840622,7840851,7841684,7840917,7841506,7842962,7840828,7841821,7841039,7842307,7843496,7838191,7838214,7838211,7838239,7839569,7838179,7838206,7838169,7838168,7838138,7839567,7838204,7838199,7838207,7838289,7838196,7838275,7838182,7838299,7838096,7836663,7839103,7836686,7837049,7838216,7836673,7836817,7838048,7738596,7835438,7836060,7835465,7835387,7830090,7828360,7828517,7829628,7829592,7828429,7830232,7830451,7830122,7829644,7829927,7832838,7828554,7829617,7829781,7827613,7827691,7828285,7827590,7827977,7827669,7442844,7824134,7824833,7823950,7823247,7823695,7542572,7823835,7822695,7823490,7824843",2017/05/04 23:22:12
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Design exploration of ASIP architectures for the K-Nearest Neighbor machine-learning algorithm","D. Jamma; O. Ahmed; S. Areibi; G. Grewal; N. Molloy","University of Guelph, School of Engineering, Guelph, Canada N1G 2W1","2016 28th International Conference on Microelectronics (ICM)","20170209","2016","","","57","60","Increasingly, machine-learning algorithms are playing an important role in the context of embedded and real-time systems. Applications such as wireless sensor networks, security, and commercial enterprises are increasingly relying on machine-learning algorithms to efficiently make predictive decisions based on the large volumes of data these systems collect. Therefore, there is a need to accelerate the runtime of these algorithms, especially for real-time applications. In this paper, we propose several Application Specific Instruction Processor (ASIP) architectures for the K-Nearest Neighbor (KNN) classification algorithm. Each ASIP is developed using Cadence Tensilica tools and represents a tightly-coupled architecture. Our experimental results, based on several benchmarks, show that proposed ASIPs achieve speedups of 86×-650× over the original software implementation.","","Electronic:978-1-5090-5721-4; POD:978-1-5090-5722-1","10.1109/ICM.2016.7847907","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847907","","Algorithm design and analysis;Benchmark testing;Computer architecture;Prediction algorithms;Registers;Software algorithms;Training","application specific integrated circuits;embedded systems;learning (artificial intelligence);logic design;microprocessor chips","ASIP architectures;Cadence Tensilica tools;K-nearest neighbor machine-learning algorithm;KNN classification algorithm;application specific instruction processor;commercial enterprises;embedded systems;predictive decisions;wireless sensor networks","","","","","","","17-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Hybrid Machine Learning Approach for Planning Safe Trajectories in Complex Traffic-Scenarios","A. Chaulwar; M. Botsch; W. Utschick","Fac. of Electr. Eng., Ingolstadt Univ. of Appl. Sci., Ingolstadt, Germany","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","540","546","Planning of safe trajectories with interventions in both lateral and longitudinal dynamics of vehicles has huge potential for increasing the road traffic safety. Main challenges for the development of such algorithms are the consideration of vehicle nonholonomic constraints and the efficiency in terms of implementation, so that algorithms run in real time in a vehicle. The recently introduced Augmented CL-RRT algorithm is an approach that uses analytical models for trajectory planning based on the brute force evaluation of many longitudinal acceleration profiles to find collision-free trajectories. The algorithm considers nonholonomic constraints of the vehicle in complex road traffic scenarios with multiple static and dynamic objects, but it requires a lot of computation time. This work proposes a hybrid machine learning approach for predicting suitable acceleration profiles in critical traffic scenarios, so that only few acceleration profiles are used with the Augmented CL-RRT to find a safe trajectory while reducing the computation time. This is realized using a convolutional neural network variant, introduced as 3D-ConvNet, which learns spatiotemporal features from a sequence of predicted occupancy grids generated from predictions of other road traffic participants. These learned features together with hand-designed features of the EGO vehicle are used to predict acceleration profiles. Simulations are performed to compare the brute force approach with the proposed approach in terms of efficiency and safety. The results show vast improvement in terms of efficiency without harming safety. Additionally, an extension to the Augmented CL-RRT algorithm is introduced for finding a trajectory with low severity of injury, if a collision is already unavoidable.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0095","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838199","3D-ConvNets;Hybrid learning algorithms;Trajectory Planning in Road Traffic","Acceleration;Heuristic algorithms;Machine learning algorithms;Prediction algorithms;Roads;Trajectory;Vehicle dynamics","feedforward neural nets;learning (artificial intelligence);road safety;road traffic control;vehicle dynamics","3D-ConvNet;EGO vehicle;acceleration profile prediction;analytical models;augmented CL-RRT algorithm;brute force evaluation;collision-free trajectories;complex road traffic scenarios;computation time;convolutional neural network;hybrid machine learning approach;longitudinal acceleration profiles;multiple dynamic objects;multiple static objects;occupancy grids;road traffic safety;safe-trajectory planning;spatiotemporal features learns;vehicle efficiency;vehicle lateral dynamics;vehicle longitudinal dynamics;vehicle nonholonomic constraints","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"WeChat Text and Picture Messages Service Flow Traffic Classification Using Machine Learning Technique","M. Shafiq; X. Yu; A. A. Laghari; L. Yao; N. K. Karn; F. Abdesssamia; Salahuddin","Sch. of Comput. Sci. & Technol., Harbin Inst. of Technol., Harbin, China","2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","20170126","2016","","","58","62","Network Traffic Classification carries great importance for both internet service providers (ISPs) and quality of services (QoSs) management. During the last two decades, a lot of machine learning models have been proposed and applied on different types of real time applications to classify their real time traffic and obtain very proficient accuracy results. However, no research has been done on WeChat text and picture messages traffic classification. In this paper, WeChat text and picture messages traffics are classified using two different types of datasets and 4 well-known machine learning algorithms. These two datasets, Harbin Institute of Technology (HIT) and Dorm13, are collected from two different network environments. Having captured the traffic 50 features, they are extracted respectively. Thereafter, well-known four machine learning algorithms C4.5 decision tree, Bayes Net, Naïve Bayes and SVM are used to classify WeChat text and picture messages traffic. Experimental result analysis show that using HIT data set all the applied machine learning classifiers classify WeChat text and picture messages traffic very accurately as compared to Dorm13 dataset. Using HIT dataset, all ML classifier perform very well, but C4.5 and SVM are the ones that give very effective accuracy results of 99.91% and 99.57% respectively as compared to other ML classifiers.","","Electronic:978-1-5090-4297-5; POD:978-1-5090-4298-2","10.1109/HPCC-SmartCity-DSS.2016.0019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828360","Machine Learning;Message;Text and Picture;Traffic Classification;WeChat","Classification algorithms;Decision trees;Feature extraction;Machine learning algorithms;Message service;Support vector machines;Training","Bayes methods;Internet;belief networks;decision trees;learning (artificial intelligence);pattern classification;quality of service;support vector machines;telecommunication traffic;text analysis","Bayes net;Dorm13;HIT;Harbin Institute of Technology;ISPs;Internet service providers;QoSs;SVM;Wechat text;decision tree;machine learning;naïve Bayes;picture messages service flow traffic classification;quality of services","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Java thread and process performance for parallel machine learning on multicore HPC clusters","S. Ekanayake; S. Kamburugamuve; P. Wickramasinghe; G. C. Fox","School of Informatics and Computing, Indiana University, Bloomington","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","347","354","The growing use of Big Data frameworks on large machines highlights the importance of performance issues and the value of High Performance Computing (HPC) technology. This paper looks carefully at three major frameworks Spark, Flink and Message Passing Interface (MPI) both in scaling across nodes and internally over the many cores inside modern nodes. We focus on the special challenges of the Java Virtual Machine (JVM) using an Intel Haswell HPC cluster with 24 cores per node. Two parallel machine learning algorithms, K-Means clustering and Multidimensional Scaling (MDS) are used in our performance studies. We identify three major issues - thread models, affinity patterns, and communication mechanisms - as factors affecting performance by large factors and show how to optimize them so that Java can match the performance of traditional HPC languages like C. Further we suggest approaches that preserve the user interface and elegant dataflow approach of Flink and Spark but modify the runtime so that these Big Data frameworks can achieve excellent performance and realize the goals of HPC-Big Data convergence.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840622","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840622","Big Data;HPC;Java;Machine Learning;Multicore","Big data;Data models;Instruction sets;Java;Message systems;Parallel machines;Sparks","Big Data;Java;learning (artificial intelligence);message passing;multiprocessing systems;operating systems (computers);parallel processing;performance evaluation;virtual machines","Big Data frameworks;Flink;HPC technology;Intel Haswell HPC cluster;JVM;Java thread;Java virtual machine;K-Means clustering;MDS;MPI;Spark;communication mechanisms;dataflow approach;high performance computing;message passing interface;multicore HPC clusters;multidimensional scaling;parallel machine learning;parallel machine learning algorithms;process performance;user interface","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Review on Machine Learning and Data Mining Techniques for Residential Energy Smart Management","H. Salem; M. Sayed-Mouchaweh; A. B. Hassine","Ecole Nat. des Sci. de l'Inf., La Mannouba, Tunisia","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","1073","1076","In this paper, the different machine learning and data mining approaches used for Residential Energy Smart Management (RESM) will be discussed and classified according to some meaningful criteria. The proposed classification is an attempt to highlight the advantages and limitations of each category. Moreover, we emphasize the complementarity between approaches belonging to different categories and we point out the main challenges that still face RESM.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0195","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838299","","Data mining;Energy consumption;Hidden Markov models;Home appliances;Load modeling;Monitoring;Support vector machines","data mining;energy management systems;learning (artificial intelligence);pattern classification;power engineering computing","RESM;data mining techniques;machine learning;residential energy smart management","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach to dynamically generate context sensitive applications on top of a monolithic enterprise system","S. Kanankearachchi","99X Technology, 65, Walukarama Road, Colombo 03, Sri Lanka","2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer)","20170126","2016","","","248","249","This industrial research paper outlines an outcome of a machine leaning approach to dynamically generate context sensitive Application User Interfaces (Adaptive UI) on top of a monolithic enterprise software product.","","CD:978-1-5090-6076-4; Electronic:978-1-5090-6078-8; POD:978-1-5090-6079-5; Paper:978-1-5090-6077-1","10.1109/ICTER.2016.7829927","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829927","Adaptive UI;Machine Learning;Monolithic Enterprise Applications","Business;Context;Engines;Google;Roads;Software;User interfaces","learning (artificial intelligence);software engineering;user interfaces","adaptive UI;context sensitive application user interfaces;machine learning;monolithic enterprise software product","","","","","","","1-3 Sept. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Optimum CT-Prediction for qPCR","M. Gunay; E. Goceri; R. Balasubramaniyan","Dept. of Comput. Eng., Akdeniz Univ., Antalya, Turkey","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","588","592","Introduction of fluorescence-based Real-Time PCR (RT-PCR) is increasingly used to detect multiple pathogens simultaneously and rapidly by gene expression analysis of PCR amplification data. PCR data is analyzed often by setting an arbitrary threshold that intersect the signal curve in its exponential phase if it exists. The point at which the curve crosses the threshold is called Threshold Cycle (CT) for positive samples. On the other, when such cross of threshold does not occur, the sample is identified as negative. This simple and arbitrary however not an elagant definition of CT value sometimes leads to conclusions that are either false positive or negative. Therefore, the purpose of this paper is to present a stable and consistent alternative approach that is based on machine learning for the definition and determination of CT values.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838207","Algorithm;CT Prediction;Machine Learning;Sigmoid;qPCR","Clustering algorithms;Logistics;Mathematical model;Pathogens;Prediction algorithms;Real-time systems;Xenon","biology computing;learning (artificial intelligence)","PCR amplification data;PCR data;RT-PCR;fluorescence-based real-time PCR;machine learning;optimum CT-prediction;qPCR;threshold cycle","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"On the feasibility of an embedded machine learning processor for intrusion detection","R. Sankaran; R. A. Calix","Computer Science Division, Argonne National Laboratory, Argonne, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1082","1089","Intrusion detection and prevention systems serve a pivotal role in securing computer networks. Using machine learning for an intrusion detection system is important for stopping new attacks that do not have known signatures. Lowering the barrier to entry for microprocessor-based systems has enabled the use of specialized machine learning coprocessors to improve analysis performance. This paper proposes a machine learning approach on a small, low-powered embedded system that uses network-based features to distinguish between normal and abnormal network traffic. A hardware-based approach using a machine learning coprocessor is compared with a software-based approach. Machine learning processors can improve power consumption and processing speed especially when dealing with dig data sets. Results of the analysis show that the machine learning coprocessor obtains 66.67% classification accuracy. Additional results are presented and discussed.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840711","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840711","Embedded systems;network intrusion detection systems (NIDS);speed and energy performance","Big data;Conferences;Decision support systems","Big Data;computer network security;embedded systems;learning (artificial intelligence);microprocessor chips;power aware computing;power consumption;telecommunication traffic","Big Data sets;abnormal network traffic;classification accuracy;computer networks;embedded machine learning processor;intrusion detection;intrusion prevention systems;low-powered embedded system;microprocessor-based systems;network-based features;normal network traffic;power consumption;processing speed;specialized machine learning coprocessors","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Semantic Clone Detection Using Machine Learning","A. Sheneamer; J. Kalita","Univ. of Colorado, Colorado Springs, CO, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","1024","1028","If two fragments of source code are identical to each other, they are called code clones. Code clones introduce difficulties in software maintenance and cause bug propagation. In this paper, we present a machine learning framework to automatically detect clones in software, which is able to detect Types-3 and the most complicated kind of clones, Type-4 clones. Previously used traditional features are often weak in detecting the semantic clones The novel aspects of our approach are the extraction of features from abstract syntax trees (AST) and program dependency graphs (PDG), representation of a pair of code fragments as a vector and the use of classification algorithms. The key benefit of this approach is that our approach can find both syntactic and semantic clones extremely well. Our evaluation indicates that using our new AST and PDG features is a viable methodology, since they improve detecting clones on the IJaDataset 2.0.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0185","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838289","Abstract syntax trees (AST);Classifier algorithms;Code clones;Program dependence graphs (PDG);Software clones","Cloning;Control systems;Feature extraction;Machine learning algorithms;Measurement;Semantics;Syntactics","learning (artificial intelligence);program debugging;software maintenance;source code (software);tree data structures","AST;IJaDataset2.0;PDG;abstract syntax trees;bug propagation;classification algorithms;code fragment pair;machine learning framework;program dependency graphs;semantic code clone detection;software maintenance;source code fragments;syntactic clones;type-3 clone detection;type-4 clone detection","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Analysis for Status of the Road Accident Occurance and Determination of the Risk of Accident by Machine Learning in Istanbul","H. İ. Bülbül; T. Kaya; Y. Tulgar","Dept. of Comput. & Instructional Technol., Gazi Univ., Ankara, Turkey","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","426","430","The traffic has been transformed into the difficult structure in points of designing and managing by the reason of increasing number of vehicle. This situation has discovered road accidents problem, influenced public health and country economy and done the studies on solution of the problem. Large calibrated data agglomerations have increased by the reasons of the technological improvements and data storage with low cost. Arising the need of accession to information from this large calibrated data obtained the corner stone of the data mining. In this study, assignment of the most compatible machine learning classification techniques for road accidents estimation by data mining has been intended.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0075","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838179","Machine learning;classification techniques;data mining;road accident","Algorithm design and analysis;Classification algorithms;Data mining;Data models;Machine learning algorithms;Road accidents","data mining;learning (artificial intelligence);pattern classification;risk analysis;road accidents;road traffic;traffic engineering computing","Istanbul;accident risk determination;calibrated data agglomerations;country economy;data mining;data storage;machine learning classification techniques;public health;road accident occurance status analysis;road traffic","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"#ChennaiFloods: Leveraging Human and Machine Learning for Crisis Mapping during Disasters Using Social Media","B. Anbalagan; C. Valliyammai","MIT, Dept. of Comput. Technol., Anna Univ., Chennai, India","2016 IEEE 23rd International Conference on High Performance Computing Workshops (HiPCW)","20170202","2016","","","50","59","The recent emergence of ubiquitous smart communication devices accelerate people to post the current trending topics in real time as micro blogs, tweets, posts and multimedia content on social media sites along with geographical location tags (geo-tags). Specifically, during recent floods in Tamilnadu 2015, the early warnings about flooded areas emerged to get posted in popular social media with geo-parsed hash tags continuously. In the humanitarian view, the real-time crisis sparked great interest in designing an innovative methodology using big social media data analysis along with supervised machine learning techniques to actuate immediate disaster response and rescue efforts in near future. The proposed system performs disaster tweet collection based on trending disaster hash tags. Our system performs Naive-Bayesian (multinomial) and SSVM classification on collected tweets to identify the severity of the disaster. Based on location-to-interpolation cluster proximity, disaster geographic map is generated for the affected area. Our approach detects the tweets fitted into correct classifier label, and generate an output with detection rate of 79% to 91% of the time. The predicted disaster mapping results are highly accurate up to 89% for real time geo-parsed tweets that matched with actual location at-risk during the flood.","","Electronic:978-1-5090-5773-3; POD:978-1-5090-5774-0","10.1109/HiPCW.2016.016","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837049","Crowd sourcing;Disaster Management;Emergency Informatics;Geo-coding;Geo-parsing;Hash tags;Naive-Bayesian;SSVM Classifier;Social media","Computational modeling;Earthquakes;Floods;Real-time systems;Support vector machines;Twitter","Bayes methods;Big Data;crowdsourcing;data analysis;disasters;emergency management;floods;learning (artificial intelligence);pattern classification;social networking (online);support vector machines;text analysis;ubiquitous computing","#ChennaiFloods;India;SSVM classification;Tamilnadu;big social media data analysis;crisis mapping;disaster geographic map generation;disaster response;disaster severity identification;disaster tweet collection;flooded area early warning;geo-parsed hash tags;geo-tags;geographical location tags;human learning;location at-risk;location-to-interpolation cluster proximity;microblogs;multimedia content;multinomial classification;naive-Bayesian classification;real time geo-parsed tweets;real-time crisis;rescue effort;social media sites;supervised machine learning technique;trending disaster hash tags;trending topics;ubiquitous smart communication devices","","","","","","","19-22 Dec. 2016","","IEEE","IEEE Conference Publications"
"Hedonic Housing Theory — A Machine Learning Investigation","T. Oladunni; S. Sharma","Dept. of Comput. Sci., Bowie state Univ., Bowie, MD, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","522","527","The hedonic pricing theory suggests that house is a differ-entiated commodity, whose value depends on its hetero-geneous characteristics. Application of the theory has been well implemented using OLS Regression. Our study investigates this econometric concept using machine learning algorithms. An improved pricing will benefit buyers, sellers, investors, banks and real estate professionals. Normality test for the experiment was done using Chi-Square Quantile-Quantile plot and Henze-Zirkler's Multivariate Normality Test. Statistical relationship was based on correlation matrix, Kaiser-Meyer-Olkin and Bartlett tests. Support Vector Regression (SVR), K-Nearest Neighbor (K-NN) and Principal Component Re-gression (PCR) were used as learning algorithms. Per-formance comparison of the learning algorithms was done using spearman's rho correlation coefficient. The performance of the model showed that PCR has a slight edge over SVR and K-NN. Also, the study validated the suitability and substitutability of PCR, SVR and K-NN in the implementation of the hedonic pricing theory.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0092","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838196","Bartlett tests;Henze-Zirkler's Multivariate Normality Test;KNN;Kaiser-Meyer-Olkin;PCR;SVM;comparative market analysis;hedonic pricing model;housing prices prediction;linear regression","Correlation;Gaussian distribution;Government;Mathematical model;Pricing;Support vector machines;Urban areas","econometrics;learning (artificial intelligence);pricing;principal component analysis;property market;regression analysis;statistical testing;support vector machines","Bartlett tests;Henze-Zirkler multivariate normality test;K-NN;Kaiser-Meyer-Olkin tests;OLS regression;PCR;SVR;Spearman rho correlation coefficient;banks;chi-square quantile-quantile plot;correlation matrix;differentiated commodity;econometric concept;hedonic housing theory;hedonic pricing theory;investors;k-nearest neighbor;machine learning investigation;principal component regression;real estate professionals;statistical relationship;support vector regression","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Estimating the Number of Receiving Nodes in 802.11 Networks via Machine Learning Techniques","D. Del Testa; M. Danieletto; G. M. Di Nunzio; M. Zorzi","Dept. of Inf. Eng. (DEI), Univ. of Padova, Padua, Italy","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","7","Nowadays, most mobile devices are equipped with multiple wireless interfaces, causing an emerging research interest in device to device (D2D) communication: the idea behind the D2D paradigm is to exploit the proper interface to directly communicate with another user, without traversing any network infrastructure. A first issue related to this paradigm is the need for a coordinator, called controller, able to decide when activating a D2D connection is appropriate and to manage such connection. In this view, the paradigm of Software Defined Networking (SDN) can be exploited both to handle the data flows among the devices and to interact directly with every device. This work is focused on a scenario where a device is selected by the SDN controller, in order to become the master node of a WiFi-Direct network. The remaining nodes, called clients, can exchange data with other nodes through the master. The objective is to infer, through different Machine Learning approaches, the number of nodes actively involved in receiving data, exploiting only the information available at the client side and without modifying any standard communication protocol. The information about the number of client nodes is crucial when, e.g., a user desires a precise prediction of the transmission estimated time of arrival (ETA) while downloading a file.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841821","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841821","","Batteries;Device-to-device communication;IEEE 802.11 Standard;Protocols;Smart phones;Software;Wireless communication","learning (artificial intelligence);software defined networking;wireless LAN","D2D paradigm;IEEE 802.11 network;SDN controller;W-Fi-direct network;device to device communication;machine learning technique;multiple wireless interface;receiving node number estimation;software defined networking","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Extreme Learning Machines for Datasets with Missing Values Using the Unscented Transform","D. P. P. Mesquita; J. P. P. Gomes; L. R. Rodrigues","Dept. of Comput. Sci., Fed. Univ. of Ceara, Fortaleza, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","85","90","The existence of missing data is a common fact in real applications which can significantly affect the data analysis process. In order to overcome this problem, many methods have been proposed in the literature. Extreme Learning Machine (ELM) has become a very popular research topic in machine learning and artificial intelligence areas due to its characteristics such as fast training procedure, good generalization and universal approximation capability. Although ELM has been successfully applied in different domains, its basic formulation cannot handle datasets with missing values properly. This paper presents a variant of the Extreme Learning Machine (ELM) for datasets with missing values. In the proposed method, probability distributions for the missing values are estimated using the expectation maximization (EM) algorithm, assuming that data is normally distributed. The Unscented Transform (UT) is used to estimate the values of the hidden layer outputs, and the weights of the output layer are assigned using the Moore-Penrose Pseudoinverse. Numerical experiments are carried out in order to evaluate the performance of the proposed method in four real world and two synthetic regression datasets. The results show that the proposed method presented a good performance in terms of Average Root-Mean-Squared Error (ARMSE).","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.026","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839567","Extreme Learning Machines;Missing Values;Unscented Transform","Computational modeling;Covariance matrices;Neurons;Random variables;Training;Transforms;Two dimensional displays","approximation theory;data analysis;learning (artificial intelligence);optimisation;regression analysis;statistical distributions;transforms","ARMSE;ELM;EM algorithm;Moore-Penrose pseudoinverse;UT;artificial intelligence;average root-mean-squared error;data analysis;distributed data;expectation maximization algorithm;extreme learning machines;generalization;machine learning;missing data;probability distributions;synthetic regression datasets;training procedure;universal approximation capability;unscented transform","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Eye state prediction using ensembled machine learning models","D. Singla; P. S. Rana","Department of Computer Science and Engineering, Thapar University, Patiala, Punjab - 147004","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","2","","1","5","As electric signals are transmitted between the brain cell s for transferring of data within the brain, capturing of these signals can result in understanding the functionality of brain and other directly linked parts (like eyes, ears, spinal nerves etc) of our body. We can also capture epileptic seizures that are caused by a disruption in the working of brain, by the Electro Encephalogram Test. These electric signals are to be captured by small electrodes placed on human scalp using a standard 10/20 system on an Electro Encephalograph monitor. In this work, we will predict the state of eye (open or closed) by exploring 13 machine learning models on a 15 features dataset of an EEG test. The records of 14 electrodes are used for this prediction. Results are evaluated using 6 different machine learning parameters i.e. Sensitivity, Confusion matrix, Kappa value, Specificity, Accuracy and Receiver Operating Characteristics (ROC) curve. K-fold validation and ensembling of models will be done on best three predictive models pertaining to our dataset.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7824833","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824833","Electro Encephalogram Test;Ensembled Models;Machine Learning Models","Analytical models;Brain models;Data models;Electrodes;Object oriented modeling;Predictive models","biomedical electrodes;electroencephalography;eye;learning (artificial intelligence);matrix algebra","EEG test;Kappa value;ROC curve;brain cell;brain functionality;confusion matrix;data transfer;electric signals;electrodes;electroencephalogram test;electroencephalograph monitor;epileptic seizures;eye state prediction;features dataset;human scalp;machine learning accuracy;machine learning parameters;machine learning specificity;receiver operating characteristics","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Helping HPC Users Specify Job Memory Requirements via Machine Learning","E. R. Rodrigues; R. L. F. Cunha; M. A. S. Netto; M. Spriggs","","2016 Third International Workshop on HPC User Support Tools (HUST)","20170126","2016","","","6","13","Resource allocation in High Performance Computing (HPC) settings is still not easy for end-users due to the wide variety of application and environment configuration options. Users have difficulties to estimate the number of processors and amount of memory required by their jobs, select the queue and partition, and estimate when job output will be available to plan for next experiments. Apart from wasting infrastructure resources by making wrong allocation decisions, overall user response time can also be negatively impacted. Techniques that exploit batch scheduler systems to predict waiting time and runtime of user jobs have already been proposed. However, we observed that such techniques are not suitable for predicting job memory usage. In this paper we introduce a tool to help users predict their memory requirements using machine learning. We describe the integration of the tool with a batch scheduler system, discuss how batch scheduler log data can be exploited to generate memory usage predictions through machine learning, and present results of two production systems containing thousands of jobs.","","Electronic:978-1-5090-3874-9; POD:978-1-5090-3875-6","10.1109/HUST.2016.006","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830451","","Data models;Databases;Load modeling;Memory management;Predictive models;Program processors;Training","formal specification;formal verification;learning (artificial intelligence);parallel processing;resource allocation;scheduling;software tools;systems analysis","HPC;batch scheduler system;high performance computing;job memory requirements specification;machine learning;resource allocation;tool integration","","","","","","","13-13 Nov. 2016","","IEEE","IEEE Conference Publications"
"Comparison of machine learning algorithms for breast cancer","P. Suryachandra; P. V. S. Reddy","CSSE Department, SVEC, Tirupathi, Andhra Pradesh","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","6","Machine learning algorithms are computer programs that try to predict cancer type based on the past data. The eventual goal of Machine learning algorithms in cancer diagnosis is to have a trained machine learning algorithm that gives the gene expression levels from cancer patient, can accurately predict what type and severity of cancer they have, aiding the doctor in treating it. The existing technology compares three different machine learning algorithms are Decision Tree, Support Vector Machine, Bayesian Belief Network. The main drawback of these algorithms is unusual because the number of features (gene expressions) far exceeds the number of cases (samples taken from patients). Performance efficiency can be achieved by comparing two more algorithms are Random Forest and Naïve Bayes algorithms. Because Random forest and Naïve Bayes are used as feature selection method, Random Forest is used to rank the feature importance and applied for relevant feedback. The requirements are weka tool, Java and Relational Database.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830090","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830090","Bayesian Belief Network;Decision Tree;Machine Learning;Support Vector Machine","Algorithm design and analysis;Breast cancer;Classification algorithms;Machine learning algorithms;Prediction algorithms;Training","Bayes methods;Java;belief networks;cancer;decision trees;feature selection;learning (artificial intelligence);medical diagnostic computing;patient treatment;random processes;relational databases;support vector machines","Bayesian belief network;Java;Naïve Bayes algorithms;Weka tool;breast cancer diagnosis;decision tree;feature selection;gene expression levels;machine learning algorithms;patient treatment;random forest algorithms;relational database;support vector machine","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Reverse engineering smart card malware using side channel analysis with machine learning techniques","H. D. Tsague; B. Twala","Smart Token Research Group, Modelling and Digital Science (MDS), Council for scientific and Industrial Research (CSIR), Pretoria, South Africa","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3713","3721","From inception, side channel leakage has been widely used for the purposes of extracting secret information, such as cryptographic keys, from embedded devices. However, in a few instances it has been utilized for extracting other information about the internal state of a computing device. In this paper, we exploit side channel information to recover large parts of the Sykipot malware program executed on a smart card. We present the first methodology to recover the program code of a smart card malware by evaluating its power consumption only. Besides well-studied methods from side channel analysis, we apply a combination of dimensionality reduction techniques in the form of PCA and LDA models to compress the large amount of data generated while preserving as much variance of the original data as possible. Among feature extraction techniques, PCA and LDA are very common dimensionality reduction algorithms that have successfully been applied in many classification problems like face recognition, character recognition, speech recognition, etc. with the chief objective being to eliminate insignificant data (without losing too much information) during the pre-processing step. In addition to quantifying the potential of the created side channel based disassembler, we highlight its diverse and unique application scenarios.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841039","Electromagnetic Templates;Linear Discriminant Analysis;Multivariate Gaussian Distribution;Principal Components Analysis;Reverse Engineering;Side Channel Leakage;k-Nearest Neighbours Algorithm","Algorithm design and analysis;Covariance matrices;Data mining;Malware;Principal component analysis;Reverse engineering;Smart cards","cryptography;data compression;invasive software;learning (artificial intelligence);pattern classification;principal component analysis;reverse engineering;smart cards","LDA;PCA;Sykipot malware program;classification problem;data compression;dimensionality reduction technique;machine learning technique;power consumption;program code;reverse engineering smart card malware;side channel analysis;side channel based disassembler","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Toward Parametric Security Analysis of Machine Learning Based Cyber Forensic Biometric Systems","K. Sadeghi; A. Banerjee; J. Sohankar; S. K. S. Gupta","","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","626","631","Machine learning algorithms are widely used in cyber forensic biometric systems to analyze a subject's truthfulness in an interrogation. An analytical method (rather than experimental) to evaluate the security strength of these systems under potential cyber attacks is essential. In this paper, we formalize a theoretical method for analyzing the immunity of a machine learning based cyber forensic system against evidence tampering attack. We apply our theory on brain signal based forensic systems that use neural networks to classify responses from a subject. Attack simulation is run to validate our theoretical analysis results.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0110","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838214","cyber forensic;electroencephalogram;machine learning;security strength","Brain modeling;Electroencephalography;Feature extraction;Forensics;Iron;Machine learning algorithms;Security","biometrics (access control);brain;digital forensics;learning (artificial intelligence);neural nets","attack simulation;brain signal;cyber attacks;cyber forensic biometric systems;evidence tampering attack;immunity;machine learning algorithms;neural networks;parametric security analysis;security strength","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Review on Machine Learning Based Lesion Segmentation Methods from Brain MR Images","E. Goceri; E. Dura; M. Gunay","Dept. of Comput. Eng., Akdeniz Univ., Antalya, Turkey","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","582","587","Brain lesions are life threatening diseases. Traditional diagnosis of brain lesions is performed visually by neuro-radiologists. Nowadays, advanced technologies and the progress in magnetic resonance imaging provide computer aided diagnosis using automated methods that can detect and segment abnormal regions from different medical images. Among several techniques, machine learning based methods are flexible and efficient. Therefore, in this paper, we present a review on techniques applied for detection and segmentation of brain lesions from magnetic resonance images with supervised and unsupervised machine learning techniques.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0102","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838206","","Biological neural networks;Feature extraction;Histograms;Image segmentation;Lesions;Probabilistic logic","biomedical MRI;image segmentation;medical image processing;unsupervised learning","abnormal region detection;abnormal region segmentation;brain MR images;brain lesions;computer aided diagnosis;machine learning based lesion segmentation method;magnetic resonance imaging;medical images;unsupervised machine learning technique","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Hybrid Machine Learning Model for Range Estimation of Electric Vehicles","B. Zheng; P. He; L. Zhao; H. Li","Dept. of Electr. & Comput. Eng., Ryerson Univ., Toronto, ON, Canada","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","Data-driven solutions to Electric Vehicle (EV) range estimation is attracting attention recently due to the prevalence of Internet of Things (IoT). However, there raise the Big Data problems with the increased volume and number of sensory sources of unstructured data collected from the EV equipped with In-Vehicle Networks. This means that traditional statistical analysis and Machine Learning tools are not suitable to be directly applied to analyse and interpret data. Hence, we aim to develop a Hybrid Machine Learning Model to predict the power consumption of EV trips practically considering multivariate high- dimensional data and meanwhile extract knowledge from the historical trip features for further applications. The proposed Hybrid Model is a modified Self-Organizing Maps (SOM) integrating Regression Trees (RT) to predict the power consumption of EV trips. The experimental results, including both cross-validation and mathematical accuracy measuring criteria, demonstrate that our Hybrid Model could not only provide a better power consumption estimation of EV trips but also reveal the inherent of the EV Big Data.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841506","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841506","","Data models;Estimation;Kernel;Neurons;Power demand;Regression tree analysis;Training","Big Data;Internet of Things;electric vehicles;learning (artificial intelligence);power engineering computing;regression analysis;self-organising feature maps;trees (mathematics)","EV range estimation;Internet of Things;IoT;SOM;electric vehicle range estimation;hybrid machine learning model;multivariate high-dimensional data;power consumption estimation;power consumption prediction;regression trees;self-organizing maps","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Novel Method for Tuning Configuration Parameters of Spark Based on Machine Learning","G. Wang; J. Xu; B. He","Sch. of Comput. & Control Eng., Univ. of Chinese Acad. of Sci., Beijing, China","2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","20170126","2016","","","586","593","Apache Spark is an open source distributed data processing platform, which can use distributed memory abstraction to process large volume of data efficiently. With the application of Apache Spark more and more widely, some problems are exposed. One of the most important aspects is the performance problem. Apache Spark has more than 180 configuration parameters, which can be adjusted by users according to their own specific application so as to optimize the performance. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. In this paper, in order to make the parameter tuning process of Spark more effective, a novel method for tuning configuration of Spark based on machine learning is proposed, which is composed of binary classification and multi-classification. This method can be used to auto-tune the configuration parameters of Spark. Furthermore, several common machine learning algorithms based on the proposed method are explored, and experimental results show that decision tree model (C5.0) is the best model considering the accuracy and computational efficiency. Finally, the experimental results also show that the performance can get average 36% gain with the proposed method compared with the default configuration of Spark.","","Electronic:978-1-5090-4297-5; POD:978-1-5090-4298-2","10.1109/HPCC-SmartCity-DSS.2016.0088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828429","Apache spark;configuration parameter;machine learning","Big data;Computational modeling;Data models;Optimization methods;Predictive models;Sparks;Tuning","data structures;decision trees;distributed processing;learning (artificial intelligence);pattern classification;public domain software","Apache Spark;binary classification;decision tree model;distributed memory abstraction;machine learning;multiclassification;open source distributed data processing;tuning configuration parameters","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"A novel 2D to 3D video conversion system based on a machine learning approach","J. L. Herrera; C. R. del-Blanco; N. García","Grupo de Tratamiento de Im&#225;genes, Universidad Polit&#233;cnica de Madrid, Madrid, Spain","IEEE Transactions on Consumer Electronics","20170202","2016","62","4","429","436","There has been recently a significant increase in the number of available 3D displays and players. Nevertheless, the amount of 3D content has not increased in the same magnitude, creating a gap between 3D offer and demand. To reduce this difference, many algorithms have appeared that perform 2D-to-3D image and video conversion. These algorithms usually require several images from the same scene to perform the conversion. In this paper, an automatic algorithm for estimating the 3D structure of a scene from a single color image is proposed. It is based on the key assumption that color images with similar structure will likely present similar depth structures. The conversion algorithm is split into an offline and an online module to be easily deployable into consumer devices, such as smartphones or TVs. The offline module pre-processes a color+depth image database to speed up the subsequent depth estimation. The online module infers a depth prior from a color query image using the previous database as training data. Then, it is refined through a segmentation-guided filtering. The conversion algorithm has been evaluated in three publicly available databases, and compared with several state-of-theart algorithms to prove its efficiency.","0098-3063;00983063","","10.1109/TCE.2016.7838096","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838096","2D-to-3D conversion;clustering;depth extraction;depth maps;machine learning","Color;Databases;Estimation;Image color analysis;Machine learning algorithms;Three-dimensional displays;Two dimensional displays","image colour analysis;image filtering;image segmentation;learning (artificial intelligence)","2D-to-3D image conversion;2D-to-3D video conversion;3D displays;3D structure estimation;automatic algorithm;color query image;color-depth image database;depth estimation;machine learning approach;offline module pre-process;segmentation-guided filtering;single color image","","","","","","","November 2016","","IEEE","IEEE Journals & Magazines"
"Machine learning in physical design","B. Li; P. D. Franzon","Electrical and Computer Department, North Carolina State University, Raleigh, United States","2016 IEEE 25th Conference on Electrical Performance Of Electronic Packaging And Systems (EPEPS)","20170130","2016","","","147","150","Machine learning, a powerful technique for building models, can rapidly provide accurate predictions. Since Integrated Circuit (IC) design and manufacturing have tremendously high complexity and enormous data, there is a surge in adapting machine learning approach in IC Design stages, as machine learning can provide fast predictions. Recently, machine learning has been used in some IC Design stages (e.g. Physical Verification), but not in Physical Design. In this research, machine learning is adapted to Physical Design. Surrogate Modeling is implemented to predict results after GR in Physical Design. Machine learning models for predicting Detailed Route (DR) results using Global Route (GR) results are also discussed. With surrogate models and machine learning methods, circuit performances after Physical Design (e.g. hold violation check and area) would be predicted quickly.","","Electronic:978-1-5090-6110-5; POD:978-1-5090-6111-2; USB:978-1-5090-2273-1","10.1109/EPEPS.2016.7835438","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835438","Detailed Route;Global Route;Physical Design;Surrogate Modeling;decision tree;machine learning;neural network;prediction;relationship","Adaptation models;Decision trees;Integrated circuit modeling;Physical design;Predictive models;Semiconductor process modeling;Training","circuit complexity;electronic engineering computing;integrated circuit design;integrated circuit manufacture;learning (artificial intelligence)","DR results;GR results;IC design;circuit complexity;detailed route results;global route results;integrated circuit design;integrated circuit manufacturing;machine learning;physical design;surrogate modeling","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"A machine learning based approach for the detection and recognition of Bangla sign language","M. Hasan; T. H. Sajib; M. Dey","Department of Electrical and Electronic Engineering, Chittagong University of Engineering and Technology, 4349, Bangladesh","2016 International Conference on Medical Engineering, Health Informatics and Technology (MediTec)","20170130","2016","","","1","5","Speech impaired people are detached from the mainstream society due to the lacking of proper communication aid. Sign language is the primary means of communication for them which normal people do not understand. In order to facilitate the conversation conversion of sign language to audio is very necessary. This paper aims at conversion of sign language to speech so that disabled people have their own voice to communicate with the general people. In this paper, Hand Gesture recognition is performed using HOG (Histogram of Oriented Gradients) for extraction of features from the gesture image and SVM (Support Vector Machine) as classifier. Finally, predict the gesture image with output text. This output text is converted into audible sound using TTS (Text to Speech) converter.","","Electronic:978-1-5090-5421-3; POD:978-1-5090-5422-0","10.1109/MEDITEC.2016.7835387","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835387","BdSL;Classification;Contouring;Feature;HOG;Prediction;Recognition rate;SVM;TTS Engine","Assistive technology;Databases;Feature extraction;Gesture recognition;Support vector machines;Testing;Training","feature extraction;language translation;medical image processing;pattern classification;sign language recognition;speech synthesis;support vector machines","Bangla sign language detection;Bangla sign language recognition;Histogram of Oriented Gradients;SVM classifier;audible sound;feature extraction;gesture image;hand gesture recognition;machine learning;sign language conversation conversion;speech impaired people;support vector machine;text-to-speech converter","","","","","","","17-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Cloud-based machine learning for predictive analytics: Tool wear prediction in milling","D. Wu; C. Jennings; J. Terpenny; S. Kumara","Department of Industrial and Manufacturing Engineering, The Pennsylvania State University, University Park, PA 16802, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2062","2069","The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840831","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840831","cloud computing;machine learning;prognostics and health management;tool wear prediction","Decision trees;Machine learning algorithms;Maintenance engineering;Prediction algorithms;Predictive models;Training data;Vegetation","Internet of Things;cloud computing;condition monitoring;learning (artificial intelligence);mechanical engineering computing;milling;parallel algorithms","Amazon Elastic Compute Cloud;IIoT;MapReduce framework;classical model-based prognostics;cloud-based parallel machine learning algorithms;data-driven PHM;data-driven methods;dry milling operations;health management;industrial Internet of Things;machinery prognostics;model-based methods;parallel algorithms;parallel random forest algorithm;predictive analytics;predictive models;random processes;real-time monitoring systems;stochastic processes;tool wear prediction","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for student assessment in E-learning using Quinlan's C4.5, Naive Bayes and Random Forest algorithms","T. Mahboob; S. Irfan; A. Karamat","Department of Software Engineering, Fatima Jinnah Women University, Rawalpindi, Pakistan","2016 19th International Multi-Topic Conference (INMIC)","20170206","2016","","","1","8","Student Assessment on e-learning platforms is a debated subject. The focal emphasis of this research study is to predict fair/transparent student evaluation using machine learning algorithms. A prediction on students' final grade showing whether the student will pass or fail would benefit the student/instructor and act as a guide for future recommendations/evaluations on performance. An in depth study on the assessment techniques for e-learning such as Markov Model, metacognitive perspectives has been conducted. A proposed model for fair/transparent student evaluation/performance has also been presented. Specific parameters have been defined that are then efficaciously tested by applying machine learning algorithms. In this study, classifiers such as Decision Trees-J48, Naive Bayes and Random Forest are used to progress the excellence of student data by initially eradicating noisy data, and consequently getting better prognostic accuracy. The scope of the paper has been set for undergraduate programs. The experimental results endow with set of guidelines to those students who have low grades. Performance testing has also been conducted for verification, accuracy and validity of results.","","Electronic:978-1-5090-4300-2; POD:978-1-5090-4301-9","10.1109/INMIC.2016.7840094","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840094","E-learning;HMM (Hidden Markov Model);J48;Naive Bayes;ROC (receiving operating characteristics);Random Forest","Data mining;Data models;Electronic learning;Hidden Markov models;Machine learning algorithms;Physiology;Prediction algorithms","Bayes methods;Markov processes;computer aided instruction;decision trees;further education;learning (artificial intelligence);pattern classification","Markov model;Naive Bayes;Quinlan C4;classifiers;decision trees-J48;e-learning;fair/transparent student evaluation prediction;machine learning;metacognitive perspectives;performance recommendations/evaluations;random forest algorithms;student assessment;student data;undergraduate programs","","","","","","","5-6 Dec. 2016","","IEEE","IEEE Conference Publications"
"Parallel and Distributed Methods for Constrained Nonconvex Optimization-Part II: Applications in Communications and Machine Learning","G. Scutari; F. Facchinei; L. Lampariello; S. Sardellitti; P. Song","School of Industrial Engineering, Purdue University, West-Lafayette, IN, USA","IEEE Transactions on Signal Processing","20170207","2017","65","8","1945","1960","In Part I of this paper, we proposed and analyzed a novel algorithmic framework for the minimization of a nonconvex objective function, subject to nonconvex constraints, based on inner convex approximations. This Part II is devoted to the (nontrivial) application of the framework to the following relevant large-scale problems ranging from communications to machine learning: 1) (generalizations of) the rate profile maximization in MIMO interference broadcast networks; 2) the max-min fair multicast multigroup beamforming problem in a multicell environment; and 3) a general nonconvex constrained bi-criteria formulation for k-sparse variable selection in statistical learning; the two criteria are a nonconvex loss objective function, measuring the fitness of the model to data, and the latter is a nonconvex sparsity-inducing constraint in the general form of difference-of-convex (DC) functions, which allows to accomodate in a unified fashion convex and nonconvex surrogates of the 10 function. The proposed algorithms outperform current state-of-the-art schemes for 1)-3) both theoretically and numerically. For instance, they are the first distributed schemes for the class of problems 1) and 2); and they also lead to subproblems enjoying closed form solutions.","1053-587X;1053587X","","10.1109/TSP.2016.2637314","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776965","Interference networks;multicast beamforming;nonconvex optimization;statistical learning;successive convex approximation","Approximation algorithms;Covariance matrices;Electronic mail;Interference;Linear programming;Signal processing algorithms;Statistical learning","MIMO communication;broadcasting;concave programming;interference (signal);learning (artificial intelligence);minimax techniques;minimisation;telecommunication computing","MIMO interference broadcast networks;constrained nonconvex optimization;convex approximations;difference-of-convex functions;distributed methods;general nonconvex constrained bi-criteria formulation;k-sparse variable selection;machine learning;max-min fair multicast multigroup beamforming problem;minimization;multicell environment;nonconvex objective function;parallel methods;rate profile maximization;sparsity-inducing constraint;statistical learning","","","","","","20161207","April15, 15 2017","","IEEE","IEEE Journals & Magazines"
"Differentiation and Integration of Machine Learning Feature Vectors","X. Mu; A. B. Pavel; M. Kon","Dept. of Math. & Stat., Boston Univ., Boston, MA, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","611","616","This paper presents a new approach to the production of feature maps for the improvement of classification in machine learning. The idea is based on a calculus of differentiation and integration of feature vectors, which can be viewed as functions on a metric space or network. Based on this we propose a novel network-based binary machine learning classifier. We illustrate our method using molecular networks alone to distinguish phenotypes, including cancer types and subtypes. We include feature sets derived from disease-specific gene co-expression networks in different cancer data sets using The Cancer Genome Atlas (TCGA) along with other previously published studies. We also illustrate our network-based predictor on another data type, based on infrared spectroscopy of lung cancer tissue.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838211","cancer;classification;gene coexpression networks;kernel method","Cancer;Correlation;Gene expression;Kernel;Laplace equations;Lungs;Proteins","cancer;genomics;learning (artificial intelligence);medical computing;pattern classification","TCGA;cancer data sets;cancer genome atlas;cancer types;disease-specific gene co-expression networks;feature map production;infrared spectroscopy;lung cancer tissue;machine learning feature vector differentiation;machine learning feature vector integration;molecular networks;network-based binary machine learning classifier;network-based predictor;subtypes","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Exploiting randomness in sketching for efficient hardware implementation of machine learning applications","Ye Wang; C. Caramanis; M. Orshansky","Department of Electrical and Computer Engineering, The University of Texas at Austin, USA, 78712","2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20170123","2016","","","1","8","Energy-efficient processing of large matrices for big-data applications using hardware acceleration is an intense area of research. Sketching of large matrices into their lower-dimensional representations is an effective strategy. For the first time, this paper develops a highly energy-efficient hardware implementation of a class of sketching methods based on random projections, known as Johnson Lindenstrauss (JL) transform. Crucially, we show how the randomness inherent in the projection matrix can be exploited to create highly efficient fixed-point arithmetic realizations of several machine-learning applications. Specifically, the transform's random matrices have two key properties that allow for significant energy gains in hardware implementations. The first is the smoothing property that allows us to drastically reduce operand bit-width in computation of the JL transform itself. The second is the randomizing property that allows bit-width reduction in subsequent machine-learning applications. Further, we identify a random matrix construction method that exploits the special sparsity structure to result in the most hardware-efficient realization and implement the highly optimized transform on an FPGA. Experimental results on (1) the k-nearest neighbor (KNN) classification and (2) the principal component analysis (PCA) show that with the same bit-width the proposed flow utilizing random projection achieves an up to 7× improvement in both latency and energy. Furthermore, by exploiting the smoothing and randomizing properties we are able to use a 1-bit instead of a 4-bit multiplier within KNN, which results in additional 50% and 6% improvement in area and energy respectively. The proposed I/O streaming strategy along with the hardware-efficient JL algorithm identified by us is able to achieve a 50% runtime reduction, a 17% area reduction in the stage of random projection compared to a standard design.","","Electronic:978-1-4503-4466-1; POD:978-1-5090-3421-5","10.1145/2966986.2967038","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827691","","Algorithm design and analysis;Hardware;Principal component analysis;Signal processing algorithms;Smoothing methods;Sparse matrices;Transforms","Big Data;energy conservation;field programmable gate arrays;learning (artificial intelligence);matrix algebra;pattern classification;power aware computing;principal component analysis;random processes;transforms","Big-Data applications;FPGA;I/O streaming;JL transform;Johnson Lindenstrauss transform;KNN;PCA;energy-efficient hardware implementation;hardware acceleration;hardware-efficient JL algorithm;highly efficient fixed-point arithmetic;k-nearest neighbor classification;machine-learning applications;operand bit-width reduction;principal component analysis;projection matrix;random matrix construction;random projections;randomizing property;sketching;smoothing property;sparsity structure","","","","","","","7-10 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach for Fault Detection in Vehicular Cyber-Physical Systems","A. Sargolzaei; C. D. Crane; A. Abbaspour; S. Noei","Dept. of Electr. Eng., Florida Polytech. Univ., Lakeland, FL, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","636","640","A network of vehicular cyber-physical systems (VCPSs) can use wireless communications to interact with each other and the surrounding environment to improve transportation safety, mobility, and sustainability. However, cloud-oriented architectures are vulnerable to cyber attacks, which may endanger passenger and pedestrian safety and privacy, and cause severe property damage. For instance, a hacker can use message falsification attack to affect functionality of a particular application in a platoon of VCPSs. In this paper, a neural network-based fault detection technique is applied to detect and track fault data injection attacks on the cooperative adaptive cruise control layer of a platoon of connected vehicles in real time. A decision support system was developed to reduce the probability and severity of any consequent accident. A case study with its design specifications is demonstrated in detail. The simulation results show that the proposed method can improve system reliability, robustness, and safety.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838216","NN-based fault detection algorith;cloud-oriented architecture;connected vehicles;cooperative adaptive cruise control;cyber-security vulnerabilities;decision support system;fault data injection;vehicular cyber-physical systems","Adaptive systems;Artificial neural networks;Cruise control;Cyber-physical systems;Fault detection;Safety;Sensors","adaptive control;cloud computing;control engineering computing;cyber-physical systems;decision support systems;fault diagnosis;learning (artificial intelligence);neural nets;probability;robust control;safety;security of data;traffic engineering computing;transportation;vehicles;vehicular ad hoc networks;velocity control","VCPS;accident probability reduction;cloud-oriented architectures;cooperative adaptive cruise control;cyber attacks;decision support system;fault data injection attack detection;machine learning;message falsification attack;mobility;neural network-based fault detection technique;passenger safety;pedestrian privacy;property damage;robustness;system reliability;transportation safety;vehicular cyber-physical systems;wireless communications","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Security Perspective of Biometric Recognition and Machine Learning Techniques","B. Arslan; E. Yorulmaz; B. Akca; S. Sagiroglu","Dept. of Comput. Eng., Gazi Univ., Ankara, Turkey","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","492","497","Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study;the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric applications are investigated for the security perspective, the advantages and disadvantages that these techniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0087","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838191","Biometric;face;fingerprint;iris;machine learning;recognition;security;teeth;voice","Biomedical imaging;Databases;Face;Iris recognition;Security;Support vector machines","authorisation;data protection;learning (artificial intelligence);security of data","access security;biometric aplications;biometric recognition;machine learning techniques;personal data protection;remote access model","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach for predicting end price of online auction","M. R. Khadge; M. V. Kulkarni","Dept of Computer Science and Engineering, Vishwakarma Institute of Technology, Pune, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","5","An online auction is an auction held over the internet. In today's era of the internet, economies have changed the way than they were a few years back. The scope and reach of online auctions have been propelled by internet to the prominent level because online auctions break down and remove the physical limitations of traditional auctions such as geography, presence, time, space, etc. As online auction has become one of the fastest growing modes of online commerce transaction, sellers and buyers have started preferring to go online for purchasing and selling products respectively. As eBay has been the leading online auction marketplace for nearly two decades, the proposed system collected huge auction data from eBay and machine learning algorithms are used to predict end price of auction items. The proposed system is trained with 70% of dataset and 30% of dataset is used for testing. The proposed system used Naive Bayes which gives 99.33% accuracy in classification of whether the item will sell or not and kernel mapping SVM gives 96.3% accuracy for predicting whether an item maximize profit or not.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830232","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830232","Machine learning;Naive Bayes(NB);Support Vector Machine(SVM);Uniform Prior Naive Bayes(UPNB);k-Nearest Neighbour(k-NN)","Clustering algorithms;Decision trees;Feature extraction;Internet;Machine learning algorithms;Prediction algorithms;Support vector machines","Bayes methods;Internet;electronic commerce;learning (artificial intelligence);pricing;purchasing;retail data processing;support vector machines;transaction processing","Internet;end price prediction;kernel mapping SVM;machine learning;naive Bayes;online auction;online commerce transaction;product purchasing;product selling","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"A Machine-Learning Approach to Automatic Detection of Delimiters in Tabular Data Files","S. Saurav; P. Schwarz","Viterbi Sch. of Eng., Univ. of Southern California, Los Angeles, CA, USA","2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","20170126","2016","","","1501","1503","Detection of string and column delimiters is a critical first step in the automated ingestion of files containing tabular data. In this paper we present an algorithm that uses a logistic-regression classifier to evaluate whether a particular choice of delimiters is correct. The delimiter choice that is given the highest score by the classifier is chosen as the one most likely to be correct. The algorithm makes the correct choice over 90% of the time on a test data set of files with a variety of different delimiters.","","Electronic:978-1-5090-4297-5; POD:978-1-5090-4298-2","10.1109/HPCC-SmartCity-DSS.2016.0213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828554","data ingestion;delimiters;logistic regression","Automation;Classification algorithms;Conferences;Data models;Logistics;Standards;Training","learning (artificial intelligence);pattern classification;regression analysis;string matching","automated file ingestion;automatic column delimiter detection;automatic string delimiter detection;logistic-regression classifier;machine-learning approach;tabular data files","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Forecasting PM2.5 Concentration Using Spatio-Temporal Extreme Learning Machine","B. Liu; S. Yan; J. Li; Y. Li","Sch. of Software Eng., Beijing Univ. of Technol., Beijing, China","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","950","953","In recent years, air quality has become a severe environmental problem in China. Since bad air quality brought significant influences on traffic and people's daily life, how to predict the future air quality precisely and subtly, has been an urgent and important problem. In this paper, a Spatio-Temporal Extreme Learning Machine (STELM) method is proposed for air quality prediction. STELM considers temporal and spatial characteristics of air quality data and related meteorological data, constructs a prediction model based on ELM, and realizes air quality prediction with more than 80% precision. A prototype system is implemented and the experiments on practical air quality data in Beijing validate the effectiveness of our method and system.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838275","PM2.5 concentration;extreme learning machine;prediction","Air quality;Atmospheric modeling;Data models;Monitoring;Predictive models;Training;Wind forecasting","air quality;environmental science computing;learning (artificial intelligence);weather forecasting","PM2.5 concentration forecasting;STELM method;air quality prediction;meteorological data;spatio-temporal extreme learning machine","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Multiple human skeleton recognition in RGB and depth images with graph theory, anatomic refinement of point clouds and machine learning","E. Gedat; P. Fechner; R. Fiebelkorn; R. Vandenhouten","Telematics Research Group, Wildau Technical University of Applied Sciences, Germany 15745","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000627","000631","Computer visual recognition of multiple human poses infers technological benefit in a variety of systems, including security surveillance, medical therapeutics, sports analytics and many more. For this goal the set of detected body parts on color or depth images must be aligned to reconstruct the skeletons of the humans. Here, an algorithm is introduced that models the body part point clouds using principal component analysis to obtain anatomically correct positions of joints, and that assembles the redundant and/or incomplete number of candidate joints with graph theoretical methods using Suurballe's k-shortest disjoint paths algorithm to build the skeletons. The computations were applied to MOCAP database motions rendered in Blender to produce idealized classified point clouds, and to real human depth images classified with decision forests similar to Shotton et al. For MOCAP data, in 68 images showing 3 persons all 204 skeletons were correctly aligned using 4.285 of 4.682 joints with no false assignment. For 33 real human images each showing 3 people, 71 skeletons were correctly detected with 1 false detection and 17 misses, which is promising with respect to non-perfect body part classification in real world.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844310","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844310","","Algorithm design and analysis;Buildings;Head;Legged locomotion;Neck;Skeleton;Three-dimensional displays","computer vision;decision trees;image classification;image colour analysis;image motion analysis;image reconstruction;image thinning;learning (artificial intelligence);object detection;object recognition;pose estimation;principal component analysis","Blender;MOCAP database motion;RGB image;Suurballe k-shortest disjoint paths algorithm;anatomically correct joint position;body part detection;body part point clouds;color image;computer visual recognition;decision forest;graph theory;human depth image classification;human pose recognition;human skeleton reconstruction;idealized classified point clouds;machine learning;medical therapeutics;multiple human skeleton recognition;nonperfect body part classification;point clouds anatomic refinement;principal component analysis;security surveillance;sports analytics","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Efficient Distributed Machine Learning with Trigger Driven Parallel Training","S. Li; J. Xue; Z. Yang; Y. Dai","Comput. Sci. Dept., Peking Univ., Beijing, China","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","Distributed machine learning is becoming increasingly popular for large scale data mining on large scale cluster. To mitigate the interference of straggler machines, recent distributed machine learning systems support flexible model consistency, which allows worker using a local stale model to compute model update without waiting for the newest model, while limiting the asynchronous step in a certain bound to guarantee the algorithm correctness. However, bounded asynchronous computing can not tolerate consistent straggler. We explore that the root cause of this problem derives from the worker driven parallel training mechanism in existing systems. To address the straggler problem fundamentally and fully leverage the asynchronous efficiency, we propose a novel trigger driven parallel training mechanism, where model server proactively triggers to collect updates from workers instead of passively receiving them, which can inherently avoid the coordinating issue among workers. Besides, we devise a dynamic load balancing strategy to make the sampling frequency of each data equal. Furthermore, bounded asynchronous computing is introduced to achieve the algorithm efficiency, as well as the convergence guarantee. Finally, we integrate the above techniques into a distributed machine learning system called Squirrel. Squirrel provides simple programming interface and can easily deploy machine learning algorithms on distributed cluster. In comparison with traditional worker driven parallel training mechanism, trigger driven mechanism can improve up to 4x faster convergence speed of machine learning algorithm.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841777","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841777","","Clustering algorithms;Computational modeling;Data models;Load modeling;Machine learning algorithms;Servers;Training","data mining;learning (artificial intelligence);resource allocation","Squirrel;bounded asynchronous computing;convergence guarantee;distributed machine learning systems;dynamic load balancing strategy;flexible model consistency;large scale cluster;large scale data mining;local stale model;model server;model update;programming interface;sampling frequency;straggler machines;trigger driven parallel training;trigger driven parallel training mechanism;worker driven parallel training mechanism","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"System-Level Test Case Prioritization Using Machine Learning","R. Lachmann; S. Schulze; M. Nieke; C. Seidl; I. Schaefer","Tech. Univ. Braunschweig, Braunschweig, Germany","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","361","368","Regression testing is the common task of retesting software that has been changed or extended (e.g., by new features) during software evolution. As retesting the whole program is not feasible with reasonable time and cost, usually only a subset of all test cases is executed for regression testing, e.g., by executing test cases according to test case prioritization. Although a vast amount of methods for test case prioritization exist, they mostly require access to source code (i.e., white-box). However, in industrial practice, system-level testing is an important task that usually grants no access to source code (i.e., black-box). Hence, for an effective regression testing process, other information has to be employed. In this paper, we introduce a novel technique for test case prioritization for manual system-level regression testing based on supervised machine learning. Our approach considers black-box meta-data, such as test case history, as well as natural language test case descriptions for prioritization. We use the machine learning algorithm SVM Rank to evaluate our approach by means of two subject systems and measure the prioritization quality. Our results imply that our technique improves the failure detection rate significantly compared to a random order. In addition, we are able to outperform a test case order given by a test expert. Moreover, using natural language descriptions improves the failure finding rate.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0065","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838169","Black-Box Testing;Supervised Machine Learning;System-Level Testing;Test Case Prioritization","Dictionaries;Natural languages;Software;Support vector machines;Testing;Training;Training data","learning (artificial intelligence);natural languages;program testing;regression analysis;source code (software)","SVM rank;black-box meta-data;failure detection rate;failure finding rate;industrial practice;natural language descriptions;natural language test case descriptions;prioritization quality;random order;regression testing;retesting software;software evolution;source code;supervised machine learning;system-level test case prioritization;test case history;test case prioritization;test expert","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Approach for the Predicting Performance of SpMV on GPU","A. Benatia; W. Ji; Y. Wang; F. Shi","Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol. Beijing, Beijing, China","2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)","20170119","2016","","","894","901","Sparse Matrix-Vector Multiplication (SpMV) kernel dominates the computing cost in numerous scientific applications. Many implementations based on different sparse formats were proposed recently for optimizing this kernel on the GPU side. Since the performance of the SpMV varies significantly according to the sparsity characteristics of the input matrix and the hardware features, developing an accurate performance model for this kernel is a challenging task. The traditional approach of building such models by analytical modeling is difficult in practice and requires a thorough understanding of the interaction between the GPU hardware and the sparse code. In this paper, we propose to use a machine learning approach to predict the performance of the SpMV kernel using several sparse formats (COO, CSR, ELL, and HYB) on GPU. We used two popular machine learning algorithms, Support Vector Regression (SVR) and Multilayer Perceptron neural network (MLP). Our experimental results on two different GPUs (Fermi GTX 512 and Maxwell GTX 980 Ti) show that the SVR models deliver the best accuracy with average prediction error ranging between 7% and 14%.","1521-9097;15219097","Electronic:978-1-5090-4457-3; POD:978-1-5090-5382-7","10.1109/ICPADS.2016.0120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823835","GPU computing;Multilayer Perceptron (MLP);Performance modeling;Sparse Matrix-Vector multiplication (SpMV);Support Vector Regression (SVR)","Adaptation models;Arrays;Computational modeling;Graphics processing units;Kernel;Machine learning algorithms;Sparse matrices","graphics processing units;linear algebra;mathematics computing;multilayer perceptrons;regression analysis;subroutines;support vector machines","GPU;MLP;SVR models;SpMV kernel;machine learning;multilayer perceptron neural network;sparse matrix-vector multiplication;support vector regression","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Based Botnet Identification Traffic","A. Azab; M. Alazab; M. Aiash","Telstra, Australia","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","1788","1794","The continued growth of the Internet has resulted in the increasing sophistication of toolkit and methods to conduct computer attacks and intrusions that are easy to use and publicly available to download, such as Zeus botnet toolkit. Botnets are responsible for many cyber-attacks, such as spam, distributed denial-of-service (DDoS), identity theft, and phishing. Most of existence botnet toolkits release updates for new features, development and support. This presents challenges in the detection and prevention of bots. Current botnet detection approaches mostly ineffective as botnets change their Command and Control (C&C) server structures, centralized (e.g., IRC, HTTP), distributed (e.g., P2P), and encryption deterrent. In this paper, based on real world data sets we present our preliminary research on predicting the new bots before they launch their attack. We propose a rich set of features of network traffic using Classification of Network Information Flow Analysis (CONIFA) framework to capture regularities in C&C communication channels and malicious traffic. We present a case study of applying the approach to a popular botnet toolkit, Zeus. The experimental evaluation suggest that it is possible to detect effectively botnets during the botnet C&C communication generated from new updated Zeus botnet toolkit by building the classifier using machine learning from an earlier version and before they launch their attacks using traffic behaviors. Also, show that there is similarity in C&C structures various Botnet toolkit versions and that the network characteristics of botnet C&C traffic is different from legitimate network traffic. Such methods could reduce many different resources needed to identify C&C communication channels and malicious traffic.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0275","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847158","Botnet;Classification;Cyber security;Malware;Network Security","Buildings;Feature extraction;Malware;Monitoring;Servers;Telecommunication traffic;Training","Internet;computer network security;invasive software;learning (artificial intelligence);pattern classification;telecommunication traffic","C&C communication channels;C&C server structures;CONIFA;DDoS;Internet;Zeus botnet toolkit;botnet C&C communication;classification of network information flow analysis;command and control server structures;computer attacks;computer intrusions;cyber-attacks;distributed denial-of-service;encryption deterrent;identity theft;machine learning based botnet identification traffic;malicious traffic;network traffic;phishing;spam","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Modeling the Biomechanical Behavior of Human Soft Tissue","J. D. Martín-Guerrero; M. J. Rupérez-Moreno; F. Martinez-Martínez; D. Lorente-Garrido; A. J. Serrano-López; C. Monserrat; S. Martínez-Sanchis; M. Martínez-Sober","Intell. Data Anal. Lab. (IDAL), Univ. of Valencia, Valencia, Spain","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","247","253","An accurate modeling of the biomechanical properties of human soft tissue is crucial in many clinical applications, such as, radiotherapy administration or surgery. The finite element method (FEM) is the usual choice to carry out such modeling due to its high accuracy. However, FEM is computationally very costly, and hence, its application in real-time or even off-line with short delays are still challenges to overcome. This paper proposes a framework based on Machine Learning to learn FEM modeling, thus having a tool able to yield results that may be sufficiently fast for clinical applications. In particular, the use of ensembles of Decision Trees has shown its suitability in modeling the behavior of the liver and the breast.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0042","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836673","Biomechanics;Breast;Ensembles of decision trees;Human soft tissue;Liver","Biological system modeling;Breast;Data models;Deformable models;Finite element analysis;Liver","biological tissues;biomechanics;decision trees;finite element analysis;learning (artificial intelligence);liver;medical computing","FEM modeling;biomechanical properties;breast behavior modeling;clinical applications;decision trees;finite element method;human soft tissue;liver behavior modeling;machine learning","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Classification of breast masses using Tactile Imaging System and machine learning algorithms","V. Oleksyuk; F. Saleheen; D. F. Caroline; S. A. Pascarella; C. H. Won","Department of Electrical and Computer Engineering, Temple University, Philadelphia, PA 19122, USA","2016 IEEE Signal Processing in Medicine and Biology Symposium (SPMB)","20170209","2016","","","1","4","In this study, we used Tactile Imaging System (TIS) and machine learning algorithms to classify breast masses in vivo as malignant or benign. When the silicone probe at the front end of TIS is compressed against the breast mass, the indentation profile of this waveguide is captured by a CCD camera. Then TIS algorithm determines the size and stiffness of inclusions based on the acquired tactile images. The size and stiffness results are then used as the input features for breast tumor classification algorithms. We compared three tumor classification algorithms: k-nearest neighbor, support vector machine, and Naïve Bayes, which are known to work well for limited data set. We tested these algorithms on twelve human breast tumors. The results were evaluated using the leave-one-out cross validation technique. Among the three algorithms, k-nearest neighbor classifier performed the best with sensitivity of 86% and specificity of 100%.","","Electronic:978-1-5090-6713-8; POD:978-1-5090-6714-5","10.1109/SPMB.2016.7846857","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846857","breast cancer screening;breast tissue stiffness;breast tumor classification;tactile breast imaging","Cancer;Classification algorithms;Feature extraction;Imaging;Indexes;Niobium;Sensitivity","Bayes methods;CCD image sensors;haptic interfaces;image classification;learning (artificial intelligence);medical image processing;support vector machines;tumours","CCD camera;NaIve Bayes classifier;TIS algorithm;benign breast mass;breast mass classification;breast tumor classification;inclusion size;inclusion stiffness;indentation profile;input features;k-nearest neighbor classifier;leave-one-out cross validation;machine learning algorithms;malignant breast mass;sensitivity;silicone probe;specificity;support vector machine;tactile images;tactile imaging system","","","","","","","3-3 Dec. 2016","","IEEE","IEEE Conference Publications"
"Evaluation of rehearsal effects of multimedia content based on EEG using machine learning algorithms","M. Mazher; A. A. Aziz; A. S. Malik","Centre for intelligent Signal and Imaging Research (CISIR) Universiti Teknologi PETRONAS Tronoh, Perak, Malaysia","2016 6th International Conference on Intelligent and Advanced Systems (ICIAS)","20170119","2016","","","1","6","Rehearsal is a common phenomenon of practicing something to make it more resilient in long-term memory. This paper will present the rehearsal effects based on electroencephalography (EEG) recorded data for multimedia contents. Three frequency based features are used to discriminate the three learning states mentioned as L1, L2 and L3 using machine learning algorithms. From these three learning states, L1 is the first learning state whether L2 and L3 are the rehearsal states of L1. The set of spectral features that are used for analysis are based on the intensity weighted mean frequency (IWMF), its bandwidth (IWBW), and spectral power density (PSD). For the analysis, the three brain waves investigated are the alpha waves, theta waves and delta waves. The results of the study show that the alpha waves produce de-synchronization from rest to learning state as compared to other EEG recorded waves. This de-synchronization lead to mental effort imposed by working memory during a learning task. The Alpha wave shows more accuracy in L1 using SVM classifier that is 85% using PSD features, 86% for IWFM and 78.4% using IBWB feature. The results also mention that L3 produces less classifier accuracy value as compared to the L2 and L1 for each of three extracted features. This indicates that L3 requires less mental effort during learning. The findings proved the rehearsal as a good phenomenon of long-term memorized learning.","","Electronic:978-1-5090-0845-2; POD:978-1-5090-0846-9","10.1109/ICIAS.2016.7824134","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824134","Alpha wave;Classifiers;Delta Wave;Multimedia Learning;Spectral Features;Theta wave","Bandwidth;Discrete wavelet transforms;Electroencephalography;Feature extraction;Frequency-domain analysis;Machine learning algorithms;Multimedia communication","electroencephalography;learning (artificial intelligence);medical signal processing;multimedia computing;signal classification;support vector machines","EEG;IWBW;IWMF;PSD;SVM classifier;electroencephalography;intensity weighted bandwidth frequency;intensity weighted mean frequency;machine learning;multimedia content;rehearsal effect evaluation;spectral power density","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Solar irradiance forecasting by machine learning for solar car races","X. Shao; S. Lu; T. G. van Kessel; H. F. Hamann; L. Daehler; J. Cwagenberg; A. Li","IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2209","2216","Solar car race competitions offer realistic conditions to test and demonstrate the state-of-the-art technologies in multidisciplinary fields. In such races the solar panels mounted on the car produce the energy required to power the vehicle. A simulator runs during the race determines the optimal race speed based on the predicted availability of solar energy and other parameters as well as road conditions. The accuracy of the forecasts, especially the solar irradiance forecasts, has a significant impact on the race strategy. Here we report on the experience of providing irradiance forecasts for two races run by the University of Michigan Solar Car Team at the Bridgestone World Solar Challenge 2015 in Australia and at the American Solar Challenge 2016 from Ohio to South Dakota. The probabilistic forecasts of hourly solar irradiance generated from machine learning algorithms were deployed to optimally decide on the race strategy. This work showcases an example of real time decision making based on insights derived from machine learning utilizing big geospatial data - weather models and measurement data from weather station networks.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840851","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840851","big data;machine learning;probabilistic forecast;race strategy;renewable;solar car;solar energy","Atmospheric modeling;Automobiles;Clouds;Predictive models;Wind forecasting","Big Data;decision making;geophysics computing;learning (artificial intelligence);road vehicles;weather forecasting","American Solar Challenge 2016;Australia;Bridgestone World Solar Challenge 2015;Ohio;South Dakota;University of Michigan Solar Car Team;big geospatial data;machine learning;real time decision making;road conditions;road vehicle;solar car races;solar irradiance forecasting;weather station networks","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Efficient statistical validation of machine learning systems for autonomous driving","Weijing Shi; M. B. Alawieh; X. Li; Huafeng Yu; N. Arechiga; N. Tomatsu","ECE Department, Carnegie Mellon University, Pittsburgh, PA 15213, United States","2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20170123","2016","","","1","8","Today's automotive industry is making a bold move to equip vehicles with intelligent driver assistance features. A modern automobile is now equipped with a powerful computing platform to run multiple machine learning algorithms for environment perception (e.g., pedestrian detection) and motion control (e.g., vehicle stabilization). These machine learning systems must be highly robust with extremely small failure rate in order to ensure safe and reliable driving. In this paper, we propose a novel Subset Sampling (SUS) algorithm to efficiently validate a machine learning system. In particular, a Markov Chain Monte Carlo algorithm based on graph mapping is developed to accurately estimate the rare failure rate with a minimal amount of test data, thereby minimizing the validation cost. Our numerical experiments show that SUS achieves 15.2× runtime speed-up over the conventional brute-force Monte Carlo method.","","Electronic:978-1-4503-4466-1; POD:978-1-5090-3421-5","10.1145/2966986.2980077","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827613","","Algorithm design and analysis;Computational modeling;Feature extraction;Learning systems;Machine learning algorithms;Monte Carlo methods;Vehicles","Markov processes;Monte Carlo methods;driver information systems;failure analysis;graph theory;intelligent transportation systems;learning (artificial intelligence);minimisation;motion control;sampling methods;statistical analysis","Markov chain Monte Carlo algorithm;SUS algorithm;automotive industry;autonomous driving;environment perception;failure rate estimation;graph mapping;intelligent driver assistance features;machine learning algorithms;motion control;statistical validation;subset sampling algorithm;validation cost minimization","","","","","","","7-10 Nov. 2016","","IEEE","IEEE Conference Publications"
"Large-Scale Cover Song Retrieval System Developed Using Machine Learning Approaches","T. H. Huang; P. C. Chang","Commun. Eng. Dept., Nat. Central Univ., Taoyuan, Taiwan","2016 IEEE International Symposium on Multimedia (ISM)","20170119","2016","","","591","596","Large-scale cover song retrieval systems should be able to calculate song-to-song similarity and accommodate differences in timing, key, and tempo. Simple vector distance measure is not adequately powerful to perform cover song recognition, and expensive solutions such as dynamic time warping do not scale to millions of instances, making cover song retrieval inappropriate for commercial-scale applications. In this work, we used the content-based music features of songs as input and transformed them into vectors by using the 2D Fourier transform approach. Furthermore, we explored different machine learning approaches to reinforce the pattern of these vectors. By projecting the songs into a semantic vector space, we can use the efficient nearest neighbor algorithm to compare the similarity of songs and retrieve the most similar songs from the large-scale database. The proposed system is not only efficient enough to perform scalable content-based music retrieval but can also develop the potential of machine learning approaches, making similar music recognition applications faster and more accurate.","","Electronic:978-1-5090-4571-6; POD:978-1-5090-4572-3","10.1109/ISM.2016.0128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823695","2D Fourier transform;large-scale music information retrieval;machine learning;million song dataset","Feature extraction;Fourier transforms;Machine learning algorithms;Multiple signal classification;Principal component analysis;Two dimensional displays","Fourier transforms;content-based retrieval;learning (artificial intelligence);music","2D Fourier transform;content-based music features;content-based music retrieval;large scale cover song retrieval system;large-scale database;machine learning;nearest neighbor algorithm;semantic vector space","","","","","","","11-13 Dec. 2016","","IEEE","IEEE Conference Publications"
"Predicting daily mean solar power using machine learning regression techniques","F. Jawaid; K. NazirJunejo","Karachi Institute of Economics and Technology, Pakistan","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","355","360","Daily mean solar irradiance is the most critical parameter in sizing the installation of solar power generation units. The average solar irradiation on a specific location can help predict the amount of electricity that will be generated through solar panels and an accurate forecast can help in calculating the size of the system, return on investment (ROI) and system load measurements. To predict the mean solar irradiation Wh/m<sup>2</sup> various regression algorithms have been used in conjunction with various parameters related to solar irradiance. In this paper we present a comparative analysis of forecasting through artificial neural networks (ANN) against the standard regression algorithms. Furthermore, we show that incorporation of azimuth and zenith parameters in the model significantly improves the performance.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845051","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845051","","Azimuth;Prediction algorithms;Predictive models;Solar energy;Support vector machines;Weather forecasting","electric power generation;learning (artificial intelligence);load forecasting;neural nets;power engineering computing;regression analysis;solar cell arrays;solar power","ANN;ROI;artificial neural networks;azimuth parameters;daily mean solar irradiance;daily mean solar power prediction;electricity prediction;machine learning regression techniques;mean solar irradiation;return on investment;solar panels;solar power generation units installation sizing;system load measurements;zenith parameters","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Improving backfilling by using machine learning to predict running times","E. Gaussier; D. Glesser; V. Reis; D. Trystram","LIG, Univ. Grenoble-Alpes, Grenoble, France","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","20170126","2015","","","1","10","The job management system is the HPC middleware responsible for distributing computing power to applications. While such systems generate an ever increasing amount of data, they are characterized by uncertainties on some parameters like the job running times. The question raised in this work is: To what extent is it possible/useful to take into account predictions on the job running times for improving the global scheduling? We present a comprehensive study for answering this question assuming the popular EASY backfilling policy. More precisely, we rely on some classical methods in machine learning and propose new cost functions well-adapted to the problem. Then, we assess our proposed solutions through intensive simulations using several production logs. Finally, we propose a new scheduling algorithm that outperforms the popular EASY backfilling algorithm by 28% considering the average bounded slowdown objective.","","Electronic:978-1-4503-3723-6; POD:978-1-5090-0273-3","10.1145/2807591.2807646","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832838","","Estimation;Hidden Markov models;Prediction algorithms;Resource management;Scheduling algorithms;Uncertainty","learning (artificial intelligence);middleware;parallel processing;scheduling","EASY backfilling policy;HPC middleware;cost functions;distributing computing power;global scheduling;job management system;job running time prediction;machine learning;production logs","","1","","","","","15-20 Nov. 2015","","IEEE","IEEE Conference Publications"
"Using the machine learning approach to predict patient survival from high-dimensional survival data","Wenbin Zhang; Jian Tang; Nuo Wang","Department of Computer Science, Memorial University of Newfoundland, St. John's, NL, Canada","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1234","1238","Survival analysis with high-dimensional data deals with the prediction of patient survival based on their gene expression data and clinical data. A crucial task for the accuracy of survival analysis in this context is to select the features highly correlated with the patient's survival time. Since the information about class labels is hidden, existing feature selection methods in machine learning are not applicable. In contrast to classical statistical methods which address this issue with the Cox score, we propose to tackle this problem by discretizing the survival time of patients into a suitable number of subgroups via silhouettes clustering validity. To cope with patients' censoring, we use “k-nearest neighbor” based on clinical parameters. Feature selection is then accomplished using Fast Correlation-Based Filtering approach from machine learning community. The effectiveness and efficiency of the proposed method are demonstrated through comparisons with classical statistical methods on real-world datasets and simulation datasets.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822695","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822695","Survival prediction;high-dimensional survival data;machine learning;statistical method","Cancer;Computers","bioinformatics;feature extraction;genetics;learning (artificial intelligence);statistical analysis","classical statistical methods;fast correlation-based filtering approach;feature selection methods;gene expression data;high-dimensional survival data;k-nearest neighbor;machine learning approach;patient survival prediction","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Stick Must Fall: Using Machine Learning to Predict Human Error in Virtual Balancing Task","I. Zgonnikova; A. Zgonnikov; S. Kanemoto","Fac. of Appl. Math. & Control Processes, St.-Petersburg State Univ., St. Petersburg, Russia","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","173","177","This work presents a new approach to prediction of human control error in unstable systems. We consider virtual inverted pendulum (stick) as a characteristic example of such system. The proposed approach is based on applying classification via machine learning to distinguish between the samples of human control corresponding to successful balancing and critical control errors (resulting in stick fall). To illustrate the approach, we analyze the previously collected data on human balancing of virtual overdamped stick. The obtained results demonstrate that, at least in the considered balancing problem, as much as 73% of human control errors can be successfully predicted in advance (as early as one second before the stick fall).","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0032","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836663","Human error prediction;Human operator performance;Human-machine interaction;Machine learning;Stick balancing","Conferences;Control systems;Feature extraction;Prediction algorithms;Real-time systems;Standards;Time series analysis","human factors;learning (artificial intelligence);nonlinear systems;pattern classification;pendulums","balancing errors;critical control errors;data classification;human control error prediction;machine learning;unstable systems;virtual balancing task;virtual inverted pendulum;virtual overdamped stick","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Multiclass mood classification on Twitter using lexicon dictionary and machine learning algorithms","G. Gaikwad; D. J. Joshi","Vishwakarma Institute of Technology, Pune","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","1","","1","6","Sentiment Analysis (SA), an application of Natural Language processing (NLP), has evolved a lot over the past decade. It is also known as opinion mining, mood extraction and emotion analysis. Social Media provides a good platform for people to share their thoughts, feelings and views. Social Media like Twitter is flooded everyday with tweets from users. These tweets can be effectively used for extracting mood of a person. Extraction of mood from texts is a challenging task as it involves understanding the underlying semantic. In this paper a new approach is been proposed that uses lexicon database to assign each word in a text a value called as `Impact Factor'. The Impact Factor is nothing but how a single word is affecting the whole sentence in which it is used. Every word in a sentence has its own Impact Factor and it tries to influence the overall semantic of the sentence. Higher the value of Impact Factor of a word in the sentence, the more influential it is. The approach proposed in this paper makes use of lexicon based approach as well as machine based learning. It uses AFINN lexicon database to assign Impact Factor to words and Support Vector Machine (SVM), k-Nearest Neighbors (KNN) and Naive Bayesian (NB) machine learning algorithms for training and testing the model.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7823247","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823247","AFINN;Impact Factor;Sentiment analysis;emoticon;lexicon approach;machine learning;twitter social network","Feature extraction;Mood;Semantics;Sentiment analysis;Social network services;Support vector machines;Training","database management systems;dictionaries;learning (artificial intelligence);pattern classification;sentiment analysis;social networking (online);support vector machines","AFINN lexicon database;KNN;NB machine learning algorithm;NLP;SA;SVM;Twitter;impact factor;k-nearest neighbor;lexicon dictionary algorithm;multiclass mood classification;naive Bayesian machine learning algorithm;natural language processing;sentiment analysis;social media;support vector machine","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"QED: Groupon's ETL management and curated feature catalog system for machine learning","D. C. Spell; L. Y. Wang; R. T. Shomer; B. Nooraei; J. Waggoner; X. H. T. Zeng; J. Y. Chung; K. C. Cheng; D. Kirsche","Groupon, Inc.","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1639","1646","In today's technology industry where machine learning has become essential, the effectiveness of algorithms ultimately depends on a robust data pipeline, and fast model prototyping and tuning require easy feature discovery and consumption. Careful management of ETL processes and their produced datasets is key to both model development in the research stage and model execution in the production environment. In this paper we present QED, an ETL management and curated feature catalog system that provides robust, streamlined machine learning pipelines. First, QED promises dynamic, reliable, and timely data delivery to the production pipeline. Its enhanced ETL process persists data from upstream sources in local data stores and ensures their correctness. Second, in contrast to previous systems, QED is capable not only of producing a daily scoring dataset, but also a training dataset with minimized bias by preserving the historical observations of feature values. Third, QED's multiple data store design allows batch process of large datasets as well as fast random access to single records. Finally, its curated feature catalog system enables sharing and reuse of machine learning features. QED serves as the data backend for a variety of machine learning models that provide key insights into the global business, and optimize the daily operations of Groupon.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840776","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840776","big data management;data pipeline;feature catalog;machine learning","Data mining;Feature extraction;Machine learning algorithms;Metadata;Pipelines;Training","batch processing (computers);data handling;learning (artificial intelligence);marketing data processing","ETL management;Groupon;QED;batch process;curated feature catalog system;data backend;data delivery;extract-transform-load management;machine learning;multiple data store design","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Based Approaches for Sex Identification in Bioarchaeology","D. L. Miholca; G. Czibula; I. G. Mircea; I. G. Czibula","Fac. of Math. & Comput. Sci., Babes-Bolyai Univ., Cluj-Napoca, Romania","2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","20170126","2016","","","311","314","In this paper we approach from a machine learning perspective the problem of identifying the sex of archaeological remains from anthropometric data, an important problem with in the field of bio archaeology. As the conditions for detecting the sex of a skeleton are not entirely known, machine learning based data mining models are appropriate to address this problem since they are able to capture unobservable patterns in data. These patterns could be relevant for classifying a skeletal remain as male or female. We propose two machine learning models based on artificial neural networks for identifying the sex of human skeletons from bone measurements. The proposed models are experimentally evaluated on case studies generated from two data sets publicly available in the archaeological literature. The obtained results show that the proposed data mining models are effective for detecting the sex of archaeological remains, confirming the potential of our proposal.","2470-881X;2470881X","Electronic:978-1-5090-5707-8; POD:978-1-5090-5708-5","10.1109/SYNASC.2016.056","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829628","bioarchaeology;machine learning;radial basis function networks;self-organizing maps;sex identification","Bones;Computational modeling;Data mining;Data models;Self-organizing feature maps;Training","anthropometry;archaeology;bone;data mining;learning (artificial intelligence);neural nets","anthropometric data;archaeological remains;artihcial neural networks;bioarchaeology;bone measurements;machine learning based approach;machine learning-based data mining model;skeleton sex detection","","","","","","","24-27 Sept. 2016","","IEEE","IEEE Conference Publications"
"Co-MLM: A SSL Algorithm Based on the Minimal Learning Machine","W. L. Caldas; J. P. P. Gomes; M. G. Cacais; D. P. P. Mesquita","Comput. Sci. Dept., Fed. Univ. of Ceara, Fortaleza, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","97","102","Semi-supervised learning is a challenging topic in machine learning that has attracted much attention in recent years. The availability of huge volumes of data and the work necessary to label all these data are two of the reasons that can explain this interest. Among the various methods for semi-supervised learning, the co-training framework has become popular due to its simple formulation and promising results. In this work, we propose Co-MLM, a semi-supervised learning algorithm based on a recently supervised method named Minimal Learning Machine (MLM), built upon co-training framework. Experiments on UCI data sets showed that Co-MLM has promising performance in compared to other co-training style algorithms.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.028","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839569","Co-Training;Minimal Learning Machine;Semi-supervised Learning","Cost function;Intelligent systems;Predictive models;Semisupervised learning;Standards;Support vector machines;Training","learning (artificial intelligence)","Co-MLM;MLM;SSL algorithm;UCI data sets;co-training framework;minimal learning machine;semisupervised learning algorithm","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Energy efficient link adaptation using machine learning techniques for wireless OFDM","T. Desai; H. Shah","Communication Systems Engineering, G.H. Patel College of Engineering & Technology, Vallabh Vidyanagar, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","4","Energy Efficiency in wireless communication is very important due to the slow progress in battery technology with improvement in technology. In this paper, we applied energy efficient link adaptation using Machine learning techniques in Orthogonal Frequency Division Multiplexing (OFDM). We sounding the channel condition periodically and observing the channel parameters. Our aim is to select the optimal mode of the channel which maximizes energy efficiency or throughput of data subject to a given quality of service (QoS) constraint. Simulation results show that the proposed solution achieves significant improvement over existing link adaptation algorithms. Presented work aims on maximizing the throughput and provides orders of magnitude gain in energy efficiency linked to poorly chosen fixed modes when used for energy efficiency maximization purposes.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830122","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830122","Energy Efficiency;Link Adaptation;Machine Learning Techniques;OFDM","Classification algorithms;Energy consumption;Energy efficiency;OFDM;Throughput;Transmitters;Wireless communication","OFDM modulation;channel allocation;learning (artificial intelligence);power aware computing;quality of service;radiocommunication","QoS constraint;battery technology;channel condition;energy efficiency maximization;energy efficient link adaptation;link adaptation algorithms;machine learning;orthogonal frequency division multiplexing;quality of service;wireless OFDM;wireless communication","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning-based clinical decision support system for early diagnosis from real-time physiological data","M. M. Baig; H. G. Hosseini; M. Lindén","Department of Electrical and Electronic Engineering, Auckland University of Technology, New Zealand","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2943","2946","This research aims to design a self-organizing decision support system for early diagnosis of key physiological events. The proposed system consists of pre-processing, clustering and diagnostic system, based on self-organizing fuzzy logic modeling. The clustering technique was employed with empirical pattern analysis, particularly when the information available is incomplete or the data model is affected by vagueness, which is mostly the case with medical/clinical data. Clustering module can be viewed as unsupervised learning from a given dataset. This module partitions the patient vital signs to identify the key relationships, patterns and clusters among the medical data. Secondly, it uses self-organizing fuzzy logic modeling for early symptom and event detection. Based on the clustering outcome, when detecting abnormal signs, a high level of agreement was observed between system interpretation and human expert diagnosis of the physiological events and signs.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848584","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848584","Clinical decision support system;Clustering analysis;Early diagnosis of clinical events;Machine learning;Patient monitoring;Self-organising fuzzy system","Biomedical monitoring;Decision support systems;Fuzzy logic;Hypertension;Medical diagnostic imaging;Monitoring","decision support systems;fuzzy logic;fuzzy neural nets;medical information systems;pattern clustering;self-organising feature maps;unsupervised learning","abnormal sign detection;clustering module;diagnostic system module;empirical pattern analysis;human expert diagnosis;machine learning-based clinical decision support system;patient vital sign partitioning;physiological event diagnosis;preprocessing module;real-time physiological data;self-organizing decision support system;self-organizing fuzzy logic modeling;unsupervised learning","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Predicting Thread Profiles across Core Types via Machine Learning on Heterogeneous Multiprocessors","C. V. Li; V. Petrucci; D. Mossé","Dept. of Comput. Sci., Univ. of Pittsburgh, Pittsburgh, PA, USA","2016 VI Brazilian Symposium on Computing Systems Engineering (SBESC)","20170123","2016","","","56","62","Given that energy consumption has become one of the most important issues in computer systems, Heterogeneous Multiprocessors (HMPs) have been introduced, where large high performing and small power-efficient cores can co-exist on the same platform and share the processing of the workload. Clearly, the concept is the same whether it is multiple processors on a board or a chip multiprocessor with several cores on a chip. With the advent of HMPs, thread scheduling becomes much more challenging, while having to deal with thread to processor-type mapping. In particular, it is important that the operating system is able to understand the workload behavior when a thread is to be migrated to a core of a different type. In this paper, we describe a thread characterization method that explores machine learning techniques to automate and improve the accuracy of predicting thread execution across different processor types. We use hardware performance counters and use machine learning to predict performance when moving a thread to another core type on heterogeneous processors. We show that our characterization scheme achieves higher structural similarity (SSIM) values when predicting performance indicators, such as instructions per cycle and last-level cache misses, commonly used to determine the mapping of threads to processor types at runtime. We also show that support vector regression achieves higher SSIM values when compared to linear regression, and has very low (1%) overhead.","","Electronic:978-1-5090-2653-1; POD:978-1-5090-2654-8","10.1109/SBESC.2016.017","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828285","","Benchmark testing;Data models;Hardware;Instruction sets;Predictive models;Radiation detectors","cache storage;microprocessor chips;multiprocessing systems;processor scheduling;regression analysis;support vector machines","HMP;SSIM values;chip multiprocessor;computer systems;core types;energy consumption;heterogeneous multiprocessors;instructions-per-cycle;last-level cache misses;machine learning techniques;operating system;structural similarity values;support vector regression;thread characterization method;thread execution prediction;thread profile prediction;thread scheduling;thread-to-processor-type mapping;workload behavior;workload processing","","","","","","","1-4 Nov. 2016","","IEEE","IEEE Conference Publications"
"Dynamic Line Rating Using Numerical Weather Predictions and Machine Learning: A Case Study","J. L. Aznarte; N. Siebert","Department of Artificial Intelligence, 28040, Spain","IEEE Transactions on Power Delivery","20170120","2017","32","1","335","343","In this paper, a dynamic line-rating experiment is presented in which four machine-learning algorithms (generalized linear models, multivariate adaptive regression splines, random forests and quantile random forests) are used in conjunction with numerical weather predictions to model and predict the ampacity up to 27 h ahead in two conductor lines located in Northern Ireland. The results are evaluated against reference models and show a significant improvement in performance for point and probabilistic forecasts. The usefulness of probabilistic forecasts in this field is shown through the computation of a safety-margin forecast which can be used to avoid risk situations. With respect to the state of the art, the main contributions of this paper are an in depth look at explanatory variables and their relation to ampacity, the use of machine learning with numerical weather predictions to model ampacity, the development of a probabilistic forecast from standard point forecasts, and a favorable comparison to standard reference models. These results are directly applicable to protect and monitor transmission and distribution infrastructures, especially if renewable energy sources and/or distributed power generation systems are present.","0885-8977;08858977","","10.1109/TPWRD.2016.2543818","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442844","Dynamic line rating;forecasting;machine learning;time series","Conductors;Correlation;Forecasting;Power system dynamics;Predictive models;Temperature measurement;Weather forecasting","geophysics computing;learning (artificial intelligence);meteorology;numerical analysis;regression analysis","Northern Ireland;distributed power generation systems;distribution infrastructures;dynamic line rating;machine-learning algorithms;multivariate adaptive regression splines;numerical weather predictions;probabilistic forecasts;quantile random forests;renewable energy sources;safety-margin forecast;standard point forecasts;transmission infrastructures","","","","","","20160329","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Automatic labelling of clusters with discrete and continuous data using supervised machine learning","J. M. de Sousa; R. L. de Sales Santos; L. A. Lopes; V. P. Machado; I. S. Silva","Departamento de Computa&#x00E7;&#x00E3;o Universidade, Federal do Piau&#x00ED;, Teresina, Piau&#x00ED;","2016 35th International Conference of the Chilean Computer Science Society (SCCC)","20170130","2016","","","1","10","The clustering problem has been considered one of the most relevant problems in the research area of unsupervised learning. However, the comprehension and definition of such clusters is not a trivial task, making necessary their identification, i.e., assign a label to each cluster. To address the problem of labelling learning, this paper presents a methodology based on techniques for supervised learning, unsupervised learning and a discretization model, aimed to increasing the speed and accuracy of the algorithm. Thus, a method with unsupervised learning algorithm is applied to the clustering problem, and the supervised learning algorithm is responsible for detecting the meaningful attributes to define each formed cluster. Some strategies are used to form a methodology that presents a label (based on attributes and values) for each provided cluster. Such methodology is applied to one database, in which acceptable results were achieved with an average that exceeds 92.89% of correctly labelled elements.","","Electronic:978-1-5090-3339-3; POD:978-1-5090-3340-9","10.1109/SCCC.2016.7836060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836060","Clustering;Machine learning;Pattern recognition","Clustering algorithms;Computational modeling;Electronic mail;Labeling;RNA;Supervised learning;Unsupervised learning","pattern clustering;unsupervised learning","automatic clusters labelling;clustering problem;continuous data;discrete data;discretization model;supervised machine learning;unsupervised learning","","","","","","","10-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Consensus-Based Parallel Extreme Learning Machine for Indoor Localization","Z. Qiu; H. Zou; H. Jiang; L. Xie; Y. Hong","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","In the era of Internet of Things, WiFi fingerprinting based indoor positioning system (IPS) has been recognized as the most promising IPS for indoor location-based service. Fingerprinting-based algorithms critically rely on a fingerprint database built from machine learning methods, and extreme machine learning (ELM) is preferred for its fast training speed. However, traditional WiFi based IPS usually requires a central server to collect and process data, which is tremendously vulnerable to server breakdown and communication link failure. To address this issue, we propose Consensus-based Parallel ELM (CPELM) to enhance the robustness by distributing the data on different computation nodes. Specifically, each node keeps updating the corresponding terms in the ELM regression equation as a weighted average of those from neighboring nodes based on the distributed consensus iterative scheme. Upon the agreement of the regression equation within the network, the output weight of ELM can be calculated on some nodes and propagated to other nodes. Extensive simulation with real data has demonstrated that CPELM is able to produce same level of localization accuracy as centralized ELM without incurring additional computational cost, and in the meanwhile provides more robustness to the entire IPS in case of server breakdown and link failures.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841684","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841684","","Electric breakdown;IEEE 802.11 Standard;IP networks;Machine learning algorithms;Robustness;Servers;Training","Internet of Things;failure analysis;indoor navigation;iterative methods;learning (artificial intelligence);radionavigation;regression analysis;telecommunication computing;telecommunication network reliability;wireless LAN","CPELM;ELM regression equation;IPS;Internet of Things;WiFi fingerprinting based indoor positioning system;central server;communication link failure;consensus-based parallel extreme machine learning;data collection;data processing;distributed consensus iterative scheme;fingerprint database;indoor localization;neighboring nodes;server breakdown","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Hourly Solar Irradiance Forecasting Based on Machine Learning Models","F. N. Melzi; T. Touati; A. Same; L. Oukhellou","IRT SystemX, Palaiseau, France","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","441","446","In recent years, many research studies are conducted into the use of smart meters data for developping decision-making tools including both analytical, forecasting and display purposes. Forecasting energy generation or forecasting energy consumption demand are indeed central problems for urban stakeholders (electricity companies and urban planners). These issues are helpful to allow them ensuring an efficient planning and optimization of energy resources. This paper investigates the problem for forecasting the hourly solar irradiance within a Machine Learning (ML) framework using Similarity method (SIM), Support Vector Machine (SVM) and Neural Network (NN). These approaches rely on a methodology which takes into account the previous hours of the predicting day and also the days having the same number of sunshine hours in the history. The study is conducted on a real data set collected on the Paris suburb of Alfortville. A comparison with two time series approaches namely Naive method and Autoregressive Moving Average Model (ARMA) is performed. This study is the first step towards the development of the hourly solar irradiance forecasting hybrid models.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0078","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838182","","Autoregressive processes;Forecasting;Hidden Markov models;Numerical models;Predictive models;Time series analysis;Weather forecasting","autoregressive moving average processes;learning (artificial intelligence);support vector machines","ARMA;ML framework;NN;Naive method;SIM;SVM;autoregressive moving average model;decision making tools;electricity companies;energy resources;forecasting energy consumption demand;forecasting energy generation;hourly solar irradiance forecasting hybrid models;machine learning models;neural network;similarity method;smart meters data;support vector machine;urban planners;urban stakeholders","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Re-architecting the on-chip memory sub-system of machine-learning accelerator for embedded devices","Y. Wang; Huawei Li; Xiaowei Li","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20170123","2016","","","1","6","The rapid development of deep learning are enabling a plenty of novel applications such as image and speech recognition for embedded systems, robotics or smart wearable devices. However, typical deep learning models like deep convolutional neural networks (CNNs) consume so much on-chip storage and high-throughput compute resources that they cannot be easily handled by mobile or embedded devices with thrifty silicon and power budget. In order to enable large CNN models in mobile or more cutting-edge devices for IoT or cyberphysics applications, we proposed an efficient on-chip memory architecture for CNN inference acceleration, and showed its application to our in-house general-purpose deep learning accelerator. The redesigned on-chip memory subsystem, Memsqueezer, includes an active weight buffer set and data buffer set that embrace specialized compression methods to reduce the footprint of CNN weight and data set respectively. The Memsqueezer buffer can compress the data and weight set according to their distinct features, and it also includes a built-in redundancy detection mechanism that actively scans through the work-set of CNNs to boost their inference performance by eliminating the data redundancy. In our experiment, it is shown that the CNN accelerators with Memsqueezer buffers achieves more than 2× performance improvement and reduces 80% energy consumption on average over the conventional buffer design with the same area budget.","","Electronic:978-1-4503-4466-1; POD:978-1-5090-3421-5","10.1145/2966986.2967068","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827590","","Bandwidth;Buffer storage;Hardware;Indexes;Machine learning;System-on-chip;Throughput","convolution;learning (artificial intelligence);neural nets","CNN inference acceleration;CNNs;IoT;Memsqueezer buffer;active weight buffer set;built-in redundancy detection mechanism;cyberphysics applications;data buffer set;data compression;data redundancy;deep convolutional neural networks;embedded devices;in-house general-purpose deep learning accelerator;machine learning accelerator;mobile device;on-chip memory architecture;on-chip memory subsystem;on-chip storage;specialized compression methods","","","","","","","7-10 Nov. 2016","","IEEE","IEEE Conference Publications"
"User Movement Prediction: The Contribution of Machine Learning Techniques","S. Banitaan; M. Azzeh; A. B. Nassif","Dept. of Math., Univ. of Detroit Mercy, Detroit, MI, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","571","575","Ambient Assisted Living (AAL) aims to increase the time older people or disabled people can live in their home environment by assisting them in performing activities of daily living by the use of intelligent products. Localization and tracking of users in indoor environment are the main components of AAL. Wireless sensor networks is an effective technology to accomplish these services by using Received Signal Strength (RSS) information. This work seeks to investigate the effect of machine learning techniques on the accuracy of user movement prediction. Five base classifiers and two ensemble learning approaches are employed and the results are evaluated in terms of precision recall, and F-measure. A real-life benchmark dataset in the area of AAL is used for evaluation. The results show that J48 is the best performing model compared to the other base-level classifiers. It also shows that Bagged J48 achieves the best performance.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0100","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838204","Bagging;Boosting;Classification;Ensemble Learning;Indoor User Movement;Machine Learning","Bagging;Benchmark testing;Boosting;Radar tracking;Tracking;Training;Wireless sensor networks","RSSI;handicapped aids;learning (artificial intelligence);wireless sensor networks","AAL;RSS information;ambient assisted living;disabled people;home environment;indoor environment;intelligent products;machine learning techniques;movement prediction;real-life benchmark dataset;received signal strength;user movement prediction;wireless sensor networks","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Biomarker discovery and validation for Parkinson's Disease: A machine learning approach","D. Bazazeh; R. M. Shubair; W. Q. Malik","Electrical & Computer Engineering Department, Khalifa University, UAE","2016 International Conference on Bio-engineering for Smart Technologies (BioSMART)","20170130","2016","","","1","6","Parkinson's disease is recognized as the second most common neurodegenerative disorder, after Alzheimer's, however, diagnosis remains a challenging task with no gold standard clinical test available. The use of machine learning (ML) techniques has been seen across a wide array of applications in bioinformatics, including biomarker discovery and validation. Biomarkers are defined as an objective measure of biological parameters that can diagnose a disease, monitor its progression, or predict therapeutic pathologies. Biomarkers range from genetic and biochemical, to neuroimaging and clinical. Biomarker identification is a sequential and repetitive process that consists of many critical steps, including data-preprocessing, feature extraction, model selection and biomarker validation. This paper provides an overview of the symptoms of Parkison's disease, both motor and non-motor, as well as a description of the state-of-the-art biomarkers currently in use. Additionally, the paper provides a synopsis showcasing the several subdivisions of the discovery and validation process.","","Electronic:978-1-5090-4568-6; POD:978-1-5090-4569-3; USB:978-1-5090-4567-9","10.1109/BIOSMART.2016.7835465","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835465","","Diseases;Fluids;Neuroimaging;Proteins;Single photon emission computed tomography;Sleep","bioinformatics;diseases;feature extraction;learning (artificial intelligence)","Alzheimer disease;Parkison disease symptom;bioinformatics;biomarker discovery;biomarker validation;data preprocessing;feature extraction;machine learning technique;model selection;neurodegenerative disorder;neuroimaging","","","","","","","4-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automatic Algorithm Selection in Computational Software Using Machine Learning","M. C. Simpson; Q. Yi; J. Kalita","NC State Univ., Raleigh, NC, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","355","360","Computational software programs, such as Maple and Mathematica, heavily rely on superfunctions and meta-algorithms to select the optimal algorithm for a given task. These meta-algorithms may require intensive mathematical proof to formulate, incur large computational overhead, or fail to consistently select the best algorithm. Machine learning demonstrates a promising alternative for automatic algorithm selection by easing the design process and overhead while also attaining high accuracy in selection. In a case study on the resultant superfunction, a trained neural network is able to select the best algorithm out of the four available 86% of the time in Maple and 78% of the time in Mathematica. When used as a replacement for pre-existing meta-algorithms, the neural network brings about a 68% runtime improvement in Maple and 49% improvement in Mathematica. Random forests, k-nearest neighbors, and both linear and RBF kernel SVMs are also compared to the neural network model, the latter of which offers the best performance out of the tested machine learning methods.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0064","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838168","Maple;Mathematica;algorithm;automatic;choice;improvement;meta-algorithms;resultant;runtime;selection;superfunctions","Algorithm design and analysis;Heuristic algorithms;Machine learning algorithms;Mathematical model;Runtime;Software;Software algorithms","learning (artificial intelligence);neural nets;software engineering","Maple software;Mathematica software;automatic algorithm selection;computational software;machine learning;meta-algorithms;neural network model;trained neural network","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Detecting BGP anomalies using machine learning techniques","Qingye Ding; Zhida Li; P. Batta; L. Trajković","Simon Fraser University, Vancouver, British Columbia, Canada","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","003352","003355","Border Gateway Protocol (BGP) anomalies affect network operations and, hence, their detection is of interest to researchers and practitioners. Various machine learning techniques have been applied for detection of such anomalies. In this paper, we first employ the minimum Redundancy Maximum Relevance (mRMR) feature selection algorithms to extract the most relevant features used for classifying BGP anomalies and then apply the Support Vector Machine (SVM) and Long Short-Term Memory (LSTM) algorithms for data classification. The SVM and LSTM algorithms are compared based on accuracy and F-score. Their performance was improved by choosing balanced data for model training.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844751","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844751","Border gateway protocol;feature selection;long short-term memory;machine learning;routing anomalies;support vector machine","Classification algorithms;Feature extraction;Hidden Markov models;Logic gates;Mathematical model;Support vector machines;Training","feature selection;internetworking;learning (artificial intelligence);pattern classification;protocols;support vector machines;telecommunication network routing","BGP anomaly classification;BGP anomaly detection;Border Gateway Protocol anomaly;F-score;LSTM algorithm;SVM;data classification;long short-term memory algorithm;machine learning technique;minimum redundancy maximum relevance feature selection algorithm;model training;network operation;support vector machine","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Cardiovascular risk prediction based on Retinal Vessel Analysis using machine learning","K. M. Fathalla; A. Ekárt; S. Seshadri; D. Gherghel","Computer Engineering Department, Arab Academy for Science and Technology, Alexandria, Egypt","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000880","000885","Cardiovascular risk prediction is a vital aspect of personalized health care. In this study, retinal vascular function is assessed in asymptomatic participants who are classified into risk groups based on Framingham Risk Score. Feature selection, oversampling and state-of-the-art classification methods are applied to provide a sound individual risk prediction based on Retinal Vessel Analysis (RVA) data obtained by non-invasive methods. The results indicate that the RVA based cardiovascular risk prediction models are competitive with well established Framingham and Qrisk based models.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844352","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844352","","Artificial neural networks;Classification algorithms;Conferences;Data mining;Feature extraction;Retinal vessels;Support vector machines","cardiovascular system;data mining;diseases;eye;feature selection;health care;learning (artificial intelligence);pattern classification;sampling methods","Framingham based model;Framingham risk score;Qrisk based model;RVA;cardiovascular risk prediction;classification method;feature selection;health care;machine learning;noninvasive method;oversampling method;retinal vascular function;retinal vessel analysis;risk groups","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Postsilicon Trace Signal Selection Using Machine Learning Techniques","K. Rahmani; S. Ray; P. Mishra","University of Florida, Gainesville, FL, USA","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","20170119","2017","25","2","570","580","A key problem in postsilicon validation is to identify a small set of traceable signals that are effective for debug during silicon execution. Structural analysis used by traditional signal selection techniques leads to a poor restoration quality. In contrast, simulation-based selection techniques provide superior restorability but incur significant computation overhead. In this paper, we propose an efficient signal selection technique using machine learning to take advantage of simulation-based signal selection while significantly reducing the simulation overhead. The basic idea is to train a machine learning framework with a few simulation runs and utilize its effective prediction capability (instead of expensive simulation) to identify beneficial trace signals. Specifically, our approach uses: (1) bounded mock simulations to generate training vectors for the machine learning technique and (2) a compound search-space exploration approach to identify the most profitable signals. Experimental results indicate that our approach can improve restorability by up to 143.1% (29.2% on average) while maintaining or improving runtime compared with the state-of-the-art signal selection techniques.","1063-8210;10638210","","10.1109/TVLSI.2016.2593902","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542572","Feature selection;postsilicon debug;simulation;supervised learning","Computational modeling;Integrated circuit modeling;Machine learning algorithms;Prediction algorithms;Predictive models;Silicon;Training","electronic engineering computing;integrated circuit design;learning (artificial intelligence);search problems;signal restoration;vectors","compound search space exploration;machine learning;postsilicon trace signal selection;postsilicon validation;restoration quality;silicon execution debug;simulation-based signal selection;structural analysis;traceable signals;training vectors","","","","","","20160812","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Modeling behavior of Computer Generated Forces with Machine Learning Techniques, the NATO Task Group approach","A. Toubman; J. J. Roessingh; J. van Oijen; R. A. Løvlid; Ming Hou; C. Meyer; L. Luotsinen; R. Rijken; J. Harris; M. Turčaník","NLR, Amsterdam, Netherlands","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001906","001911","Commercial/Military-Off-The-Shelf (COTS/MOTS) Computer Generated Forces (CGF) packages are widely used in modeling and simulation for training purposes. Conventional CGF packages often include artificial intelligence (AI) interfaces, but lack behavior generation and other adaptive capabilities. We believe Machine Learning (ML) techniques can be beneficial to the behavior modeling process, yet such techniques seem to be underused and perhaps under-appreciated. This paper aims at bridging the gap between users in academia and the military/industry at a high level when it comes to ML and AI. We address specific requirements and desired capabilities for applying machine learning to CGF behavior modeling applications. The paper is based on the work of the NATO Research Task Group IST-121 RTG-060 Machine Learning Techniques for Autonomous Computer Generated Entities.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844517","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844517","","Adaptation models;Computational modeling;Computers;Conferences;Learning (artificial intelligence);Training","computer based training;learning (artificial intelligence);military computing","AI technique;COTS/MOTS-CGF package;IST-121 RTG-060-NATO research task groupmachine learning techniques;ML technique;artificial intelligence interfaces;autonomous computer generated entities;commercial/military-off-the-shelf package;computer generated forces behavior modeling","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Fall recognition using wearable technologies and machine learning algorithms","A. Harris; H. True; Z. Hu; J. Cho; N. Fell; M. Sartipi","Department of Computer Science and Engineering, University of Tennessee at Chattanooga, Chattanooga, TN 37403, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3974","3976","Falls are common and dangerous for the elderly or individuals with decreased independence or functional limitations. Fall recognition is extremely important for fallers, healthcare providers, and society. Immediate fall recognition triggers emergency services and potentially decreases individuals time with injury without care. Acute post-fall intervention works to mitigate life threatening fall consequences, decrease fall risk through rehabilitation, and improve quality of life. Extended from our research on real-time fall risk estimation with the functional reach test and Timed Up and Go test built in mStroke, a real-time and automatic mobile health system for post-stroke recovery and rehabilitation, our investigation here is expanded to include fall recognition by taking advantage of wearable technologies and machine learning algorithms. Up to three wearable sensors are employed to acquire raw motion data related to activities of daily living or falls. Feature selection and classification on the basis of machine learning algorithms are explored for fall recognition. The fall recognition performances are presented to justify their accuracy and reliability. Meanwhile, the effects of sensor placement/location and the feature number on the recognition performance are also discussed in this paper.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841080","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841080","fall recognition;feature selection;machine learning;wearable","Data analysis;Legged locomotion;Machine learning algorithms;Real-time systems;Robustness;Sternum;Wearable sensors","data acquisition;feature selection;geriatrics;health care;image motion analysis;learning (artificial intelligence);mobile computing;object recognition;patient rehabilitation;sensor placement","acute post-fall intervention;automatic mobile health system;emergency services;fall recognition;fall risk;feature classification;feature selection;life threatening fall consequence mitigation;mStroke;machine learning algorithms;post-stroke recovery;post-stroke rehabilitation;quality-of-life;raw motion data acquisition;reach test;real-time fall risk estimation;sensor location;sensor placement;timed up-and-go test;wearable sensors;wearable technologies","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Detecting Malicious URLs: A Semi-Supervised Machine Learning System Approach","A. D. Gabriel; D. T. Gavrilut; B. I. Alexandru; P. A. Stefan","Bitdefender Lab., Al.I. Cuza Univ., Iasi, Romania","2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","20170126","2016","","","233","239","As malware industry grows, so does the means of infecting a computer or device evolve. One of the most common infection vector is to use the Internet as an entry point. Not only that this method is easy to use, but due to the fact that URLs come in different forms and shapes, it is really difficult to distinguish a malicious URL from a benign one. Furthermore, every system that tries to classify or detect URLs must work on a real time stream and needs to provide a fast response for every URL that is submitted for analysis (in our context a fast response means less than 300-400 milliseconds/URL). From a malware creator point of view, it is really easy to change such URLs multiple times in one day. As a general observation, malicious URLs tend to have a short life (they appear, serve malicious content for several hours and then they are shut down usually by the ISP where they reside in). This paper aims to present a system that analyzes URLs in network traffic that is also capable of adjusting its detection models to adapt to new malicious content. Every correctly classified URL is reused as part of a new dataset that acts as the backbone for new detection models. The system also uses different clustering techniques in order to identify the lack of features on malicious URLs, thus creating a way to improve detection for this kind of threats.","2470-881X;2470881X","Electronic:978-1-5090-5707-8; POD:978-1-5090-5708-5","10.1109/SYNASC.2016.045","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829617","big data;data streams;malicious URLs;semi-supervised learning","Adaptation models;Classification algorithms;Data mining;Electronic mail;Feature extraction;Malware;Uniform resource locators","Internet;invasive software;learning (artificial intelligence);pattern clustering","Internet;clustering techniques;infection vector;malicious URL detection;malware industry;network traffic;semisupervised machine learning system;threats","","","","","","","24-27 Sept. 2016","","IEEE","IEEE Conference Publications"
"Automatic clustering of eye gaze data for machine learning","K. Naqshbandi; T. Gedeon; U. A. Abdulla","Research School of Computer Science, Australian National University, Canberra, Australia","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001239","001244","Eye gaze patterns or scanpaths of subjects looking at art while answering questions related to the art have been used to decode those tasks with the use of certain classifiers and machine learning techniques. Some of these techniques require the artwork to be divided into several Areas or Regions of Interest. In this paper, two ways of clustering the static visual stimuli - k-means and the density based clustering algorithm called OPTICS - were used for this purpose. These algorithms were used to cluster the gaze points before classification. The classification success rates were then compared. While it was observed that both k-means and OPTICS gave better success rates than manual clustering, which is itself higher than chance level, OPTICS consistently gave higher success rates than k-means given the right parameter settings. OPTICS also formed clusters that look more intuitive and consistent with the heat map readings than k-means, which formed clusters that look unintuitive and less consistent with the heat map.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844411","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844411","OPTICS;density-based clustering;eye gaze;k-means clustering;task decoding","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Conferences;Cybernetics;Gaze tracking;Optics","computer vision;gaze tracking;image classification;learning (artificial intelligence);pattern clustering","OPTICS;density based clustering;eye gaze data automatic clustering;eye gaze patterns;k-means clustering;machine learning;static visual stimuli","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Plant Disease Incidence and Severity Measurements from Leaf Images","E. Mwebaze; G. Owomugisha","Johann Bernoulli Inst. for Math. & Comput. Sci., Univ. of Groningen, Groningen, Netherlands","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","158","163","In many fields, superior gains have been obtained by leveraging the computational power of machine learning techniques to solve expert tasks. In this paper we present an application of machine learning to agriculture, solving a particular problem of diagnosis of crop disease based on plant images taken with a smartphone. Two pieces of information are important here, the disease incidence and disease severity. We present a classification system that trains a 5 class classification system to determine the state of disease of a plant. The 5 classes represent a health class and 4 disease classes. We further extend the classification system to classify different severity levels for any of the 4 diseases. Severity levels are assigned classes 1 - 5, 1 being a healthy plant, 5 being a severely diseased plant. We present ways of extracting different features from leaf images and show how different extraction methods result in different performance of the classifier. We finally present the smartphone-based system that uses the classification model learnt to do real-time prediction of the state of health of a farmers garden. This works by the farmer uploading an image of a plant in his garden and obtaining a disease score from a remote server.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0034","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838138","Machine learning;computer vision;feature selection;image-based diagnosis","Africa;Agriculture;Diseases;Feature extraction;Histograms;Image color analysis;Microorganisms","agriculture;feature extraction;image classification;learning (artificial intelligence);plant diseases;smart phones","agriculture;classification system;crop disease diagnosis;disease severity measurements;feature extraction;leaf images;machine learning techniques;plant disease incidence;plant images;smartphone-based system","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"SPARK – A Big Data Processing Platform for Machine Learning","J. Fu; J. Sun; K. Wang","Sch. of Autom., Wuhan Univ. of Technol., Wuhan, China","2016 International Conference on Industrial Informatics - Computing Technology, Intelligent Technology, Industrial Information Integration (ICIICII)","20170119","2016","","","48","51","Apache Spark is a distributed memory-based computing framework which is natural suitable for machine learning. Compared to Hadoop, Spark has a better ability of computing. In this paper, we analyze Spark's primary framework, core technologies, and run a machine learning instance on it. Finally, we will analyze the results and introduce our hardware equipment.","","Electronic:978-1-5090-3575-5; POD:978-1-5090-3576-2; USB:978-1-5090-3574-8","10.1109/ICIICII.2016.0023","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823490","Apache Spark;Iteration;MLlib;Machine Learning;Memory computing","Big data;Computers;Data models;Feature extraction;Libraries;Measurement;Sparks","Big Data;distributed memory systems;learning (artificial intelligence);parallel processing","Apache Spark;Big Data processing platform;distributed memory-based computing framework;machine learning","","","","","","","3-4 Dec. 2016","","IEEE","IEEE Conference Publications"
"Data analysis to predictive modeling of marine engine performance using machine learning","T. K. Chan; C. Chin","School of Marine Science and Technology, Newcastle University, Newcastle upon Tyne, NE17RU, United Kingdom","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2076","2080","Modeling a complex marine engine system is always very challenging and often subjected to model uncertainties and time-varying inputs. A non-model based predictive modeling of marine engine system performance is therefore required. Predictive analytics tools such as Neural Network, Multiple Linear Regression, and Bagged Regression Tree Model are jointly used to model the marine engine system using different filtered input parameters. The unsupervised machine learnings such as Fuzzy C-Means amongst the K-Means Clustering and Self-Organizing Maps are used to reduce the root-mean-square error of the predicted model further by approximately 50%. With exploratory data analysis and applications of the proposed multiple learning schemes on the real-time engine data leads to a more robust and comparative approach to real-time engine model prediction for data engineers.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848391","Fuzzy C-Means;Self-Organizing Maps;bagged regression tree model;exploratory data analysis;k-means clustering;marine engine;multiple linear regression;neural network;predictive modeling","Analytical models;Data models;Engines;Mathematical model;Predictive models;Shafts;Training","data analysis;engines;fuzzy set theory;learning (artificial intelligence);marine engineering;mechanical engineering computing;regression analysis;trees (mathematics)","bagged regression tree model;complex marine engine system;data analysis;data engineers;fuzzy c-means;k-means clustering;marine engine performance;marine engine system performance;model uncertainties;multiple learning schemes;multiple linear regression;neural network;non-model based predictive modeling;predictive analytics tools;real-time engine data;real-time engine model prediction;root-mean-square error;self-organizing maps;time-varying inputs;unsupervised machine learnings","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"An Empirical Study on Machine Learning Models for Wind Power Predictions","Y. Liu; H. Zhang","Fac. of Comput. Sci., Univ. of New Brunswick, Fredericton, NB, Canada","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","758","763","Wind power prediction is of great importance in the utilization of renewable wind power. A lot of research has been done attempting to improve the accuracy of wind power predictions and has achieved desirable performance. However, there is no complete performance evaluation of machine learning methods. This paper presents an extensive empirical study of machine learning methods for wind power predictions. Nine various models are considered in this study which also includes the application and evaluation of deep learning techniques. The experimental data consists of seven datasets based on wind farms in Ontario, Canada. The results indicate that SVM, followed by ANN, has the best overall performance, and that k-NN method is suitable for longer ahead predictions. Despite the findings that deep learning fails to give improvement in basic predictions, it shows the potential for more abstract prediction tasks, such as spatial correlation predictions.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0135","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838239","","Machine learning;Neurons;Support vector machines;Training;Wind farms;Wind power generation;Wind speed","learning (artificial intelligence);neural nets;support vector machines;wind power plants","ANN;Canada;Ontario;SVM;deep learning technique;empirical analysis;k-NN method;machine learning model;renewable wind power utilization;spatial correlation prediction;wind farms;wind power predictions","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Dynamic gesture recognition using machine learning techniques and factors affecting its accuracy","F. A. Zuberi; S. Khatri; K. N. Junejo","University of Freiburg, Freiburg im Briesgau, Germany","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","310","313","Kinect, a motion sensing input device for gaming consoles has been successfully utilized for video games, and rehabilitation of paralyzed patients. We use this device to make learning a fun activity for children. Children learn to draw shapes by moving their hands in front of the Kinect device. We automatically recognize and classify their dynamic hand gestures into predefined shapes, namely; rectangles, triangles, and circles. To decrease over fitting and the cost of generating sample shapes a novel feature engineering approach is also proposed that increases the performance by more than 11%. We used three different machine learning algorithms and successfully classified the shapes with an accuracy of more than 97%.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845067","","Decision trees;Gesture recognition;Machine learning algorithms;Niobium;Sensors;Shape;Training data","computer games;gesture recognition;image sensors;learning (artificial intelligence);patient rehabilitation","Kinect;accuracy factors;dynamic gesture recognition;feature engineering;gaming consoles;hand gesture classification;machine learning;motion sensing input device;paralyzed patient rehabilitation;video games","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition with Groebner Bases","Z. Huang; M. England; J. H. Davenport; L. C. Paulson","Comput. Lab., Univ. of Cambridge, Cambridge, UK","2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","20170126","2016","","","45","52","Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance. In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic.","2470-881X;2470881X","Electronic:978-1-5090-5707-8; POD:978-1-5090-5708-5","10.1109/SYNASC.2016.020","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829592","computer algebra;cylindrical algebraic decomposition;groebner bases;machine learning;preconditioning;support vector machine","Algebra;Complexity theory;Computers;Electronic mail;Machine learning algorithms;Measurement;Support vector machines","computational complexity;computational geometry;learning (artificial intelligence);polynomials;support vector machines","CAD algorithm;CAD performance;CAD problems;GB preconditioning;GB theory;Groebner Basis;Groebner bases;SVM;computational algebraic geometry;human-made heuristic;input polynomials;machine learned choice;machine learning;precondition cylindrical algebraic decomposition;quantifier elimination;support vector machine;worst case complexity","","","","","","","24-27 Sept. 2016","","IEEE","IEEE Conference Publications"
"IR-UWB Radar Sensor for Human Gesture Recognition by Using Machine Learning","J. Park; S. H. Cho","Dept. of Electron. & Comput. Eng., Hanyang Univ. Seoul, Seoul, South Korea","2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","20170126","2016","","","1246","1249","In this paper, we propose a human gesture recognition algorithm using impulse radio ultra-wideband (IR-UWB) radar. The radar signal is transmitted into a three dimensional space, however, the received signal is only expressed in one dimensional. Therefore, it is difficult to classify 3-D gestures by analyzing specific features, such as power, peak value, index of peak value, and other values of received signal. To resolve this problem, a new human gesture recognition algorithm using machine learning is proposed. Two machine learning technics are used in this paper. One is unsupervised learning technic which is used for extracting features from received radar signal is principal component analysis, and the other one is supervised learning which is used for classifying gestures. The features are extracted by using the principal component analysis (PCA) method, then neural network method is used for training and classifying gestures using the extracted features. In training and classifying step, other method can be used, such as supporting vector machine (SVM), however, this method is hard to recognize noise gesture which means untrained gesture. To resolve this problem, we use neural network method in this paper, then in order to classy noise gestures and trained gestures, a noise determining algorithm is used.","","Electronic:978-1-5090-4297-5; POD:978-1-5090-4298-2","10.1109/HPCC-SmartCity-DSS.2016.0176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828517","hand gesture recognition;impulse radio ultra-wideband (IR-UWB);machine learning;motion recognition;radar sensor","Feature extraction;Gesture recognition;Machine learning algorithms;Neural networks;Principal component analysis;Radar;Training","feature extraction;gesture recognition;neural nets;principal component analysis;radar receivers;radar signal processing;ultra wideband radar;unsupervised learning","3D gestures classification;IR-UWB radar sensor;PCA;feature extraction;human gesture recognition;impulse radio ultra-wideband radar;machine learning;motion recognition;neural network;noise determining algorithm;principal component analysis;received radar signal;unsupervised learning","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning-Guided Etch Proximity Correction","S. Shim; Y. Shin","Department of Electrical Engineering, KAIST, Daejeon, South Korea","IEEE Transactions on Semiconductor Manufacturing","20170201","2017","30","1","1","7","Rule- and model-based methods of etch proximity correction (EPC) are widely used, but they are insufficiently accurate for technologies below 20 nm. Simple rules are no longer adequate for the complicated patterns in layouts; and models based on a few empirically determined parameters cannot reflect etching phenomena physically. We introduce machine learning to EPC: each segment of interest, together with its surroundings, is characterized by geometric and optical parameters, which are then submitted to an artificial neural network that predicts the etch bias. We have implemented this new approach to EPC using a commercial OPC tool, and applied it to a DRAM gate layer in 20-nm technology, achieving predictions that are 34% more accurate than model-based EPC.","0894-6507;08946507","","10.1109/TSM.2016.2626304","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738596","Etch proximity correction (EPC);artificial neural network;machine learning","Etching;Kernel;Layout;Optical imaging;Resists;Semiconductor device measurement;Semiconductor device modeling","DRAM chips;etching;learning (artificial intelligence);neural nets;proximity effect (lithography);semiconductor device manufacture","DRAM gate layer;machine learning-guided etch proximity correction","","","","","","20161108","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"A Combined Analytical Modeling Machine Learning Approach for Performance Prediction of MapReduce Jobs in Cloud Environment","E. Ataie; E. Gianniti; D. Ardagna; A. Movaghar","Dept. of Comput. Eng., Sharif Univ. of Technol., Tehran, Iran","2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)","20170126","2016","","","431","439","Nowadays MapReduce and its open source implementation, Apache Hadoop, are the most widespread solutions for handling massive dataset on clusters of commodity hardware. At the expense of a somewhat reduced performance in comparison to HPC technologies, the MapReduce framework provides fault tolerance and automatic parallelization without any efforts by developers. Since in many cases Hadoop is adopted to support business critical activities, it is often important to predict with fair confidence the execution time of submitted jobs, for instance when SLAs are established with end-users. In this work, we propose and validate a hybrid approach exploiting both queuing networks and support vector regression, in order to achieve a good accuracy without too many costly experiments on a real setup. The experimental results show how the proposed approach attains a 21% improvement in accuracy over applying machine learning techniques without any support from analytical models.","2470-881X;2470881X","Electronic:978-1-5090-5707-8; POD:978-1-5090-5708-5","10.1109/SYNASC.2016.072","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829644","Analytical performance modeling;MapReduce;cloud computing;machine learning","Analytical models;Cloud computing;Computational modeling;Data models;Noise measurement;Predictive models;Training","cloud computing;data handling;fault tolerance;learning (artificial intelligence);parallel processing;queueing theory;regression analysis;support vector machines","Apache Hadoop;MapReduce jobs;analytical modeling machine learning;automatic parallelization;business critical activity support;cloud environment;fault tolerance;massive dataset handling;open source implementation;performance prediction;queuing networks;support vector regression","","","","","","","24-27 Sept. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach to fall detection algorithm using wearable sensor","C. Y. Hsieh; C. N. Huang; K. C. Liu; W. C. Chu; C. T. Chan","Department of Biomedical Engineering, National Yang-Ming University, Taipei, 112 Taiwan","2016 International Conference on Advanced Materials for Science and Engineering (ICAMSE)","20170206","2016","","","707","710","Falls are the primary cause of accidents for the elderly in living environment. Falls frequently cause fatal and non-fatal injuries that are associated with a large amount of medical costs. Reduction hazards in living environment and doing exercise for training balance and muscle are the common strategies for fall prevention. But falls cannot be avoided completely; fall detection provides the alarm in time that can decrease the injuries or death caused by no rescue. We propose machine learning-based fall detection algorithm using multi-SVM with linear, quadratic or polynomial kernel function, and k-NN classifier. Eight kinds of falling postures and seven types of daily activities arranged in the experiment are used to explore the performance of the machine learning-based fall detection algorithm. The emulated falls were performed on a soft mat by ten healthy young subjects wearing protectors. The k-nearest neighbor method with 0.1 second window size has the highest accuracy, which is 96.26%. The results show that the proposed machine learning fall detection algorithm can fulfill the requirements of adaptability and flexibility for the individual differences.","","Electronic:978-1-5090-3869-5; POD:978-1-5090-3870-1","10.1109/ICAMSE.2016.7840209","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840209","fall detection;machine learning;wearable sensor","Acceleration;Detection algorithms;Feature extraction;Sensitivity;Support vector machines;Training;Wearable sensors","accident prevention;geriatrics;hazards;learning (artificial intelligence);medical computing;pattern classification;polynomials;support vector machines","accidents;elderly;fall detection algorithm;hazard reduction;k-NN classifier;linear kernel function;machine learning;medical costs;multi-SVM;polynomial kernel function;quadratic kernel function;support vector machines;wearable sensor","","","","","","","12-13 Nov. 2016","","IEEE","IEEE Conference Publications"
"Short-term electrical load forecasting using predictive machine learning models","K. P. Warrior; M. Shrenik; N. Soni","Electrical and Electronics Engineering, BMS College of Engineering, Bangalore, India","2016 IEEE Annual India Conference (INDICON)","20170202","2016","","","1","6","Availability of cheap power through alternative means such as energy exchanges and bilateral agreements is resulting in short-term load forecasting gaining importance among industries, residential complexes and corporate buildings. Short-term forecasting over an hour or a day requires non-linear predictive models. Machine learning algorithms such as neural networks are inherently non-linear and are suitable for accurate forecasting. This paper compares neural networks, decision trees and Conditional Restricted Boltzmann Machines algorithms for forecasting short-term demand. The algorithms are tested on power consumption data acquired from two test sites with different consumption profiles.","","Electronic:978-1-5090-3646-2; POD:978-1-5090-3647-9","10.1109/INDICON.2016.7839103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839103","Conditional Restricted Boltzmann machine;Decision Trees;Demand Forecasting;Neural Networks","Biological neural networks;Data models;Decision trees;Forecasting;Machine learning algorithms;Power demand","Boltzmann machines;decision trees;demand forecasting;learning (artificial intelligence);load forecasting;power consumption;power engineering computing","conditional restricted Boltzmann machines;corporate buildings;decision trees;neural networks;nonlinear predictive models;power consumption consumption profiles;power consumption data;predictive machine learning model;residential complexes;short-term demand forecasting;short-term electrical load forecasting","","","","","","","16-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"MaSiF: Machine learning guided auto-tuning of Parallel Skeletons","A. Collins; C. Fensch; H. Leather","University of Edinburgh, School of Informatics, Scotland","2012 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)","20170206","2012","","","437","438","We present MaSiF, a novel tool to auto-tune parallelization parameters of skeleton parallel programs. It reduces the cost of searching the optimization space using a combination of machine learning and linear dimensionality reduction. To auto-tune a new program, a set of program features is determined statically and used to compute k nearest neighbors from a set of training programs. Previously collected performance data for the nearest neighbors is used to reduce the size of the search space using Principal Components Analysis. This results in a set of eigenvectors that are used to search the reduced space. MaSiF achieves 88% of the performance of the oracle, which searches a random set of 10,000 parameter values. MaSiF searches just 45 points, or 0.45% of the optimization space, to achieve this performance. MaSiF provides an average speedup of 1.18x over parallelization parameters chosen by a human expert.","","Electronic:978-1-4503-1182-3; POD:978-1-5090-6609-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842962","Auto-tuning;FastFlow;Machine Learning;Multi-core;Parallel Skeletons","Feature extraction;Optimization;Principal component analysis;Programming;Skeleton;Training;Tuning","eigenvalues and eigenfunctions;learning (artificial intelligence);parallel programming;principal component analysis","MaSiF;eigenvectors;k nearest neighbors;linear dimensionality reduction;machine learning;parallelization parameters;principal components analysis;program auto-tuning;skeleton parallel programs","","","","","","","19-23 Sept. 2012","","IEEE","IEEE Conference Publications"
"Machine Learning with Memristors via Thermodynamic RAM","T. W. Molter; M. A. Nugent","","CNNA 2016; 15th International Workshop on Cellular Nanoscale Networks and their Applications","20170123","2016","","","1","2","Thermodynamic RAM (kT-RAM) is a neuromemristive co-processor design based on the theory of AHaH Computing and implemented via CMOS and memristors. The co-processor is a 2-D array of differential memristor pairs (synapses) that can be selectively coupled together (neurons) via the digital bit addressing of the underlying CMOS RAM circuitry. The chip is designed to plug into existing digital computers and be interacted with via a simple instruction set. Anti-Hebbian and Hebbian (AHaH) computing forms the theoretical framework from which a nature-inspired type of computing architecture is built where, unlike von Neumann architectures, memory and processor are physically combined for synaptic operations. Through exploitation of AHaH attractor states, memristor-based circuits converge to attractor basins that represents machine learning solutions such as unsupervised feature learning, supervised classification and anomaly detection. Because kT-RAM eliminates the need to shuttle bits back and forth between memory and processor and can operate at very low voltage levels, it can significantly surpass CPU, GPU, and FPGA performance for synaptic integration and learning operations. Here, we present a memristor technology developed for use in kT-RAM, in particular bi-directional incremental adaptation of conductance via short low-voltage (<1.0 V, <1.0 muS) pulses.","","Paper:978-3-8007-4252-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827977","","","","","","","","","","","23-25 Aug. 2016","","VDE","VDE Conference Publications"
"A review on opinionated sentiment analysis based upon machine learning approach","N. Bansal; A. Singh","Dept. of Computer Science & Engg., Thapar University, Patiala, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","2","","1","6","Sentiment analysis is a rapidly emerging field and has attracted a large community of researchers to analyze an amalgamated form of people expressions shared on the internet. There are several sources like e-commerce sites, social networking sites, micro blogging sites which help in capturing user reactions corresponding to different market products and other important issues in the form of feedback. The opinions extracted from these feedbacks are useful for both individual and organizations to predict user opinionated sentiments and customer preferences. The key emphasis is put on the employing of an accurate method in the experiments to obtain correctness in the results. The main focus of this paper is to review Sentiment Analysis domain by using machine learning approach. The study is carried out by considering different facets like granularity, algorithms, polarity, applications, and data type related to the domain, which is presented and discussed in a simple manner.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7824843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824843","Machine learning;Sentiment Analysis;Sentiment Classification Opinion mining;Web Content","Blogs;Classification algorithms;Feature extraction;Machine learning algorithms;Sentiment analysis;Social network services;Support vector machines","learning (artificial intelligence);sentiment analysis","Internet;customer preferences;machine learning;opinion extraction;opinionated sentiment analysis","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Supervised Machine Learning-Based Routing for Named Data Networking","L. Mekinda; L. Muscariello","","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","Named Data Networking (NDN) ambitions the rank of Future Internet Architecture in uniquely addressing content items by their name. In NDN, routers forward Interests for content after finding Longest-Prefix Matches (LPM) of content names in their Forwarding Information Base (FIB). However, the scalability of this structure is challenged by the huge global Internet namespace. In this paper, we propose a novel approach to interest forwarding that compresses the FIB data structure into Artificial Neural Networks (ANNs). A bitwise trie splits the namespace and indexes ANNs. ANNs are offline trained by the control plane from the Routing Information Base and matching Interests. Then, they are made available to the data plane for interrogation. We demonstrate that this approach accelerates packet forwarding by several orders of magnitude. Noteworthily, leveraging ANNs as memory and processor for directing packets towards next hops reminds of Asking For Directions to people in the street, incurring similar reliability regards.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7842307","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842307","","Internet;Neurons;Ribs;Routing;Routing protocols;Supervised learning;Training","Internet;data structures;learning (artificial intelligence);neural nets;telecommunication network routing","ANN;FIB;LPM;NDN;artificial neural networks;forwarding information base;future Internet architecture;longest-prefix matches;named data networking;routing information base;supervised machine learning-based routing","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Human identification from brain EEG signals using advanced machine learning method EEG-based biometrics","M. K. Bashar; I. Chiaki; H. Yoshida","Faculty of General Educational Research, Ochanomizu University, Tokyo, Japan","2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)","20170206","2016","","","475","479","EEG-based human recognition is increasingly becoming a popular modality for biometric authentication. Two important features of EEG signals are liveliness and the robustness against falsification. However, a comprehensive study on human authentication using EEG signal is still remains. On the other hand, low-cost wireless EEG recording devices are now growing in the market places. Although these devices have the potential to many applications, researches have yet to be done to find the feasibility of these devices. In this study, we propose a method for human identification using EEG signals obtained from such low-cost devices. EEG signal is first preprocessed to remove noise and artifacts using Bandpass FIR filter. These signals are then divided into disjoint segments. Three feature extraction methods, namely multiscale shape description (MSD), multiscale wavelet packet statistics (WPS) and multiscale wavelet packet energy statistics (WPES) are then applied. These features are finally used to train a supervised error-correcting output code multiclass model (ECOC) using support vector machine (SVM) classifier, which ultimately can recognize humans from test EEG signals. A preliminary experiment with 9 EEG records from 9 subjects shows the true positive rate of 94.44% of the proposed method.","","Electronic:978-1-4673-7791-1; POD:978-1-4673-7792-8","10.1109/IECBES.2016.7843496","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843496","EEG signals;human recognition;low-cost wireless EEG recording devices;multiscale features;supervised classification","Biometrics (access control);Cutoff frequency;Electroencephalography;Feature extraction;Finite impulse response filters;Support vector machines;Wavelet packets","FIR filters;band-pass filters;biometrics (access control);electroencephalography;error correction codes;feature extraction;learning (artificial intelligence);medical signal processing;signal denoising;support vector machines","ECOC;MSD;SVM classifier;WPES;WPS;advanced machine learning method;artifact removal;bandpass FIR filter;biometrics;brain EEG signals;electroencephalography;feature extraction;human identification;human recognition;multiscale shape description;multiscale wavelet packet energy statistics;multiscale wavelet packet statistics;noise removal;supervised error-correcting output code multiclass model;support vector machine","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for indirect human presence detection using IOT devices","R. Madeira; L. Nunes","Instituto de Telecomunica&#x00E7;&#x00F5;es, Lisbon Portugal Department of Information Science and Technology, ISCTE-IUL Lisbon, Portugal","2016 Eleventh International Conference on Digital Information Management (ICDIM)","20170126","2016","","","145","150","This paper describes the construction of a system that uses information from several home automation devices, to detect the presence of a person in the space where the devices are located. The detection however doesn't rely on the information of devices that explicitly detect human presence, like motion detectors or smart cameras. The information used is the one available in the Muzzley system, which is a mobile application that allows the monitoring and control of several types of devices from a single program. The provided information was anonymized at the source. The first step was to extract adequate features for this problem. A labeling step is introduced using a combination of heuristics to assert the likelihood of anyone being home at a given time, based on all information available, including, but not limited to, direct presence detectors. The solution rests mainly on the use of supervised learning algorithms to train models that detect the presence without any information based on direct presence detectors. The model should be able to detect patterns of usage when the owner is at home rather than rely only on direct sensors. Results show that detection in this context is difficult, but we believe these results shed some light on possible paths to improve the system's accuracy.","","Electronic:978-1-5090-2641-8; POD:978-1-5090-2642-5","10.1109/ICDIM.2016.7829781","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829781","ambient intelligence;human presence detection;internet of things;sensor fusion","Ambient intelligence;Detectors;Home automation;Intelligent sensors;Measurement;Monitoring","Internet of Things;ambient intelligence;feature extraction;home automation;learning (artificial intelligence);object detection;sensor fusion","IoT devices;Muzzley system;ambient intelligence;device control;device monitoring;direct presence detectors;feature extraction;home automation devices;indirect human presence detection;labeling step;machine learning approach;mobile application;model training;pattern detection;supervised learning algorithms","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Evaluating machine learning algorithms for anomaly detection in clouds","A. Gulenko; M. Wallschläger; F. Schmidt; O. Kao; F. Liu","Complex and Distributed IT-Systems, TU Berlin, Berlin, Germany","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2716","2721","Critical services in the field of Network Function Virtualization require elaborate reliability and high availability mechanisms to meet the high service quality requirements. Traditional monitoring systems detect overload situations and outages in order to automatically scale out services or mask faults. However, faults are often preceded by anomalies and subtle misbehaviors of the services, which are overlooked when detecting only outages. We propose to exploit machine learning techniques to detect abnormal behavior of services and hosts by analysing metrics collected from all layers and components of the cloud infrastructure. Various algorithms are able to compute models of a hosts normal behavior that can be used for anomaly detection at runtime. An offline evaluation of data collected from anomaly injection experiments shows that the models are able to achieve very high precision and recall values.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840917","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840917","anomaly detection;cloud;machine learning;network function virtualization","Cloud computing;Data collection;Decision trees;Machine learning algorithms;Measurement;Monitoring;Vegetation","cloud computing;fault tolerant computing;learning (artificial intelligence);system monitoring","anomaly injection experiment;availability mechanism;cloud anomaly detection;cloud infrastructure;critical services;fault masking;machine learning algorithm;metrics analysis;monitoring system;network function virtualization;outage detection;overload situation detection;reliability mechanism;service anomaly;service misbehavior;service quality requirement","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"TENET: A Machine Learning-Based System for Target Characterization in Signaling Networks","H. E. Chua; S. S. Bhowmick; L. Tucker-Kellogg; C. F. Dewey","Complexity Inst., Nanyang Technol. Univ., Singapore, Singapore","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","1288","1291","Target characterization of a biological network identifies characteristics that distinguish targets (nodes that can serve as molecular targets of drugs) from other nodes. In this demonstration, we present TENET (Target charactErization using NEtwork Topology), a software that facilitates topological features-based characterization of known targets in signaling networks modelling dynamic interactions within biological systems. TENET is based on a support vector machine (SVM)-based approach and generates a characterization model. These models specify topological features that can discriminate known targets and how these features are combined to quantify the likelihood of a node being a target. Hence, TENET can be used for prioritizing targets and for identifying novel candidate targets that share similar characteristics with known targets. The interactive user interface that TENET provides facilitates users' study and understanding of topological characteristics of targets in signaling networks.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0186","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836817","Signaling network;support vector machine;target characterization;topological features;visualization","Biological system modeling;Computational modeling;Diseases;Graphical user interfaces;Predictive models;Support vector machines","bioinformatics;learning (artificial intelligence);support vector machines","SVM;TENET;biological systems;interactive user interface;machine learning-based system;signaling networks;support vector machine;target characterization;target characterization using network topology;topological features-based characterization","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Using Machine Learning to Accelerate Data Wrangling","S. Ahuja; M. Roth; R. Gangadharaiah; P. Schwarz; R. Bastidas","IBM Res. - Almaden, San Jose, CA, USA","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","343","349","70% Of the time spent on data analytics is not actually spent on data analytics, but rather, in data wrangling: the process of finding, interpreting, extracting, preparing and recombining the data to be analyzed. For data that is collected as free-form text, the lack of standards or competing standards often results in a variety of formats for expressing the same type of data, making the data wrangling step a tedious and error-prone process. For example, US street addresses may be expressed with a house number, PO Box, rural or military route, and/or a direction - all of which can be abbreviated or spelled out in a variety of ways. In this paper, we present an algorithm that uses machine learning to efficiently and automatically identify categories of attributes, such as geo-spatial, that are present in a data file and we discuss results on a variety of real data sets. Our implementation can be used to automatically prepare data for consumption by other tools and services, such as mapping and visualization tools, and is motivated by and in support of a customizable severe weather alerting service.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0055","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836686","column label detection;data extraction;data wrangling;geo-prepping;geospatial","Data analysis;Data mining;Geospatial analysis;Machine learning algorithms;Meteorology;Standards;Urban areas","data analysis;learning (artificial intelligence)","customizable severe weather alerting service;data analytics;data wrangling;free-form text;machine learning","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach to fab-of-origin attestation","A. Ahmadi; M. M. Bidmeshki; A. Nahar; B. Orr; M. Pas; Y. Makris","Dept. of Electrical Engineering, The University of Texas at Dallas, Richardson, 75080, United States of America","2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20170123","2016","","","1","6","We introduce a machine learning approach for distinguishing between integrated circuits fabricated in a ratified facility and circuits originating from an unknown or undesired source based on parametric measurements. Unlike earlier approaches, which seek to achieve the same objective in a general, design-independent manner, the proposed method leverages the interaction between the idiosyncrasies of the fabrication facility and a specific design, in order to create a customized fab-of-origin membership test for the circuit in question. Effectiveness of the proposed method is demonstrated using two large industrial datasets from a 65nm Texas Instruments RF transceiver manufactured in two different fabrication facilities.","","Electronic:978-1-4503-4466-1; POD:978-1-5090-3421-5","10.1145/2966986.2966992","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7827669","","Density measurement;Electrical engineering;Fabrication;Foundries;Integrated circuits;Semiconductor device measurement","integrated circuit manufacture;learning (artificial intelligence);production engineering computing;production facilities;supply chain management","electronics supply chain;fab-of-origin attestation;fabrication facility;integrated circuit fabrication;machine learning approach","","","","","","","7-10 Nov. 2016","","IEEE","IEEE Conference Publications"
"YouTube QoE Estimation Based on the Analysis of Encrypted Network Traffic Using Machine Learning","I. Orsolic; D. Pevec; M. Suznjevic; L. Skorin-Kapov","Fac. of Electr. Eng. & Comput., Univ. of Zagreb, Zagreb, Croatia","2016 IEEE Globecom Workshops (GC Wkshps)","20170209","2016","","","1","6","The widespread use of encryption in the delivery of Over-The-Top video streaming services poses challenges for network operators looking to monitor service performance and detect potential customer perceived Quality of Experience (QoE) degradations. While monitoring solutions deployed on client devices provide insight into application layer KPIs (e.g., video quality levels, buffer underruns, stalling duration) which can be further mapped to user QoE, network providers commonly rely primarily on passive traffic monitoring solutions deployed within their network to obtain insight into user perceived degradations and their potential causes. In this paper we present a methodology for the estimation of end users' QoE when watching YouTube videos which is based only on statistical properties of encrypted network traffic. We have developed a system called YouQ which includes tools for monitoring and analysis of application-layer KPIs and corresponding traffic traces, and the subsequent use of this data for the development of machine learning models for QoE estimation based on traffic features. To test this approach, we have collected a dataset of 1060 different YouTube video traces using 39 different bandwidth scenarios. All video traces are annotated with application-layer KPIs and classified into one of three QoE classes. The dataset was used to test various machine learning algorithms, and results showed that up to 84% QoE classification accuracy could be achieved using only features extracted from encrypted traffic.","","Electronic:978-1-5090-2482-7; POD:978-1-5090-2483-4","10.1109/GLOCOMW.2016.7849088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849088","","Androids;Feature extraction;Humanoid robots;Monitoring;Servers;Streaming media;YouTube","cryptography;feature extraction;learning (artificial intelligence);quality of experience;social networking (online);video streaming","YouQ;YouTube QoE estimation;encrypted network traffic;feature extraction;machine learning;over-the-top video streaming services;quality of experience","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning, linear and Bayesian models for logistic regression in failure detection problems","B. Pavlyshenko","SoftServe, Inc., Ivan Franko National University of Lviv, Lviv, Ukraine","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2046","2050","In this work, we study the use of logistic regression in manufacturing failures detection. As a data set for the analysis, we used the data from Kaggle competition “Bosch Production Line Performance”. We considered the use of machine learning, linear and Bayesian models. For machine learning approach, we analyzed XGBoost tree based classifier to obtain high scored classification. Using the generalized linear model for logistic regression makes it possible to analyze the influence of the factors under study. The Bayesian approach for logistic regression gives the statistical distribution for the parameters of the model. It can be useful in the probabilistic analysis, e.g. risk assessment.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840828","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840828","Bayesian inference;XGBoost;failure detection;logistic regression","Analytical models;Bayes methods;Boosting;Correlation coefficient;Logistics;Manufacturing","Bayes methods;learning (artificial intelligence);logistics;manufacturing processes;pattern classification;regression analysis","Bayesian models;Bosch production line performance;Kaggle competition;XGBoost tree based classifier;logistic regression;machine learning;manufacturing failure detection problems;statistical distribution","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning, Statistics, and Data Analytics","E. Alpaydin","","Machine Learning:The New AI","20170208","2016","","","","","This chapter contains sections titled: Learning to Estimate the Price of a Used Car, Randomness and Probability, Learning a General Model, Model Selection, Supervised Learning, Learning a Sequence, Credit Scoring, Expert Systems, Expected Values","","97802623375","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7845219.pdf&bkn=7845169&pdfType=chapter","","","","","","","","","2016","","","","MIT Press","MIT Press eBook Chapters"
"Machine learning algorithms for document clustering and fraud detection","S. Yaram","Data Architect, CSC India Limited, Hyderabad, India","2016 International Conference on Data Science and Engineering (ICDSE)","20170119","2016","","","1","6","Machine Learning plays very important role in processing of large amounts of structured and unstructured data. A set of algorithms can be used to get meaningful insights into the data that are helpful in making effective business decisions. Document clustering is one of the popular machine learning technique used to group unstructured data (text documents) based on its content and further analyze the data to understand the patterns in it. The unstructured data gets transformed into semi-structured data and structured data in stages by using text mining and clustering (k-means) techniques. Classification is another machine learning technique that can be implemented for use cases like ""fraud detection and cross-sell & up-sell opportunity identification"" in banking, financial services and insurance industry. This paper focuses on the implementation of both document clustering algorithm and a set of classification algorithms (Decision Tree, Random Forest and Naïve Bayes), along with appropriate industry use cases. Also, the performance of three classification algorithms will be compared by calculation of ""Confusion Matrix"" which in turn helps us to calculate performance measures such as, ""accuracy"", ""precision"", and ""recall"".","","Electronic:978-1-5090-1281-7; POD:978-1-5090-1282-4","10.1109/ICDSE.2016.7823950","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823950","accuracy;class variable or dependent variable;classification;clustering;confusion matrix;decision tree;document-term matrix;entropy;euclidean distance;feature variable or independent variable;information gain;inverse document frequency;machine learning;naïve bayes;precision;random forest;recall;supervised learning;term frequency;unsupervised learning","Classification algorithms;Decision trees;Entropy;Feature extraction;Insurance;Machine learning algorithms;Prediction algorithms","Bayes methods;data mining;decision making;decision trees;fraud;learning (artificial intelligence);matrix algebra;pattern classification;pattern clustering;text analysis","Naive Bayes;business decision making;classification algorithm set;clustering techniques;confusion matrix;decision tree;document clustering;fraud detection;machine learning algorithms;random forest;semistructured data;text documents;text mining;unstructured data;up-sell opportunity identification","","","","","","","23-25 Aug. 2016","","IEEE","IEEE Conference Publications"
"Why We are Interested in Machine Learning","E. Alpaydin","","Machine Learning:The New AI","20170208","2016","","","","","This chapter contains sections titled: The Power of the Digital, Computers Store Data, Computers Exchange Data, Mobile Computing, Social Data, All That Data: The Dataquake, Learning versus Programming, Artificial Intelligence, Understanding the Brain, Pattern Recognition, What We Talk about When We Talk about Learning, History","","97802623375","","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7845186.pdf&bkn=7845169&pdfType=chapter","","","","","","","","","2016","","","","MIT Press","MIT Press eBook Chapters"
"Multi-domain Alias Matching Using Machine Learning","M. Ashcroft; F. Johansson; L. Kaati; A. Shrestha","Dept. of Inf. Technol., Uppsala Univ., Uppsala, Sweden","2016 Third European Network Intelligence Conference (ENIC)","20170202","2016","","","77","84","We describe a methodology for linking aliases belonging to the same individual based on a user's writing style (stylometric features extracted from the user generated content) and her time patterns (time-based features extracted from the publishing times of the user generated content). While most previous research on social media identity linkage relies on matching usernames, our methodology can also be used for users who actively try to choose dissimilar usernames when creating their aliases. In our experiments on a discussion forum dataset and a Twitter dataset, we evaluate the performance of three different classifiers. We use the best classifier (AdaBoost) to evaluate how well it works on different datasets using different features. Experiments show that combining stylometric and time-based features yield good results on our synthetic datasets and a small-scale evaluation on real-world blog data confirm these results, yielding a precision over 95%. The use of emotion-related and Twitter-related features yield no significant impact on the results.","","Electronic:978-1-5090-3455-0; POD:978-1-5090-3456-7","10.1109/ENIC.2016.019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838048","alias matching;authorship analysis","Blogs;Couplings;Electronic mail;Feature extraction;Social network services;User-generated content;Writing","learning (artificial intelligence);social networking (online)","Twitter dataset;Twitter-related features;discussion forum dataset;dissimilar usernames;machine learning;multidomain alias matching;real-world blog data;social media identity linkage;stylometric features;synthetic datasets;time patterns;time-based features;user writing style","","","","","","","5-7 Sept. 2016","","IEEE","IEEE Conference Publications"
"Using machine learning to identify major shifts in human gut microbiome protein family abundance in disease","M. Yazdani; B. C. Taylor; J. W. Debelius; W. Li; R. Knight; L. Smarr","California Institute for Telecommunications and Information Technology, UC San Diego, California, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1272","1280","Inflammatory Bowel Disease (IBD) is an autoimmune condition that is observed to be associated with major alterations in the gut microbiome taxonomic composition. Here we classify major changes in microbiome protein family abundances between healthy subjects and IBD patients. We use machine learning to analyze results obtained previously from computing relative abundance of ~10,000 KEGG orthologous protein families in the gut microbiome of a set of healthy individuals and IBD patients. We develop a machine learning pipeline, involving the Kolomogorv-Smirnov test, to identify the 100 most statistically significant entries in the KEGG database. Then we use these 100 as a training set for a Random Forest classifier to determine ~5% the KEGGs which are best at separating disease and healthy states. Lastly, we developed a Natural Language Processing classifier of the KEGG description files to predict KEGG relative over-or under-abundance. As we expand our analysis from 10,000 KEGG protein families to one million proteins identified in the gut microbiome, scalable methods for quickly identifying such anomalies between health and disease states will be increasingly valuable for biological interpretation of sequence data.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840731","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840731","ibd;kegg;machine learning;microbiome;pca;random forest","Bioinformatics;Databases;Diseases;Genomics;Proteins;Sequential analysis;Training","biology computing;diseases;learning (artificial intelligence);natural language processing;proteins","IBD;IBD patients;KEGG database;KEGG description files;Kolomogorv-Smirnov test;disease;healthy subjects;human gut microbiome protein family abundance;inflammatory bowel disease;machine learning;machine learning pipeline;microbiome protein family;microbiome taxonomic composition;natural language processing classifier","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
