"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7782419,7850111,7850117,7851858,7850047,7852343,7849764,7849921,7852388,7852271,7852251,7850130,7846953,7845025,7848630,7844994,7740016,7844703,7849183,7848343,7846603,7847173,7846426,7844153,7846652,7844424,7844524,7849132,7844740,7849067,7848593,7844325,7848560,7844444,7844673,7846791,7844359,7846607,7846237,7844558,7847445,7841045,7742323,7840687,7841503,7840796,7840648,7840857,7840747,7840733,7840112,7840756,7840805,7840795,7843428,7841019,7840832,7841715,7841036,7841082,7840589,7840826,7840810,7841059,7842073,7840868,7843542,7840668,7841100,7837848,7838716,7837936,7836713,7836661,7837988,7836658,7837950,7837957,7839559,7838573,7837887,7814247,7832066,7837946,7836805,7814145,7839617,7839600,7837874,7836806,7839580,7831994,7839988,7837928,7832108,7836843,7831854,7769237,7752798,7744687",2017/05/05 22:27:12
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Spectrum-Revealing Cholesky Factorization for Kernel Methods","J. Xiao; M. Gu","Dept. of Math., UC Berkeley, Berkeley, CA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1293","1298","Kernel methods represent some of the most popular machine learning tools for data analysis. Since exact kernel methods can be prohibitively expensive for large problems, reliable low-rank matrix approximations and high-performance implementations have become indispensable for practical applications of kernel methods. In this work, we introduce spectrum-revealing Cholesky factorization, a reliable low-rank matrix factorization, for kernel matrix approximation. We also develop an efficient and effective randomized algorithm for computing this factorization. Our numerical experiments demonstrate that this algorithm is as effective as other Cholesky factorization based kernel methods on machine learning problems, but significantly faster.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837988","Spectrum-Revealing Cholesky factorization;kernel methods;low-rank approximation","Algorithm design and analysis;Approximation algorithms;Complexity theory;Kernel;Machine learning algorithms;Prediction algorithms;Reliability","approximation theory;data analysis;learning (artificial intelligence);matrix decomposition","data analysis;kernel matrix approximation;kernel methods;machine learning;matrix approximations;matrix factorization;spectrum-revealing Cholesky factorization","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Water levels forecast in Thailand: A case study of Chao Phraya river","K. Pasupa; S. Jungjareantrat","Faculty of Information Technology, King Mongkut's Institute of Technology, Ladkrabang Bangkok 10520, Thailand","2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)","20170202","2016","","","1","6","It is always desirable to be able to manage level of water in river, dam, and reservoir. Models have been constructed for predicting the level of these bodies of water, and good models can help increase the effectiveness of water management. Presently, the model that is employed by the Hydrographic Department of the Royal Thai Navy for predicting the level of water in Chao Phraya river is a harmonic method of tidal modeling. This model can predict the overall trend well but with high individual prediction error. Many machine learning algorithms for making predictions have also been introduced in recent years. Therefore, it was attempted in this study to compare the prediction performance of several machine learning models to that of the Royal Thai Navys model. These models were the following: linear regression, kernel regression, support vector regression, k-nearest neighbors, and random forest. The data input into these models were water level time series data of past 24, 48, and 72 hours measured at the Royal Thai Navy headquarters station, Phra Chulachomklao Fort, thirteen other stations along the river, and the output were predictions for the next 24 hours. It was found that all of the machine learning techniques were able to achieve better performances than that of the harmonic method of tidal modeling. The support vector regression model with Radial basis function kernel and 72-hour past time series data yielded prediction results with the least errors, at 0.117 m and 0.116 m for the water levels at the Royal Thai Navy headquarters station and Phra Chulachomklao Fort, respectively.","","Electronic:978-1-5090-3549-6; POD:978-1-5090-3550-2; USB:978-1-5090-4757-4","10.1109/ICARCV.2016.7838716","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838716","","Harmonic analysis;Kernel;Machine learning algorithms;Predictive models;Rivers;Telemetry;Water resources","environmental science computing;forecasting theory;learning (artificial intelligence);radial basis function networks;random processes;regression analysis;reservoirs;support vector machines;time series","Chao Phraya river;Phra Chulachomklao Fort;Royal Thai Navy Hydrographic Department;Thailand;dam;k-nearest neighbors;kernel regression;linear regression;machine learning;radial basis function kernel;random forest;reservoir;support vector regression;tidal modeling;water level forecast;water level time series data;water management","","","","","","","13-15 Nov. 2016","","IEEE","IEEE Conference Publications"
"Efficient instance selection algorithm for classification based on fuzzy frequent patterns","A. S. Alvar; M. S. Abadeh","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2016 IEEE 17th International Symposium on Computational Intelligence and Informatics (CINTI)","20170209","2016","","","000319","000324","Most of the instance selection methods seek to obtain subset of data for instance-based learning algorithms. These methods can improve classification performance, reduce memory requirements, and reduce execution time for these learning algorithms. In this paper, we introduce an instance selection algorithm (FF-IS) which is based on fuzzy frequent patterns and two thresholds. This method preserves appropriate border instances. The aim of this algorithm is to reserve important instances that are closer to border of the classes. We have used K-Nearest Neighbor (KNN) classifier to evaluate the performance of the proposed instance selection algorithm. We have compared our method with several well-known instance selection algorithms. Results indicate that this algorithm selects fewer and hence more representative instances. In comparison to other instance selection techniques, using the proposed instance selection algorithm enables the final KNN classifier to achieve better classification accuracy.","","Electronic:978-1-5090-3909-8; POD:978-1-5090-3910-4; USB:978-1-5090-3908-1","10.1109/CINTI.2016.7846426","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846426","","Algorithm design and analysis;Classification algorithms;Computational intelligence;Data mining;Databases;Machine learning algorithms;Training","fuzzy set theory;learning (artificial intelligence);pattern classification","KNN classifier;fuzzy frequent patterns;instance selection algorithm;instance-based learning algorithms;k-nearest neighbor classifier;memory requirements","","","","","","","17-19 Nov. 2016","","IEEE","IEEE Conference Publications"
"Transfer learning algorithms for autonomous reconfiguration of wearable systems","R. Saeedi; H. Ghasemzadeh; A. H. Gebremedhin","School of Electrical Engineering and Computer Science, Washington State University","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","563","569","Wearables have emerged as a revolutionary technology in many application domains including healthcare and fitness. Machine learning algorithms, which form the core intelligence of wearables, traditionally deduce a computational model from a set of training examples to detect events of interest (e.g. activity type). However, in the dynamic environment in which wearables typically operate in, the accuracy of a computational model drops whenever changes in configuration of the system (such as device type and sensor orientation) occur. Therefore, there is a need to develop systems which can adapt to the new configuration autonomously. In this paper, using transfer learning as an organizing principle, we develop several algorithms for data mapping. The data mapping algorithms employ effective signal similarity methods and are used to adapt the system to the new configuration. We demonstrate the efficacy of the data mapping algorithms using a publicly available dataset on human activity recognition.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840648","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840648","","Activity recognition;Algorithm design and analysis;Computational modeling;Dictionaries;Machine learning algorithms;Signal processing algorithms;Time series analysis","health care;learning (artificial intelligence);wearable computers","autonomous wearable system reconfiguration;computational model;data mapping;fitness;healthcare;human activity recognition;machine learning;sensor orientation;signal similarity methods;transfer learning;wearables intelligence","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Novel Bayesian Ensemble Pruning Method","Z. Jiang; H. Liu; B. Fu; Z. Wu","Electron. Eng. & Comput. Sci., Peking Univ., Beijing, China","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","1205","1212","In ensemble learning, ensemble pruning is a procedure that aims at removing the unnecessary base classifiers and retaining the best subset of the base classifiers. We presented a two-step ensemble pruning framework, in which the optimal size of the pruned ensemble is first decided, and then with the optimal size as input, the optimal ensemble is selected. For the first step to find the optimal ensemble size, we presented an algorithm that can be proved to be able to find the Bayesian optimal ensemble size. For the second step, we developed two greedy forward pruning methods, i.e., the Bayesian Pruning (BP) method and the Bayesian Independent Pruning (BIP) method. In the BP method, we assumed that the probability of a candidate ensemble to be the optimal ensemble follows the Generalized Beta distribution. And in the BIP method, we further assumed that whether a base classifier belongs to the optimal ensemble is independent to the other base classifiers. Experimental results on twenty benchmark data sets showed that the BP and BIP methods achieved competitive performance in contrast to other state-of-the-art algorithms.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0174","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836805","Bayesian framework;ensemble learning;ensemble pruning;ordering-based","Bayes methods;Conferences;Electronic mail;Machine learning algorithms;Prediction algorithms;Sampling methods;Training","Bayes methods;learning (artificial intelligence);pattern classification;statistical distributions","BIP method;BP method;Bayesian independent pruning method;base classifier;ensemble learning;generalized beta distribution;greedy forward pruning methods;novel Bayesian ensemble pruning method;probability;two-step ensemble pruning framework","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Fully Deep Blind Image Quality Predictor","J. Kim; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Journal of Selected Topics in Signal Processing","20170213","2017","11","1","206","220","In general, owing to the benefits obtained from original information, full-reference image quality assessment (FR-IQA) achieves relatively higher prediction accuracy than no-reference image quality assessment (NR-IQA). By fully utilizing reference images, conventional FR-IQA methods have been investigated to produce objective scores that are close to subjective scores. In contrast, NR-IQA does not consider reference images; thus, its performance is inferior to that of FR-IQA. To alleviate this accuracy discrepancy between FR-IQA and NR-IQA methods, we propose a blind image evaluator based on a convolutional neural network (BIECON). To imitate FR-IQA behavior, we adopt the strong representation power of a deep convolutional neural network to generate a local quality map, similar to FR-IQA. To obtain the best results from the deep neural network, replacing hand-crafted features with automatically learned features is necessary. To apply the deep model to the NR-IQA framework, three critical problems must be resolved: 1) lack of training data; 2) absence of local ground truth targets; and 3) different purposes of feature learning. BIECON follows the FR-IQA behavior using the local quality maps as intermediate targets for conventional neural networks, which leads to NR-IQA prediction accuracy that is comparable with that of state-of-the-art FR-IQA methods.","1932-4553;19324553","","10.1109/JSTSP.2016.2639328","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782419","Convolutional neural network;deep learning;image quality assessment;no-reference image quality assessment","Distortion;Feature extraction;Image quality;Machine learning;Measurement;Neural networks;Training","image enhancement;neural nets","BIECON;FR-IQA method;NR-IQA method;blind image evaluator;deep convolutional neural network;full-reference image quality assessment;fully deep blind image quality predictor;no-reference image quality assessment","","1","","","","20161213","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"A 5.5GHz 0.84TOPS/mm<sup>2</sup> neural network engine with stream architecture and resonant clock mesh","S. Lu; Z. Zhang; M. Papaefthymiou","University of Michigan, Ann Arbor, MI, USA","2016 IEEE Asian Solid-State Circuits Conference (A-SSCC)","20170209","2016","","","133","136","This paper presents an ultra-high-performance neural network engine fabricated in a 65nm CMOS technology. The 0.9mm<sup>2</sup> core relies on an energy-efficient resonant clock mesh running at 5.5GHz to achieve 0.76 8-bit TOPS, improving throughput by over 4x, area efficiency by over 8×, and energy-delay-area product by over 1.8× compared to previous state-of-the-art neural network designs. Achieving a charge recovery rate of 63%, the resonant clock mesh enables the deployment of a deeply-pipelined stream architecture and high-speed stream buffers with a sub-5W power consumption.","","Electronic:978-1-5090-3700-1; POD:978-1-5090-3701-8; Paper:978-1-5090-3699-8","10.1109/ASSCC.2016.7844153","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844153","Application-Specific Integrated Circuits;High-Performance Clocking;Neural Network;Stream Architecture","Clocks;Computer architecture;Conferences;Machine learning;Neural networks;Registers;Solid state circuits","CMOS integrated circuits;low-power electronics;neural nets","CMOS technology;charge recovery rate;deeply-pipelined stream architecture;energy-delay-area product;energy-efficient resonant clock mesh;frequency 5.5 GHz;high-speed stream buffers;power 5 W;power consumption;size 65 nm;ultra-high-performance neural network engine","","","","","","","7-9 Nov. 2016","","IEEE","IEEE Conference Publications"
"Interactive Multi-task Relationship Learning","K. Lin; J. Zhou","Dept. of Comput. Sci. & Eng., Michigan State Univ., East Lansing, MI, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","241","250","Multi-task learning (MTL) is a learning paradigm that provides a principled way to improve the generalization performance of a set of related machine learning tasks by transferring knowledge among the tasks. The past decade has witnessed many successful applications of MTL in different domains. In the center of MTL algorithms is how the relatedness of tasks are modeled and encoded in learning formulations to facilitate knowledge transfer. Among the MTL algorithms, the multi-task relationship learning (MTRL) attracted much attention in the community because it learns task relationship from data to guide knowledge transfer, instead of imposing a prior task relatedness assumption. However, this method heavily depends on the quality of training data. When there is insufficient training data or the data is too noisy, the algorithm could learn an inaccurate task relationship that misleads the learning towards suboptimal models. To address the aforementioned challenge, in this paper we propose a novel interactive multi-task relationship learning (iMTRL) framework that efficiently solicits partial order knowledge of task relationship from human experts, effectively incorporates the knowledge in a proposed knowledge-aware MTRL formulation. We propose an efficient optimization algorithm for kMTRL and comprehensively study query strategies that identify the critical pairs that are most influential to the learning. We present extensive empirical studies on both synthetic and real datasets to demonstrate the effectiveness of proposed framework.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0035","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837848","","Covariance matrices;Data models;Diseases;Knowledge transfer;Machine learning algorithms;Predictive models;Training","interactive systems;learning (artificial intelligence);optimisation","iMTRL;interactive multitask relationship learning;kMTRL;knowledge-aware MTRL formulation;optimization algorithm;partial order knowledge;query strategies;task relationship","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Overview: Video recognition from handcrafted method to deep learning method","X. Xiao; D. Xu; W. Wan","School of Communication and Information Engineering, Shanghai University, 200444, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","646","651","With the development of information technology, the automatic recognition of human action from video becomes a very popular research topic. In this paper, we review recent state-of-the-art of human action recognition methods in videos. First, we compare several notable handcrafted methods. Then we introduce some deep learning action recognition models. As deep learning becomes hot spot of research in recent years, more and more papers have utilized this method to explore the spatiotemporal features representation. We find that the deep learning methods outperform handcrafted methods at large scale recognition especially in cluttered background. But the networks still have much disadvantage. We expect our overview provides a fairly clear guidance for future research in this domain.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846652","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846652","deep learning method;handcrafted method;human action recognition","Computer vision;Feature extraction;Image motion analysis;Image recognition;Machine learning;Optical imaging;Streaming media","feature extraction;image recognition;image representation;learning (artificial intelligence);video signal processing","deep learning action recognition models;handcrafted method;human action automatic recognition;spatiotemporal features representation;video recognition","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"An efficient parallel topic-sensitive expert finding algorithm using spark","Y. M. Yang; C. D. Wang; J. H. Lai","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P. R. China","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3556","3562","Expert finding is an important technique to obtain the user authority ranking in community question answering (CQA) websites. ZhihuRank is a topic-sensitive expert finding algorithm, which is based on both LDA and PageRank. Currently, with the amount of participants and documents increasing rapidly in CQA websites, how to parallel expert finding algorithms for big data analysis has received significant attention. In this paper, we find that the Spark framework is more suitable for paralleling expert finding algorithms than the MapReduce framework, which is a memory-based parallel computing model to support complicated iterative algorithms. As an example, we parallel ZhihuRank using MLlib's LDA and GraphX's PageRank in Spark. Experiments have been conducted on large-scale real data from Zhihu<sup>1</sup> (the most popular CQA website in China). And the experimental results confirmed the effectiveness and scalability of our proposed approach.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841019","Community Question Answering;Distributed Computing;Expert Finding;LDA;PageRank","Algorithm design and analysis;Computational modeling;Heuristic algorithms;Libraries;Machine learning algorithms;Programming;Sparks","Big Data;Web sites;data analysis;iterative methods;parallel algorithms;question answering (information retrieval);search engines","CQA Web sites;LDA;MapReduce framework;PageRank;Spark framework;ZhihuRank;big data analysis;community question answering Web sites;iterative algorithms;memory-based parallel computing model;parallel topic-sensitive expert finding algorithm;user authority ranking","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Comparative study between deep learning and bag of visual words for wild-animal recognition","E. Okafor; P. Pawara; F. Karaaba; O. Surinta; V. Codreanu; L. Schomaker; M. Wiering","Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, The Netherlands","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","Most research in image classification has focused on applications such as face, object, scene and character recognition. This paper examines a comparative study between deep convolutional neural networks (CNNs) and bag of visual words (BOW) variants for recognizing animals. We developed two variants of the bag of visual words (BOW and HOG-BOW) and examine the use of gray and color information as well as different spatial pooling approaches. We combined the final feature vectors extracted from these BOW variants with a regularized L2 support vector machine (L2-SVM) to distinguish between classes within our datasets. We modified existing deep CNN architectures: AlexNet and GoogleNet, by reducing the number of neurons in each layer of the fully connected layers and last inception layer for both scratch and pre-trained versions. Finally, we compared the existing CNN methods, our modified CNN architectures and the proposed BOW variants on our novel wild-animal dataset (Wild-Anim). The results show that the CNN methods significantly outperform the BOW techniques.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850111","","Character recognition;Computer architecture;Feature extraction;Image recognition;Machine learning;Neurons;Visualization","feedforward neural nets;image classification;image colour analysis;learning (artificial intelligence);support vector machines","AlexNet;GoogleNet;HOG-BOW;L2-SVM;bag of visual words;color information;deep CNN architectures;deep convolutional neural networks;deep learning;final feature vectors;gray information;image classification;regularized L2 support vector machine;spatial pooling approaches;wild-animal dataset;wild-animal recognition","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Counting Apples and Oranges With Deep Learning: A Data-Driven Approach","S. W. Chen; S. S. Shivakumar; S. Dcunha; J. Das; E. Okon; C. Qu; C. J. Taylor; V. Kumar","GRASP Laboratory, University of Pennsylvania, Philadelphia, PA, USA","IEEE Robotics and Automation Letters","20170202","2017","2","2","781","788","This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowdsourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruits in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.","2377-3766;23773766","","10.1109/LRA.2017.2651944","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814145","Agricultural automation;categorization;object detection;segmentation;visual learning","Image segmentation;Labeling;Lighting;Machine learning;Neural networks;Pipelines;Training","agricultural products;crowdsourcing;feature extraction;feedforward neural nets;learning (artificial intelligence);object detection;performance evaluation;regression analysis","agricultural automation;apple counting;blob detector;candidate region extraction;counting algorithm;crowdsourcing platform;data-driven approach;fruit counting pipeline;fully convolutional network;green apples;human generated label utilization;illumination changes;large data sets;linear regression model;object detection;occlusions;orange counting;performance analysis","","","","","","20170111","April 2017","","IEEE","IEEE Journals & Magazines"
"ExploreKit: Automatic Feature Generation and Selection","G. Katz; E. C. R. Shin; D. Song","Univ. of California, Berkeley, Berkeley, CA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","979","984","Feature generation is one of the challenging aspects of machine learning. We present ExploreKit, a framework for automated feature generation. ExploreKit generates a large set of candidate features by combining information in the original features, with the aim of maximizing predictive performance according to user-selected criteria. To overcome the exponential growth of the feature space, ExploreKit uses a novel machine learning-based feature selection approach to predict the usefulness of new candidate features. This approach enables efficient identification of the new features and produces superior results compared to existing feature selection solutions. We demonstrate the effectiveness and robustness of our approach by conducting an extensive evaluation on 25 datasets and 3 different classification algorithms. We show that ExploreKit can achieve classification-error reduction of 20% overall. Our codeis available at https://github.com/giladkatz/ExploreKit.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837936","","Analytical models;Diabetes;Machine learning;Machine learning algorithms;Prediction algorithms;Pregnancy;Space exploration","feature selection;learning (artificial intelligence);pattern classification","ExploreKit;automatic feature generation;automatic feature selection;classification algorithms;classification-error reduction;machine learning-based feature selection;predictive performance maximization;user-selected criteria","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Predict failures in production lines: A two-stage approach with clustering and supervised learning","D. Zhang; B. Xu; J. Wood","International Center for Automotive Research, Clemson University, Greenville, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2070","2074","The implementation of advanced technologies in manufacturing has created large amounts of data. The data can be utilized to create predictive models for quality control, which allows manufacturers to produce higher quality products at a lower cost. Bosch has provided a large-scale data set of a production line and hosted a challenge on Kaggle aiming to predict the manufacturing failures using the anonymized features. We proposed a two-stage method first to cluster the data into groups based on the manufacturing process and then use supervised learning to predict the failed product in each cluster. This approach reduces the sparsity of the data set. Various algorithms were compared. The random forest algorithm achieved the highest performance score and was chosen as the final model.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840832","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840832","clustering;manufacturing;quality control;supervised learning","Classification algorithms;Clustering algorithms;Machine learning algorithms;Manufacturing processes;Prediction algorithms;Supervised learning","data handling;failure analysis;learning (artificial intelligence);manufacturing data processing;pattern clustering;product quality;production engineering computing;quality control","Bosch;Kaggle;data clustering;data set sparsity reduction;manufacturing failure prediction;manufacturing process;predictive model;product quality;production line failure prediction;quality control;random forest algorithm;supervised learning","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep Model Based Domain Adaptation for Fault Diagnosis","W. Lu; B. Liang; Y. Cheng; D. Meng; J. Yang; T. Zhang","Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Industrial Electronics","20170209","2017","64","3","2296","2305","In recent years, machine learning techniques have been widely used to solve many problems for fault diagnosis. However, in many real-world fault diagnosis applications, the distribution of the source domain data (on which the model is trained) is different from the distribution of the target domain data (where the learned model is actually deployed), which leads to performance degradation. In this paper, we introduce domain adaptation, which can find the solution to this problem by adapting the classifier or the regression model trained in a source domain for use in a different but related target domain. In particular, we proposed a novel deep neural network model with domain adaptation for fault diagnosis. Two main contributions are concluded by comparing to the previous works: first, the proposed model can utilize domain adaptation meanwhile strengthening the representative information of the original data, so that a high classification accuracy in the target domain can be achieved, and second, we proposed several strategies to explore the optimal hyperparameters of the model. Experimental results, on several real-world datasets, demonstrate the effectiveness and the reliability of both the proposed model and the exploring strategies for the parameters.","0278-0046;02780046","","10.1109/TIE.2016.2627020","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740016","Deep neural network (DNN);domain adaptation;fault diagnosis","Adaptation models;Data models;Fault diagnosis;Feature extraction;Machine learning;Neural networks;Training","fault diagnosis;learning (artificial intelligence);neural nets;pattern classification;regression analysis","classifier model;deep model based domain adaptation;fault diagnosis;machine learning techniques;novel deep neural network model;optimal model hyperparameters;performance degradation;regression model;source domain data distribution;target domain data distribution","","","","","","20161109","March 2017","","IEEE","IEEE Journals & Magazines"
"Local modes-based free-shape data partitioning","P. Angelov; X. Gu","School of Computing and Communications, Lancaster University, LA1 4WA, UK","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","In this paper, a new data partitioning algorithm, named “local modes-based data partitioning”, is proposed. This algorithm is entirely data-driven and free from any user input and prior assumptions. It automatically derives the modes of the empirically observed density of the data samples and results in forming parameter-free data clouds. The identified focal points resemble Voronoi tessellations. The proposed algorithm has two versions, namely, offline and evolving. The two versions are both able to work separately and start “from scratch”, they can also perform a hybrid. Numerical experiments demonstrate the validity of the proposed algorithm as a fully autonomous partitioning technique, and achieve better performance compared with alternative algorithms.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850117","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850117","data cloud;data partitioning;data- driven;evolving clustering;parameterfree","Algorithm design and analysis;Chebyshev approximation;Clustering algorithms;Filtering;Machine learning algorithms;Partitioning algorithms;Standards","cloud computing;data handling","Voronoi tessellations;autonomous partitioning technique;data samples;focal point identification;local-mode-based free-shape data partitioning;parameter-free data clouds","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Guide data generation for on-line learning of DBM-initialized MLP","Y. Kaneda; Q. Zhao; Y. Liu","Dept. of Computer Science and Engineering, The University of Aizu, Aizu-Wakamatsu, Fukushima, Japan","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001340","001345","In this paper, we propose a method for generating guide data, and investigate its efficiency and efficacy for on-line learning with guide data. On-line learning in this research updates a learning model initialized by the decision boundary making algorithm proposed by us in our earlier study. The problem is that, if the guide data are not properly generated, on-line learning may require high computational cost in terms of time, and the learning process may not converge to good result. To solve this problem, we propose to use k-means to cluster all candidates of guide data, and use one datum from each cluster as the guide datum. We conducted experiments on several public databases, using different settings, and confirmed the performance of the proposed method. Specifically, if k=5, we can obtain good models with low computational cost through on-line learning.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844424","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844424","Backpropagation Algorithm;Decision Boundary Making;Multilayer Perceptron;On-Line Learning","Computational efficiency;Computational modeling;Data models;Databases;Machine learning algorithms;Support vector machines;Training","decision making;learning (artificial intelligence);multilayer perceptrons;pattern clustering","DBM-initialized MLP;decision boundary making algorithm;guide data generation;k-means clustering;learning model;online learning","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"DARC: Timely Classification with Randomly Delayed Features","J. Xu; Q. Cai; C. Shen","Dept. of Electr. & Comput. Eng., Univ. of Miami, Miami, FL, USA","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","Many emerging Big Data applications involve realtime classification in which data instances arriving sequentially over time need to be classified based on their feature vectors. A common and implicit assumption in existing works is that the features become available instantly with the instance and simultaneously with each other, which, however, rarely holds in practice. Instead, features of an instance may experience various random delays to be available. In such scenarios, an important trade-off emerges between accurate classification and timely classification. In this paper, we provide a first formulation of this important problem and propose efficient online algorithms, namely DAlay-aware Real-time Classification (DARC) algorithms, that maximize the classification accuracy given an average classification delay constraint. The algorithms are developed based on the Lyapunov stochastic optimization technique which provides strong performance guarantee. Numerical results on an intrusion detection dataset are provided to show the effectiveness of the proposed algorithm.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841715","","Big data applications;Delays;Feature extraction;Intrusion detection;Machine learning algorithms;Optimization;Real-time systems","data communication;telecommunication security","DARC;Lyapunov stochastic optimization technique;big data applications;dalay-aware real-time classification;intrusion detection;randomly delayed features;timely classification","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Estimate user meaningful places through low-energy mobile sensing","F. Corno; L. De Russis; T. Montanaro","Politecnico di Torino Corso Duca degli Abruzzi, 24, Italy 10129","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","003039","003044","Due to the increasing spread of location-aware applications, developers interest in user location estimation has grown in recent years. As users spend the majority of their time in few meaningful places (i.e., groups of near locations that can be considered as a unique place, such as home, school or the workplace), this paper presents a new energy efficient method to estimate user presence in a meaningful place. Specifically, instead of using commonly used but energy hungry methods such as GPS and network positioning techniques, the proposed method applies a Machine Learning algorithm based on Decision Trees, to predict the user presence in a meaningful place by collecting and analyzing: a) user activity, b) information from received notifications (receipt time, generating service, sender-receiver relationship), and c) device status (battery level and ringtone mode). The results demonstrate that, using 20 days of training data and testing the system with data coming from 14 persons, the accuracy (percentage of correct predictions) is 89.40% (standard deviation: 8.27%) with a precision of 89.04% and a recall of 89.40%. Furthermore, the paper analyzes the importance of each considered feature, by comparing the prediction accuracy obtained with different combinations of features.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844703","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844703","","Batteries;Decision trees;Estimation;Global Positioning System;Machine learning algorithms;Receivers;Sensors","energy conservation;learning (artificial intelligence);mobile computing;mobility management (mobile radio);telecommunication power management","decision tree;device status;energy efficient method;energy hungry method;location-aware applications;low-energy mobile sensing;machine learning algorithm;user activity;user location estimation;user meaningful place estimation;user presence prediction","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Distributed embedded deep learning based real-time video processing","W. Zhang; Dehai Zhao; L. Xu; Z. Li; Wenjuan Gong; Jiehan Zhou","Department of Software Engineering, China University of Petroleum, No. 66 Changjiang West Road, Qingdao, China, 266580","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001945","001950","There arises the needs for fast processing of continuous video data using embedded devices, for example the one needed for UAV aerial photography. In this paper, we proposed a distributed embedded platform built with NVIDIA Jetson TX1 using deep learning techniques for real time video processing, mainly for object detection. We design a Storm based distributed real-time computation platform and ran object detection algorithm based on convolutional neural networks. We have evaluated the performance of our platform by conducting real-time object detection on surveillance video. Compared with the high end GPU processing of NVIDIA TITAN X, our platform achieves the same processing speed but a much lower power consumption when doing the same work. At the same time, our platform had a good scalability and fault tolerance, which is suitable for intelligent mobile devices such as unmanned aerial vehicles or self-driving cars.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844524","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844524","Distributed embedded platform;Stream processing;low power consumption;video processing","Machine learning;Neural networks;Object detection;Power demand;Real-time systems;Storms;Streaming media","autonomous aerial vehicles;fault tolerance;graphics processing units;learning (artificial intelligence);object detection;photography;video signal processing","NVIDIA Jetson TX1;NVIDIA TITAN X;UAV aerial photography;continuous video data processing;convolutional neural networks;distributed embedded deep learning;fault tolerance;high-end GPU processing;intelligent mobile devices;object detection algorithm;power consumption","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Modeling Grasp Motor Imagery Through Deep Conditional Generative Models","M. Veres; M. Moussa; G. W. Taylor","School of Engineering, University of Guelph, Guelph, ON, Canada","IEEE Robotics and Automation Letters","20170202","2017","2","2","757","764","Grasping is a complex process involving knowledge of the object, the surroundings, and of oneself. While humans are able to integrate and process all of the sensory information required for performing this task, equipping machines with this capability is an extremely challenging endeavor. In this paper, we investigate how deep learning techniques can allow us to translate high-level concepts such as motor imagery to the problem of robotic grasp synthesis. We explore a paradigm based on generative models for learning integrated object-action representations and demonstrate its capacity for capturing and generating multimodal multifinger grasp configurations on a simulated grasping dataset.","2377-3766;23773766","","10.1109/LRA.2017.2651945","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814247","Deep learning;generative models;grasping;multifingered hands;visual learning","Encoding;Generators;Grasping;Machine learning;Robot sensing systems;Visualization","end effectors;learning (artificial intelligence)","deep conditional generative models;deep learning techniques;grasp motor imagery modeling;integrated object-action representations;multimodal multifinger grasp configurations;robotic grasp synthesis","","","","","","20170111","April 2017","","IEEE","IEEE Journals & Magazines"
"Automatic Detection and Classification of Colorectal Polyps by Transferring Low-Level CNN Features From Nonmedical Domain","R. Zhang; Y. Zheng; T. W. C. Mak; R. Yu; S. H. Wong; J. Y. W. Lau; C. C. Y. Poon","Department of Surgery, The Chinese University of Hong Kong, Shatin, Hong Kong SAR","IEEE Journal of Biomedical and Health Informatics","20170201","2017","21","1","41","47","Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide. Although polypectomy at early stage reduces CRC incidence, 90% of the polyps are small and diminutive, where removal of them poses risks to patients that may outweigh the benefits. Correctly detecting and predicting polyp type during colonoscopy allows endoscopists to resect and discard the tissue without submitting it for histology, saving time, and costs. Nevertheless, human visual observation of early stage polyps varies. Therefore, this paper aims at developing a fully automatic algorithm to detect and classify hyperplastic and adenomatous colorectal polyps. Adenomatous polyps should be removed, whereas distal diminutive hyperplastic polyps are considered clinically insignificant and may be left in situ . A novel transfer learning application is proposed utilizing features learned from big nonmedical datasets with 1.4-2.5 million images using deep convolutional neural network. The endoscopic images we collected for experiment were taken under random lighting conditions, zooming and optical magnification, including 1104 endoscopic nonpolyp images taken under both white-light and narrowband imaging (NBI) endoscopy and 826 NBI endoscopic polyp images, of which 263 images were hyperplasia and 563 were adenoma as confirmed by histology. The proposed method identified polyp images from nonpolyp images in the beginning followed by predicting the polyp histology. When compared with visual inspection by endoscopists, the results of this study show that the proposed method has similar precision (87.3% versus 86.4%) but a higher recall rate (87.6% versus 77.0%) and a higher accuracy (85.9% versus 74.3%). In conclusion, automatic algorithms can assist endoscopists in identifying polyps that are adenomatous but have been incorrectly judged as hyperplasia and, therefore, enable timely resection of these polyps at an early stage before they develop into invasive cancer.","2168-2194;21682194","","10.1109/JBHI.2016.2635662","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769237","Colorectal cancer;deep learning;health informatics;polyp diagnosis","Biomedical imaging;Cancer;Colonoscopy;Feature extraction;Machine learning;Neurons;Training","biological tissues;biomedical optical imaging;cancer;endoscopes","NBI endoscopic polyp images;adenomatous colorectal polyps;cancer deaths;colonoscopy;colorectal cancer;colorectal polyps automatic detection;colorectal polyps classification;deep convolutional neural network;endoscopic images;endoscopic nonpolyp images;fully automatic algorithm;human visual observation;hyperplasia;hyperplastic colorectal polyps;learning application;low-level CNN features;narrowband imaging endoscopy;nonmedical datasets;optical magnification;polyp histology;polypectomy;tissue;white-light endoscopy","","","","","","20161205","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Classification and diagnosis of the parkinson disease by stacked autoencoder","H. Badem; A. Caliskan; A. Basturk; M. E. Yuksel","Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Erciyes &#x00DC;niversitesi, Kayseri, T&#x00FC;rkiye","2016 National Conference on Electrical, Electronics and Biomedical Engineering (ELECO)","20170213","2016","","","499","502","Parkinson disease is neuro-degenerative motion disorder this disease causes voice and speech dysfunctions. Therefore, disease can be diagnosed by determining dysphonia produced by a subject with parkinson diseases. In this paper we have introduced a new classification method of the parkinson disease which is based on the stacked autoencoder. The method is tested with two data sets and results show that the performance of the stacked autoencoder is significantly better than the-state-art-methods.","","POD:978-605-01-0923-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851858","","Dogs;Machine learning;Neural networks;Parkinson's disease;Random access memory;Speech","biomechanics;diseases;encoding;medical disorders;medical signal processing;neurophysiology;patient diagnosis;signal classification","Parkinson disease diagnosis;classification method;dysphonia;neurodegenerative motion disorder;speech dysfunction;stacked autoencoder;voice dysfunction","","","","","","","1-3 Dec. 2016","","IEEE","IEEE Conference Publications"
"Supervised Learning of Semantics-Preserving Hash via Deep Convolutional Neural Networks","H. F. Yang; K. Lin; C. S. Chen","","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2017","PP","99","1","1","This paper presents a simple yet effective supervised deep hash approach that constructs binary hash codes from labeled data for large-scale image search. We assume that the semantic labels are governed by several latent attributes with each attribute on or off, and classification relies on these attributes. Based on this assumption, our approach, dubbed supervised semantics-preserving deep hashing (SSDH), constructs hash functions as a latent layer in a deep network and the binary codes are learned by minimizing an objective function defined over classification error and other desirable hash codes properties. With this design, SSDH has a nice characteristic that classification and retrieval are unified in a single learning model. Moreover, SSDH performs joint learning of image representations, hash codes, and classification in a point-wised manner, and thus is scalable to large-scale datasets. SSDH is simple and can be realized by a slight enhancement of an existing deep architecture for classification; yet it is effective and outperforms other hashing approaches on several benchmarks and large datasets. Compared with state-of-the-art approaches, SSDH achieves higher retrieval accuracy, while the classification performance is not sacrificed.","0162-8828;01628828","","10.1109/TPAMI.2017.2666812","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849132","Image retrieval;binary codes;convolutional neural networks;deep learning;supervised hashing","Binary codes;Convolutional codes;Machine learning;Neural networks;Semantics;Synchronous digital hierarchy;Training","","","","","","","","20170209","","","IEEE","IEEE Early Access Articles"
"Predicting bug inducing source code change patterns","A. Khan; S. N. Ahsan","Faculty of Engineering, Science and Technology Main Campus, Iqra University, Karachi, Pakistan","2016 International Conference on Open Source Systems & Technologies (ICOSST)","20170202","2016","","","29","35","A change in source code without the prior analysis of its impact may generate one or more defects. Fixing of such defects consumes maintenance time which ultimately increases the cost of software maintenance. Therefore, in the recent years, several research works have been done to develop techniques for the automatic impact analysis of changes in source code. In this paper, we propose to use Frequent Pattern Mining (FPM) technique of machine learning for the automatic impact analysis of those changes in source code which may induce bugs. Therefore, to find patterns associated with some specific types of software changes, we applied FPM's algorithms' Apriori and Predictive Apriori on the stored data of software changes of the following three Open-Source Software (OSS) projects: Mozilla, GNOME, and Eclipse. We grouped the data of software changes into two major categories: changes to meet bug fixing requirements and changes to meet requirements other than bug fixing. In the case of bug fixing requirements, we predict source files which are frequently changed together to fix any one of the following four types of bugs related to: memory (MEMORY), variables locking (LOCK), system (SYSTEM) and graphical user interface (UI). Our experimental results predict several interesting software change patterns which may induce bugs. The obtained bug inducing patterns have high confidence and accuracy value i.e., more than 90%.","","CD:978-1-5090-5585-2; Electronic:978-1-5090-5586-9; POD:978-1-5090-5587-6","10.1109/ICOSST.2016.7838573","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838573","Frequent Pattern Mining;Impact Analysis;Software Changes;Specific Types of Bugs;Transaction Data","Computer bugs;Data mining;Databases;Machine learning algorithms;Prediction algorithms;Software maintenance","data mining;object-oriented methods;program debugging;public domain software;software maintenance;source code (software)","Eclipse;FPM technique;GNOME;Mozilla;OSS projects;automatic impact analysis;bug fixing requirements;bug inducing source code change patterns;frequent pattern mining technique;graphical user interface;machine learning;memory;open-source software projects;predictive apriori algorithm;software change patterns;source files;variables locking","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Phishing detection based on newly registered domains","X. Li; G. Geng; Z. Yan; Y. Chen; X. Lee","China Internet Network Information Center","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3685","3692","Phishing is a security attack that involves the creation of websites that mimic legitimate websites, and these fraud websites bring Internet users a lot of loss. Traditional anti-phishing methods usually worked in a passive way by receiving report data of user. Due to the growing shorter survival time of phishing, this kind of methods is not efficient enough to find and take down new phishing attacks. In this paper, we propose an Intelligent Phishing Detection (IPD) system to address phishing detection problem actively. Specially, IPD first generates the detection dataset from the global massive domain name registration data automatically; then it applies the Naïve Bayes algorithm which is optimized by position-based features to achieve the high precision detection; finally, in order to find more phishing websites, IPD expands detection dataset by generating Uniform Resource Locator (URL) templates based on the detection results. The experimental results of IPD demonstrate the effectiveness and timeliness in detection phishing websites.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841036","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841036","Naïve Bayes;URL template expandsion;newly registered domain;phishing detection;position-based feature","Algorithm design and analysis;Big data;Data acquisition;Feature extraction;Internet;Machine learning algorithms;Uniform resource locators","Web sites;computer crime","IPD system;Internet;URL templates;antiphishing methods;fraud Web sites;intelligent phishing detection system;naïve Bayes algorithm;newly registered domains;security attack;uniform resource locator templates","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep learning with consumer preferences for recommender system","T. Gao; X. Li; Y. Chai; Y. Tang","Department of Automation, Tsinghua University, Beijing 100084, China","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1556","1561","With the arrival of big data era and the fast development of E-commerce, recommender systems(RSs) are used in more and more application domains to assist customers in the search for their favorite products. Collaborative filtering(CF) is one of the most successful and widely used recommendation approaches. Poor recommender quality is a major challenge in traditional CF. And one reason causing this circumstance is the sparsity of data. In this paper, we propose to a deep learning model to predict the values of null ratings. Moreover, we investigate personal recommendation based on customer preferences and search the neighbors through the customer preferences. Accordingly, it has effectively solved recommendation quality problems of traditional CF algorithm under the condition of data sparse and poor result quality.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7832066","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832066","Collaborative filtering Deep learning;Consumer Preferences;Recommender systems","Clustering algorithms;Collaboration;History;Machine learning;Machine learning algorithms;Motion pictures;Recommender systems","Big Data;collaborative filtering;consumer behaviour;electronic commerce;learning (artificial intelligence);recommender systems","CF;E-commerce;big data era;collaborative filtering;consumer preferences;data sparse;deep learning model;null ratings;personal recommendation;recommendation quality problems;recommender quality;recommender system","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"An ensemble of deep learning architectures for automatic feature extraction","F. Shaheen; B. Verma","Centre for Intelligent Systems, Central Queensland University, Brisbane, Australia","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","5","This paper presents a novel ensemble of deep learning architectures for automatic feature extraction. Many ensemble techniques have been recently proposed and successfully applied to real world applications. The existing ensemble techniques can achieve high accuracy however the accuracy depends on features they use and features are extracted by a separate model for feature extraction. As deep learning architectures such as Convolutional Neural Networks (CNNs) can automatically extract features, it is a good idea to explore their feature extraction ability in an ensemble. Therefore the purpose of this research is to propose an ensemble of CNNs and find out the answer of whether or not an ensemble of CNNs can perform better than the traditional ensemble techniques which use a separate feature extraction. To find an answer of the research question, an ensemble of CNNs, an ensemble of MLPs, a CNN and an MLP are implemented and evaluated on the same benchmark datasets. A large number of experiments were conducted and the results showed that the proposed ensemble of CNNs can automatically extract features and achieve better accuracy but takes a higher number of epochs than other ensembles on some real-world image datasets.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850047","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850047","Convolutional Neural Network;Deep Learning;Ensemble of CNNs;Feature Extraction","Computer architecture;Cows;Feature extraction;Machine learning;Neurons;Training;Vegetation mapping","convolution;feature extraction;learning (artificial intelligence);multilayer perceptrons","CNN;MLP;automatic feature extraction;convolutional neural networks;deep learning architectures;multilayer perceptron","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Fuzzy Support Vector Machine Algorithm for Cooperative Spectrum Sensing with Noise Uncertainty","Y. D. Huang; Y. C. Liang; G. Yang","Univ. of Electron. Sci. & Technol. of China, Chengdu, China","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","In cognitive radio networks, the performance of energy detection will be degraded significantly due to the cluster overlapping caused by noise uncertainty. To alleviate the noise uncertainty effect, a novel machine learning algorithm is proposed in this paper for cooperative spectrum sensing. The proposed algorithm incorporates fuzzy support vector machine and nonparallel hyperplane support vector machine. For membership assignment, kernel shadow c-means (KSCM) algorithm is utilized. Furthermore, the test statistics collected by the second users are arranged into a feature vector instead of being combined through weighted sum. Simulations results have shown that the proposed scheme, called NP-FSVM, is more robust to noise uncertainty than the existing methods.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7841503","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841503","","Cascading style sheets;Cognitive radio;Kernel;Machine learning algorithms;Sensors;Support vector machines;Uncertainty","cognitive radio;fuzzy set theory;learning (artificial intelligence);radio spectrum management;signal detection;support vector machines;telecommunication computing","KSCM algorithm;NP-FSVM;cluster overlapping;cognitive radio networks;cooperative spectrum sensing;energy detection;fuzzy support vector machine algorithm;kernel shadow c-means;membership assignment;noise uncertainty;nonparallel hyperplane support vector machine;novel machine learning algorithm","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep Sparse Coding for Non-Intrusive Load Monitoring","S. Singh; A. Majumdar","","IEEE Transactions on Smart Grid","","2017","PP","99","1","1","Energy disaggregation is the task of segregating the aggregate energy of the entire building (as logged by the smartmeter) into the energy consumed by individual appliances. This is a single channel (the only channel being the smart-meter) blind source (different electrical appliances) separation problem. The traditional way to address this is via stochastic finite state machines (e.g. Factorial Hidden Markov Model). In recent times dictionary learning based approaches have shown promise in addressing the disaggregation problem. The usual technique is to learn a dictionary for every device and use the learnt dictionaries as basis for blind source separation during disaggregation. Prior studies in this area are shallow learning techniques, i.e. they learn a single layer of dictionary for every device. In this work, we propose a deep learning approach – instead of learning one level of dictionary, we learn multiple layers of dictionaries for each device. These multi-level dictionaries are used as a basis for source separation during disaggregation. Results on two benchmark datasets and one actual implementation show that our method outperforms state-of-the-art techniques.","1949-3053;19493053","","10.1109/TSG.2017.2666220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847445","Deep Learning;Dictionary Learning;Energy Disaggregation;Non-intrusive Load Monitoring","Automata;Dictionaries;Encoding;Hidden Markov models;Home appliances;Loading;Machine learning","","","","","","","","20170208","","","IEEE","IEEE Early Access Articles"
"Universal Steganography Detector Based on an Artificial Immune System for JPEG Images","J. D. J. S. Pérez; M. S. Rosales; N. Cruz-Cortés","Centro de Investigaciun en Computaciun, Inst. Politec. Nac. Mexico City, Mexico City, Mexico","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","1896","1903","Steganography is a hiding information technique heavily used nowadays. Though initially it was used to establish hidden communication channels, modern steganography has been found useful to hide code inside multimedia objects, mostly images. Its goal is to infiltrate malware into organizations or personal devices. This kind of malware is called stegomalware. As countermeasure, modern steganalysis methods employing different Computational Intelligence techniques such as Support Vector Machine, Machine Learning, Fisher Linear Discriminant, and others have been utilized. In this work we present a new stegananalysis method based on an Artificial Immune System (AIS), to detect JPEG images modified with three well known steganographic tools: F5, Outguess, or Steghide. It is also proposed the usage of Haar Wavelets to extract a feature vector that best describes the analyzed image, this is due the Haar Wavelets fast calculation and information synthesis. Our experimentation results are competitive against techniques representative of the state of the art.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847173","Artificial Immune Systems;Haar Wavelets;Image Analysis;Steganalysis;Steganography","Artificial intelligence;Detectors;Feature extraction;Immune system;Machine learning algorithms;Transform coding;Wavelet analysis","Haar transforms;artificial immune systems;feature extraction;multimedia systems;steganography;wavelet transforms","F5;Haar wavelets;JPEG image detection;Outguess;Steghide;artificial immune system;computational intelligence;feature vector extraction;hidden communication channels;information hiding;information synthesis;multimedia objects;steganalysis;stegomalware;universal steganography detector","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"A Meta-Learning Method to Select Under-Sampling Algorithms for Imbalanced Data Sets","R. F. A. B. d. Morais; P. B. C. Miranda; R. M. A. Silva","Univ. Fed. de Pernambuco, Recife, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","385","390","Imbalanced data sets originating from real world problems, such as medical diagnosis, can be found pervasive. Learning from imbalanced data sets poses its own challenges, as common classifiers assume a balanced distribution of examples' classes in the data. Sampling techniques overcome the imbalance in the data by modifying the examples' classes distribution. Unfortunately, selecting a sampling technique together with its parameters is still an open problem. Current solutions include the brute-force approach (try as many techniques as possible), and the random search approach (choose the most appropriate from a random subset of techniques). In this work, we propose a new method to select sampling techniques for imbalanced data sets. It uses Meta-Learning and works by recommending a technique for an imbalanced data set based on solutions to previous problems. Our experimentation compared the proposed method against the brute-force approach, all techniques with their default parameters, and the random search approach. The results of our experimentation show that the proposed method is comparable to the brute-force approach, outperforms the techniques with their default parameters most of the time, and always surpasses the random search approach.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.076","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839617","Algorithm selection;Meta-learning;Sampling algorithms","Gold;Machine learning algorithms;Measurement;Optimization;Prediction algorithms;Search methods;Software algorithms","data handling;learning (artificial intelligence);sampling methods;search problems","brute-force approach;class distribution;data balanced distribution;imbalanced pervasive data sets;medical diagnosis;metalearning method;random search approach;sampling techniques;under-sampling algorithm selection","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"A novel multiprocessor architecture for k-means clustering algorithm based on network-on-chip","S. G. Khawaja; M. U. Akram; S. A. Khan; A. Ajmal","College of Electrical and Mechanical Engineering, National University of Sciences & Technology, Islamabad, Pakistan","2016 19th International Multi-Topic Conference (INMIC)","20170206","2016","","","1","5","The k-means clustering is one of the widely used algorithms in Data Mining and Machine Learning domains due to the simplicity, efficiency and scalability involved. The algorithm allocates N data-points or samples to k-clusters employing the minimum distances from respective cluster centroids. Distance calculation is intrinsically a computationally intensive task which is usually accelerated by using specific hardware platforms like Field Programmable Gate Arrays (FPGAs) and Graphic Processing Unit (GPUs) etc. Hardware implementations absolve k-means from these exhaustive computations by using the inherent parallelism of the k-means clustering technique. In this paper we propose a Multi-Processor based sequentially unfolded architecture for k-means clustering using N-tiles in a collaborative working environment. The tiles work independently in parallel and largely interchange data at the end of iteration. In proposed framework the exchange of data between titles is carried out using a Network-on-Chip (NoC) inter-connect to elevate communication bottleneck caused by concurrent working of tiles. The modularity of the proposed model permits scalability with respect to the number of working tiles. The performance evaluation of proposed architecture is done using Speed, Area and average Throughput.","","Electronic:978-1-5090-4300-2; POD:978-1-5090-4301-9","10.1109/INMIC.2016.7840112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840112","Multiprocessor;NoC;k-means;parallel","Clustering algorithms;Computer architecture;Field programmable gate arrays;Hardware;Machine learning algorithms;Parallel processing;Throughput","data mining;learning (artificial intelligence);multiprocessing systems;network-on-chip;pattern clustering","FPGA;GPU;N data-points;N-tiles;NoC;collaborative working environment;data mining;distance calculation;field programmable gate arrays;graphic processing unit;hardware platforms;k-clusters;k-means clustering algorithm;machine learning;multiprocessor architecture;network-on-chip;respective cluster centroids","","","","","","","5-6 Dec. 2016","","IEEE","IEEE Conference Publications"
"Asynchrony begets momentum, with an application to deep learning","I. Mitliagkas; C. Zhang; S. Hadjis; C. Ré","Dept. of Computer Science, Stanford University, United States","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","997","1004","Asynchronous methods are widely used in deep learning, but have limited theoretical justification when applied to non-convex problems. We show that running stochastic gradient descent (SGD) in an asynchronous manner can be viewed as adding a momentum-like term to the SGD iteration. Our result does not assume convexity of the objective function, so it is applicable to deep learning systems. We observe that a standard queuing model of asynchrony results in a form of momentum that is commonly used by deep learning practitioners. This forges a link between queuing theory and asynchrony in deep learning systems, which could be useful for systems builders. For convolutional neural networks, we experimentally validate that the degree of asynchrony directly correlates with the momentum, confirming our main result. An important implication is that tuning the momentum parameter is important when considering different levels of asynchrony. Furthermore, our theory suggests ways of counter-acting the adverse effects of asynchrony. We see that a simple mechanism like using negative algorithmic momentum can be beneficial under high asynchrony. Since asynchronous methods have better hardware efficiency, this result may shed light on when asynchronous execution is more efficient for deep learning systems.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852343","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852343","","Computer science;Electronic mail;Hardware;Machine learning;Optimization;Training;Tuning","feedforward neural nets;gradient methods;learning (artificial intelligence);queueing theory","SGD iteration;asynchronous methods;convolutional neural networks;deep learning;deep learning systems;momentum-like term;negative algorithmic momentum;nonconvex problems;queuing theory;standard queuing model;stochastic gradient descent","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Identifying trolls and determining terror awareness level in social networks using a scalable framework","B. Mutlu; M. Mutlu; K. Oztoprak; E. Dogdu","Department of Computer Engineering, KTO Karatay University, Konya, Turkey 42020","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1792","1798","Trolls in social media are `malicious' users trying to propagate an opinion or distort the general perceptions. Identifying trolls in social media is a task of interest for many big data applications since data cannot be analyzed effectively without eliminating such users from the crowd. In this paper, we present a solution for troll detection and also the results of measuring terror awareness among social media users. We used Twitter platform only, and applied several machine learning techniques and big data methodologies. For machine learning we used k-Nearest Neighbour (kNN), Naive Bayes, and C4.5 decision tree algorithms. Hadoop/Mahout and Hadoop/Hive platforms were used for big data processing. Our tests show that C4.5 has a better performance on troll detection.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840796","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840796","C4.5;Naive Bayes;Troll detection;kNN;terrorism awareness","Big data;Classification algorithms;Machine learning algorithms;Terrorism;Training;Twitter","Big Data;decision trees;learning (artificial intelligence);parallel processing;pattern classification;sentiment analysis;social networking (online);social sciences computing;terrorism","Big Data;C4.5 decision tree algorithm;Hadoop/Hive platform;Hadoop/Mahout platform;Twitter platform;k-nearest neighbour algorithm;kNN algorithm;machine learning;naive Bayes algorithm;opinion propagation;social network;terror awareness level determination;troll identification","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based Ensembles for Time Series Classification","J. Lines; S. Taylor; A. Bagnall","Univ. of East Anglia, Norwich, UK","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1041","1046","There have been many new algorithms proposed over the last five years for solving time series classification (TSC) problems. A recent experimental comparison of the leading TSC algorithms has demonstrated that one approach is significantly more accurate than all others over 85 datasets. That approach, the Flat Collective of Transformation-based Ensembles (Flat-COTE), achieves superior accuracy through combining predictions of 35 individual classifiers built on four representations of the data into a flat hierarchy. Outside of TSC, deep learning approaches such as convolutional neural networks (CNN) have seen a recent surge in popularity and are now state of the art in many fields. An obvious question is whether CNNs could be equally transformative in the field of TSC. To test this, we implement a common CNN structure and compare performance to Flat-COTE and a recently proposed time series-specific CNN implementation. We find that Flat-COTE is significantly more accurate than both deep learning approaches on 85 datasets. These results are impressive, but Flat-COTE is not without deficiencies. We improve the collective by adding new components and proposing a modular hierarchical structure with a probabilistic voting scheme that allows us to encapsulate the classifiers built on each transformation. We add two new modules representing dictionary and interval-based classifiers, and significantly improve upon the existing frequency domain classifiers with a novel spectral ensemble. The resulting classifier, the Hierarchical Vote Collective of Transformation-based Ensembles (HIVE-COTE) is significantly more accurate than Flat-COTE and represents a new state of the art for TSC. HIVE-COTE captures more sources of possible discriminatory features in time series and has a more modular, intuitive structure.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0133","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837946","deep learning;ensemble classifiers;time series;time series classification","Classification algorithms;Dictionaries;Machine learning;Machine learning algorithms;Prediction algorithms;Time series analysis;Training","neural nets;pattern classification;time series","CNN structure;HIVE-COTE;TSC algorithms;convolutional neural networks;data representations;dictionary classifier;frequency domain classifier;hierarchical vote collective of transformation-based ensembles;interval-based classifier;modular hierarchical structure;probabilistic voting;time series classification","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Exploring Unsupervised Features in Conditional Random Fields for Spanish Named Entity Recognition","J. Copara; J. Ochoa; C. Thorne; G. Glavaš","Res. Center in Comput. Sci., Univ. Catolica San Pablo, Arequipa, Peru","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","283","288","Unsupervised features such as word representations mostly given by word embeddings have been shown significantly improve semi supervised Named Entity Recognition (NER) for English language. In this work we investigate whether unsupervised features can boost (semi) supervised NER in Spanish. To do so, we use word representations and collocations as additional features in a linear chain Conditional Random Field (CRF) classifier. Experimental results (82.44% F-score on the CoNLL-2002 corpus and 65.72% F-score on Ancora Corpus) show that our approach is comparable to some state-of-art Deep Learning approaches for Spanish, in particular when using cross-lingual Word Representations.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.059","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839600","Conditional Random Fields;NER for Spanish;Unsupervised features;Word Representations;Word embeddings","Computational modeling;Graphical models;Machine learning;Mutual information;Probabilistic logic;Prototypes;Training","natural language processing;pattern classification;random processes;unsupervised learning","English language;Spanish named entity recognition;cross-lingual word representations;linear chain conditional random field classifier;semisupervised named entity recognition;unsupervised features;word collocations;word embeddings","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Scalable Hybrid Ensemble model for text classification","B. Singh; N. Kushwaha; O. P. Vyas","Indian Institute of Information Technology Allahabad, India 211012","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","3148","3152","Text classification is a major problem and an evolving area of wide research with many algorithms had been already proposed. With the recent advancements in the field of Ensemble Learning there are many new techniques consistently emerging that need to be implemented for text classification in search of a better classifier. In this paper, a machine learning model BagBoo, which is a combination of Bag + Boo, where Bag and Boo are representing the Bagging and Boosting respectively. BagBoo model in essence gains its performance from using Bagged ensemble of Boosted trees. In this paper, it is a specific model we use to build our own text classifier using the already existing or some self-modified Bagging and Boosting techniques. We run evaluations on Reuters 21578 data sets 10 most frequent classes, SMS Spam Collection [1] data set, DBWorld e-mails Data Set and shown through results whether our implementation of BagBoo gives a better performance for text based classification. Results show that the proposed method can achieve an accuracy of 78.94% and 94.21% with SVM and J48 classifier on DBword datasets.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848630","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848630","Algorithim;Bagging;Boosting;Classification;Ensemble Learning","Bagging;Boosting;Data models;Machine learning algorithms;Prediction algorithms;Support vector machines;Vegetation","learning (artificial intelligence);pattern classification;support vector machines;text analysis;trees (mathematics)","BagBoo model;DBWorld e-mails Data Set;J48 classifier;Reuters data sets;SMS Spam Collection data set;SVM classifier;boosted tree bagged ensemble;ensemble learning;machine learning model;scalable hybrid ensemble model;self-modified bagging techniques;self-modified boosting techniques;text classification;text classifier","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Convolutional Neural Network for Automatic Characterization of Plaque Composition in Carotid Ultrasound","K. Lekadir; A. Galimzianova; À. Betriu; M. del Mar Vila; L. Igual; D. L. Rubin; E. Fernández; P. Radeva; S. Napel","Department of Radiology, Stanford University School of Medicine, Stanford, CA, USA","IEEE Journal of Biomedical and Health Informatics","20170201","2017","21","1","48","55","Characterization of carotid plaque composition, more specifically the amount of lipid core, fibrous tissue, and calcified tissue, is an important task for the identification of plaques that are prone to rupture, and thus for early risk estimation of cardiovascular and cerebrovascular events. Due to its low costs and wide availability, carotid ultrasound has the potential to become the modality of choice for plaque characterization in clinical practice. However, its significant image noise, coupled with the small size of the plaques and their complex appearance, makes it difficult for automated techniques to discriminate between the different plaque constituents. In this paper, we propose to address this challenging problem by exploiting the unique capabilities of the emerging deep learning framework. More specifically, and unlike existing works which require a priori definition of specific imaging features or thresholding values, we propose to build a convolutional neural network (CNN) that will automatically extract from the images the information that is optimal for the identification of the different plaque constituents. We used approximately 90 000 patches extracted from a database of images and corresponding expert plaque characterizations to train and to validate the proposed CNN. The results of cross-validation experiments show a correlation of about 0.90 with the clinical assessment for the estimation of lipid core, fibrous cap, and calcified tissue areas, indicating the potential of deep learning for the challenging task of automatic characterization of plaque composition in carotid ultrasound.","2168-2194;21682194","","10.1109/JBHI.2016.2631401","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752798","Atherosclerosis;carotid artery;convolutional neural networks (CNNs);plaque composition;ultrasound","Atherosclerosis;Feature extraction;Imaging;Lipidomics;Machine learning;Neural networks;Ultrasonic imaging","biomedical ultrasonics;blood vessels;cardiovascular system;feature extraction;image segmentation;learning (artificial intelligence);medical image processing;neural nets","calcified tissue;cardiovascular events;carotid plaque composition;carotid ultrasound;cerebrovascular events;convolutional neural network;deep learning framework;fibrous cap;fibrous tissue;image noise;imaging features;lipid core;plaque constituents;thresholding values","","","","","","20161122","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"A Deep Learning Model for Robust Wafer Fault Monitoring With Sensor Measurement Noise","H. Lee; Y. Kim; C. O. Kim","Department of Information and Industrial Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Semiconductor Manufacturing","20170201","2017","30","1","23","31","Standard fault detection and classification (FDC) models detect wafer faults by extracting features useful for fault detection from time-indexed measurements of the equipment recorded by in situ sensors (sensor signals) and feeding the extracted information into a classifier. However, the preprocessing-and-classification approach often results in the loss of information in the sensor signals that is important for detecting wafer faults. Furthermore, the sensor signals usually contain noise induced by mechanical and electrical disturbances. In this paper, we propose the use of a stacked denoising autoencoder (SdA), which is a deep learning algorithm, to establish an FDC model for simultaneous feature extraction and classification. The SdA model can identify global and invariant features in the sensor signals for fault monitoring and is robust against measurement noise. Through experiments using wafer samples collected from a work-site photolithography tool, we confirmed that as the sensor measurement noise severity increased, the SdA's classification accuracy could be as much as 14% higher than those of the twelve models considered for comparison, each of which employed one of three feature extractors and one of four classifiers.","0894-6507;08946507","","10.1109/TSM.2016.2628865","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744687","Semiconductor manufacturing;deep learning;fault detection and classification;sensor measurement noise;stacked denoising autoencoder","Biological neural networks;Feature extraction;Machine learning;Noise reduction;Semiconductor device measurement;Semiconductor device modeling","fault diagnosis;feature extraction;semiconductor device manufacture;semiconductor device measurement;semiconductor device noise;sensors","deep learning model;feature classification;feature extraction;robust wafer fault monitoring;sensor measurement noise;stacked denoising autoencoder;standard fault detection and classification models;time-indexed measurements","","","","","","20161115","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Body joints regression using deep convolutional neural networks","A. Abobakr; M. Hossny; S. Nahavandi","Institute for Intelligent Systems Research and Innovation (IISRI), Deakin University, Australia","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","003281","003287","Human pose estimation is a well-known computer vision problem that receives intensive research interest. The reason for such interest is the wide range of applications that the successful estimation of human pose offers. Articulated pose estimation includes real time acquisition, analysis, processing and understanding of high dimensional visual information. Ensemble learning methods operating on hand-engineered features have been commonly used for addressing this task. Deep learning exploits representation learning methods to learn multiple levels of representations from raw input data, alleviating the need to hand-crafted features. Deep convolutional neural networks are achieving the state-of-the-art in visual object recognition, localization, detection. In this paper, the pose estimation task is formulated as an offset joint regression problem. The 3D joints positions are accurately detected from a single raw depth image using a deep convolutional neural networks model. The presented method relies on the utilization of the state-of-the-art data generation pipeline to generate large, realistic, and highly varied synthetic set of training images. Analysis and experimental results demonstrate the generalization performance and the real time successful application of the proposed method.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844740","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844740","ConvNet and pose estimation;Kinect;RGB-D sensors;deep learning","Cameras;Joints;Machine learning;Pipelines;Pose estimation","computer vision;feedforward neural nets;image sensors;learning (artificial intelligence);object detection;object recognition;pose estimation;regression analysis","3D joint position detection;articulated pose estimation;body joint regression;computer vision problem;data generation pipeline;deep convolutional neural networks;deep convolutional neural networks model;generalization performance;high-dimensional visual information acquisition;human pose estimation;offset joint regression problem;raw depth image;synthetic set;visual information analysis;visual information processing;visual object detection;visual object localization;visual object recognition","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"From Training to Match Performance: A Predictive and Explanatory Study on Novel Tracking Data","J. Fernández; D. Medina; A. Gómez; M. Arias; R. Gavaldá","Sports Sci. & Health Dept., Futbol Club Barcelona, Barcelona, Spain","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","136","143","The recent FIFA approval of the use of Electronic Performance and Tracking Systems (EPTS) during competition, has provided the availability of novel data regarding physical player performance. The analysis of this kind of information will provide teams with competitive advantages, by gaining a deeper understanding of the relation between training and match load, and individual player's fitness characteristics. In order to make sense of this physical data, which is inherently complex, machine learning algorithms that exploit both non-linear and linear relations among variables could be of great aid on building predictive and explanatory models. Also, the increasing availability of information brings the necessity and the challenge for successful interpretation of these models in order to be able to translate the findings into information that can be quickly applied by fast-paced practitioners, such as physical coaches. For season 2015-2016 F. C. Barcelona has collected both physical information from both training sessions and matches using EPTS devices. This study focuses primarily on evaluating up to what extent is possible to predict match performance from training and match physical information. Different machine learning algorithms are applied for building predictive regression models, in combination with feature selection techniques and Principal Component Analysis (PCA) for dimensionality reduction. Physical Variables are segmented into three groups: Locomotor, Metabolic and Mechanical variables, reaching successful prediction rates in 11 out of 17 total variables, based on a threshold determined by expert physical coaches. A normalized root mean square error metric is proposed that allows better understanding of results for practitioners. The second part of this study is focused on understanding the predictor variables that better explain each of the 17 analyzed match variables. It was found that specific variables can act as representatives of the set of hi- hly correlated ones, so reducing greatly the amount of variables needed in the periodical physical analysis carried out by coaches, passing from 17 to 4 variables in average.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0027","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836658","epts;feature selection;football analytics;gps;regression;sports analytics","Accelerometers;Correlation;Global Positioning System;Machine learning algorithms;Performance evaluation;Training","feature selection;learning (artificial intelligence);principal component analysis;sport","dimensionality reduction;electronic performance and tracking systems;feature selection techniques;machine learning algorithms;normalized root mean square error metric;novel tracking data;periodical physical analysis;principal component analysis","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Evaluation of distributed processing of caffe framework using poor performance device","A. Ichinose; M. Oguchi; A. Takefusa; H. Nakada","Ochanomizu University","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3980","3982","The spread of various sensors and Cloud technologies has made it easy to acquire life-logs and accumulate data. As a result, many life-log analysis applications, which transfer data from sensors, especially cameras to a Cloud and analyze them in the Cloud, have been developed. Cameras with a server function called network cameras have become cheap and readily available for security services and the monitoring of pets and children from remote locations. In these services, raw data from sensors, including cameras, are generally transferred to a Cloud and processed there. However, it is difficult to transfer raw data from sensors to a Cloud because of the limitation of network bandwidth between sensors and a Cloud and privacy issues caused by sending raw sensor data to a Cloud.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841082","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841082","","Bandwidth;Cameras;Convolution;Data privacy;Distributed processing;Machine learning;Sensors","cloud computing;data privacy;learning (artificial intelligence)","caffe framework;cloud technologies;deep learning;distributed processing;life-log analysis applications;privacy issues","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep belief networks and stacked autoencoders for the P300 Guilty Knowledge Test","J. P. Kulasingham; V. Vibujithan; A. C. De Silva","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Moratuwa 10400, Sri Lanka","2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)","20170206","2016","","","127","132","The P300 wave is an Event Related Potential that is elicited in the brain when a subject is presented with a familiar stimulus. The Guilty Knowledge Test is used to determine if certain information is stored in the brain by detecting the P300 wave. In this paper, an improved Guilty Knowledge Test has been conducted on 14 subjects. Although a variety of feature extraction and classification algorithms for detecting the P300 have been used thus far, this paper introduces the use of deep learning techniques for the Guilty Knowledge Test. Two deep learning techniques; Deep Belief Networks and Stacked Autoencoders, have been compared. The input to these classifiers was the filtered EEG signal without any feature extraction. A mean accuracy of 86.9% was achieved for the Deep Belief Network and 86.01% for the Stacked Autoencoder on our dataset, outperforming the conventional Support Vector Machine.","","Electronic:978-1-4673-7791-1; POD:978-1-4673-7792-8","10.1109/IECBES.2016.7843428","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843428","Deep Belief Networks;EEG;Guilty Knowledge Test;P300;Stacked Autoencoders","Conferences;Electroencephalography;Feature extraction;Machine learning;Probes;Support vector machines;Training","belief networks;electroencephalography;filters;learning (artificial intelligence);medical signal processing;pattern classification;support vector machines","EEG signal filtering;brain;classification algorithm;deep belief network;deep learning technique;event related potential;feature extraction;guilty knowledge test;stacked autoencoder;support vector machine","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"An approach to machine classification based on stacked generalization and instance selection","I. Czarnowski; P. Jędrzejowicz","Department of Information Systems, Gdynia Maritime University, Morska 83, 81-225, Poland","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","004836","004841","This paper focuses on the machine classification with data reduction. The aim of the data reduction techniques is decreasing the quantity of information required to learn a high quality classifiers. In this paper the data reduction is carried out by selection of relevant instances, called prototypes. To solve the machine classification problem with data reduction an agent-based population learning algorithm is proposed. The discussed approach bases on the assumption that the selection of prototypes is carried-out by a team of agents and that the prototype instances are selected from clusters of instances. The proposed procedure is called the stack generalization. It aims at improving the quality of the learning process and increasing the generalization capacity of the learning model. The paper includes the description of the approach and the discussion of the validating experiment results.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844994","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844994","data reduction;learning from data;multi-agent population learning algorithm;stacked generalization","Classification algorithms;Machine learning algorithms;Prototypes;Sociology;Stacking;Statistics;Training","data reduction;generalisation (artificial intelligence);learning (artificial intelligence);multi-agent systems;pattern classification","agent-based population learning algorithm;data reduction;high-quality classifier learning;instance selection;machine classification problem;stacked generalization capacity","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Deep learning in the automotive industry: Applications and tools","A. Luckow; M. Cook; N. Ashcraft; E. Weill; E. Djerekarov; B. Vorster","BMW Group, IT Research Center, Information Management Americas, Greenville, SC 29607, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3759","3768","Deep Learning refers to a set of machine learning techniques that utilize neural networks with many hidden layers for tasks, such as image classification, speech recognition, language understanding. Deep learning has been proven to be very effective in these domains and is pervasively used by many Internet services. In this paper, we describe different automotive uses cases for deep learning in particular in the domain of computer vision. We surveys the current state-of-the-art in libraries, tools and infrastructures (e. g. GPUs and clouds) for implementing, training and deploying deep neural networks. We particularly focus on convolutional neural networks and computer vision use cases, such as the visual inspection process in manufacturing plants and the analysis of social media data. To train neural networks, curated and labeled datasets are essential. In particular, both the availability and scope of such datasets is typically very limited. A main contribution of this paper is the creation of an automotive dataset, that allows us to learn and automatically recognize different vehicle properties. We describe an end-to-end deep learning application utilizing a mobile app for data collection and process support, and an Amazon-based cloud backend for storage and training. For training we evaluate the use of cloud and on-premises infrastructures (including multiple GPUs) in conjunction with different neural network architectures and frameworks. We assess both the training times as well as the accuracy of the classifier. Finally, we demonstrate the effectiveness of the trained classifier in a real world setting during manufacturing process.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841045","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841045","Automotive;Cloud Computing;Deep Learning;Manufacturing","Automotive engineering;Computational modeling;Libraries;Machine learning;Neural networks;Sparks;Training","automotive engineering;cloud computing;computer vision;data analysis;feedforward neural nets;graphics processing units;image classification;industrial plants;inspection;learning (artificial intelligence);manufacturing processes;mobile computing;production engineering computing","Amazon-based cloud backend;automotive dataset;automotive industry;cloud computing;computer vision;convolutional neural networks;curated datasets;data collection;end-to-end deep learning application;image classification;inspection process;labeled datasets;language understanding;machine learning techniques;manufacturing plants;manufacturing process;mobile app;multiple GPU;social media data analysis;speech recognition","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Lucene and deep learning based commodity information analysis system","J. Cao; J. Lin; S. Wu; M. Guan; Q. Dai; W. Feng","School of Information Engineering, Guangdong University of Technology, Guangzhou, Guangdong Province, China","2016 IEEE International Conference on Consumer Electronics-China (ICCE-China)","20170213","2016","","","1","4","E-commerce is profoundly changing the economic growth and people's lifestyles, but it also become a zone of fake, shoddy and other violations of intelligent property. In view of this phenomenon, we developed an information analysis system for electronic commodities. The system adopts the web crawler technology to capture the information of the commodities, which is used to form a resource library with the patent information. Based on the resource library, the Lucene engine and deep learning are used to analyze the commodity information from text and image respectively. Experiments show that the information analyzed from the system has a good reference value to a comprehensive understanding of commodity information.","","Electronic:978-1-5090-6193-8; POD:978-1-5090-6194-5","10.1109/ICCE-China.2016.7849764","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849764","Crawler technology;Deep learning;E-commerce;Lucene engine","Crawlers;Feature extraction;Indexes;Machine learning;Patents;Rendering (computer graphics)","electronic commerce;information analysis;learning (artificial intelligence);search engines","Lucene engine;Web crawler technology;commodity information analysis system;deep learning;e-commerce;electronic commodities;intelligent property;patent information;resource library;violations","","","","","","","19-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"Traffic Speed Prediction and Congestion Source Exploration: A Deep Learning Method","J. Wang; Q. Gu; J. Wu; G. Liu; Z. Xiong","Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","499","508","Traffic speed prediction is a long-standing and critically important topic in the area of Intelligent Transportation Systems (ITS). Recent years have witnessed the encouraging potentials of deep neural networks for real-life applications of various domains. Traffic speed prediction, however, is still in its initial stage without making full use of spatio-temporal traffic information. In light of this, in this paper, we propose a deep learning method with an Error-feedback Recurrent Convolutional Neural Network structure (eRCNN) for continuous traffic speed prediction. By integrating the spatio-temporal traffic speeds of contiguous road segments as an input matrix, eRCNN explicitly leverages the implicit correlations among nearby segments to improve the predictive accuracy. By further introducing separate error feedback neurons to the recurrent layer, eRCNN learns from prediction errors so as to meet predictive challenges rising from abrupt traffic events such as morning peaks and traffic accidents. Extensive experiments on real-life speed data of taxis running on the 2nd and 3rd ring roads of Beijing city demonstrate the strong predictive power of eRCNN in comparison to some state-of-the-art competitors. The necessity of weight pre-training using a transfer learning notion has also been testified. More interestingly, we design a novel influence function based on the deep learning model, and showcase how to leverage it to recognize the congestion sources of the ring roads in Beijing.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0061","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837874","Convolutional neural network;Deep learning;Intelligent transportation systems;Spatio-temporal;Time series prediction","Biological neural networks;Convolution;Feature extraction;Machine learning;Neurons;Roads;Training","learning (artificial intelligence);recurrent neural nets;road traffic;traffic engineering computing","Beijing City;ITS;congestion source exploration;deep learning method;eRCNN;error-feedback recurrent convolutional neural network structure;intelligent transportation systems;spatio-temporal traffic information;traffic speed prediction;transfer learning","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Traffic Matrix Prediction and Estimation Based on Deep Learning for Data Center Networks","L. Nie; D. Jiang; L. Guo; S. Yu; H. Song","Sch. of Comput. Sci. & Eng., Northeastern Univ., Shenyang, China","2016 IEEE Globecom Workshops (GC Wkshps)","20170209","2016","","","1","6","Network traffic analysis is a crucial technique for systematically operating a data center network. Many network management functions rely on exact network traffic information. Although a great number of works to obtain network traffic have been carried out in traditional ISP networks, they cannot be employed effectively in data center networks. Motivated by that, we focus on the problem of network traffic prediction and estimation in data center networks. We involve deep learning techniques in the network traffic prediction and estimation fields, and propose two deep architectures for network traffic prediction and estimation, respectively. We first use a deep architecture to explore the time-varying property of network traffic in a data center network, and then propose a novel network traffic prediction approach based on a deep belief network and a logistic regression model. Meanwhile, to deal with the highly ill-pose property of network traffic estimation, we further propose a network traffic estimation method using the deep belief network trained by link counts. We validate the effectiveness of our methodologies by real traffic data.","","Electronic:978-1-5090-2482-7; POD:978-1-5090-2483-4","10.1109/GLOCOMW.2016.7849067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849067","","Computer architecture;Estimation;Logistics;Machine learning;Predictive models;Probability distribution;Routing","belief networks;computer centres;learning (artificial intelligence);regression analysis;telecommunication network management;telecommunication traffic","data center networks;deep belief network;deep learning techniques;logistic regression model;network management functions;network traffic estimation method;novel network traffic prediction approach;time-varying property","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Traffic flow prediction with Long Short-Term Memory Networks (LSTMs)","H. Shao; B. H. Soong","Interdisciplinary Graduate School, Nanyang Technological University, Singapore","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2986","2989","Accurate traffic flow information is crucial for an intelligent transportation system management and deployment. Over the past few years, many existing models have been designed for short-term traffic flow prediction. However, they fail to provide favorable results due to their shallow architectures or incapability to extract the sequence correlations in the data. In this paper, we explore the application of Long Short-Term Memory Networks (LSTMs) in short-term traffic flow prediction. As a deep learning approach, LSTMs are able to learn more abstract representations in the non-linear traffic flow data. The intrinsic feature of capturing long-term dependencies in a sequential data also makes it a suitable choice in traffic prediction. Experiments on real traffic data sets indicate a good performance of our model. The LSTMs architecture is also compared with state-of-the-art models and experiments show that our model achieves desirable results by lowering the MAPE metrics to 5.4%.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848593","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848593","","Computer architecture;Forecasting;Machine learning;Measurement;Neurons;Predictive models;Roads","intelligent transportation systems;learning (artificial intelligence);neural net architecture;road traffic control","LSTM architecture;MAPE metrics;deep learning approach;intelligent transportation system deployment;intelligent transportation system management;intrinsic feature;long-short-term memory networks;long-term dependencies;nonlinear traffic flow data;sequential data;short-term traffic flow prediction;traffic flow information;traffic flow prediction;traffic prediction","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Customer shopping pattern prediction: A recurrent neural network approach","H. Salehinejad; S. Rahnamayan","Department of Electrical, Computer, and Software Engineering University of Ontario Institute of Technology, Oshawa, Canada","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","6","Customer relationship management is a popular and strategic topic in marketing and quality of service. The availability of big transactions data as well as computing systems have provided a great opportunity to model and predict customer behaviour. However, there is a lack of modern modelling and analytical methods to perform analysis on such data. Deep learning techniques can assist marketing decision makers to provide more reliable and practical marketing strategic plans. In this paper, we propose a customer behaviour prediction model using recurrent neural networks (RNNs) based on the client loyalty number (CLN), recency, frequency, and monetary (RFM) variables. The experiment results show that RNNs can predict RFM values of customers efficiently. This model can be later used in recommender systems for exclusive promotional offers and loyalty programs management.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849921","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849921","Customer Behaviour Prediction;Deep Learning;Recency Frequency Monetary (RFM);Recommender System;Recurrent Neural Networks;Shopping Pattern","Computational modeling;Data models;Logic gates;Machine learning;Predictive models;Recurrent neural networks;Training","consumer behaviour;marketing data processing;recurrent neural nets","RFM values;RNN;client loyalty number;customer behaviour prediction model;customer shopping pattern prediction;recency-frequency-and-monetary variables;recurrent neural networks","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Materials discovery: Understanding polycrystals from large-scale electron patterns","R. Liu; A. Agrawal; W. k. Liao; A. Choudhary; M. De Graef","Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2261","2269","This paper explores the idea of modeling a large image data collection of polycrystal electron patterns, in order to detect insights in understanding materials discovery. There is an emerging interest in applying big data processing, management and modeling methods to scientific images, which often come in a form and with patterns only interpretable to domain experts. While large-scale machine learning approaches have demonstrated certain superiority in analyzing, summarizing, and providing an understandable route to data types like natural images, speeches and texts, scientific images is still a relatively unexplored area. Deep convolutional neural networks, despite their recent triumph in natural image understanding, are still rarely seen adapted to experimental microscopic images, especially in a large scale. To the best of our knowledge, we present the first deep learning solution towards a scientific image indexing problem using a collection of over 300K microscopic images. The result obtained is 54% better than a dictionary lookup method which is state-of-the-art in the materials science society.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840857","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840857","Deep learning;EBSD;convolutional neural networks;electronic images;materials design;materials discovery","Biological neural networks;Computational modeling;Electron microscopy;Indexing;Machine learning","Big Data;convolution;crystals;image processing;learning (artificial intelligence);materials science computing;neural nets","big data processing;deep convolutional neural networks;experimental microscopic images;image data collection;large-scale machine learning;large-scale polycrystal electron patterns;materials discovery;materials science society;scientific image indexing problem;scientific images","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Deep Learning Approach to Android Malware Feature Learning and Detection","X. Su; D. Zhang; W. Li; K. Zhao","Hunan Provincial Key Lab. of Network Investigational Technol., Hunan Police Acad., Changsha, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","244","251","The growing amount and diversity of Android malware has significantly weakened the effectiveness of the conventional defense mechanisms, and thus Android platform often remains unprotected from new and unknown malware. To address these limitations, we propose DroidDeep, a malware detection approach for the Android platform based on the deep learning model. Deep learning emerges as a new area of machine learning research that has attracted increasing attention in artificial intelligence. To implement this, we first extract five types of features from the static analysis of Android apps. Then, we build the deep learning model to learn features from Android apps. Finally, the learned features are used to detect unknown Android malware. In an experiment with 3,986 benign apps and 3,986 malware, DroidDeep outperforms several existing malware detection approaches and achieves a 99.4% detection accuracy. Moreover, DroidDeep can achieve a remarkable run-time efficiency which makes it very easy to adapt to a lager scale of real-world Android malware detection.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0070","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846953","Android malware;Deep learning;Security;Static analysis","Androids;Feature extraction;Humanoid robots;Machine learning;Malware;Mobile communication;Smart phones","invasive software;learning (artificial intelligence);mobile computing;program diagnostics","Android apps;Android malware detection;Android malware feature learning;DroidDeep;deep learning model;machine learning;static analysis","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Improving the Prediction Cost of Drift Handling Algorithms by Abstaining","P. X. Loeffel; V. Lemaire; C. Marsala; M. Detyniecki","IBS, Warsaw, Poland","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","1213","1222","The problem considered in this paper is regression with a constraint on the precision of each prediction in the framework of data streams subject to concept drifts (when the hidden distribution which generates the observations can change over time). Concept drifts can diminish the reliability of the predictions over time and it might not be possible to output a prediction which satisfies the constraints on the precision. In this case, we claim that if the costs associated with a good and with a bad prediction are known beforehand, the overall prediction cost can be improved by allowing the regressor to abstain. To this end, we propose a generic method, compatible with any regressor, which uses an ensemble of reliability estimators to estimate whether the constraints on the precision of a given prediction can be met or not. In the later case, the regressor is allowed to abstain. Empirical results on 30 datasets including different types of drifts back our claim.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836806","Concept Drift;Data Stream;Prediction with a Reject Option","Conferences;Cost function;Data mining;Electron tubes;Machine learning algorithms;Prediction algorithms;Reliability","data analysis;learning (artificial intelligence);regression analysis;reliability","abstaining;beforehand prediction;concept drifts;data streams;drift handling algorithms;generic method;overall prediction cost;regression problem;reliability estimators","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Empirical evaluations of preprocessing parameters' impact on predictive coding's effectiveness","R. Chhatwal; N. Huber-Fliflet; R. Keeling; J. Zhang; H. Zhao","Legal AT&T Services, Inc., Washington, D.C. USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1394","1401","Predictive coding, once used in only a small fraction of legal and business matters, is now widely deployed to quickly cull through increasingly vast amounts of data and reduce the need for costly and inefficient human document review. Previously, the sole front-end input used to create a predictive model was the exemplar documents (training data) chosen by subject-matter experts. Many predictive coding tools require users to rely on static preprocessing parameters and a single machine learning algorithm to develop the predictive model. Little research has been published discussing the impact preprocessing parameters and learning algorithms have on the effectiveness of the technology. A deeper dive into the generation of a predictive model shows that the settings and algorithm can have a strong effect on the accuracy and efficacy of a predictive coding tool. Understanding how these input parameters affect the output will empower legal teams with the information they need to implement predictive coding as efficiently and effectively as possible. This paper outlines different preprocessing parameters and algorithms as applied to multiple real-world data sets to understand the influence of various approaches.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840747","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840747","e-discovery;ediscovery;electronic discovery;predictive coding;technology assisted review","Electronic mail;Law;Machine learning algorithms;Predictive coding;Predictive models;Training","business data processing;information management;learning (artificial intelligence)","business matters;empirical evaluations;exemplar documents;human document review;information management;legal matters;predictive coding effectiveness;predictive coding tools;preprocessing parameters;single machine learning algorithm;static preprocessing parameters;subject matter experts","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Forecasting of Multivariate Time Series via Complex Fuzzy Logic","O. Yazdanbakhsh; S. Dick","Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB T6G 1H9, Canada.","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","PP","99","1","12","Multivariate time series consist of sequential vector-valued observations of some phenomenon over time. Time series forecasting (for both univariate and multivariate case) is a well-known, high-value machine learning problem, in which the goal is to predict future observations of the time series based on prior ones. Several learning algorithms based on complex fuzzy logic have recently been shown to be very accurate and compact forecasting models. However, these models have only been tested on univariate and bivariate datasets. There has as yet been no investigation of more general multivariate datasets. We report on the extension of the adaptive neuro-complex-fuzzy inferential system learning architecture to the multivariate case. We investigate single-input-single-output, multiple-input-single-output, and multiple-input-multiple-output variations of the architecture, exploring their performance on four multivariate time series. We also explore modifications to the forward- and backward-pass computations in the architecture. We find that our best designs are superior to the published results on these datasets, and at least as accurate as kernel-based prediction algorithms.","2168-2216;21682216","","10.1109/TSMC.2016.2630668","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849183","Complex fuzzy logic;complex fuzzy sets (CFSs);neuro-fuzzy systems;time series forecasting","Computer architecture;Forecasting;Fuzzy logic;Fuzzy sets;MIMO;Machine learning algorithms;Time series analysis","","","","","","","","20170209","","","IEEE","IEEE Early Access Articles"
"Online Eye state recognition from EEG data using Deep architectures","T. K. Reddy; L. Behera","Department of Electrical Engineering, Indian Institute of Technology Kanpur, Uttar Pradesh, India 208016","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000712","000717","In the past decade, improvements in the production of in-expensive PC equipment and software has permitted more refined real-time signal processing in BCI systems. In the literature, Deep learning concepts have not been applied to EEG data analysis in a systematic manner. This paper applies various existing Deep learning architectures and algorithms for the classification of EEG data applied to eye state detection. The deep learning based classifier systems presented in this work are comparable to the state of the art classifiers devised by Roesler and Suenderman (2013), and Cameron et al. (2015). The goal of this work is to construct a system producing accuracies comparable to Roesler's K* classifier, Cameron et al.'s (RRF+K*) classifiers and at the same time providing enough speed to be used in an online BCI framework. In order to meet the constraints, following architectures were designed: A Multi layered neural network with ReLU and drop-out, deep belief networks based unsupervised learning, drop-out masks on deep neural networks. Specifically, we compare our results with K*, RRF, (K*+RRF), ada(RJ48F) classifiers. Also an in-depth analysis of binary class features has been done using t-SNE based visualizations while fitting elliptical contours to the features. Prior research suggests that instance-based/lazy learners like the K* algorithm are likely to be too slow to be used in a BCI framework, with ada(RJ48F) model performing decently well. But our chosen deep neural network architectures produce higher classification accuracies and have lower convergence times making them even faster within the time specifications of real-time classification and control applications.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844325","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844325","Electroencephalogram (EEG);Eye-state;Restricted Boltzmann Machine (RBM);drop-out","Computer architecture;Electroencephalography;Machine learning;Real-time systems;Testing;Training;Vehicles","belief networks;electroencephalography;eye;learning (artificial intelligence);medical signal processing;neural nets;signal classification","EEG data classification;K* classifier;K*+RRF classifier;RRF classifier;ReLU;ada(RJ48F) classifier;binary class features;classifier systems;deep belief networks;deep learning;deep neural network architectures;drop-out;electroencephalogram;eye state detection;instance-based learner;lazy learner;multilayered neural network;online eye state recognition;t-SNE;unsupervised learning","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Mini-apps for high performance data analysis","S. R. Sukumar; M. A. Matheson; R. Kannan; S. H. Lim","Oak Ridge National Laboratory, 1 Bethel Valley Road, Oak Ridge, TN 37831, United States of America","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1483","1492","Scaling-up scientific data analysis and machine learning algorithms for data-driven discovery is a grand challenge that we face today. Despite the growing need for analysis from science domains that are generating `Big Data' from instruments and simulations, building high-performance analytical workflows of data-intensive algorithms have been daunting because: (i) the `Big Data' hardware and software architecture landscape is constantly evolving, (ii) newer architectures impose new programming models, and (iii) data-parallel kernels of analysis algorithms and their performance facets on different architectures are poorly understood. To address these problems, we have: (i) identified scalable data-parallel kernels of popular data analysis algorithms, (ii) implemented `Mini-Apps' of those kernels using different programming models (e.g. Map Reduce, MPI, etc.), (iii) benchmarked and validated the performance of the kernels in diverse architectures. In this paper, we discuss two of those Mini-Apps and show the execution of principal component analysis built as a workflow of the Mini-Apps. We show that Mini-Apps enable scientists to (i) write domain-specific data analysis code that scales on most HPC hardware and (ii) and offers the ability (most times with over a 10x speed-up) to analyze data sizes 100 times the size of what off-the-shelf desktop/workstations of today can handle.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840756","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840756","Big Data;analytical motifs;data analysis kernels;high performance data analytics;mini-apps","Algorithm design and analysis;Big data;Computer architecture;Data analysis;Kernel;Machine learning algorithms;Software algorithms","Big Data;data analysis;learning (artificial intelligence);parallel processing;principal component analysis;software architecture","Big data;data analysis algorithms;data analysis code;data intensive algorithms;data-driven discovery;data-parallel kernels;high performance data analysis;high-performance analytical workflows;machine learning algorithms;miniapps;new programming models;principal component analysis;software architecture landscape","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Improved multimodal sentiment detection using stressed regions of audio","H. Abburi; M. Shrivastava; S. V. Gangashetty","Language Technologies Research Center, IIIT Hyderabad, India","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2834","2837","Recent advancement of social media has led people to share the product reviews through various modalities such as audio, text and video. In this paper, an improved approach to detect the sentiment of an online spoken reviews based on its multi-modality natures (audio and text) is presented. To extract the sentiment from audio, Mel Frequency Cepstral Coefficients (MFCC) features are extracted at stressed significant regions which are detected based on the strength of excitation. Gaussian Mixture Models (GMM) classifier is employed to develop a sentiment model using these features. From results, it is observed that MFCC features extracted at stressed significance regions perform better than the features extracted from the whole audio input. Further from the transcript of the audio input, textual features are computed by Doc2vec vectors. Support Vector Machine (SVM) classifier is used to develop a sentiment model using these textual features. From experimental results it is observed that combining both the audio and text features results in improvement in the performance for detecting the sentiment of a review.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848560","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848560","Audio features;Gaussian mixture model;Lyric features;Sentiment analysis;Stressed significant regions;Support vector machine","Databases;Feature extraction;Machine learning algorithms;Mel frequency cepstral coefficient;Sentiment analysis;Speech;Support vector machines","Gaussian processes;audio signal processing;cepstral analysis;feature extraction;mixture models;sentiment analysis;signal classification;social networking (online);support vector machines","Doc2vec vectors;GMM classifier;Gaussian mixture model classifier;MFCC feature extraction;SVM classifier;audio features;improved multimodal sentiment detection;mel frequency cepstral coefficient feature extraction;multimodality natures;online spoken reviews;review sentiment detection;social media;stressed audio regions;support vector machine classifier;text features","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Sublinear Dual Coordinate Ascent for Regularized Loss Minimization","L. Liu; D. Tao","Centre for Artificial Intell., Univ. of Technol. Sydney, Ultimo, NSW, Australia","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1065","1070","We present a sublinear version of the dual coordinate ascent method for solving a group of regularized loss minimization problems in machine learning. The proposed method seamlessly integrates sampling techniques, the dual coordinate ascent method, and a multiplicative update algorithm. The sampling techniques choose the ""expected"" examples, and estimate the corresponding inner products. The dual coordinate ascent method generates an updated iterative step, which outperforms the time-learning step used in the previous sublinear perceptron algorithm. The multiplicative update algorithm updates the example weighting. The proposed method is implemented with an iterative step of order O(log(n)), where n is the size of examples, and achieves a better result than other methods, with high probability. We present a theoretical analysis of the sublinear iterative in order to justify its benefits. We then apply the proposed optimization method to support vector machine and conduct experiments on three large-scale datasets. Our experimental results validate our theoretical findings.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0137","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837950","Dual coordinate ascent;multiplicative update;sampling","Algorithm design and analysis;Australia;Cost function;Machine learning algorithms;Minimization;Runtime","computational complexity;iterative methods;learning (artificial intelligence);minimisation;probability;sampling methods;support vector machines","iterative step;machine learning;regularized loss minimization problems;sampling techniques;sublinear dual coordinate ascent;sublinear iterative;sublinear perceptron algorithm;support vector machine;time-learning step","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Photo aesthetic quality assessment via label distribution learning","Xiaowei Zhang; F. Gao; Di Huang; Min Tan; J. Yu","Key Laboratory of Complex Systems Modeling and Simulation, School of Computer Science and Technology, Hangzhou Dianzi University, China","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001467","001470","Automatic prediction of photo aesthetic quality is useful for many practical purposes. Current computational approaches typically solved this problem by assigning a categorical label (good or bad) to a photo. However, due to the subjectivity and complexity of humans aesthetic judgments, only a categorical label is insufficient to represent humans perceived aesthetic quality of a photo. This paper focuses on an interesting problem: is it possible to predict the crowed opinions about the aesthetic quality of a photo? The crowed opinion here is expressed by the distribution of scores given by a number of subjects. For each given photo, a deep convolutional neural network (DCNN) is utilized to calculate its feature representation. Afterwards, the crowed opinion prediction problem is formulated as one of label distribution learning (LDL). Experiments show that the proposed method is highly effective and outperforms state-of-the-art algorithms.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844444","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844444","convolutional neural network;deep learning;label distribution learning;label distribution support regressor;photo quality assessment","Conferences;Cybernetics;Databases;Feature extraction;Machine learning;Neural networks;Quality assessment","feedforward neural nets;image representation;learning (artificial intelligence)","DCNN;LDL;automatic photo aesthetic quality prediction;categorical label;crowed opinion prediction;deep-convolutional neural network;feature representation;label distribution learning;score distribution","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Deep Learning Approach for the Prediction of Retail Store Sales","Y. Kaneko; K. Yada","Data Sci. Lab., Kansai Univ., Suita, Japan","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","531","537","The purpose of this research is to construct a sales prediction model for retail stores using the deep learning approach, which has gained significant attention in the rapidly developing field of machine learning in recent years. Using such a model for analysis, an approach to store management could be formulated. The present study uses three years' worth of point-of-sale (POS) data from a retail store to construct a sales prediction model that, given the sales of a particular day, predicts the changes in sales on the following day. As a result, a deep learning model that considers the L1 regularization achieved a sale forecasting accuracy rate of 86%. The products at the retail store have been finely categorized. Even if the attributes of the product categories are increased in number from tens to thousands, the predictive accuracy did not fall by more than about 7%. In contrast, the accuracy decreased by around 13% when the logistic regression model was used. These results indicate that deep learning is highly suitable for constructing models that include multi-attribute variables. The present research demonstrates that deep learning is effective for analyzing the POS data of retail stores.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0082","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836713","Deep Learning;Logistic Regression;Marketing;POS data;Sales Prediction Model","Analytical models;Business;Data models;Google;Machine learning;Predictive models;Water","learning (artificial intelligence);regression analysis;retail data processing","POS data;deep learning approach;logistic regression model;machine learning;multi attribute variables;point-of-sale data;retail store sales;sale forecasting;sales prediction model;store management","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"User and entity behavior analytics for enterprise security","M. Shashanka; M. Y. Shen; J. Wang","Charles Schwab","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1867","1874","This paper presents an overview of an intelligence platform we have built to address threat hunting and incident investigation use-cases in the cyber security domain. Specifically, we focus on User and Entity Behavior Analytics (UEBA) modules that track and monitor behaviors of users, IP addresses and devices in an enterprise. Anomalous behavior is automatically detected using machine learning algorithms based on Singular Values Decomposition (SVD). Such anomalous behavior indicative of potentially malicious activity is alerted to analysts with relevant contextual information for further investigation and action. We provide a detailed description of the models, algorithms and implementation underlying the module and demonstrate the functionality with empirical examples.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840805","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840805","Anomaly Detection;Mahalanobis Distance;Singular Value Decomposition (SVD);User and Entity Behavior Analytics","Covariance matrices;Machine learning algorithms;Monitoring;Security;Servers;Standards;Training data","enterprise resource planning;learning (artificial intelligence);security of data;singular value decomposition","IP addresses;SVD;UEBA modules;cyber security;enterprise security;incident investigation use-cases;intelligence platform;machine learning;malicious activity;singular value decomposition;threat hunting;user and entity behavior analytics modules","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep Learning for solar power forecasting — An approach using AutoEncoder and LSTM Neural Networks","A. Gensler; J. Henze; B. Sick; N. Raabe","Intelligent Embedded Systems Group, University of Kassel, Germany","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","002858","002865","Power forecasting of renewable energy power plants is a very active research field, as reliable information about the future power generation allow for a safe operation of the power grid and helps to minimize the operational costs of these energy sources. Deep Learning algorithms have shown to be very powerful in forecasting tasks, such as economic time series or speech recognition. Up to now, Deep Learning algorithms have only been applied sparsely for forecasting renewable energy power plants. By using different Deep Learning and Artificial Neural Network algorithms, such as Deep Belief Networks, AutoEncoder, and LSTM, we introduce these powerful algorithms in the field of renewable energy power forecasting. In our experiments, we used combinations of these algorithms to show their forecast strength compared to a standard MLP and a physical forecasting model in the forecasting the energy output of 21 solar power plants. Our results using Deep Learning algorithms show a superior forecasting performance compared to Artificial Neural Networks as well as other reference models such as physical models.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844673","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844673","Forecasting;Multi-layer neural network;Power system analysis computing;Recurrent neural networks;Solar Energy","Forecasting;Machine learning;Neural networks;Neurons;Power generation;Predictive models;Weather forecasting","belief networks;electric power generation;learning (artificial intelligence);load forecasting;neural nets;power engineering computing;power grids;solar power stations","AutoEncoder;LSTM neural networks;artificial neural network;deep belief networks;deep learning;power generation;power grid;renewable energy power forecasting;renewable energy power plants;solar power forecasting","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Hyper-Parameter Tuning of a Decision Tree Induction Algorithm","R. G. Mantovani; T. Horváth; R. Cerri; J. Vanschoren; A. C. P. L. F. d. Carvalho","Inst. of Math. & Comput. Sciencez, Univ. of Sao Paulo, Sao Carlos, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","37","42","Supervised classification is the most studied task in Machine Learning. Among the many algorithms used in such task, Decision Tree algorithms are a popular choice, since they are robust and efficient to construct. Moreover, they have the advantage of producing comprehensible models and satisfactory accuracy levels in several application domains. Like most of the Machine Leaning methods, these algorithms have some hyper-parameters whose values directly affect the performance of the induced models. Due to the high number of possibilities for these hyper-parameter values, several studies use optimization techniques to find a good set of solutions in order to produce classifiers with good predictive performance. This study investigates how sensitive decision trees are to a hyper-parameter optimization process. Four different tuning techniques were explored to adjust J48 Decision Tree algorithm hyper-parameters. In total, experiments using 102 heterogeneous datasets analyzed the tuning effect on the induced models. The experimental results show that even presenting a low average improvement over all datasets, in most of the cases the improvement is statistically significant.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.018","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839559","","Machine learning algorithms;Optimization;Prediction algorithms;Predictive models;Sociology;Statistics;Tuning","decision trees;learning (artificial intelligence);optimisation;pattern classification","J48 decision tree algorithm hyperparameters;classifiers;decision tree induction algorithm;hyperparameter optimization process;hyperparameter tuning;machine learning;supervised classification","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"A deep learning approach for seamless integration of cognitive skills for humanoid robots","J. Hwang; M. Jung; J. Kim; J. Tani","Korea Advanced Institute of Science and Technology, Daejeon, South Korea","2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)","20170209","2016","","","59","65","This study investigates the seamless integration of cognitive skills, such as visual recognition, attention switching, action preparation and generation for a humanoid robot. In our preliminary study [1], the deep dynamic neural network model was introduced to process spatio-temporal visuomotor patterns. In the current study, we extended the previous model further to enhance its capability of handling sequential visuomotor information as well as forming visuomotor representation. We conducted synthetic robotic experiments in which a robot learned goal-directed actions of reaching to grasp objects under two different experimental settings. In the first experiment, a task of reaching to grasp objects was conducted under parameterized visual occlusion condition for the purpose of examining the memory capability in the model. In the second experiment, the action of reaching to grasp objects was incorporated with visual recognition of human gesture patterns with using the working memory. The experimental results revealed that the proposed model was able to generalize its reaching and grasping skills in the novel situations. Furthermore, the analysis using the dimensionality reduction technique on neuron activation verified that the proposed model was capable of manipulating high dimensional spatio-temporal visuomotor patterns by forming their dynamic link to the actional intention developed in the higher level of the model via iterative learning.","","Electronic:978-1-5090-5069-7; POD:978-1-5090-5070-3","10.1109/DEVLRN.2016.7846791","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846791","Deep learning;developmental robotics;humanoids;visuomotor coordination","Biological neural networks;Grasping;Machine learning;Neurons;Robot kinematics;Visualization","gesture recognition;humanoid robots;learning (artificial intelligence);manipulators;neural nets;robot vision","cognitive skill seamless integration;deep dynamic neural network;dimensionality reduction technique;grasp objects;grasping skills;high-dimensional spatio-temporal visuomotor patterns;human gesture patterns;humanoid robots;iterative learning;neuron activation;parameterized visual occlusion condition;sequential visuomotor information handling capability enhancement;spatio-temporal visuomotor patterns;visual recognition","","","","","","","19-22 Sept. 2016","","IEEE","IEEE Conference Publications"
"Datalography: Scaling datalog graph analytics on graph processing systems","W. E. Moustafa; V. Papavasileiou; K. Yocum; A. Deutsch","LinkedIn Corporation","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","56","65","This paper presents the first Datalog evaluation engine for executing graph analytics over BSP-style graph processing engines. Building on recent advances in Datalog that support efficient evaluation of aggregates functions, it is now easy for data scientists to author many important graph algorithms succinctly. Without the burden of low-level parallelization and optimization, data scientists can avoid programming to the quirks of the latest high-performance distributed computing framework. Where prior approaches build bespoke evaluation engines or modify generalized dataflow processing engines to achieve performance, this work shows how to efficiently evaluate Datalog directly on BSP-style graph processing engines such as Giraph. Datalography incorporates both traditional Datalog optimizations, such as semi-naive evaluation, and new evaluation algorithms and optimization techniques for efficient distributed evaluation of Datalog queries on graph processing engines. In particular we develop evaluation techniques that take advantage of super vertices, eager aggregation, and asynchronous execution to optimize graph processing on Pregel-like systems. We implement our algorithms on top of Apache Giraph and our results indicate that Datalography competes with native, tuned implementations, with some analytics running up to 9 times faster.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840589","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840589","","Aggregates;Computational modeling;Distributed databases;Engines;Machine learning algorithms;Optimization","DATALOG;graph theory;query processing","Apache Giraph;BSP-style graph processing engines;Datalog evaluation engine;Datalog graph analytics;Datalog optimizations;Datalog queries;Pregel-like systems;aggregate function evaluation;asynchronous execution;datalography;eager aggregation;graph processing engines;high-performance distributed computing framework;seminaive evaluation;super vertices","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Deep wavelet network for image classification","S. Said; O. Jemai; S. Hassairi; R. Ejbali; M. Zaied; C. Ben Amar","Higher Institute of Computer Science and Multimedia, University of Gabes, Tunisia","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000922","000927","The success of the deep learning and specifically learning layer by layer led to many impressive results in several contexts that include neural network. This gave us the idea to apply this principle of learning on wavelet network because it is an active research topic at the moment. This paper present our approach for image classification by the combination of two techniques of learning: the wavelet network and the deep learning. We try to classify images in a supervised way following by an unsupervised learning using the principle of autoencoder. Experiments on two databases COIL-100 and MNIST show that our approach gives good results for the two classifiers that we used.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844359","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844359","autoencoder;deep learning;supervised classification;unsupervised learning;wavelet network","Conferences;Databases;Machine learning;Neurons;Training;Wavelet analysis;Wavelet transforms","image classification;unsupervised learning;wavelet neural nets","autoencoder;deep learning;deep wavelet network;image classification;neural network;unsupervised learning","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Dynamic metric learning from pairwise comparisons","K. Greenewald; S. Kelley; A. O. Hero","Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","1327","1334","Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retroinitialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, demonstrate RICE-OCELAD on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852388","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852388","","Clustering algorithms;Learning systems;Machine learning algorithms;Pollution measurement;Principal component analysis;Robustness","convex programming;data mining;learning (artificial intelligence)","COMID;OCELAD framework;composite objective mirror descent;data mining;dynamic metric learning;online convex ensemble strongly adaptive dynamic learning;pairwise dissimilarity;pairwise similarity","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Using big data to enhance the bosch production line performance: A Kaggle challenge","A. Mangal; N. Kumar","Materials Science and Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania 15289","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2029","2035","This paper describes our approach to the Bosch production line performance challenge run by Kaggle.com. Maximizing the production yield is at the heart of the manufacturing industry. At the Bosch assembly line, data is recorded for products as they progress through each stage. Data science methods are applied to this huge data repository consisting records of tests and measurements made for each component along the assembly line to predict internal failures. We found that it is possible to train a model that predicts which parts are most likely to fail. Thus a smarter failure detection system can be built and the parts tagged likely to fail can be salvaged to decrease operating costs and increase the profit margins.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840826","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840826","Manufacturing automation;data science;failure analysis;predictive models","Big data;Error analysis;Machine learning algorithms;Manufacturing;Numerical models;Predictive models;Production","Big Data;assembling;automobile industry;failure analysis;production engineering computing","Big Data;Bosch assembly line;Bosch production line performance enhancement;data repository;data science method;internal failure prediction;manufacturing industry;operating cost reduction;production yield maximization;profit margins","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Solving cold-start problem in large-scale recommendation engines: A deep learning approach","J. Yuan; W. Shalaby; M. Korayem; D. Lin; K. AlJadda; J. Luo","Department of Computer Science, University of Rochester, New York","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1901","1910","Collaborative Filtering (CF) is widely used in large-scale recommendation engines because of its efficiency, accuracy and scalability. However, in practice, the fact that recommendation engines based on CF require interactions between users and items before making recommendations, make it inappropriate for new items which haven't been exposed to the end users to interact with. This is known as the cold-start problem. In this paper we introduce a novel approach which employs deep learning to tackle this problem in any CF based recommendation engine. One of the most important features of the proposed technique is the fact that it can be applied on top of any existing CF based recommendation engine without changing the CF core. We successfully applied this technique to overcome the item cold-start problem in Careerbuilder's CF based recommendation engine. Our experiments show that the proposed technique is very efficient to resolve the cold-start problem while maintaining high accuracy of the CF recommendations.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840810","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840810","Cold-Start;Deep Learning;Document Similarity;Job Search;Recommendation System","Big data;Correlation;Electronic mail;Engines;Filtering;Machine learning;Semantics","collaborative filtering;learning (artificial intelligence);recommender systems","Careerbuilder CF based recommendation engine;collaborative filtering;deep learning;item cold-start problem;large-scale recommendation engines","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Recognition of individual object in focus people group based on deep learning","L. Hui-bin; W. Fei; C. Qiang; P. Yong","School of Electronic and Electrical Engineering, Shanghai University of Engineering Science, 201620, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","615","619","Deep leaning has become a hot research topic with the rapid development of big data technology. As an important branch of deep learning, convolutional neural network has been widely used in image recognition, and has achieved great success. Convolutional architecture for fast feature embedding (Caffe) with features like speed, extendibility and openness is currently top popular tool of deep learning. In this paper, the authors use Caffe to realize the recognition of individual object in a focus people group. The training images can be obtained from the video recorded by the camera through the method of normalized cross-correlation histogram. The experimental results show that the individual object can be matched accurately by using pre training model. It can be used in practical work like attendance system, criminal investigation field etc.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846607","CNN;Caffe;deep learning;focus group;object recognition","Biological neural networks;Convolution;Data models;Histograms;Machine learning;Neurons;Training","Big Data;correlation methods;feature extraction;image recognition;learning (artificial intelligence);neural nets;object recognition","Caffe;big data technology;convolutional neural network;deep learning;fast feature embedding;focus people group;image recognition;individual object recognition;normalized cross-correlation histogram","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"Revisiting the Sequential Symbolic Regression Genetic Programming","L. O. V. B. Oliveira; F. E. B. Otero; L. F. Miranda; G. L. Pappa","Comput. Sci. Dept., Fed. Univ. of Minas Gerais, Belo Horizonte, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","163","168","Sequential Symbolic Regression (SSR) is a technique that recursively induces functions over the error of the current solution, concatenating them in an attempt to reduce the error of the resulting model. As proof of concept, the method was previously evaluated in one-dimensional problems and compared with canonical Genetic Programming (GP) and Geometric Semantic Genetic Programming (GSGP). In this paper we revisit SSR exploring the method behaviour in higher dimensional, larger and more heterogeneous datasets. We discuss the difficulties arising from the application of the method to more complex problems, e.g., overfitting, along with suggestions to overcome them. An experimental analysis was conducted comparing SSR to GP and GSGP, showing SSR solutions are smaller than those generated by the GSGP with similar performance and more accurate than those generated by the canonical GP.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839580","Genetic programming;Geometric semantic crossover;Problem transformation;Symbolic regression","Boosting;Genetic programming;Machine learning algorithms;Measurement;Semantics;Time series analysis;Training","genetic algorithms;regression analysis","GP;GSGP;SSR;geometric semantic genetic programming;sequential symbolic regression genetic programming","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Totally automated keyword extraction","T. Pay","Graduate Center of New York, New York, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3859","3863","We develop and analyze an unsupervised and domain-independent method for extracting keywords from single documents. Our approach differs from the previous ones in the way of identifying candidate keywords, pruning the list of candidate keywords with several filtering heuristics and selecting keywords from the list of candidate keywords according to dynamic threshold functions. We were able to obtain better precision and recall on the same data set with our method than the previously studied ones.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841059","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841059","data mining;text analysis;text mining","Approximation algorithms;Data mining;Feature extraction;Heuristic algorithms;Machine learning algorithms;Measurement;Robots","data mining;information filtering;text analysis","automated keyword extraction;dynamic threshold functions;filtering heuristics;keywords identification;keywords selection;single documents;text mining","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Bayesian predictor of airline class seats based on multinomial event model","B. Liu; Y. Tan; H. Zhou","Ctrip.com, Shanghai, China","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1787","1791","The allocation of class seats on a flight in airline industry is closely connected to multiple correlating factors, including yield management and airfare strategy from airlines, policy regulation and price modification from travel agencies, booking and reservation behavior from customers. Different from various machine learning methods targeting at direct fare or price prediction, we constructed a state predictor of class seats by applying a Naïve Bayes algorithm based on Multinomial Event Model on the core flight reservations inventory big data, to tell the probability of class availability within the next several hours or days. Four fundamental models and one integrated model are developed to propose an optimal decision to the airfare search engine layer, which makes the engine be capable of forecasting a smart buy-or-wait suggestion to customers. In our experimental route from SHA to TYO, the integrated model reaches an average of 95.42% accuracy.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840795","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840795","Multinomial Event Model;Naïve Bayes;air fare;class seats prediction;generic learning algorithm","Atmospheric modeling;Business;Data models;Machine learning algorithms;Prediction algorithms;Predictive models;Probability","Bayes methods;Big Data;consumer behaviour;learning (artificial intelligence);pricing;search engines;travel industry","Bayesian predictor;Naïve Bayes algorithm;SHA;TYO;airfare search engine layer;airfare strategy;airline class seats allocation;airline industry;booking behavior;class availability probability;core flight reservations inventory big data;direct fare prediction;machine learning methods;multinomial event model;multiple correlating factors;policy regulation;price modification;price prediction;reservation behavior;smart buy-or-wait suggestion forecasting;travel agencies;yield management","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Budgeted Batch Bayesian Optimization","V. Nguyen; S. Rana; S. K. Gupta; C. Li; S. Venkatesh","Deakin Univ., Geelong, VIC, Australia","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1107","1112","Parameter settings profoundly impact the performance of machine learning algorithms and laboratory experiments. The classical trial-error methods are exponentially expensive in large parameter spaces, and Bayesian optimization (BO) offers an elegant alternative for global optimization of black box functions. In situations where the functions can be evaluated at multiple points simultaneously, batch Bayesian optimization is used. Current batch BO approaches are restrictive in fixing the number of evaluations per batch, and this can be wasteful when the number of specified evaluations is larger than the number of real maxima in the underlying acquisition function. We present the budgeted batch Bayesian optimization (B3O) for hyper-parameter tuning and experimental design - we identify the appropriate batch size for each iteration in an elegant way. In particular, we use the infinite Gaussian mixture model (IGMM) for automatically identifying the number of peaks in the underlying acquisition functions. We solve the intractability of estimating the IGMM directly from the acquisition function by formulating the batch generalized slice sampling to efficiently draw samples from the acquisition function. We perform extensive experiments for benchmark functions and two real world applications - machine learning hyper-parameter tuning and experimental design for alloy hardening. We show empirically that the proposed B3O outperforms the existing fixed batch BO approaches in finding the optimum whilst requiring a fewer number of evaluations, thus saving cost and time.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0144","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837957","batch Bayesian optimization;experimental design;hyper-parameter tuning;parallel global optimization","Bayes methods;Gaussian mixture model;Machine learning algorithms;Metals;Optimization;Tuning","Bayes methods;learning (artificial intelligence);mixture models;optimisation","B3O;IGMM;acquisition function;alloy hardening;batch generalized slice sampling;black box functions;budgeted batch Bayesian optimization;experimental design;global optimization;hyperparameter tuning;infinite Gaussian mixture model;laboratory experiments;machine learning algorithms","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automated atrial fibrillation detection based on deep learning network","C. Yuan; Y. Yan; L. Zhou; J. Bai; L. Wang","Information and communication engineering, Wuhan university of technology, Wuhan, Hubei Province China","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1159","1164","Aiming at the shorting of the existing atrial fibrillation (AF) detection algorithms and improve the ability of intelligent recognition and extraction of AF signals. Recently, deep learning theory with massive data has been used on image, voice and other filed widely. In this paper, a method based on the stack sparse autoencoder neural network, a instance of deep learning strategy, was proposed for AF detection. Greedy layer-wise training algorithms and massive unlabeled hotter data from a hospital were used to train the deep learning system, and Back Propagation algorithm and half of the MIT-BIH standard databases were applied to optimized the whole system. Another half of the standard data were used to evaluated the performance of this method. The autoencoder learns the high level features which can describe the necessary information better from the raw data The experimental results show that the accuracy of the algorithm based on stack sparse autoencoder is 98.309%, so this approach is of great significance on the real-time monitoring of atrial fibrillation signal in electrocardiogram.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7831994","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831994","AF detection;autoencoder;deep learning;intelligent recognition","Atrial fibrillation;Biological neural networks;Databases;Electrocardiography;Feature extraction;Machine learning;Training","backpropagation;diseases;electrocardiography;feature extraction;graph theory;medical signal detection;network theory (graphs);neural nets;patient diagnosis","AF detection;AF signal extraction;AF signal recognition;atrial fibrillation detection;back propagation algorithm;deep learning network;electrocardiogram;greedy layer-wise training algorithm;stack sparse autoencoder neural network","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"Scalable Iterative Classification for Sanitizing Large-Scale Datasets","B. Li; Y. Vorobeychik; M. Li; B. Malin","Vanderbilt University, Nashville, TN","IEEE Transactions on Knowledge and Data Engineering","20170206","2017","29","3","698","711","Cheap ubiquitous computing enables the collection of massive amounts of personal data in a wide variety of domains. Many organizations aim to share such data while obscuring features that could disclose personally identifiable information. Much of this data exhibits weak structure (e.g., text), such that machine learning approaches have been developed to detect and remove identifiers from it. While learning is never perfect, and relying on such approaches to sanitize data can leak sensitive information, a small risk is often acceptable. Our goal is to balance the value of published data and the risk of an adversary discovering leaked identifiers. We model data sanitization as a game between 1) a publisher who chooses a set of classifiers to apply to data and publishes only instances predicted as non-sensitive and 2) an attacker who combines machine learning and manual inspection to uncover leaked identifying information. We introduce a fast iterative greedy algorithm for the publisher that ensures a low utility for a resource-limited adversary. Moreover, using five text data sets we illustrate that our algorithm leaves virtually no automatically identifiable sensitive instances for a state-of-the-art learning algorithm, while sharing over 93 percent of the original data, and completes after at most five iterations.","1041-4347;10414347","","10.1109/TKDE.2016.2628180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742323","Privacy preserving;game theory;weak structured data sanitization","Data models;Games;Inspection;Machine learning algorithms;Predictive models;Privacy;Security","data privacy;iterative methods;learning (artificial intelligence);pattern classification;set theory;ubiquitous computing","iterative greedy algorithm;large-scale dataset sanitization;leaked identifiers;machine learning;manual inspection;personal data;personally identifiable information;scalable iterative classification;sensitive information;text data sets;ubiquitous computing","","","","","","20161111","March 1 2017","","IEEE","IEEE Journals & Magazines"
"OnSeS: A Novel Online Short Text Summarization Based on BM25 and Neural Network","J. Niu; Q. Zhao; L. Wang; H. Chen; M. Atiquzzaman; F. Peng","State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","The last decade has witnessed a dramatic growth of social networks, such as Twitter, Sina Microblog, etc. Messages/short texts on these platforms are generally of limited length, causing difficulties for machines to understand. Moreover, it is rarely possible for users to read and understand all the content due to the large quantity. So it is imperative to cluster and extract the viewpoints of these short texts. To solve this, the representation of a word is enriched with additional features from external, but it is demanding in terms of computational and time resources. In this paper, we proposed OnSeS, a novel short text summarization method which makes full use of word2vec to represent a word and utilizes neural network model to generate each word of the summary. OnSeS consists of three phrases: 1) clustering short texts using the K-means algorithm; 2) ranking content of each cluster by building a graph-based ranking model using BM25; 3) generating main point of each cluster with the help of neural machine translation model on the top ranked sentence. The experimental results reveal that our proposed fully data-driven approach outperforms state-of-the-art method.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7842073","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842073","","Clustering algorithms;Feature extraction;Machine learning;Natural language processing;Neural networks;Semantics;Solid modeling","graph theory;language translation;neural nets;social networking (online);text analysis","BM25;OnSeS;graph-based ranking model;k-means algorithm;neural machine translation model;neural network;online short text summarization;social networks;top ranked sentence","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"On the interplay of network structure and gradient convergence in deep learning","V. K. Ithapu; S. N. Ravi; V. Singh","Computer Sciences, University of Wisconsin-Madison, United States of America","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","488","495","The regularization and output consistency behavior of dropout and layer-wise pretraining for learning deep networks have been fairly well studied. However, our understanding of how the asymptotic convergence of backpropagation in deep architectures is related to the structural properties of the network and other design choices (like denoising and dropout rate) is less clear at this time. An interesting question one may ask is whether the network architecture and input data statistics may guide the choices of learning parameters and vice versa. In this work, we explore the association between such structural, distributional and learnability aspects vis-à-vis their interaction with parameter convergence rates. We present a framework to address these questions based on convergence of backpropagation for general nonconvex objectives using first-order information. This analysis suggests an interesting relationship between feature denoising and dropout. Building upon these results, we obtain a setup that provides systematic guidance regarding the choice of learning parameters and network sizes that achieve a certain level of convergence (in the optimization sense) often mediated by statistical attributes of the inputs. Our results are supported by a set of experimental evaluations as well as independent empirical observations reported by other groups.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852271","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852271","","Backpropagation;Convergence;Machine learning;Noise reduction;Optimization;Training;Transforms","backpropagation;convergence;gradient methods;neural nets;statistics","asymptotic convergence;backpropagation;data statistics;deep architectures;deep learning;deep network learning;dropout output consistency behavior;dropout regularization;feature denoising;first-order information;gradient convergence;layer wise pretraining;network architecture;network structure;parameter convergence rates","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"I-Vector estimation as auxiliary task for Multi-Task Learning based acoustic modeling for automatic speech recognition","G. Pironkov; S. Dupont; T. Dutoit","TCTS Lab, University of Mons, Belgium","2016 IEEE Spoken Language Technology Workshop (SLT)","20170209","2016","","","1","7","I-Vectors have been successfully applied in the speaker identification community in order to characterize the speaker and its acoustic environment. Recently, i-vectors have also shown their usefulness in automatic speech recognition, when concatenated to standard acoustic features. Instead of directly feeding the acoustic model with i-vectors, we here investigate a Multi-Task Learning approach, where a neural network is trained to simultaneously recognize the phone-state posterior probabilities and extract i-vectors, using the standard acoustic features. Multi-Task Learning is a regularization method which aims at improving the network's generalization ability, by training a unique network to solve several different, but related tasks. The core idea of using i-vector extraction as an auxiliary task is to give the network an additional inter-speaker awareness, and thus, reduce overfitting. Overfitting is a commonly met issue in speech recognition and is especially impacting when the amount of training data is limited. The proposed setup is trained and tested on the TIMIT database, while the acoustic modeling is performed using a Recurrent Neural Network with Long Short-Term Memory cells.","","Electronic:978-1-5090-4903-5; POD:978-1-5090-4904-2; USB:978-1-5090-4902-8","10.1109/SLT.2016.7846237","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846237","LSTM;TIMIT;automatic speech recognition;i-vector;multi-task learning","Acoustics;Feature extraction;Machine learning;Speech;Speech recognition;Standards;Training","learning (artificial intelligence);recurrent neural nets;speech recognition","TIMIT database;automatic speech recognition;i-vector estimation;inter-speaker awareness;long short-term memory cells;multitask learning based acoustic modeling;neural network;recurrent neural network;regularization method;speaker identification community","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"HogWild++: A New Mechanism for Decentralized Asynchronous Stochastic Gradient Descent","H. Zhang; C. J. Hsieh; V. Akella","Dept. of Electr. & Comput. Eng., Univ. of California, Davis, Davis, CA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","629","638","Stochastic Gradient Descent (SGD) is a popular technique for solving large-scale machine learning problems. In order to parallelize SGD on multi-core machines, asynchronous SGD (Hogwild!) has been proposed, where each core updates a global model vector stored in a shared memory simultaneously, without using explicit locks. We show that the scalability of Hogwild! on modern multi-socket CPUs is severely limited, especially on NUMA (Non-Uniform Memory Access) system, due to the excessive cache invalidation requests and false sharing. In this paper we propose a novel decentralized asynchronous SGD algorithm called HogWild++ that overcomes these drawbacks and shows almost linear speedup on multi-socket NUMA systems. The main idea in HogWild++ is to replace the global model vector with a set of local model vectors that are shared by a cluster (a set of cores), keep them synchronized through a decentralized token-based protocol that minimizes remote memory access conflicts and ensures convergence. We present the design and experimental evaluation of HogWild++ on a variety of datasets and show that it outperforms state-of-the-art parallel SGD implementations in terms of efficiency and scalability.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0074","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837887","Decentralized algorithm;Non-uniform memory access (NUMA) architecture;Stochastic gradient descent","Algorithm design and analysis;Computational modeling;Instruction sets;Machine learning algorithms;Multicore processing;Scalability;Sockets","gradient methods;learning (artificial intelligence);parallel processing;pattern clustering;protocols;shared memory systems;stochastic processes","HOGWILD!;HogWild++;NUMA system;asynchronous SGD;cache invalidation requests;decentralized asynchronous SGD;decentralized asynchronous stochastic gradient descent;decentralized token-based protocol;global model vector;large-scale machine learning;local model vectors;multicore machines;multisocket CPU;multisocket NUMA systems;nonuniform memory access system;remote memory access conflict minimisation;shared memory","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Travel-time prediction with deep learning","C. Siripanpornchana; S. Panichpapiboon; P. Chaovalit","Faculty of Information Technology, King Mongkut's Institute of Technology Ladkrabang, Bangkok 10520, Thailand","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","1859","1862","Travel time prediction is a challenging problem in Intelligent Transportation Systems (ITS). Accurate travel time information helps motorists plan their routes more wisely. This, in turn, alleviates traffic congestion and improves operation efficiency. A number of travel time prediction techniques exist; however, most of them are based on shallow learning architectures. In contrast to deep learning architectures, shallow learning architectures are lack of features-learning capability. In this paper, we propose an effective travel time prediction technique based on a concept of Deep Belief Networks (DBN). In our method, a stack of Restricted Boltzmann Machines (RBM) is used to automatically learn generic traffic features in an unsupervised fashion, and then a sigmoid regression is used to predict travel time in a supervised fashion. The experimental results, based on real traffic data, show that the proposed method can achieve great performance in terms of prediction accuracy.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848343","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848343","deep belief networks (DBN);deep learning;travel-time prediction","Data models;Machine learning;Measurement;Predictive models;Real-time systems;Testing","Boltzmann machines;intelligent transportation systems;learning (artificial intelligence);road traffic","DBN;Deep Belief Networks;ITS;RBM;Restricted Boltzmann Machines;deep learning architectures;features learning capability;generic traffic features;intelligent transportation systems;operation efficiency;shallow learning architectures;traffic congestion;travel time information;travel time prediction","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"VHT: Vertical hoeffding tree","N. Kourtellis; G. De Francisci Morales; A. Bifet; A. Murdopo","Telefonica I&#x002B;D, Spain","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","915","922","IoT big data requires new machine learning methods able to scale to large size of data arriving at high speed. Decision trees are popular machine learning models since they are very effective, yet easy to interpret and visualize. In the literature, we can find distributed algorithms for learning decision trees, and also streaming algorithms, but not algorithms that combine both features. In this paper we present the Vertical Hoeffding Tree (VHT), the first distributed streaming algorithm for learning decision trees. It features a novel way of distributing decision trees via vertical parallelism. The algorithm is implemented on top of Apache SAMOA, a platform for mining big data streams, and thus able to run on real-world clusters. Our experiments to study the accuracy and throughput of VHT prove its ability to scale while attaining superior performance compared to sequential decision trees.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840687","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840687","Apache SAMOA;IoT;big data;distributed streaming decision tree;hoeffding tree;vertical parallelism","Big data;Decision trees;Machine learning algorithms;Parallel processing;Partitioning algorithms;Radiation detectors;Vegetation","Big Data;Internet of Things;decision trees;distributed algorithms;learning (artificial intelligence);parallel processing","Apache SAMOA;Big Data streams mining;Internet of Things;IoT Big Data;VHT;decision trees;distributed streaming algorithm;machine learning;vertical Hoeffding tree;vertical parallelism","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"PinterNet: A thematic label curation tool for large image datasets","R. Liu; D. Palsetia; A. Paul; R. Al-Bahrani; D. Jha; W. k. Liao; A. Agrawal; A. Choudhary","Electrical Engineering and Computer Science, Northwestern University","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2353","2362","Recent progress in big data and computer vision with deep learning models has gained a lot of attention. Deep learning has been performed on tasks such as image classification, object detection, image segmentation, image captioning, visual question and answering, using large collections of annotated images. This calls for more curated large image datasets with clearer descriptions, cleaner contents, and diversified usability. However, the curation and labeling of such datasets can be labor-intensive. In this paper, we present PinterNet, an algorithm for automatic curation and label generation from noisy textual descriptions, and also publish a big image dataset containing over 110K images automatically labeled with their themes. Our dataset is hierarchical in nature, it has high level category information which we refer as verticals with fine-grained thematic labels at lower level. This advocates a new type of hierarchical theme classification problem closer to human cognition and of business value. We provide benchmark performances using deep learning models based on AlexNet architecture with different pre-training schemes for this novel task and new data.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840868","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840868","Computer vision;Dataset;Image classification;Label curation;Theme classification","Big data;Computational modeling;Data models;Itemsets;Labeling;Machine learning;Noise measurement","image classification;image texture;learning (artificial intelligence);visual databases","AlexNet architecture;PinterNet;automatic thematic label curation tool;big image dataset;business value;deep learning models;fine-grained thematic labels;hierarchical dataset;hierarchical theme classification problem;high level category information;human cognition;label generation;large image datasets;large-image datasets;noisy textual descriptions;pretraining scheme","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Intelligent feature selection using hybrid based feature selection method","S. Nisar; M. Tariq","Department of Electrical Engineering, FAST-NUCES, Peshawar, Pakistan","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","168","172","Feature selection and reduction has assumed the position of a leading approach for many preprocessing step in machine learning. It is widely used as preprocessing in classification due to an exponential growth in data set as well as in feature set. The aim of this paper is to contribute in the domain of feature selection, which directly reduces the complexity and speed up the learning algorithm by improving the predictive accuracy. In machine learning as the dimensionality increases, the classifiers performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in performance, and increase in the complexity and computational time of the classifier. This degrades the overall performance of the classifiers. In this work, a new model is proposed, hybrid based feature selection (HBFS) that identifies relevant features with respect to labels as well as eliminate the redundancy between relevant features. Features selection under the proposed method is compared with the widely used techniques such as correlation based feature selection, mutual information based feature selection, Kullback Leibler based feature selection and improves the accuracy by 5.75%. The proposed model is evaluated using 10 fold cross validation technique. The results show that the proposed method not only reduces the number of features but also achieves accuracy up to 87.01%.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845025","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845025","","Computational modeling;Correlation;Entropy;Machine learning algorithms;Mutual information;Principal component analysis;Training","computational complexity;feature selection;learning (artificial intelligence);pattern classification","HBFS;Kullback Leibler based feature selection;classifier complexity;classifier computational time;classifier performance;feature set;hybrid based feature selection method;intelligent feature selection;machine learning","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"A review on optimization algorithm for deep learning method in bioinformatics field","S. N. M. Yousoff; A. Baharin; A. Abdullah","Synthetic Biology Research Group, Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM, Johor, Malaysia","2016 IEEE EMBS Conference on Biomedical Engineering and Sciences (IECBES)","20170206","2016","","","707","711","In the past few years, deep learning has been used widely in bioinformatics area to solve common problems such as protein sequence prediction, phylogenic inferences, multiple sequence alignment and many more. It has been in the spotlight as a powerful approach which makes significant advances in taking care of the issues that haunt artificial intelligence community for many years. However, several weaknesses such as trap at local minima, lower performance and high computational time still occur in deep learning. Therefore, global optimization technique such as differential search algorithm can be used to assist deep learning method in order to get best finding result and data. This review will cover fundamental of deep learning and their involvement in bioinformatics field as well as implementation of differential search algorithm and their involvement in bioinformatics field.","","Electronic:978-1-4673-7791-1; POD:978-1-4673-7792-8","10.1109/IECBES.2016.7843542","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843542","backpropagation;bioinformatics;deep learning;differential search algorithm;neural network;optimization algorithm","Algorithm design and analysis;Backpropagation;Bioinformatics;Biological neural networks;Machine learning;Optimization;Organisms","bioinformatics;learning (artificial intelligence);optimisation;reviews","artificial intelligence;bioinformatics;deep learning method;differential search algorithm;global optimization algorithm;multiple sequence alignment;phylogenic inferences;protein sequence prediction;review","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Adaptive neuron apoptosis for accelerating deep learning on large scale systems","C. Siegel; J. Daily; A. Vishnu","Pacific Northwest National Laboratory, Richland, WA 99352","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","753","762","We present novel techniques to accelerate the convergence of Deep Learning algorithms by conducting low overhead removal of redundant neurons - apoptosis of neurons - which do not contribute to model learning, during the training phase itself. We provide in-depth theoretical underpinnings of our heuristics (bounding accuracy loss and handling apoptosis of several neuron types), and present the methods to conduct adaptive neuron apoptosis. Specifically, we are able to improve the training time for several datasets by 2-3x, while reducing the number of parameters by up to 30× (4-5× on average) on datasets such as ImageNet classification. For the Higgs Boson dataset, our implementation improves the accuracy (measured by Area Under Curve (AUC)) for classification from 0.88/1 to 0.94/1, while reducing the number of parameters by 3x in comparison to existing literature. The proposed methods achieve a 2.44x speedup in comparison to the default (no apoptosis) algorithm.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840668","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840668","","Biological neural networks;Machine learning;Network topology;Neurons;Topology;Training","Big Data;large-scale systems;learning (artificial intelligence);neural nets","Higgs Boson dataset;ImageNet classification;adaptive neuron apoptosis;deep learning acceleration;large scale systems;low overhead removal;redundant neurons","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A MIC-based acceleration model of Deep Learning","J. Zhang; X. Zheng; W. Shen; D. Zhou; F. Qiu; H. Zhang","School of Computer Engineering and Science, Shanghai University, 200444, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","608","614","In the era of Computational Intelligence developing rapidly, Deep Learning (DL) has gradually won acceptance from the world of Artificial Intelligence (AI) and it has been widely applied to the industry. However, the training of the network requires a considerable amount of time. For instances, the training of Convolution Neural Network (CNN) and Deep Belief Network (DBN) may take one week or even longer. Therefore, a new challenge has been put forward to the world of Artificial Intelligence, which demands decrease on the training time of Deep Learning algorithm effectively. And in this paper, we proposed a Deep Learning acceleration model based on MIC, which can reduce the training time significantly by using Restricted Boltzmann Machine (RBM) and Logistic Regression (LR). First, it conducts vectorization on the program, and then accelerates it by using the model we proposed in this paper. And the paper mainly consists of the design of the parallel model, which comprising data parallelism, model parallelism, a hybrid of data and model parallelism and so on. And experiments showed that the MIC-based acceleration model can reduce the training time to 1/10 of the original.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846603","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846603","Deep Belief Network;Deep learning;Parallel Computation on MIC;Restricted Boltzmann Machine;Vectorization Computation","Acceleration;Computational modeling;Data models;Feature extraction;Machine learning;Parallel processing;Training","Boltzmann machines;artificial intelligence;convolution;graphics processing units;microprocessor chips;multiprocessing systems;regression analysis;vectors","CNN;DBN;MIC-based acceleration model;RBM;artificial intelligence;computational intelligence;convolution neural network;data parallelism;deep belief network;deep learning acceleration model;logistic regression;many integrated core;model parallelism;restricted Boltzmann machine;vectorization","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"Visualization of Driving Behavior Based on Hidden Feature Extraction by Using Deep Learning","H. Liu; T. Taniguchi; Y. Tanaka; K. Takenaka; T. Bando","Graduate School of Information Science and Engineering, Ritsumeikan University, Kusatsu 525-8577, Japan, and with the Japan Society for the Promotion of Science.","IEEE Transactions on Intelligent Transportation Systems","","2017","PP","99","1","13","In this paper, we propose a visualization method for driving behavior that helps people to recognize distinctive driving behavior patterns in continuous driving behavior data. Driving behavior can be measured using various types of sensors connected to a control area network. The measured multi-dimensional time series data are called driving behavior data. In many cases, each dimension of the time series data is not independent of each other in a statistical sense. For example, accelerator opening rate and longitudinal acceleration are mutually dependent. We hypothesize that only a small number of hidden features that are essential for driving behavior are generating the multivariate driving behavior data. Thus, extracting essential hidden features from measured redundant driving behavior data is a problem to be solved to develop an effective visualization method for driving behavior. In this paper, we propose using deep sparse autoencoder (DSAE) to extract hidden features for visualization of driving behavior. Based on the DSAE, we propose a visualization method called a driving color map by mapping the extracted 3-D hidden feature to the red green blue (RGB) color space. A driving color map is produced by placing the colors in the corresponding positions on the map. The subjective experiment shows that feature extraction method based on the DSAE is effective for visualization. In addition, its performance is also evaluated numerically by using pattern recognition method. We also provide examples of applications that use driving color maps in practical problems. In summary, it is shown the driving color map based on DSAE facilitates better visualization of driving behavior.","1524-9050;15249050","","10.1109/TITS.2017.2649541","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839988","Data visualization;deep learning;driving behavior analysis;feature extraction.","Data mining;Data visualization;Feature extraction;Image color analysis;Machine learning;Sensors;Time series analysis","","","","","","","","20170202","","","IEEE","IEEE Early Access Articles"
"Learning to decode linear codes using deep learning","E. Nachmani; Y. Be'ery; D. Burshtein","School of Electrical Engineering, Tel-Aviv University, Israel","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","341","346","A novel deep learning method for improving the belief propagation algorithm is proposed. The method generalizes the standard belief propagation algorithm by assigning weights to the edges of the Tanner graph. These edges are then trained using deep learning techniques. A well-known property of the belief propagation algorithm is the independence of the performance on the transmitted codeword. A crucial property of our new method is that our decoder preserved this property. Furthermore, this property allows us to learn only a single codeword instead of exponential number of codewords. Improvements over the belief propagation algorithm are demonstrated for various high density parity check codes.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852251","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852251","","Belief propagation;Machine learning;Maximum likelihood decoding;Neural networks;Parity check codes;Training","decoding;learning (artificial intelligence);linear codes;parity check codes","Tanner graph;belief propagation algorithm;deep learning;high density parity check codes;linear codes;transmitted codeword","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Analysis of teamwork dialogue: A data mining approach","A. Shibani; E. Koh; V. Lai; K. J. Shim","National Institute of Education, Nanyang Technological University, Singapore","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","4032","4034","With the advent of the Internet and wide-spread popularity of online technology-enhanced learning platforms, many pedagogical activities today involve learners in online discussions such as synchronous chat. In this study, we describe a text mining method used for analyzing teamwork from such chat dialogue of students. The steps in the text mining method such as pre-processing and classification are described and the results of our analysis are presented in this paper.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841100","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841100","chat;dialogue;learning analytics;teamwork;text mining","Algorithm design and analysis;Classification algorithms;Collaborative work;Encoding;Feature extraction;Machine learning algorithms;Teamwork","Internet;computer aided instruction;data mining;groupware;text analysis","Internet;data mining;online discussions;online technology-enhanced learning platforms;student chat dialogue;teamwork dialogue;text mining","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Characterization of In-season Elite Football Trainings by GPS Features: The Identity Card of a Short-Term Football Training Cycle","A. Rossi; E. Perri; A. Trecroci; M. Savino; G. Alberti; M. F. Iaia","Dept. of Biomed. Sci. for health, Univ. degli Studi di Milano, Milan, Italy","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","160","166","Football training periodization is widely acknowledged as crucial to obtain the best performance throughout matches and to reduce the risk of injuries. Thus, the aim of this study is to detect the in-season short-term training cycles in an Italian elite football team. 80 trainings of 26 elite football players were monitored during 23 in-season weeks by a global position system (GPS). Machine learning process and autocorrelation analyses were performed in order to detect pattern inside in-season football trainings. Extra tree random forest classifier (ETRFC) was used to create a supervised machine learning process able to describe the football trainings cycle. This analytical model allows us to produce reliable decisions and results learning from historical relationships and trends in the data. In addition, the autocorrelation analysis allows us to detect similarity between observations between the data. Based on these analysis, it was found that the in-season football trainings are characterized by a series of short-term cycles. This kind of periodization follows a sinusoidal model because the short-term cycle detected in the in-season trainings is composed of two parts with different training loads. In particular, in the days long before the match football players perform higher training loads than in the close ones. To enhance performance and reduce risk of injuries, it would be essential to provide correct stimuli in each short-term cycle per day. Thus, developing a valid method able to define the correct training loads in each training day may be central for coaches and athletic trainers to periodize correctly the football trainings.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0030","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836661","Data analysis;Short-term cycle;Training loads","Acceleration;Algorithm design and analysis;Correlation;Feature extraction;Global Positioning System;Machine learning algorithms;Training","Global Positioning System;computer based training;learning (artificial intelligence);pattern classification;sport","ETRFC;GPS;Global Position System;Italian in-season elite football trainings;analytical model;autocorrelation analysis;extra-tree random forest classifier;football training periodization;identity card;in-season short-term football training cycle;injury risk reduction;pattern inside in-season football training detection;sinusoidal model;supervised machine learning process;training loads","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"SOAL: Second-Order Online Active Learning","S. Hao; P. Zhao; J. Lu; S. C. H. Hoi; C. Miao; C. Zhang","Inst. of High Performance Comput., Agency for Sci. Technol. & Res., Singapore, Singapore","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","931","936","This paper investigates the problem of online active learning for training classification models from sequentially arriving data. This is more challenging than conventional online learning tasks since the learner not only needs to figure out how to effectively update the classifier but also needs to decide when is the best time to query the label of an incoming instance given limited label budget. The existing online active learning approaches are often based on first-order online learning methods which generally fall short in slow convergence rate and sub-optimal exploitation of available information when querying the labeled data. To overcome the limitations, in this paper, we present a new framework of Second-order Online Active Learning (SOAL), which fully exploits both first-order and second-order information to achieve high learning accuracy with low labeling cost. We conduct both theoretical analysis and empirical studies for evaluating the proposed SOAL algorithm extensively. The encouraging results show clear advantages of the proposed algorithm over a family of state-of-the-art online active learning algorithms.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837928","active learning;online learning","Algorithm design and analysis;Computational modeling;Labeling;Learning systems;Machine learning algorithms;Mathematical model;Training","learning (artificial intelligence);pattern classification","SOAL algorithm;classification models;first-order online learning methods;second-order online active learning;slow convergence rate;sub-optimal exploitation","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"TV logo classification based on convolutional neural network","D. Pan; P. Shi; Z. Qiu; Y. Sha; X. Zhongdi; J. Zhoushao","College of Information Engineering, Communication University of China, Beijing, China","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","1793","1796","This paper presents region-based, convolutional neural network for accurate and efficient TV logo classification. Although many previous methods have been applied to TV logo classification, most of them only process the video images. For images captured by smartphone, these methods often can't achieve satisfactory performance. Three contributions are given to the problem of TV logo detection and classification. Firstly, maximally stable extremal region (MSER) is used as a method of generating candidate boxes per image. Geometry constraintGeometry constraint is introduced to filter certain boxes which are obviously different from TV logo shape in geometry space. Secondly, we design an efficient Convolutional Neural Network (CNN) for classifying TV logo. Thirdly, we create a large TV logo dataset, all of which are collected from video streaming. Some augmentation methods have been taken to improve the diversity of TV logo images. Experimental results show that the proposed method outperformed previous methods.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7832108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832108","TV logo;classification;component;convolutional neural network;detection;mser","Biological neural networks;Feature extraction;Geometry;Machine learning;Streaming media;TV","feedforward neural nets;geometry;image classification;object detection;video streaming","CNN;MSER;TV logo classification;TV logo dataset;TV logo detection;TV logo image diversity improvement;augmentation methods;convolutional neural network;geometry constraint;geometry space;maximally stable extremal region;video streaming","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"Performance of MPI Codes Written in Python with NumPy and mpi4py","R. Smith","Engility Corp., Aberdeen Proving Ground, MD, USA","2016 6th Workshop on Python for High-Performance and Scientific Computing (PyHPC)","20170202","2016","","","45","51","Python is an interpreted language that has become more commonly used within HPC applications. Python benefits from the ability to write extension modules in C, which can further use optimized libraries that have been written in other compiled languages. For HPC users, two of the most common extensions are NumPy and mpi4py. It is possible to write a full computational kernel in a compiled language and then build that kernel into an extension module. However, this process requires not only the kernel be written in the compiled language, but also the interface between the kernel and Python be implemented. If possible, it would be preferable to achieve similar performance by writing the code directly in Python using readily available performant modules. In this work the performance differences between compiled codes and codes written using Python3 and commonly available modules, most notably NumPy and mpi4py, are investigated. Additionally, the performance of an open source Python stack is compared to the recently announced Intel Python3 distribution.","","Electronic:978-1-5090-5220-2; POD:978-1-5090-5221-9","10.1109/PyHPC.2016.010","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836843","HPC;Intel Python;MPI;Python;mpi4py","Benchmark testing;Kernel;Libraries;Machine learning algorithms;Workstations","application program interfaces;high level languages;message passing","Intel Python3;MPI codes;NumPy;mpi4py;open source Python stack","","","","","","","14-14 Nov. 2016","","IEEE","IEEE Conference Publications"
"Multiple feature construction in classification on high-dimensional data using GP","B. Tran; M. Zhang; B. Xue","School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, 6140, New Zealand","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","Feature construction and feature selection are common pre-processing techniques to obtain smaller but better discriminating feature sets than the original ones. These two techniques are essential in high-dimensional data with thousands or tens of thousands of features where there may exist many irrelevant and redundant features. Genetic programming (GP) is a powerful technique that has shown promising results in feature construction and feature selection. However, constructing multiple features for high-dimensional data is still challenging due to its large search space. In this paper, we propose a GP-based method that simultaneously performs multiple feature construction and feature selection to automatically transform high-dimensional datasets into much smaller ones. Experiment results on six datasets show that the size of the generated feature set is less than 4% of the original feature set size and it significantly improves the performance of K-Nearest Neighbour, Naive Bayes and Decision Tree algorithms on 15 out of 18 comparisons. Compared with the single feature construction method using GP, the proposed method has better performance on half cases and similar on the other half. Comparisons between the constructed features, the selected features and the combination of both constructed and selected features by the propose method reveal different preferences of the three learning algorithms on these feature sets.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850130","","Classification algorithms;Computational efficiency;Decision trees;Genetic programming;Machine learning algorithms;Radio frequency;Training data","Bayes methods;decision trees;feature selection;genetic algorithms;learning (artificial intelligence);pattern classification;set theory","GP-based method;K-nearest neighbour;Naive Bayes;decision tree algorithms;feature selection;genetic programming;high-dimensional data;high-dimensional datasets;learning algorithms;multiple feature construction;redundant features","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automatic false positive canceling for indoor human detection","D. Zhu; M. Q. H. Meng","Department of Electrical Engineering, the Chinese University of Hong Kong, Shatin, N.T., Hong Kong","2016 IEEE International Conference on Information and Automation (ICIA)","20170202","2016","","","381","384","Humans are the most common mobile obstacles for an office robot. A robust human detector usually plays important roles in robots' perception phase. Especially, Tasks like dynamic path planning and obstacle avoidance have to employ such a detector to help make real-time decisions. Thus, an efficient human detector is meaningful for office robots. However, while deep learning based detectors are not always available in embedded applications, shallow model based ones suffer much from high false positive rates. In this paper, we present a novel approach that can automatically canceling most false positives from shallow detectors. This method regards the detection result as candidates. Then, an algorithm called rival penalized competitive learning is applied to evaluate the inner geometry relationship of each candidate. The one with low evaluation scores, which indicates possible false positives, will be discarded. Experiments show that this approach has the ability to decrease false positive rate of detectors running in office environment.","","Electronic:978-1-5090-4102-2; POD:978-1-5090-4103-9; USB:978-1-5090-4101-5","10.1109/ICInfA.2016.7831854","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831854","","Clustering algorithms;Detectors;Feature extraction;Geometry;Machine learning;Reliability;Robots","collision avoidance;geometry;mobile robots;object detection;robot vision;unsupervised learning","automatic false positive canceling;deep learning based detectors;dynamic path planning;indoor human detection;inner geometry relationship;mobile obstacles;obstacle avoidance;office robot;rival penalized competitive learning;robot perception phase;robust human detector;shallow detectors","","","","","","","1-3 Aug. 2016","","IEEE","IEEE Conference Publications"
"Nonlinear system modeling with deep neural networks and autoencoders algorithm","E. De la Rosa; W. Yu; X. Li","Departamento de Control Automatico, CINVESTAV-IPN, Mexico City, Mexico","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","002157","002162","Deep learning techniques have been successfully used for pattern classification. These advantage methods are still not applied in nonlinear systems identification. In this paper, the neural model has deep architecture which is obtained by a random search method. The initial weights of this deep neural model is obtained from the denoising autoencoders model. We propose special unsupervised learning methods for this deep learning model with input data. The normal supervised learning is used to train the weights with the output data. The deep learning identification algorithms are validated with three benchmark examples.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844558","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844558","","Computational modeling;Data models;Machine learning;Neural networks;Noise reduction;Nonlinear systems;Training","modelling;neural nets;nonlinear systems;pattern classification;search problems;unsupervised learning","autoencoders algorithm;deep architecture;deep learning identification algorithms;deep neural networks;nonlinear system identification;nonlinear system modeling;normal supervised learning;pattern classification;random search method;unsupervised learning","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Automated IT system failure prediction: A deep learning approach","K. Zhang; J. Xu; M. R. Min; G. Jiang; K. Pelechrinis; H. Zhang","School of Information Sciences, University of Pittsburgh","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1291","1300","In mission critical IT services, system failure prediction becomes increasingly important; it prevents unexpected system downtime, and assures service reliability for end users. While operational console logs record rich and descriptive information on the health status of those IT systems, existing system management technologies mostly use them in a labor-intensive forensics approach, i.e., identifying what went wrong after the fact. Recent efforts on log-based system management take an automation approach with text mining techniques, such as term frequency - inverse document frequency (TF-IDF). However, those techniques lead to a high-dimensional feature space, and are not easily generalizable to heterogeneous log formats. In this paper, we present a novel system that automatically parses streamed console logs and detects early warning signals for IT system failure prediction. In particular, our solution includes a log pattern extraction method by clustering together logs with similar format and content. We then resemble the TF-IDF idea by considering each pattern as a word and the set of patterns in each discretized epoch as a document. This leads to a feature space with significantly lower dimensionality that can provide robust signals for the status of the system. As system failures tend to occur very rare, we apply a recurrent neural network, namely, Long Short-Term Memory (LSTM), to deal with the “rarity” of labeled data in the training process. LSTM is able to capture the long-range dependency across sequences, therefore outperforms traditional supervised learning methods in our application domain. We evaluated and compared our proposed technology with state-of-the-art machine learning approaches using real log traces from two large enterprise systems. The results showed the advantage and potentials of our system in prediction of complex IT failures. To our knowledge, our work is the first that employs LSTM f- r log-based system failure prediction.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840733","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840733","Deep Learning;Failure Prediction;LSTM;Log Analysis;Recurrent Neural Network;System Management;Text Mining","Feature extraction;Layout;Machine learning;Recurrent neural networks;Supervised learning;Text mining;Training","data mining;digital forensics;failure analysis;learning (artificial intelligence);recurrent neural nets;system recovery;text analysis","LSTM;TF-IDF;automated IT system failure prediction;complex IT failures;deep learning;discretized epoch;early warning signals;enterprise systems;health status;heterogeneous log formats;high-dimensional feature space;labor-intensive forensics;log pattern extraction method;log-based system failure prediction;log-based system management;long short-term memory;machine learning;mission critical IT services;operational console logs;recurrent neural network;robust signals;service reliability;streamed console logs;supervised learning methods;system failures;term frequency-inverse document frequency;text mining techniques;unexpected system downtime","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
