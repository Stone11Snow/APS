"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7237086,7231904,7230448,7231140,7237069,7238056,7230438,7238146,7226609,7219432,7098448,7219856,7224807,7225690,7223616,7219571,7225396,7218098,7208200,7208259,7207229,7140780,7195695,7203069,7196536,7176101,7182561,7180094,7178323,7173365,7172253,7166655,7164701,7164053,7163984,7163983,7163871,7163987,7163037,7160901,7160063,7155867,7155943,7154810,7154691,7153242,7154933,7150650,7151383,7149644,7148359,7133122,7129986,7130292,7125003,7127484,7125310,7123502,7120564,7118405,7116886,7110511,7110810,7105612,7103951,7100862,7094840,7091324,7092894,7087040,7081844,7081953,7079125,7073443,7054550,7072812,7074261,7069923,7065546,7065596,7064276,7058056,7059023,7058088,7057937,7053081,7055079,7052853,7045716,7044861,7032619,6926850,6805148,7039146,7035623,7034832,7032299,7035780,7033353,7031328",2017/05/05 22:39:47
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Automatic Filling Values to Services: A Road Map","S. Wang; Y. Zou; J. Ng; T. Ng","Queen's Univ., Kingston, ON, Canada","2015 IEEE World Congress on Services","20150817","2015","","","275","277","It is common for users to explicitly or implicitly compose on-line services to accomplish daily tasks, such as shopping for a pair of shoes on-line. However, unnecessary and repetitive data typing into the services would negatively impact the user experience and decrease the efficiency of service composition. Recent studies have proposed several approaches to help users fill in values to services automatically. However, existing approaches suffer the following two drawbacks: 1) poor accuracy of filling values to services, 2) not designed for service composition. In this position paper, we first present the recent approaches improving the process of filling values to services. We then present our recent achievements and results of using our proposed approaches to help users fill values to services. Lastly, we discuss the various opportunities and challenges in the research field of filling values to services.","2378-3818;23783818","Electronic:978-1-4673-7275-6; POD:978-1-4673-7276-3","10.1109/SERVICES.2015.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7196536","information reuse;input parameter value recommendation;service composition","Accuracy;Browsers;Context;Filling;Footwear;Machine learning algorithms;Web services","Web services","Web services;information reuse;service composition;services automatic filling values","","0","","21","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"GraphSC: Parallel Secure Computation Made Easy","K. Nayak; X. S. Wang; S. Ioannidis; U. Weinsberg; N. Taft; E. Shi","","2015 IEEE Symposium on Security and Privacy","20150720","2015","","","377","394","We propose introducing modern parallel programming paradigms to secure computation, enabling their secure execution on large datasets. To address this challenge, we present Graph SC, a framework that (i) provides a programming paradigm that allows non-cryptography experts to write secure code, (ii) brings parallelism to such secure implementations, and (iii) meets the need for obliviousness, thereby not leaking any private information. Using Graph SC, developers can efficiently implement an oblivious version of graph-based algorithms (including sophisticated data mining and machine learning algorithms) that execute in parallel with minimal communication overhead. Importantly, our secure version of graph-based algorithms incurs a small logarithmic overhead in comparison with the non-secure parallel version. We build Graph SC and demonstrate, using several algorithms as examples, that secure computation can be brought into the realm of practicality for big data analysis. Our secure matrix factorization implementation can process 1 million ratings in 13 hours, which is a multiple order-of-magnitude improvement over the only other existing attempt, which requires 3 hours to process 16K ratings.","1081-6011;10816011","Electronic:978-1-4673-6949-7; POD:978-1-4673-6950-3","10.1109/SP.2015.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163037","graph algorithms;oblivious algorithms;parallel algorithms;secure computation","Algorithm design and analysis;Clustering algorithms;Computational modeling;Data mining;Machine learning algorithms;Parallel processing;Programming","matrix decomposition;parallel programming;security of data","GraphSC framework;graph-based algorithms;logarithmic overhead;matrix factorization;parallel programming paradigm;parallel secure computation;secure code writing","","5","","78","","","17-21 May 2015","","IEEE","IEEE Conference Publications"
"A Unified Approach to Universal Prediction: Generalized Upper and Lower Bounds","N. D. Vanli; S. S. Kozat","Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Neural Networks and Learning Systems","20150216","2015","26","3","646","651","We study sequential prediction of real-valued, arbitrary, and unknown sequences under the squared error loss as well as the best parametric predictor out of a large, continuous class of predictors. Inspired by recent results from computational learning theory, we refrain from any statistical assumptions and define the performance with respect to the class of general parametric predictors. In particular, we present generic lower and upper bounds on this relative performance by transforming the prediction task into a parameter learning problem. We first introduce the lower bounds on this relative performance in the mixture of experts framework, where we show that for any sequential algorithm, there always exists a sequence for which the performance of the sequential algorithm is lower bounded by zero. We then introduce a sequential learning algorithm to predict such arbitrary and unknown sequences, and calculate upper bounds on its total squared prediction error for every bounded sequence. We further show that in some scenarios, we achieve matching lower and upper bounds, demonstrating that our algorithms are optimal in a strong minimax sense such that their performances cannot be improved further. As an interesting result, we also prove that for the worst case scenario, the performance of randomized output algorithms can be achieved by sequential algorithms so that randomized output algorithms do not improve the performance.","2162-237X;2162237X","","10.1109/TNNLS.2014.2317552","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805148","Online learning;sequential prediction;worst-case performance;worst-case performance.","Learning systems;Machine learning algorithms;Polynomials;Prediction algorithms;Signal processing algorithms;Upper bound;Vectors","learning (artificial intelligence);minimax techniques;prediction theory","computational learning theory;general parametric predictors;lower bound;parameter learning problem;parametric predictor;sequential learning algorithm;sequential prediction;squared error loss;strong minimax sense;universal prediction;upper bound","","3","","17","","20140424","March 2015","","IEEE","IEEE Journals & Magazines"
"Exploiting Ordinal Class Structure in Multiclass Classification: Application to Ovarian Cancer","B. Misganaw; M. Vidyasagar","Erik Jonsson School of Engineering and Computer Science, The University of Texas at Dallas, Richardson, TX, USA","IEEE Life Sciences Letters","20150819","2015","1","1","15","18","In multiclass machine learning problems, one needs to distinguish between the nominal labels that do not have any natural ordering and the ordinal labels that are ordered. Ordinal labels are pervasive in biology, and some examples are given here. In this note, we point out the importance of making use of the order information when it is inherent to the problem. We demonstrate that algorithms that use this additional information outperform the algorithms that do not, on a case study of assigning one of four labels to the ovarian cancer patients on the basis of their time of progression-free survival. As an aside, it is also pointed out that the algorithms that make use of ordering information require fewer data normalizations. This aspect is important in biological applications, where data are plagued by variations in platforms and protocols, batch effects, and so on.","","","10.1109/LLS.2015.2451291","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7140780","Ordinal classification;Ovarian cancer;ordinal classification;ovarian cancer","Biological systems;Biology;Cancer;Learning systems;Machine learning;Support vector machines;Tumors","cancer;gynaecology;learning (artificial intelligence)","biological applications;multiclass machine learning problems;ordinal class structure;ovarian cancer patients","","1","","19","","20150630","June 2015","","IEEE","IEEE Journals & Magazines"
"Study of soft sensor modeling based on deep learning","Yujun Lin; W. Yan","Department of automation of Shanghai Jiaotong University, 200240, China","2015 American Control Conference (ACC)","20150730","2015","","","5830","5835","Soft sensor are widely used to estimate process variables which are difficult to measure online in industrial process control. This paper proposes a new soft sensor modeling method based on a deep learning method, which integrates denoising auto-encoders (DAE) with support vector regression (SVR) method. The denoising auto-encoders are designed to capture robust high-level feature representation of import data and the SVR model is employed to precisely estimate output data based on the feature representation obtained from DAE. In case study, the method combining denoising auto-encoders with support vector regression (DAE-SVR) is applied to the estimation of oxygen-content in flue gasses in ultra-supercritical units. The results show DAE-SVR is a promising modeling method for soft sensors.","0743-1619;07431619","CD-ROM:978-1-4799-8685-9; Electronic:978-1-4799-8684-2; POD:978-1-4799-1773-0","10.1109/ACC.2015.7172253","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172253","","Computational modeling;Data models;Machine learning;Noise reduction;Support vector machines;Testing;Training","flue gases;process control;regression analysis;sensors;support vector machines","deep learning;denoising auto-encoders;flue gasses;industrial process control;oxygen-content;robust high-level feature representation;soft sensor modeling;support vector regression","","0","","25","","","1-3 July 2015","","IEEE","IEEE Conference Publications"
"Active semi-supervised affinity propagation clustering algorithm based on pair-wise constraints","Lei Qi; Yu Huiping; Wu Min","School of Information Science and Engineering, Central South University, Changsha, 410083, China","Proceeding of the 11th World Congress on Intelligent Control and Automation","20150305","2014","","","2304","2309","Pair-wise constraints are widely used in semi-supervised clustering to aid unsupervised learning, but traditional semi-supervised clustering algorithm lacks the ability to find the useful constraint information. This paper presents a semi-supervised affinity propagation(AP) clustering algorithm based on active learning, which can select informative pair-wise constraints to find constraint information that cannot be noticed by the clustering algorithm easily. The constraint information obtained with the active learning method is used to adjust the similarity matrix in the AP clustering algorithm and make it semi-supervised with side information. We compare our method with the AP clustering algorithm and K-means algorithm, both with constraints selected randomly. Experimental results on the UCI Machine Learning Repository indicate that the new clustering algorithm proposed in this paper can improve the clustering performance significantly.","","Electronic:978-1-4799-5825-2; POD:978-1-4799-5826-9; USB:978-1-4799-5824-5","10.1109/WCICA.2014.7053081","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053081","Active learning;Affinity propagation;Evaluation index;Pair-wise constraint","Accuracy;Availability;Clustering algorithms;Clustering methods;Indexes;Learning systems;Machine learning algorithms","constraint handling;learning (artificial intelligence);matrix algebra;pattern clustering","AP clustering algorithm;K-means algorithm;UCI machine learning repository;active semisupervised affinity propagation clustering algorithm;constraint information;pair-wise constraints;similarity matrix;unsupervised learning","","0","","21","","","June 29 2014-July 4 2014","","IEEE","IEEE Conference Publications"
"A comparative study of various feature selection techniques in high-dimensional data set to improve classification accuracy","K. P. Shroff; H. H. Maheta","Department of Information Technology, Dharmsinh Desai University, Nadiad, India","2015 International Conference on Computer Communication and Informatics (ICCCI)","20150824","2015","","","1","6","The performance of machine learning algorithm depends on features considered from the dataset. High dimensional dataset degrades the performance of learning algorithm as learning algorithm try to analyze and accommodate all the features. Feature selection technique is used as a pre-processing step to analyze and compress large data set. The main objective of feature selection technique is to identify relevant features and removes redundant features from high dimensional dataset. The main goal of feature selection technique is to reduce dimensions of dataset, improve classification accuracy, reduce computational cost, and better visualization of data. By considering only useful features, the performance of classification algorithm can be improved. To select reduced set of relevant features from set of all features, various search techniques such as complete search, random search and sequential search etc. are used. Each generated subset of features is validated using various evaluation techniques such as filter, wrapper, and hybrid approach. The main aim of any search technique is to generate optimal subset of features. A general methodology of feature selection process is summarized in this paper on the basis of search and evaluation techniques. In this paper, we provide a comprehensive review of the recent developments in feature selection techniques. Classification with appropriate feature selection technique has shown better performance in the field of machine learning.","","Electronic:978-1-4799-6805-3; POD:978-1-4799-6806-0","10.1109/ICCCI.2015.7218098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7218098","Classification;Evaluation Techniques;Feature Selection;High dimensional datasets;Search Techniques","Accuracy;Classification algorithms;Filtering algorithms;Genetic algorithms;Machine learning algorithms;Search problems;Time complexity","data visualisation;feature selection;learning (artificial intelligence);pattern classification","classification accuracy;data visualization;feature selection techniques;high dimensional dataset;high-dimensional data set;machine learning algorithm","","0","","45","","","8-10 Jan. 2015","","IEEE","IEEE Conference Publications"
"DBSCAN on Resilient Distributed Datasets","I. Cordova; T. S. Moh","Department of Computer Science, San Jos&#x00E9; State University, California, U.S.A.","2015 International Conference on High Performance Computing & Simulation (HPCS)","20150903","2015","","","531","540","DBSCAN is a well-known density-based data clustering algorithm that is widely used due to its ability to find arbitrarily shaped clusters in noisy data. However, DBSCAN is hard to scale which limits its utility when working with large data sets. Resilient Distributed Datasets (RDDs), on the other hand, are a fast data-processing abstraction created explicitly for in-memory computation of large data sets. This paper presents a new algorithm based on DBSCAN using the Resilient Distributed Datasets approach: RDD-DBSCAN. RDD-DBSCAN overcomes the scalability limitations of the traditional DBSCAN algorithm by operating in a fully distributed fashion. The paper also evaluates an implementation of RDD-DBSCAN using Apache Spark, the official RDD implementation.","","CD-ROM:978-1-4673-7811-6; Electronic:978-1-4673-7813-0; POD:978-1-4673-7814-7","10.1109/HPCSim.2015.7237086","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237086","Apache Spark;DBSCAN;MapReduce;Resilient Distributed Datasets;data clustering;data partition;parallel systems","Clustering algorithms;Distributed databases;Machine learning algorithms;Noise;Partitioning algorithms;Prediction algorithms;Sparks","data handling;distributed processing;pattern clustering","Apache Spark;RDD-DBSCAN algorithm;arbitrarily shaped clusters;data-processing abstraction;density-based data clustering algorithm;in-memory computation;official RDD implementation;resilient distributed datasets approach","","2","","19","","","20-24 July 2015","","IEEE","IEEE Conference Publications"
"Untangling fine-grained code changes","M. Dias; A. Bacchelli; G. Gousios; D. Cassou; S. Ducasse","RMoD Inria Lille-Nord Europe, University of Lille ? CRIStAL, France","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","20150409","2015","","","341","350","After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.","1534-5351;15345351","Electronic:978-1-4799-8469-5; POD:978-1-4799-8470-1","10.1109/SANER.2015.7081844","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081844","","Clustering algorithms;Java;Machine learning algorithms;Reliability;Software;Testing;Training","software maintenance","EpiceaUntangler;fine-grained code change information;fine-grained code change untangling;version control system","","8","","28","","","2-6 March 2015","","IEEE","IEEE Conference Publications"
"Modified K-means algorithm for automatic stimation of number of clusters using advanced visual assessment of cluster tendency","D. Sharmilarani; N. Kousika; G. Komarasamy","Dept of CSE Sri Krishna College of Engg & Tech, Coimbatore, India","2014 IEEE 8th International Conference on Intelligent Systems and Control (ISCO)","20150511","2014","","","236","239","One of the major problems in cluster analysis is the determination of the number of clusters in unlabeled data, which is a basic input for most clustering algorithms. In our paper, we investigate a new method for automatically estimating the number of clusters in unlabeled data sets, which is based on an existing algorithm for Spectral Visual Assessment of Cluster Tendency (SpecVAT) of a data set, using several common image and signal processing techniques. Its basic steps include 1) generating a VAT image of an input dissimilarity matrix, 2) Constructing Laplacian matrix 3) Normalize the rows and 4) Apply SpecVAT. Our new method is nearly “automatic,” depending on just one easy-to-set parameter. In this paper we propose direct visual validation method and divergence matrix for finding the automatic clustering. The experimental result shows that the proposed algorithm is much better than the other algorithms.","","Electronic:978-1-4799-3837-7; POD:978-1-4799-3838-4","10.1109/ISCO.2014.7103951","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103951","Cluster;SpecVAT;divergence matrix","Algorithm design and analysis;Clustering algorithms;Laplace equations;Machine learning algorithms;Partitioning algorithms;Signal processing algorithms;Visualization","estimation theory;matrix algebra;pattern clustering;statistical analysis","Laplacian matrix;SpecVAT;cluster analysis;cluster number estimation;direct visual validation method;divergence matrix;image processing;input dissimilarity matrix;k-means algorithm;signal processing;spectral visual assessment of cluster tendency","","0","","18","","","10-11 Jan. 2014","","IEEE","IEEE Conference Publications"
"Opinion Mining and Summarization of Hotel Reviews","V. B. Raut; D. D. Londhe","Dept. of Inf. Technol., Pune Inst. of Comput. Technol., Pune, India","2014 International Conference on Computational Intelligence and Communication Networks","20150326","2014","","","556","559","Everyday many users purchases product, book travel tickets, buy goods and services through web. Users also share their views about product, hotel, news, and topic on web in the form of reviews, blogs, comments etc. Many users read review information given on web to take decisions such as buying products, watching movie, going to restaurant etc. Reviews contain user's opinion about product, event or topic. It is difficult for web users to read and understand contents from large number of reviews. Important and useful information can be extracted from reviews through opinion mining and summarization process. We presented machine learning and Senti Word Net based method for opinion mining from hotel reviews and sentence relevance score based method for opinion summarization of hotel reviews. We obtained about 87% of accuracy of hotel review classification as positive or negative review by machine learning method. The classified and summarized hotel review information helps web users to understand review contents easily in a short time.","","Electronic:978-1-4799-6929-6; POD:978-1-4799-6930-2","10.1109/CICN.2014.126","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065546","Opinion Mining;Opinion Summarization;Text Mining","Accuracy;Classification algorithms;Data mining;Information technology;Machine learning algorithms;Motion pictures;Sentiment analysis","Web sites;consumer behaviour;data mining;hotel industry;learning (artificial intelligence);natural language processing;pattern classification;text analysis","SentiWordNet based method;Web users user opinion machine learning;blogs;hotel review classification;hotel reviews summarization;opinion mining;opinion summarization;review contents;review information;sentence relevance score based method;travel tickets","","1","","18","","","14-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"Single image main objects extraction via stacked sparse auto-encoders using sharpness information","Y. Sun; Z. Zhang; R. Wu; W. Wang; X. Ding; Y. Huang","Department of Communication Engineering, Xiamen University, Xiamen, China","2014 International Conference on Mechatronics and Control (ICMC)","20150903","2014","","","1953","1956","Extracting main object from photos is prerequisite for image processing and semantic image understanding in many areas especially in multimedia signal processing at internet. So far, either human interaction in single image or sequence image frames are required for the extraction and most of them still rely on hand-crafted features. In contrast, the proposed work cast the human boundary detection in the daily photos as a patch-level binary classification, where the features are learned directly and automatically from the raw pictures and corresponding sharpness information via a stacked sparse autoencoder model. The experiments on the figure database from Baidu have demonstrated that the proposed method is able to extract the human boundary in single image without any human interaction, even in the various complex backgrounds.","","Electronic:978-1-4799-2538-4; POD:978-1-4799-2539-1","10.1109/ICMC.2014.7231904","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7231904","","Computer vision;Data mining;Feature extraction;Logistics;Machine learning;Testing;Training","Internet;feature extraction;image classification;image coding;image sequences","Baidu;Internet;human boundary extraction;image processing;multimedia signal processing;patch-level binary classification;semantic image understanding;sequence image frames;single image main object extraction;stacked sparse autoencoders","","1","","18","","","3-5 July 2014","","IEEE","IEEE Conference Publications"
"Performance comparison of Word sense disambiguation approaches for Indian languages","M. R. Shree; Shambhavi B. R","Department of CSE, AIeMS, Bidadi, India","2015 IEEE International Advance Computing Conference (IACC)","20150713","2015","","","166","169","Natural Language Processing (NLP) involves many phases of which the significant one is Word-sense disambiguation (WSD). WSD includes the techniques of identifying a suitable meaning of words and sentences in a particular context by applying various computational procedures. WSD is an Artificial Intelligence problem that needs resolution for ambiguity of words. WSD is essential for many NLP applications like Machine Translation, Information Retrieval, Information Extraction and for many others. The WSD techniques are mainly categorized into knowledge-based approaches, Machine Learning based approaches and hybrid approaches. The assessment of WSD systems is discussed in this study and it includes comparisons of different WSD approaches in the context of Indian languages.","","CD-ROM:978-1-4799-8046-8; Electronic:978-1-4799-8047-5; POD:978-1-4799-8048-2","10.1109/IADCC.2015.7154691","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154691","Machine Translation;Natural Language Processing;Word Sense Disambiguation","Context;Dictionaries;Knowledge based systems;Machine learning algorithms;Natural language processing;Semantics;Training","knowledge based systems;learning (artificial intelligence);natural language processing","Indian languages;NLP;WSD;Word sense disambiguation approaches;artificial intelligence problem;hybrid approaches;knowledge-based approaches;machine learning based approaches;natural language processing;performance comparison","","0","","13","","","12-13 June 2015","","IEEE","IEEE Conference Publications"
"An Improved Approach for k-Means Using Parallel Processing","P. Swamy; M. M. Raghuwanshi; A. Gholghate","Dept. of Comput. Sci. & Eng., Rajiv Gandhi Coll. of Eng. & Res., Nagpur, India","2015 International Conference on Computing Communication Control and Automation","20150716","2015","","","358","361","Serial execution of K-means algorithm on large dataset takes more execution time and does not give accurate results. Parallel processing is one of the ways to improve the performance of K-Means algorithm. But the execution time and accuracy is largely dependent on selection of initial cluster centers. In this paper, parallel processing of K-Means is proposed using an initialization method to originate initial cluster centers, which not only reduces the execution time but also gives accurate results.","","Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0","10.1109/ICCUBEA.2015.75","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155867","K-Means;Parallel processing;Serial execution;accuracy;execution time;initial cluster centers;large dataset","Accuracy;Algorithm design and analysis;Clustering algorithms;Convergence;Machine learning algorithms;Mathematical model;Parallel processing","parallel processing;pattern classification;pattern clustering","execution time;initial cluster centers;initialization method;k-means algorithm;parallel processing","","0","","17","","","26-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"Audio recapture detection using deep learning","D. Luo; H. Wu; J. Huang","College of Information Engineering, Shenzhen University, Shenzhen, 518060, P.R. China, Shenzhen Key Laboratory of Media Security, Shenzhen, 518060, China","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","20150903","2015","","","478","482","Since the audio recapture can be used to assist audio splicing, it is important to identify whether a suspected audio recording is recaptured or not. However, few works on such detection have been reported. In this paper, we propose an method to detect the recaptured audio based on deep learning and we investigate two deep learning techniques, i.e., neural network with dropout method and stack auto-encoders (SAE). The waveform samples of audio frame is directly used as the input for the deep neural network. The experimental results show that error rate around 7.5% can be achieved, which indicates that our proposed method can successfully discriminate recaptured audio and original audio.","","Electronic:978-1-4799-1948-2; POD:978-1-4799-1949-9; USB:978-1-4799-1947-5","10.1109/ChinaSIP.2015.7230448","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230448","Audio recapture detection;Deep learning;Dropout;SAE","Artificial neural networks;Error analysis;Feature extraction;Machine learning;Speech;Training","audio coding;audio recording;neural nets;waveform analysis","SAE;audio recapture detection;audio recording;audio splicing;deep learning;deep neural network;dropout method;stack auto-encoders;waveform samples","","0","","23","","","12-15 July 2015","","IEEE","IEEE Conference Publications"
"Predictability of energy use in homes","D. Lachut; N. Banerjee; S. Rollins","Computer Science and Electrical Engineering, University of Maryland Baltimore County, Baltimore, Maryland","International Green Computing Conference","20150212","2014","","","1","10","Predictability of home energy usage forms the basis of many home energy management and demand-response systems. While existing studies focus on designing more accurate prediction algorithms, a comprehensive energy management solution requires a broad understanding of prediction accuracy at different granularities, for example appliance and home, as well as different time horizons, for example an hour, day, or week into the future. In this paper, we undertake an analysis of predictability of power draw of appliances and whole-home energy consumption at four different time horizons: an hour, a quarter-day, a day, and a week in the future. Our analysis presents two research contributions. Our first contribution is a diverse dataset, GreenHomes, that includes appliance power draw and whole-home energy consumption data from seven homes across three states in the United States over a two-year period. Our second and primary contribution is a set of insights into the predictability of home energy usage. We show that simple statistic-based algorithms perform as well as sophisticated machine learning algorithms and time-series based predictors. These simple algorithms can considerably reduce the computational need for large-scale predictive analysis of home energy data. We also show that appliance-level power draw is more predictable than whole-home energy consumption at shorter time horizons while home-level energy consumption is more predictable at longer time horizons. Finally, we show that there is large variation in predictability across homes. This variation may be attributed to home type and points to the need for personalized energy management systems.","","Electronic:978-1-4799-6177-1; POD:978-1-4799-6178-8","10.1109/IGCC.2014.7039146","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7039146","","Accuracy;Algorithm design and analysis;Energy consumption;Home appliances;Machine learning algorithms;Prediction algorithms;Support vector machines","demand side management;domestic appliances;energy management systems","GreenHomes;appliance power draw predictability;demand-response systems;energy management systems;home energy management;home energy usage predictability;large-scale predictive analysis;machine learning algorithms;statistic-based algorithms;time-series based predictors;whole-home energy consumption","","1","","38","","","3-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Pedestrian detection based on deep convolutional neural network with ensemble inference network","H. Fukui; T. Yamashita; Y. Yamauchi; H. Fujiyoshi; H. Murase","Chubu University, 1200 Matsumoto cho, Kasugai, Aichi, Japan","2015 IEEE Intelligent Vehicles Symposium (IV)","20150827","2015","","","223","228","Pedestrian detection is an active research topic for driving assistance systems. To install pedestrian detection in a regular vehicle, however, there is a need to reduce its cost and ensure high accuracy. Although many approaches have been developed, vision-based methods of pedestrian detection are best suited to these requirements. In this paper, we propose the methods based on Convolutional Neural Networks (CNN) that achieves high accuracy in various fields. To achieve such generalization, our CNN-based method introduces Random Dropout and Ensemble Inference Network (EIN) to the training and classification processes, respectively. Random Dropout selects units that have a flexible rate, instead of the fixed rate in conventional Dropout. EIN constructs multiple networks that have different structures in fully connected layers. The proposed methods achieves comparable performance to state-of-the-art methods, even though the structure of the proposed methods are considerably simpler.","1931-0587;19310587","Electronic:978-1-4673-7266-4; POD:978-1-4673-7267-1","10.1109/IVS.2015.7225690","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225690","","Accuracy;Benchmark testing;Convolution;Feature extraction;Machine learning;Robustness;Training","computer vision;driver information systems;inference mechanisms;neural nets;object detection;pedestrians","CNN-based method;classification process;deep convolutional neural network;driving assistance systems;ensemble inference network;pedestrian detection;random dropout;training process;vision-based methods","","1","","30","","","June 28 2015-July 1 2015","","IEEE","IEEE Conference Publications"
"Towards a web-based energy consumption forecasting platform","M. Taborda; J. Almeida; J. A. Oliveir-Lima; J. F. Martins","Dept. of Electrical Engineering (DEE), Universidade Nova de Lisboa (UNL), 2829-516 Caparica, Portugal","2015 9th International Conference on Compatibility and Power Electronics (CPE)","20150903","2015","","","577","580","Nowadays, energy efficiency is a major issue in modern day societies, due to increasing worldwide energy demands. Having this in mind several different solutions are emerging with the purpose of helping the control of energy in all possible ways, whether at its starting pipeline, i.e. where the energy is produced, at the middle pipeline, i.e. where and how the energy is transported, or finally, at the end of the pipeline, where the energy is consumed. At the moment, most solutions are addressing the problem at the end of the pipeline, because it is easier to control the consumption, than it is to alter all of the parts that compose an energy system. Thus, the solution proposed in this paper refers to the development of a platform capable of providing energy prediction on buildings, whether the building is commercial, industrial or residential. The platform will be composed of prediction algorithms, supported by the use of computational intelligence methods such as Artificial Neural Networks (ANN). The main objective of this platform is to use datasets previously recorded of the building energy consumption, along with a number of other parameters, to accurately predict the energy consumption of a given day, so that future, and pondered actions can be taken in order to provide a suitable response for that given day. Technically, the platform itself will be based on standard online remote communication protocols, and this platform is to be integrated with, amongst other equipment, energy meters.","2166-9538;21669538","Electronic:978-1-4799-6301-0; POD:978-1-4799-6302-7; USB:978-1-4799-6300-3","10.1109/CPE.2015.7231140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7231140","Energy efficiency;artificial neural network;computational intelligence methods;energy consumption;energy meter;energy prediction;environmental impact;online remote communication protocols","Artificial neural networks;Buildings;Energy consumption;Forecasting;Machine learning algorithms;Prediction algorithms","Web sites;building management systems;energy conservation;energy consumption;energy management systems;load forecasting;neural nets;power engineering computing;power meters;protocols","ANN;Web-based energy consumption forecasting;artificial neural networks;building energy consumption;computational intelligence methods;control of energy;energy demands;energy efficiency;energy meters;energy prediction;energy system;online remote communication protocols;prediction algorithms","","0","","11","","","24-26 June 2015","","IEEE","IEEE Conference Publications"
"Research on public opinion based on Big Data","S. Shang; M. Shi; W. Shang; Z. Hong","School of Computer Science, Communication University of China, Beijing, China","2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)","20150727","2015","","","559","562","Public opinion is the people's response for social phenomena, issues, hot topics, attitudes, emotions, and so on. It reflects the focus problems of the current time of the society. By analyzing the public opinion, we can infer what will happen in the next time, and give better decision support for governments and businesses. Big Data technology is becoming a powerful data analyzing tools for massive data in recent years. Hadoop is an open source massive data processing platform based on Big Data. Mahout is a data mining algorithms' set based on Hadoop, which is designed for processing large-scale and complex data. In most instances, the public opinion information contains many text messages. For many traditional text mining algorithms, it is almost impossible to handle high dimensional data concerns large-volume and complex data sets. Hence, this paper uses Mahout text mining algorithms to process public opinion information.","","Electronic:978-1-4799-8679-8; POD:978-1-4799-8680-4","10.1109/ICIS.2015.7166655","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166655","Big Data;Hadoop;Mahout;data mining;public opinion","Algorithm design and analysis;Big data;Classification algorithms;Clustering algorithms;Data mining;Internet;Machine learning algorithms","Big Data;Internet;data analysis;data mining;text analysis","Big Data technology;Hadoop;Mahout text mining algorithms;data analyzing tools;data mining algorithm;open source massive data processing platform;public opinion","","0","","12","","","June 28 2015-July 1 2015","","IEEE","IEEE Conference Publications"
"RSSI localization with DB-Assisted Least Error algorithm","Jongtack Jung; Kangho Kim; Seungho Yoo; Mungyu Bae; Suk Kyu Lee; Hwangnam Kim","School of Electrical Engineering, Korea University, Seoul, Korea","2015 Seventh International Conference on Ubiquitous and Future Networks","20150810","2015","","","338","343","RSSI (Received Signal Strength Indication) localization techniques using Wi-Fi presents substantial advantages compared to others. They are light weight both in terms of computation and energy consumption. RSSI localization techniques are mostly used indoors, where many APs (Access Points) are present and no GPS is available. Recently, APs are getting deployed outdoors as well, and urban canyon phenomenon degrades the capability of GPS localization even in outdoor environments, which makes RSSI localization techniques attractive as an outdoor localization solution as well. The downside of RSSI localization is that it is polarized, which means it has either high performance and economic cost or low cost and poor accuracy. Both cases are inadequate for a general deployment; high-cost algorithms can only be deployed in heavily populated area for cost feasibility and the accuracy of low-cost algorithms is nowhere near credible. In this paper, we propose a range-based RSSI localization algorithm that has reasonable accuracy yet has very low cost. The proposed algorithm consists of DB-assistance, ration base algorithm, and an elementary machine learning algorithm. This helps achieving the qualities that can provide a feasible RSSI localization solution that can be employed in a much wider area.","2165-8528;21658528","Electronic:978-1-4799-8993-5; POD:978-1-4799-8994-2; USB:978-1-4799-8992-8","10.1109/ICUFN.2015.7182561","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182561","","Accuracy;Buildings;Databases;IEEE 802.11 Standard;Least squares methods;Machine learning algorithms;Mathematical model","Global Positioning System;RSSI;learning (artificial intelligence);least squares approximations;telecommunication power management;wireless LAN","DB assisted least error algorithm;GPS localization capability;RSSI localization technique;Wi-Fi;elementary machine learning algorithm;energy consumption;received signal strength indication localization technique;urban canyon phenomenon","","0","","16","","","7-10 July 2015","","IEEE","IEEE Conference Publications"
"Generation of synthetic structural magnetic resonance images for deep learning pre-training","E. Castro; A. Ulloa; S. M. Plis; J. A. Turner; V. D. Calhoun","The Mind Research Network, Albuquerque, NM","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","1057","1060","Deep learning methods have significantly improved classification accuracy in different areas such as speech, object and text recognition. However, this field has only began to be explored in the brain imaging field, which differs from other fields in terms of the amount of data available, its data dimensionality and other factors. This paper proposes a methodology to generate an extensive synthetic structural magnetic resonance imaging (sMRI) dataset to be used at the pre-training stage of a shallow network model to address the issue of having a limited amount of data available. Our results show that by extending our dataset using 5,000 synthetic sMRI volumes for pretraining, which accounts to approximately 10 times the size of the original dataset, we can obtain a 5% average improvement on classification results compared to the regular approach on a schizophrenia dataset. While the use of synthetic sMRI data for pre-training has only been tested on a shallow network, this can be readily applied to deeper networks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7164053","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164053","deep learning;pretraining;schizophrenia;simulation;structural MRI","Data models;Machine learning;Magnetic resonance imaging;Neuroimaging;Probability density function;Support vector machines;Training","biomedical MRI;brain;image classification;learning (artificial intelligence);medical disorders;medical image processing","brain imaging field;classification accuracy;deep learning pretraining method;deeper networks;magnetic resonance imaging;object recognition;schizophrenia dataset;shallow network model;speech recognition;structural MRI;synthetic sMR image;synthetic sMRI volume;text recognition","","0","","15","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Learning Summaries of Recursive Functions","Y. F. Chen; B. Y. Wang; K. C. Yang","Inst. of Inf. Sci., Taipei, Taiwan","2014 21st Asia-Pacific Software Engineering Conference","20150423","2014","1","","303","310","We describe a learning-based approach for verifying recursive functions. The Boolean formula learning algorithm CDNF is used to automatically infer function summaries for recursive functions. In contrast to traditional iterative fix point computation-based approaches, ours can quickly guess summaries and verify purported summaries. When purported summaries are incorrect, the learning algorithm refines them by posing queries. We solve examples that are unattainable by a mature model checker for recursive programs.","1530-1362;15301362","Electronic:978-1-4799-7426-9; POD:978-1-4799-7427-6","10.1109/APSEC.2014.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091324","","Algorithm design and analysis;Approximation algorithms;Computational modeling;Encoding;Machine learning algorithms;Optimization;Semantics","Boolean functions;formal verification;iterative methods;learning (artificial intelligence);program control structures","Boolean formula learning algorithm;iterative fix point computation-based approach;learning-based approach;model checker;recursive functions;recursive programs","","0","","25","","","1-4 Dec. 2014","","IEEE","IEEE Conference Publications"
"NLP based sentiment analysis on Twitter data using ensemble classifiers","M. Kanakaraj; R. M. R. Guddeti","Dept. of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore 575025 India","2015 3rd International Conference on Signal Processing, Communication and Networking (ICSCN)","20150827","2015","","","1","5","Most sentiment analysis systems use bag-of-words approach for mining sentiments from the online reviews and social media data. Rather considering the whole sentence/ paragraph for analysis, the bag-of-words approach considers only individual words and their count as the feature vectors. This may mislead the classification algorithm especially when used for problems like sentiment classification. Traditional machine learning algorithms like Naive Bayes, Maximum Entropy, SVM etc. are widely used to solve the classification problems. These machine learning algorithms often suffer from biasness towards a particular class. In this paper, we propose Natural Language (NLP) based approach to enhance the sentiment classification by adding semantics in feature vectors and thereby using ensemble methods for classification. Adding semantically similar words and context-sense identities to the feature vectors will increase the accuracy of prediction. Experiments conducted demonstrate that the semantics based feature vector with ensemble classifier outperforms the traditional bag-of-words approach with single machine learning classifier by 3-5%.","","CD-ROM:978-1-4673-6822-3; Electronic:978-1-4673-6823-0; POD:978-1-4673-6824-7","10.1109/ICSCN.2015.7219856","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219856","Ensemble classifier;Semantic;Sentiment Analysis;Social Network Analysis;Word Sense Disambiguation","Machine learning algorithms;Semantics;Sentiment analysis;Signal processing algorithms;Support vector machines;Training;Twitter","Internet;data mining;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis","NLP;Twitter data;bag-of-words approach;ensemble method;feature vector;machine learning algorithm;natural language processing;online review;opinion mining;sentiment analysis;sentiment classification;social media data","","0","","10","","","26-28 March 2015","","IEEE","IEEE Conference Publications"
"A comparative study of α-Divergence based NMF techniques for FMRI analysis","S. Ferdowsi; V. Abolghasemi; S. Sanei","NICE Research Group, Faculty of Engineering and Physical Sciences, University of Surrey, UK","2011 19th European Signal Processing Conference","20150402","2011","","","71","75","The main objective of fMRI analysis methods is to detect the Blood Oxygenation Level Dependent (BOLD) from fMRI sequences. Algorithms which are discussed here are known as data-driven methods. The main advantage of these types of algorithms over data-based methods is that there is no need for prior information. Here, we focus on one of the powerful matrix factorization algorithms which has been recently applied to fMRI called Non-negative Matrix Factorization (NMF) [1]. There exist many different NMF techniques in the literature and no comprehensive assessment of their performances on fMRI data has been reported. So, in this work the performance in terms of BOLD detection, using a-Deivergence based methods are investigated and then compared with the commonly used Euclidean distance based method. The aim is to highlight the advantages that such techniques can have in practice. We explored the performance of these techniques for two types of real fMRI and also synthetic data. We observed that the a-Divergence based methods are also applicable to fMRI data and reveal acceptable performance.","2076-1465;20761465","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7074261","","Cost function;Data analysis;Euclidean distance;Machine learning algorithms;Signal processing;Signal processing algorithms;Visualization","biomedical MRI;blood;image sequences;matrix decomposition;medical image processing","α-divergence based NMF techniques;BOLD detection;Euclidean distance based method;blood oxygenation level dependent;data-based methods;data-driven methods;fMRI analysis;fMRI sequences;matrix factorization algorithms;nonnegative matrix factorization","","0","","18","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Classification algorithms on a large continuous random dataset using rapid miner tool","P. Sharma; D. Singh; A. Singh","Department of Computer Science and Engineering, BU, Bhopal, India","2015 2nd International Conference on Electronics and Communication Systems (ICECS)","20150618","2015","","","704","709","Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. Now a day's large amount of data is generated, that need to be analyse, and pattern have to be extracted from that to get some knowledge. Classification is a supervised machine learning task which builds a model from labelled training data. The model is used for determining the class; there are many types of classification algorithms such as tree-based algorithms (C4.5 decision tree, j48 decision tree etc.), naive Bayes and many more. These classification algorithms have their own pros and cons, depending on many factors such as the characteristics of the data. We can measure the classification performance by using several metrics, such as accuracy, precision, classification error and kappa on the testing data. We have used a random dataset in a rapid miner tool for the classification. Stratified sampling is used in different classifier such as J48, C4.5 and naïve Bayes. We analysed the result of the classifier using the randomly generated dataset and without random dataset.","","Electronic:978-1-4799-7225-8; POD:978-1-4799-7226-5","10.1109/ECS.2015.7125003","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7125003","C4.5;Classification;J48;Large Data;Naïve Bayes;Rapid Miner","Accuracy;Classification algorithms;Data mining;Data models;Decision trees;Machine learning algorithms;Training","data mining;learning (artificial intelligence);pattern classification;sampling methods;very large databases","C4.5 classifier;J48 classifier;classification algorithms;classification performance;data characteristics;data mining;large continuous random dataset;large databases;naïve Bayes classifier;rapid miner tool;stratified sampling;supervised machine learning","","0","","16","","","26-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"Deep learning with shallow architecture for image classification","A. ElAdel; R. Ejbali; M. Zaied; C. Ben Amar","Research Group in Intelligent Machines, National School of Engineers of sfax, B.P 1173, Tunisia","2015 International Conference on High Performance Computing & Simulation (HPCS)","20150903","2015","","","408","412","This paper presents a new scheme for image classification. The proposed scheme depicts a shallow architecture of Convolutional Neural Network (CNN) providing deep learning: For each image, we calculated the connection weights between the input layer and the hidden layer based on MultiResolution Analysis (MRA) at different levels of abstraction. Then, we selected the best features, representing well each class of images, with their corresponding weights using Adaboost algorithm. These weights are used as the connection weights between the hidden layer and the output layer, and will be used in the test phase to classify a given query image. The proposed approach was tested on different datasets and the obtained results prove the efficiency and the speed of the proposed approach.","","CD-ROM:978-1-4673-7811-6; Electronic:978-1-4673-7813-0; POD:978-1-4673-7814-7","10.1109/HPCSim.2015.7237069","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237069","Adaboost;deep learning;image classification;multiresolution analysis;wavelet","Algorithm design and analysis;Computer architecture;Databases;Feature extraction;Machine learning;Multiresolution analysis;Neural networks","image classification;image representation;image resolution;image retrieval;learning (artificial intelligence);neural net architecture","Adaboost algorithm;CNN;MRA;abstraction levels;connection weights;convolutional neural network;deep-learning;feature selection;hidden layer;image representation;input layer;multiresolution analysis;query image classification;shallow-architecture","","1","","22","","","20-24 July 2015","","IEEE","IEEE Conference Publications"
"Automatic detection of cerebral microbleeds via deep learning based 3D feature representation","H. Chen; L. Yu; Q. Dou; L. Shi; V. C. T. Mok; P. A. Heng","Department of Computer Science and Engineering, The Chinese University of Hong Kong","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","764","767","Clinical identification and rating of the cerebral microbleeds (CMBs) are important in vascular diseases and dementia diagnosis. However, manual labeling is time-consuming with low reproducibility. In this paper, we present an automatic method via deep learning based 3D feature representation, which solves this detection problem with three steps: candidates localization with high sensitivity, feature representation, and precise classification for reducing false positives. Different from previous methods by exploiting low-level features, e.g., shape features and intensity values, we utilize the deep learning based high-level feature representation. Experimental results validate the efficacy of our approach, which outperforms other methods by a large margin with a high sensitivity while significantly reducing false positives per subject.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163984","cerebral microbleeds;deep learning;feature representation;object detection","Biomedical imaging;Feature extraction;Machine learning;Radio frequency;Sensitivity;Three-dimensional displays;Training","biomedical MRI;blood vessels;brain;diseases;image representation;learning (artificial intelligence);medical image processing;object detection","CMB;automatic detection;candidate localization;cerebral microbleed rating;clinical identification;deep learning based 3D feature representation;deep learning based high-level feature representation;dementia diagnosis;detection problem;false positive reduction;intensity values;low-level features;manual labeling;precise classification;shape features;vascular diseases","","2","","13","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Scalable graph exploration and visualization: Sensemaking challenges and opportunities","R. Pienta; J. Abello; M. Kahng; D. H. Chau","Georgia Institute of Technology","2015 International Conference on Big Data and Smart Computing (BIGCOMP)","20150402","2015","","","271","278","Making sense of large graph datasets is a fundamental and challenging process that advances science, education and technology. We survey research on graph exploration and visualization approaches aimed at addressing this challenge. Different from existing surveys, our investigation highlights approaches that have strong potential in handling large graphs, algorithmically, visually, or interactively; we also explicitly connect relevant works from multiple research fields - data mining, machine learning, human-computer ineraction, information visualization, information retrieval, and recommender systems - to underline their parallel and complementary contributions to graph sensemaking. We ground our discussion in sensemaking research; we propose a new graph sensemaking hierarchy that categorizes tools and techniques based on how they operate on the graph data (e.g., local vs global). We summarize and compare their strengths and weaknesses, and highlight open challenges. We conclude with future research directions for graph sensemaking.","2375-933X;2375933X","Electronic:978-1-4799-7303-3; POD:978-1-4799-7304-0; USB:978-1-4799-7302-6","10.1109/35021BIGCOMP.2015.7072812","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072812","","Algorithm design and analysis;Data mining;Data visualization;Machine learning algorithms;Pattern matching;Scalability;Visualization","data visualisation;graph theory;mathematics computing","data mining;graph datasets;graph handling;graph sensemaking hierarchy;human-computer ineraction;information retrieval;information visualization;machine learning;recommender systems;scalable graph exploration approach;scalable graph visualization approach","","4","","86","","","9-11 Feb. 2015","","IEEE","IEEE Conference Publications"
"The evidence of phonological dyslexia and technology intervention: Preliminary study in special schools in South Africa","D. M. Ndombo; S. Ojo; I. O. Osunmakinde","Department of Computer Science, Tshwane University of Technology, Private BagX680, Pretoria, 0001 South Africa","2013 Pan African International Conference on Information Science, Computing and Telecommunications (PACT)","20150305","2013","","","1","6","Teaching learners with learning disabilities in general and dyslexia in particular has become a complex task in the education sector across the globe; it is even more complex when the language of education is not the first language spoken at home. Learners with learning disabilities, especially those with dyslexia, are facing greater challenge in reading and writing in schools, because they have acquired insufficient phonological awareness. The aim of this study was to investigate and examine the level of phonological skills in learners with dyslexia in primary special schools in Tshwane municipality in Gauteng province in South Africa. Our study sample group consisted of 24 learners aged between 10 and 19 years, who most of the time speak their mother tongues, but use English as the language of education. Different literacy skills assessments were carried out in the following phonological awareness components: syllable awareness, onset-rime awareness and phoneme awareness. Our findings show that there is some evidence of dyslexia in our target group and their learning levels were below average for their age. Because of this evidence we therefore propose a new integrated assistive model that is able to improve the phonological skills of dyslexic learners.","","Electronic:978-1-4799-6656-1; POD:978-1-4799-6657-8","10.1109/SCAT.2013.7055079","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7055079","Dyslexia;Education;Intelligent Assistive System;Learners;Special School;phonological awareness","Educational institutions;Hidden Markov models;Information science;Machine learning algorithms;Visualization;Writing","assisted living;computer aided instruction;educational institutions;handicapped aids;natural language processing","English language;South African special school;dyslexic learner;learning disability;onset-rime awareness;phoneme awareness;phonological skill;syllable awareness;technology intervention","","0","","14","","","13-17 July 2013","","IEEE","IEEE Conference Publications"
"Improve word sense disambiguation by proposing a pruning method for optimizing conceptual density's contexts","A. Golkar; S. Jafari; M. J. Golkar; S. M. Sadegh Dashti; S. M. Fakhrahmad","Department of Computer Science & IT Shiraz University Shiraz, Iran","2015 The International Symposium on Artificial Intelligence and Signal Processing (AISP)","20150615","2015","","","305","309","In this paper, the role of nouns in reducing the conceptual density of contexts has been examined. A new method is proposed to identify and prune nouns with negative impact on conceptual density of contexts. In the proposed method, a fitness function is offered; a fitness degree is assigned to unambiguous nouns sense within the context. Using the mean fitness degree of unambiguous nouns' sense, a threshold is produced for that context. This threshold is then used as a measure to prune the sense of nouns with lower fitness degree that reduces the conceptual density of the context. Finally, by implementing this method on the contexts produced by conceptual density method, all contexts will be optimized significantly; this significantly increases the accuracy of disambiguation.","","Electronic:978-1-4799-8818-1; POD:978-1-4799-8819-8","10.1109/AISP.2015.7123502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123502","Conceptual density;context;fitness degree;fitness function;optimization","Accuracy;Computer science;Context;Knowledge based systems;Machine learning algorithms;Presses;Semantics","natural language processing;optimisation","conceptual density context optimization;conceptual density reduction;fitness function;mean fitness degree;pruning method;word sense disambiguation","","0","","14","","","3-5 March 2015","","IEEE","IEEE Conference Publications"
"How much is your spare room worth?","D. Hill","","IEEE Spectrum","20150831","2015","52","9","32","58","How much should you charge someone to live in your house? Or how much would you pay to live in someone else's house? Would you pay more or less for a planned vacation or for a spur-of-the-moment getaway? Answering these questions isn't easy. And the struggle to do so, my colleagues and I discovered, was preventing potential rentals from getting listed on our site-Airbnb, the company that matches available rooms, apartments, and houses with people who want to book them. In focus groups, we watched people go through the process of listing their properties on our site-and get stumped when they came to the price field. Many would take a look at what their neighbors were charging and pick a comparable price; this involved opening a lot of tabs in their browsers and figuring out which listings were similar to theirs. Some people had a goal in mind before they signed up, maybe to make a little extra money to help pay the mortgage or defray the costs of a vacation. So they set a price that would help them meet that goal without considering the real market value of their listing. And some people, unfortunately, just gave up. Clearly, Airbnb needed to offer people a better way-an automated source of pricing information to help hosts come to a decision. That's why we started building pricing tools in 2012 and have been working to make them better ever since. This June, we released our latest improvements. We started doing dynamic pricing- that is, offering new price tips daily based on changing market conditions. We tweaked our general pricing algorithms to consider some unusual, even surprising characteristics of listings. And we've added what we think is a unique approach to machine learning that lets our system not only learn from its own experience but also take advantage of a little human intuition when necessary.","0018-9235;00189235","","10.1109/MSPEC.2015.7226609","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226609","","Buildings;Heuristic algorithms;Machine learning algorithms;Pricing;Urban areas","learning (artificial intelligence);marketing data processing;pricing","Airbnb;automated pricing information source;dynamic pricing;machine learning;market conditions;pricing algorithms;pricing tools","","1","","","","","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"A secondary screen architecture to accurately capture viewers' interactions in an iTV environment","R. E. V. d. S. Rosa; L. C. Cordeiro; V. F. d. Lucena Junior","Graduate Program in Electrical Engineering, Federal University of Minas Gerais, Av. Antnio Carlos 6627, 31270-901, Belo Horizonte, MG, Brazil","2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE)","20150205","2014","","","264","265","Advances in TV technology have enabled viewers to actively interact with the TV through interactive applications instead of just passively watching TV. Typically, a conventional remote control is shared by many viewers and it is difficult to accurately capture the individual interaction data from each viewer. It is also difficult to identify the context in which the interactions occur, e.g., to manage precisely who is present in the environment and what is being watched on TV. In this paper, it is presented a novel architecture that facilitates the capture of viewers' individual interactions and contextual data in shared TV environments. This architecture uses personal devices as secondary screen devices with the aim of identifying viewers and capturing their interactions with interactive TV devices. A work in progress implementation of this architecture is briefly reported as an audio-visual content rating system for interactive TV.","2378-8143;23788143","Electronic:978-1-4799-5145-1; POD:978-1-4799-5146-8","10.1109/GCCE.2014.7031328","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7031328","","Computer architecture;Context;Digital TV;Machine learning algorithms;Object recognition;Standards","audio-visual systems;interactive television","TV technology;audio-visual content rating system;contextual data;iTV environment;interactive television device;remote control;secondary screen device architecture;viewer interaction capture","","0","","6","","","7-10 Oct. 2014","","IEEE","IEEE Conference Publications"
"Methods to Segment Hard Inclusions in Soft Tissue During Autonomous Robotic Palpation","K. A. Nichols; A. M. Okamura","Department of Mechanical Engineering, Stanford University, Stanford, CA, USA","IEEE Transactions on Robotics","20150402","2015","31","2","344","354","Localizing tumors and measuring tissue mechanical properties can aid in surgical planning and evaluating the progression of disease. In this paper, autonomous robotic palpation with supervised machine learning algorithms enables mechanical localization and segmentation of stiff inclusions in artificial tissue. Elastography generates training data for the learning algorithms, providing a noninvasive, inclusion-specific characterization of tissue mechanics. Once an embedded hard inclusion was identified in the elastographic image, Gaussian discriminant analysis generated a classifier to threshold stiffness values acquired from autonomous robotic palpation. This classifier was later used to classify newly acquired points as either part of the inclusion or surrounding soft tissue. An expectation-maximization algorithm with underlying Markov random fields improved this initial classifier over successive iterations to better approximate the boundary of the inclusion. Results demonstrate robustness with respect to inclusion shape, size, and the initial classifier value. For three trials segmenting a cubic inclusion, sensitivity was above 0.95 and specificity was above 0.92.","1552-3098;15523098","","10.1109/TRO.2015.2402531","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054550","Learning and adaptive systems;medical robots and systems","Algorithm design and analysis;Biological tissues;Image segmentation;Machine learning algorithms;Robots;Surgery;Tumors","Gaussian processes;Markov processes;approximation theory;expectation-maximisation algorithm;image classification;image segmentation;learning (artificial intelligence);medical image processing;medical robotics;surgery;tumours","Gaussian discriminant analysis;Markov random field;artificial tissue;autonomous robotic palpation;boundary approximation;elastographic image classifier;expectation-maximization algorithm;soft tissue inclusion;supervised machine learning algorithm;surgical planning;threshold stiffness value;tumor segmentation","","7","","33","","20150304","April 2015","","IEEE","IEEE Journals & Magazines"
"A Recommendation System Combining LDA and Collaborative Filtering Method for Scenic Spot","S. Xie; Y. Feng","Hangzhou On Honest Tech. Co., Ltd., Hangzhou, China","2015 2nd International Conference on Information Science and Control Engineering","20150611","2015","","","67","71","Researchers have long sought to find an effective and straightforward method to bridge the gap between us and big data. Especially during the big data era, how to find the needed information with rapid speed and exact result has become the central concerns of the internet users. This paper focuses on exploring the valuable data in UGC (User Generated Content), and recommending useful information to specified users. To achieve this goal, we model the social network, and then the LDA (Linear Discriminant Analysis), PCA (Principal Component Analysis) and KNN (K-Nearest Neighbour) algorithms are adopted to calculate the recommendation items. Our algorithm avoids the disadvantages of the common collaborative filtering algorithm that only behaviors is considered but without considering the behaviour results, thus our method effectively improves the accuracy of the recommendation system. Experimental results show that our algorithm improves the accuracy comparing with the CF algorithms.","","Electronic:978-1-4673-6850-6; POD:978-1-4673-6851-3","10.1109/ICISCE.2015.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120564","K-Nearest Neighbour;Recommendation System for Scenic Spot;SVD;Topic Model","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Collaboration;Filtering;Machine learning algorithms;Matrix decomposition","collaborative filtering;learning (artificial intelligence);principal component analysis;recommender systems;social networking (online)","CF algorithms;K-nearest neighbor algorithms;KNN;LDA;PCA;Scenic Spot;UGC;collaborative filtering method;linear discriminant analysis;principal component analysis;recommendation system;social network;user generated content","","0","","20","","","24-26 April 2015","","IEEE","IEEE Conference Publications"
"Exploring Behavioral Aspects of API Calls for Malware Identification and Categorization","D. Uppal; R. Sinha; V. Mehra; V. Jain","Dept. of Comput. Eng. & Inf. Technol., Gov. Eng. Coll., Ajmer, India","2014 International Conference on Computational Intelligence and Communication Networks","20150326","2014","","","824","828","Present day scenario shows a drastic increase in the growth of the malware. According to Kaspersky Security Lab report, India ranks seventh in offline threats and ninth in online threats caused by malware, among top ten countries of the world. Advancement in the evasion techniques like code obfuscation, packing, encryption or polymorphism help malware writers to avoid detection of their malwares by Anti-Virus Scanners (AVS), as AVS primarily fails to detect unknown malwares. In this paper we elucidate a malware detection method based on mining behavioral aspects of API calls, as extraction and interpretation of API calls can help in determining the behavior and functions of a program. We propose a feature selection algorithm to select unique and distinct APIs and then we have applied machine learning techniques for categorizing malicious and benign PE files.","","Electronic:978-1-4799-6929-6; POD:978-1-4799-6930-2","10.1109/CICN.2014.176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065596","API Call;Behavioral Aspects;Data mining;Malware;Portable Executable","Algorithm design and analysis;Classification algorithms;Feature extraction;Machine learning algorithms;Malware;Software;Software algorithms","application program interfaces;data mining;feature selection;invasive software;learning (artificial intelligence)","API call behavioral aspects;AVS;India;Kaspersky Security Lab report;antivirus scanners;benign PE files;code obfuscation;data mining;encryption;feature selection algorithm;machine learning techniques;malicious PE files;malware categorization;malware identification;offline threats;online threats;polymorphism","","2","","19","","","14-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"A novel ranking algorithm based on manifold learning for CBIR system","Yao Xiao; Shenglan Liu; Lin Feng; Xiuqi Hao","School of Computer Science and Technology, Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Liaoning, China","Proceeding of the 11th World Congress on Intelligent Control and Automation","20150305","2014","","","1002","1009","At present, most of image retrieval applications use Principal Component Analysis (PCA) algorithm to reduce the low-level features of images and rank the similarity of images based on graph structure to enhance the retrieval accuracy with relevance feedback technique. However, there are two issues to consider in traditional image retrieval methods: (1) the feature space of images is probably highly non-linear, in this case, PCA always fails to uncover the intrinsic structure so that the performance of dimension reduction is unsatisfactory; (2) ranking algorithms based on manifold learning most likely ignore the global structure of image feature space. To address the issues above, this paper utilizes Linear Local Tangent Space Alignment (LLTSA) algorithm to uncover the non-linear structure of images feature space. At the ranking stage, we take advantage of the semi-supervised idea to sort the similarity of images. Such a strategy makes up the shortcomings of manifold ranking. Experiments on a large collection of images have shown the effectiveness of our proposed algorithm.","","Electronic:978-1-4799-5825-2; POD:978-1-4799-5826-9; USB:978-1-4799-5824-5","10.1109/WCICA.2014.7052853","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052853","CBIR;LLTSA;manifold ranking","Accuracy;Classification algorithms;Image retrieval;Machine learning algorithms;Manifolds;Optimization;Principal component analysis","content-based retrieval;graph theory;image retrieval;learning (artificial intelligence)","CBIR system;feature content based image retrieval;feature space;graph structure;image retrieval applications;image similarity;linear local tangent space alignment algorithm;manifold learning;manifold ranking;nonlinear structure;ranking algorithm;ranking stage","","0","","9","","","June 29 2014-July 4 2014","","IEEE","IEEE Conference Publications"
"Automatically Prioritizing Pull Requests","E. v. d. Veen; G. Gousios; A. Zaidman","Delft Univ. of Technol, Delft, Netherlands","2015 IEEE/ACM 12th Working Conference on Mining Software Repositories","20150806","2015","","","357","361","In previous work, we observed that in the pull-based development model integrators face challenges with regard to prioritizing work in the face of multiple concurrent pull requests. We present the design and initial implementation of a prototype pull request prioritisation tool called PRioritizer. PRioritizer works like a priority inbox for pull requests, recommending the top pull requests the project owner should focus on. A preliminary user study showed that Prioritize provides functionality that GitHub is currently lacking, even though users need more insight into how the priority ranking is established to make Prioritize really useful.","2160-1852;21601852","Electronic:978-0-7695-5594-2; POD:978-1-4673-7924-3","10.1109/MSR.2015.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180094","GitHub;pull based development;pull request","Electronic mail;Face;Feature extraction;Inspection;Machine learning algorithms;Software engineering;Sorting","software engineering","GitHub;PRioritizer tool;automatic pull request prioritization;multiple concurrent pull requests;priority inbox;priority ranking;project owner;pull-based development model integrator;top-pull request recommendation;work prioritization","","0","","6","","","16-17 May 2015","","IEEE","IEEE Conference Publications"
"Deep Representations for Software Engineering","M. White","Dept. of Comput. Sci., Coll. of William & Mary, Williamsburg, VA, USA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","20150817","2015","2","","781","783","Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields. We propose that software engineering (SE) research is a unique opportunity to use these transformative approaches. Our research examines applications of deep architectures such as recurrent neural networks and stacked restricted Boltzmann machines to SE tasks.","0270-5257;02705257","Electronic:978-1-4799-1934-5; POD:978-1-4799-1935-2","10.1109/ICSE.2015.248","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203069","","Computational modeling;Computer architecture;Conferences;Context;Machine learning;Software;Software engineering","Boltzmann machines;learning (artificial intelligence);recurrent neural nets;software engineering","compositional representation learning;deep architecture;deep learning subsumes algorithm;deep representation;recurrent neural networks;software engineering;stacked restricted Boltzmann machine;transformative approach","","0","","45","","","16-24 May 2015","","IEEE","IEEE Conference Publications"
"Malware detection using genetic programming","T. A. Le; T. H. Chu; Q. U. Nguyen; X. H. Nguyen","Faculty of IT, Le Quy Don University Hanoi, Vietnam","the 2014 Seventh IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA)","20150209","2014","","","1","6","Malware is any software aiming to disrupt computer operation. Malware is also used to gather sensitive information or gain access to private computer systems. This is widely seen as one of the major threats to computer systems nowadays. Traditionally, anti-malware software is based on a signature detection system which keeps updating from the Internet malware database and thus keeping track of known malwares. While this method may be very accurate to detect previously known malwares, it is unable to detect unknown malicious codes. Recently, several machine learning methods have been used for malware detection, achieving remarkable success. In this paper, we propose a method in this strand by using Genetic Programming for detecting malwares. The experiments were conducted with the malwares collected from an updated malware database on the Internet and the results show that Genetic Programming, compared to some other well-known machine learning methods, can produce the best results on both balanced and imbalanced datasets.","2329-6267;23296267","Electronic:978-1-4799-5431-5; POD:978-1-4799-5432-2; USB:978-1-4799-5430-8","10.1109/CISDA.2014.7035623","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035623","","Decision trees;Feature extraction;Learning systems;Machine learning algorithms;Malware;Support vector machines;Training","Internet;database management systems;digital signatures;genetic algorithms;invasive software;learning (artificial intelligence)","Internet malware database;antimalware software;balanced datasets;genetic programming;imbalanced datasets;machine learning methods;malware detection;private computer systems;signature detection system","","0","","30","","","14-17 Dec. 2014","","IEEE","IEEE Conference Publications"
"Deep neural networks for understanding and diagnosing partial discharge data","V. M. Catterson; B. Sheng","Institute for Energy and Environment, University of Strathclyde, Glasgow, United Kingdom","2015 IEEE Electrical Insulation Conference (EIC)","20150827","2015","","","218","221","Artificial neural networks have been investigated for many years as a technique for automated diagnosis of defects causing partial discharge (PD). While good levels of accuracy have been reported, disadvantages include the difficulty of explaining results, and the need to hand-craft appropriate features for standard two-layer networks. Recent advances in the design and training of deep neural networks, which contain more than two layers of hidden neurons, have resulted in improved results in speech and image recognition tasks. This paper investigates the use of deep neural networks for PD diagnosis. Defect samples constructed in mineral oil were used to generate data for training and testing. The paper demonstrates the improvements in accuracy and visualization of learning which can be gained from deep learning.","2334-0975;23340975","CD-ROM:978-1-4799-7351-4; Electronic:978-1-4799-7354-5; POD:978-1-4799-7355-2","10.1109/ICACACT.2014.7223616","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223616","Artificial neural networks;UHF monitoring;deep learning;defects in oil;diagnostics;partial discharge","Accuracy;Biological neural networks;Computer architecture;Machine learning;Neurons;Partial discharges;Training","data analysis;electrical maintenance;fault diagnosis;learning (artificial intelligence);neural nets;partial discharges;power engineering computing","deep learning;deep neural networks;hidden neurons;mineral oil;partial discharge data;partial discharge diagnosis","","1","","16","","","7-10 June 2015","","IEEE","IEEE Conference Publications"
"Patient-centric on-body sensor localization in smart health systems","R. Saeedi; N. Amini; H. Ghasemzadeh","Embedded & Pervasive Systems Lab, School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164-2752","2014 48th Asilomar Conference on Signals, Systems and Computers","20150427","2014","","","2081","2085","A major obstacle in widespread adoption of current wearable monitoring systems is that sensors must be worn on predefined locations on the body. In order to continuously detect sensor locations, we propose a localization algorithm that allows patients to wear the sensors on different body locations without having to adhere to a specific installation protocol. Our approach achieves localization accuracy of 90.8% even when the sensor nodes are mis-oriented. Integration of the resulting location information as a feature in an activity recognition classifier significantly increased the recognition accuracy from 23.5% to 99.5%.","","CD-ROM:978-1-4799-8295-0; Electronic:978-1-4799-8297-4; POD:978-1-4799-8298-1","10.1109/ACSSC.2014.7094840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7094840","","Accelerometers;Accuracy;Biomedical monitoring;Feature extraction;Machine learning algorithms;Monitoring;Smart phones","biomechanics;body sensor networks;medical signal processing;patient monitoring","activity recognition classifier;localization algorithm;patient-centric on-body sensor localization;pedometer;recognition accuracy;smart health systems;wearable monitoring systems","","4","","30","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Tree RE-weighted belief propagation using deep learning potentials for mass segmentation from mammograms","N. Dhungel; G. Carneiro; A. P. Bradley","ACVT, School of Computer Science, The University of Adelaide","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","760","763","In this paper, we propose a new method for the segmentation of breast masses from mammograms using a conditional random field (CRF) model that combines several types of potential functions, including one that classifies image regions using deep learning. The inference method used in this model is the tree re-weighted (TRW) belief propagation, which allows a learning mechanism that directly minimizes the mass segmentation error and an inference approach that produces an optimal result under the approximations of the TRW formulation. We show that the use of these inference and learning mechanisms and the deep learning potential functions provides gains in terms of accuracy and efficiency in comparison with the current state of the art using the publicly available datasets INbreast and DDSM-BCRP.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163983","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163983","Deep learning;Gaussian Mixture model;Mammograms;mass segmentation;tree re-weighted belief propagation","Belief propagation;Image segmentation;Machine learning;Mammography;Shape;Solid modeling;Training","belief networks;cancer;image segmentation;inference mechanisms;learning (artificial intelligence);mammography;medical image processing;minimisation;tumours","CRF model;DDSM-BCRP dataset;INbreast dataset;TRW formulation;breast masses;conditional random field;deep learning potentials;error minimization;inference approach;mammograms;mass segmentation;tree re-weighted belief propagation","","1","","20","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Distribution based ensemble for class imbalance learning","G. Mustafa; Z. Niu; A. Yousif; J. Tarus","School of Computer Science and Technology, Beijing Institute of Technology, 5 South Zhongguancun Street, Beijing 100081, China","Fifth International Conference on the Innovative Computing Technology (INTECH 2015)","20150803","2015","","","5","10","MultiBoost ensemble has been well acknowledged as an effective learning algorithm which able to reduce both bias and variance in error and has high generalization performance. However, to deal with the class imbalanced learning, the Multi- Boost shall be amended. In this paper, a new hybrid machine learning method called Distribution based MultiBoost (DBMB) for class imbalanced problems is proposed, which combines Distribution based balanced sampling with the MultiBoost algorithm to achieve better minority class performance. It minimizes the within class and between class imbalance by learning and sampling different distributions (Gaussian and Poisson) and reduces bias and variance in error by employing the MultiBoost ensemble. Therefore, DBMB could output the final strong learner that is more proficient ensemble of weak base learners for imbalanced data sets. We prove that the G-mean, F1 measure and AUC of the DBMB is significantly superior to others. The experimental verification has shown that the proposed DBMB outperforms other state-of-the-art algorithms on many real world class imbalanced problems. Furthermore, our proposed method is scalable as compare to other boosting methods.","","Electronic:978-1-4673-7551-1; POD:978-1-4673-7552-8","10.1109/INTECH.2015.7173365","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173365","Class imbalance learning;MultiBoost;distribution based resampling;ensemble learning","Accuracy;Boosting;Committees;Gaussian distribution;Machine learning algorithms;Standards;Training","Gaussian distribution;Poisson distribution;learning (artificial intelligence);sampling methods","AUC;DBMB;F1 measure;G-mean;Gaussian distributions;Poisson distributions;class imbalance learning;class imbalanced problems;distribution based balanced sampling;distribution based ensemble;distribution based multiBoost;error bias;error variance;hybrid machine learning method;learning algorithm;minority class performance;multiBoost algorithm;multiBoost ensemble","","0","","25","","","20-22 May 2015","","IEEE","IEEE Conference Publications"
"Requirement Phrasing Assistance Using Automatic Quality Assessment","A. Allahyari-Abhari; M. Soeken; R. Drechsler","Cyber-Phys. Syst., DFKI GmbH, Bremen, Germany","2015 IEEE 18th International Symposium on Design and Diagnostics of Electronic Circuits & Systems","20150817","2015","","","183","188","The design of modern hardware systems is a very complex and time consuming process. At the beginning of the design process, requirements need to be specified. Errors in that early design stage derived by misinterpretation of the requirements can be hard to detect and require significant effort and costs to get fixed. To prevent errors, requirements should be written in a comprehensive and unambiguous way. Thus, designers are interested in automatic assistance tools that help writing better requirements. Conventional approaches are usually rule-based, thus many syntactic and semantic properties are not considered. In this paper we introduce an alternative approach to ensure the quality of requirements. The approach has two stages and assists the designer by providing (i) all relevant statistics about the syntactic and semantic properties of the sentence, and (ii) a single consolidated nominal quality predicate for the sentence such as good, medium, or bad. Although such statistical quality assessment leads to already satisfying results, the algorithm prediction reliability can further be enhanced by machine learning techniques. The achieved reliability for quality assessment in combination with the overview of the metrics about syntax and semantic can help the designer to write more comprehensive and less ambiguous requirements.","","Electronic:978-1-4799-6780-3; POD:978-1-4799-6781-0","10.1109/DDECS.2015.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7195695","Hardware/Software Co-Design;natural language processing;requirements;specifications","Compounds;Machine learning algorithms;Measurement;Quality assessment;Reliability;Semantics;Syntactics","formal specification;software quality;software reliability","automatic assistance tools;automatic quality assessment;machine learning techniques;prediction reliability;requirement phrasing assistance;single consolidated nominal quality predicate","","0","","19","","","22-24 April 2015","","IEEE","IEEE Conference Publications"
"Improving Accuracy for Classifying Selected Medical Datasets with Weighted Nearest Neighbors and Fuzzy Nearest Neighbors Algorithms","M. Qasem; M. Nour","Electron. Res. Inst., Cairo, Egypt","2015 International Conference on Cloud Computing (ICCC)","20150709","2015","","","1","9","Classification algorithms are very important for several fields such as data mining, machine learning, pattern recognition, and other data analysis applications. This work presents the weighted nearest neighbors and fuzzy k-nearest neighbors algorithms to classify chosen medical datasets. This involves several distance functions to calculate the difference between any two instances. Classification approaches based on K-nearest neighbors (KNN), weighted-KNN, frequency, class probability, and fuzzy K-nearest neighbors (fuzzy-KNN) are analyzed and discussed. Some measurable criteria are adopted to evaluate the performance of such algorithms. This includes classification accuracy, time, and confidence values. The algorithms will be tested using four different medical datasets. From the results, the fuzzy-KNN achieved the best accuracy compared to the other adopted algorithms. Following that are the weighted-KNN then the KNN. The longest classification time was for the fuzzy-KNN while the smallest time was for the KNN. The class confidence values of the fuzzy approach were promising. The fuzzy-KNN was also modified using fuzzy entropy. For the chosen datasets and w.r.t. KNN, the modified algorithms improved the classification accuracy. The improvements were up to 25%, 33%, and 38% for the weighted-KNN, fuzzy-KNN, and fuzzy Entropy respectively.","","Electronic:978-1-4673-6618-2; POD:978-1-4673-6619-9","10.1109/CLOUDCOMP.2015.7149644","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149644","","Accuracy;Algorithm design and analysis;Classification algorithms;Machine learning algorithms;Prediction algorithms;Training;Tumors","entropy;fuzzy set theory;medical administrative data processing;pattern classification;pattern clustering","KNN;fuzzy entropy;fuzzy nearest neighbor algorithm;k-nearest neighbor;medical dataset classification;weighted nearest neighbor algorithm","","0","","25","","","26-29 April 2015","","IEEE","IEEE Conference Publications"
"Automatic wedge tightness classifying system by support vector machine","T. Poombansao; W. Kongprawechnon; C. Theeraworn; S. Kittipiyakul","School of Information, Computer, and communication Technology Sirindhorn International Institute of Technology, Thammasat University, Thailand","2015 6th International Conference of Information and Communication Technology for Embedded Systems (IC-ICTES)","20150525","2015","","","1","5","This paper introduces a newly developed automatic classification system for wedge tightness inside the generator by applying support vector machine (SVM) classifier. The automatic classifying system for wedge tightness of the generator consists of 4 parts including data collection, preprocessing, feature extraction, and classification. Machine learning algorithm called SVM is used with the linear and radial basis function (RBF) classifier. Each input feature is extracted in different ways to evaluate the performance of classification. The evaluation is completed by using a 10- fold cross validation technique to provide high accuracy and a low number of False Negatives (FN). By applying the proposed system, the number of tightness and looseness inside wedge generator can be classified. Based on the classification results, the signals extracted in the frequency domain gives the best performance among the time domain and the frequency domain. This paper shows that the automatic classifying method has a high potential to identify the wedge tightness inside the generator.","","Electronic:978-1-4799-8565-4; POD:978-1-4799-8566-1; USB:978-1-4799-8564-7","10.1109/ICTEmSys.2015.7110810","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7110810","pattern recognition;support vector machine;wedge tightness signal","Indexes;Knowledge based systems;Machine learning algorithms;Robots;Software;Support vector machines;Training","object-oriented programming;pattern classification;software engineering;text analysis","J48 classification method;Naive Bayes classification method;SVM classification method;design pattern recommendation;gang-of-four patterns;k-NN classification method;modular software design;novice designers;object-oriented programming;pattern usage hierarchy;reusable software design;text classification approach;textual problems","","0","","12","","","22-24 March 2015","","IEEE","IEEE Conference Publications"
"Classification of Chinese-to-English translated social network timelines using naive Bayes","X. R. Yu; Z. L. Xiang; D. K. Kang","Computer Software Institute, Weifang University of Science & Technology, Shouguang 262-700, Shandong, China","2015 17th International Conference on Advanced Communication Technology (ICACT)","20150827","2015","","","296","299","This study proposes a method that classifies Chinese social network positive-negative comments (Weibo) using naive Bayes algorithm trained from English social network (Twitter) corpus. We train our text classifier using Twitter corpus (in English language), and use this classifier to classify Chinese text. In the previous research, Chinese sentences are processed using Chinese word segmentation algorithms before the application of machine learning algorithm. Chinese word segmentation algorithms split Chinese sentences into a series of words since a Chinese word consists of several Chinese characters unlike English sentences. Therefore, the quality of word segmentation algorithm obviously influences the accuracy of Chinese text categorization problems. In our research, we eliminate Chinese word segmentation stage (a traditional preprocessing stage of Chinese text classification) to avoid the effect on the quality of segmentation algorithms. Instead of Chinese word segmentation processing, we translate Chinese text into English text via Google translator. Based on Twitter corpus, we directly generate a text classifier by using naive Bayes multinomial algorithm. Finally, the text classifier classifies a new Chinese text (a Weibo text, which has been translated into English by Google translation at preprocessing stage). We conduct an experiment comparing the performance of naive Bayes multinomial algorithm and C4.5 in terms of accuracy.","1738-9445;17389445","CD-ROM:978-8-9968-6504-9; Electronic:978-8-9968-6505-6; POD:978-1-4673-8116-1","10.1109/ICACT.2015.7224807","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7224807","Classification;Comment;Multinomial model;Naive Bayes;Text categorization;Weibo","Algorithm design and analysis;Classification algorithms;Computer science;Hidden Markov models;Machine learning algorithms;Text categorization;Twitter","Bayes methods;language translation;learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis","Chinese characters;Chinese sentence processing;Chinese social network positive-negative comments;Chinese text categorization;Chinese text classification;Chinese text translation;Chinese word segmentation algorithm;Chinese-to-English translated social network timeline classification;English sentences;English social network corpus;Google translator;Twitter corpus;Weibo text;machine learning algorithm;naive Bayes multinomial algorithm;text classifier training","","0","","9","","","1-3 July 2015","","IEEE","IEEE Conference Publications"
"Multi-Layer and Recursive Neural Networks for Metagenomic Classification","G. Ditzler; R. Polikar; G. Rosen","Department of Electrical & Computer Engineering, Drexel University, Philadelphia","IEEE Transactions on NanoBioscience","20150828","2015","14","6","608","616","Recent advances in machine learning, specifically in deep learning with neural networks, has made a profound impact on fields such as natural language processing, image classification, and language modeling; however, feasibility and potential benefits of the approaches to metagenomic data analysis has been largely under-explored. Deep learning exploits many layers of learning nonlinear feature representations, typically in an unsupervised fashion, and recent results have shown outstanding generalization performance on previously unseen data. Furthermore, some deep learning methods can also represent the structure in a data set. Consequently, deep learning and neural networks may prove to be an appropriate approach for metagenomic data. To determine whether such approaches are indeed appropriate for metagenomics, we experiment with two deep learning methods: i) a deep belief network, and ii) a recursive neural network, the latter of which provides a tree representing the structure of the data. We compare these approaches to the standard multi-layer perceptron, which has been well-established in the machine learning community as a powerful prediction algorithm, though its presence is largely missing in metagenomics literature. We find that traditional neural networks can be quite powerful classifiers on metagenomic data compared to baseline methods, such as random forests. On the other hand, while the deep learning approaches did not result in improvements to the classification accuracy, they do provide the ability to learn hierarchical representations of a data set that standard classification methods do not allow. Our goal in this effort is not to determine the best algorithm in terms accuracy-as that depends on the specific application-but rather to highlight the benefits and drawbacks of each of the approach we discuss and provide insight on how they can be improved for predictive metagenomic analysis.","1536-1241;15361241","","10.1109/TNB.2015.2461219","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219432","Comparative metagenomics;metagenomics;microbiome;neural networks","Feature extraction;Machine learning;Nanobioscience;Neural networks;Organisms;Training;Vegetation","DNA;genomics;image classification;image representation;learning (artificial intelligence);medical image processing;molecular biophysics;multilayer perceptrons;natural language processing;neurophysiology","classification accuracy;data structure;deep belief network;deep learning methods;generalization performance;hierarchical representations;image classification;language modeling;machine learning;machine learning community;metagenomic classification;metagenomic data analysis;metagenomic literature;multilayer perceptron;multilayer-recursive neural networks;natural language processing;neural networks;nonlinear feature representations;prediction algorithm;predictive metagenomic analysis;unsupervised fashion","1","4","","44","","20150824","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Twitilyzer: Designing an approach for ad-hoc search engine","H. T. Kanakia; D. R. Kalbande","Computer Engineering Department, SPIT, Mumbai, India","2015 International Conference on Communication, Information & Computing Technology (ICCICT)","20150223","2015","","","1","4","There are many micro blogging sites which provides rich source of information about personality, products, sports, politics, technology etc. As of February 2014, about 241 million tweets are being generated per day. In this paper Twitilyzer, Twitter, a micro blogging site has been used to gather the information on different topics. The information will be in the form of tweets. These tweets are extracted from the Twitter through Twitter API. These tweets are then preprocessed to remove mistakes. The Naive Bayes machine learning algorithm is developed to classify the tweets into positive and negative sentiments. The Ranking algorithm is developed which will display top positive and top negative sentiments about the particular topic. The experimental result shows statistics of positive and negative sentiments. Thus it can contribute to organization for analysis of current market trends.","","Electronic:978-1-4799-5522-0; POD:978-1-4799-5523-7","10.1109/ICCICT.2015.7045716","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045716","Naive Bayes classifier;TextBlob;Top tweets classification;Twitter","Algorithm design and analysis;Classification algorithms;Conferences;Feature extraction;Machine learning algorithms;Sentiment analysis;Twitter","Bayes methods;application program interfaces;learning (artificial intelligence);search engines;social networking (online)","Naive Bayes machine learning algorithm;Twitilyzer;Twitter API;ad-hoc search engine;market trends;micro blogging sites;ranking algorithm","","1","","10","","","15-17 Jan. 2015","","IEEE","IEEE Conference Publications"
"Easiest tricks to solve tickiest expressions of stack","Y. Gupta; R. Sharma; S. Sharma","Computer Science and Engineering, CSE- III year, IMS Engineering College, Ghaziabad, India","2015 International Conference on Advances in Computer Engineering and Applications","20150723","2015","","","231","236","This paper mainly focuses on the best and efficient methods that could simplify the stack expression solving techniques in the easiest possible and enjoyable manner. It gives a detailed description of, using shortest tricks and techniques (methodology) to solve infix expression of stack into a postfix expression. Basically, the proposed work have been made, to provide an emphasis on, how to solve stack expression, without learning the precedence of operators rather by just memorizing two very simple words that the authors have proposed in the methodology section. All in all, authors have focused on, how smart work along with hard work is beneficial in today's competitive world as compared to boring bookish methods to provide a platform in solving the expressions of stack involving logical and relational operators respectively by using various working examples. The proposed concept have been shown by the help of various self-proposed algorithms that have been validated by simulation on a proposed machine learning model to facilitate the same process.","","DVD:978-1-4673-6910-7; Electronic:978-1-4673-6911-4; POD:978-1-4673-6912-1","10.1109/ICACEA.2015.7164701","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7164701","data structure;expressions;infix;postfix;stack","Algorithm design and analysis;Bibliographies;Computer science;Computers;Data structures;Machine learning algorithms;Optimization","data structures;learning (artificial intelligence)","data structures;infix expression;logical operators;machine learning model;postfix expression;relational operators;tickiest stack expressions","","0","","14","","","19-20 March 2015","","IEEE","IEEE Conference Publications"
"Collaborative filtering recommendation algorithm based on Hadoop and Spark","B. Kupisz; O. Unold","Chair of Computer Engineering, Wroclaw University of Technology, Wyb. Wyspianskiego 27, 50-370 Wroclaw, Poland","2015 IEEE International Conference on Industrial Technology (ICIT)","20150618","2015","","","1510","1514","The aim of this work was to develop and compare recommendation systems which use the item-based collaborative filtering algorithm, based on Hadoop and Spark. Data for the research were gathered from a real social portal the users of which can express their preferences regarding the applications on offer. The Hadoop version was implemented with the use of the Mahout library which was an element of the Hadoop ecosystem. The authors original solution was implemented with the use of the Apache Spark platform and the Scala programming language. The applied similarity measure was the Tanimoto coefficient which provides the most precise results for the available data. The initial assumptions were confirmed as the solution based on the Apache Spark platform turned out to be more efficient.","","Electronic:978-1-4799-7800-7; POD:978-1-4799-7801-4; USB:978-1-4799-7799-4","10.1109/ICIT.2015.7125310","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7125310","","Big data;Collaboration;Computer languages;Computers;Libraries;Machine learning algorithms;Sparks","collaborative filtering;data handling;portals;recommender systems","Apache Spark platform;Hadoop ecosystem;Hadoop version;Mahout library;Scala programming language;Tanimoto coefficient;collaborative filtering recommendation algorithm;item-based collaborative filtering algorithm;recommendation systems","","0","","20","","","17-19 March 2015","","IEEE","IEEE Conference Publications"
"A probabilistic condensed representation of data for stream mining","M. Geilke; A. Karwath; S. Kramer","Johannes Gutenberg-Universit&#x00E4;t Mainz, Staudingerweg 9, 55128, Germany","2014 International Conference on Data Science and Advanced Analytics (DSAA)","20150312","2014","","","297","303","Data mining and machine learning algorithms usually operate directly on the data. However, if the data is not available at once or consists of billions of instances, these algorithms easily become infeasible with respect to memory and run-time concerns. As a solution to this problem, we propose a framework, called MiDEO (Mining Density Estimates inferred Online), in which algorithms are designed to operate on a condensed representation of the data. In particular, we propose to use density estimates, which are able to represent billions of instances in a compact form and can be updated when new instances arrive. As an example for an algorithm that operates on density estimates, we consider the task of mining association rules, which we consider as a form of simple statements about the data. The algorithm, called POEt (Pattern mining on Online density esTimates), is evaluated on synthetic and real-world data and is compared to state-of-the-art algorithms.","","Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1","10.1109/DSAA.2014.7058088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058088","","Algorithm design and analysis;Association rules;Inference algorithms;Itemsets;Machine learning algorithms;Probabilistic logic","data mining;data structures","MiDEO;POEt;mining association rules;mining density estimates inferred online;pattern mining on online density estimates;probabilistic condensed data representation;stream mining","","2","","19","","","Oct. 30 2014-Nov. 1 2014","","IEEE","IEEE Conference Publications"
"Empirical comparative study to supervised approaches for WSD problem: Survey","B. F. Zopon AL Bayaty; S. Joshi","Department of Computer Science, AL-Mustansiriyah University, Baghdad, Iraq","2015 IEEE Canada International Humanitarian Technology Conference (IHTC2015)","20150903","2015","","","1","7","The Internet has become the most important knowledge source, because the popularity of computers and networks. Always the users use some keywords to find out the related topics, at same time spend a lot of time to finding what they really want, and because of the imprecise results of search in the Internet, most studies of web mining method are trying to improving the accuracy of the information gotten from Web search. In this work we implemented the approaches of our proposed implement master-slave voting model, and in this model we selected five robust supervised algorithms, so in this paper we implemented empirically these approaches using WordNet, Senseval-3, and tried make comparative between them.","","Electronic:978-1-4799-8963-8; POD:978-1-4799-8964-5","10.1109/IHTC.2015.7238056","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238056","Accuracy;Natural Language Processing (NLP);Senseval- 3;Supervised Approaches;Web;Word Sense Disambiguation (WSD);WordNet","Accuracy;Algorithm design and analysis;Classification algorithms;Context;Decision trees;Machine learning algorithms;Support vector machines","Internet;data mining;learning (artificial intelligence);natural language processing;search engines","Senseval-3;WSD problem;Web mining method;Web search;WordNet;empirical analysis;information accuracy improvement;internet search;knowledge source;master-slave voting model;robust supervised algorithms;word sense disambiguation","","0","","15","","","May 31 2015-June 4 2015","","IEEE","IEEE Conference Publications"
"CoCE-SMART: Consensus clustering based on enhanced splitting-merging awareness tactics","R. Fa; B. Abu-Jamous; D. J. Roberts; A. K. Nandi","Department of Electronic and Computer Engineering, Brunel University, Uxbridge, UB8 3PH, United Kingdom","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","20150806","2015","","","2011","2015","In this paper, we propose a new consensus clustering algorithm, which is based on an existing clustering paradigm, called enhanced splitting merging awareness tactics (E-SMART). The problem of determining the number of clusters, which affects many state-of-theart consensus clustering algorithms, is addressed by the proposed CoCE-SMART algorithm. The idea behind CoCE-SMART is that SMART is used repeatedly to one dataset, resulting in different clustering results, which might have different numbers of clusters. These SMART clustering results can be combined by clustering the centroids of all clusters as the estimate of real number of clusters can be determined from the SMART clustering results. Three benchmark datasets are utilised to assess the proposed algorithm. The experimental results strongly indicate that the proposed CoCE-SMART algorithm outperforms other state-of-the-art consensus clustering algorithms.","1520-6149;15206149","Electronic:978-1-4673-6997-8; POD:978-1-4673-6998-5; USB:978-1-4673-6996-1","10.1109/ICASSP.2015.7178323","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178323","Consensus clustering;Gene expression analysis;SMART","Clustering algorithms;Gene expression;Machine learning algorithms;Merging;Noise;Partitioning algorithms;Silicon","genetic algorithms;pattern clustering","CoCE-SMART algorithm;SMART clustering;clustering paradigm;consensus clustering algorithm;enhanced splitting merging awareness tactics;enhanced splitting-merging awareness tactics","","0","","29","","","19-24 April 2015","","IEEE","IEEE Conference Publications"
"A new unsupervised convolutional neural network model for Chinese scene text detection","X. Ren; K. Chen; X. Yang; Y. Zhou; J. He; J. Sun","Institute of Image Communication and Information Processing, Shanghai Jiao Tong University","2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)","20150903","2015","","","428","432","As one of the most popular deep learning models, convolution neural network (CNN) has achieved huge success in image information extraction. Traditionally CNN is trained by supervised learning method with labeled data and used as a classifier by adding a classification layer in the end. Its capability of extracting image features is largely limited due to the difficulty of setting up a large training dataset. In this paper, we propose a new unsupervised learning CNN model, which uses a so-called convolutional sparse auto-encoder (CSAE) algorithm pre-train the CNN. Instead of using labeled natural images for CNN training, the CSAE algorithm can be used to train the CNN with unlabeled artificial images, which enables easy expansion of training data and unsupervised learning. The CSAE algorithm is especially designed for extracting complex features from specific objects such as Chinese characters. After the features of articficial images are extracted by the CSAE algorithm, the learned parameters are used to initialize the first CNN convolutional layer, and then the CNN model is fine-trained by scene image patches with a linear classifier. The new CNN model is applied to Chinese scene text detection and is evaluated with a multilingual image dataset, which labels Chinese, English and numerals texts separately. More than 10% detection precision gain is observed over two CNN models.","","Electronic:978-1-4799-1948-2; POD:978-1-4799-1949-9; USB:978-1-4799-1947-5","10.1109/ChinaSIP.2015.7230438","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7230438","","Convolutional codes;Detection algorithms;Feature extraction;Machine learning;Neural networks;Training;Unsupervised learning","feature extraction;image classification;neural nets;object detection;unsupervised learning","CSAE algorithm;Chinese scene text detection;classification layer;convolutional sparse auto-encoder algorithm;deep learning models;image features extraction;image information extraction;labeled data classification;linear classifier;multilingual image dataset;scene image patches;supervised learning method;unsupervised convolutional neural network model;unsupervised learning CNN model","","0","","21","","","12-15 July 2015","","IEEE","IEEE Conference Publications"
"Text-Based Intelligent Content Filtering on Social Platforms","S. Khurshid; S. Khan; S. Bashir","Sch. of Electr. Eng. & Comput. Sci. (SEECS), Nat. Univ. of Sci. & Technol. (NUST), Islamabad, Pakistan","2014 12th International Conference on Frontiers of Information Technology","20150608","2014","","","232","237","Social platforms have become one of the popular mediums of information sharing and communication over the Internet today. People share all types of contents such as text, images, audio and video using these social platforms. Though information gained using these social platforms can be very useful for people around the globe, some of the user generated contents are very negative as they contain abusive, racial, offensive and insulting material. Thus, there is a need for an effective online content filtering technique which blocks these negative contents while not disturbing the access of users to rest of the contents available on these sites. Current techniques simply filter on the basis of URLs blocking and keyword matching or either rely on a large database of pre-classified web addresses. The problem is how to intelligently filter the negative contents, rather than filtering entire websites using their URLs or applying simple keyword matching techniques. In this paper we review a number of existing approaches to content filtering and propose an intelligent content filtering technique that uses sentiment analysis of the text and feature engineering methods to perform text classification.","","Electronic:978-1-4799-7505-1; POD:978-1-4799-7506-8","10.1109/FIT.2014.51","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7118405","Content filtering;Feature Engineering;Sentiment Analysis;Social Platforms;Text Classification","Accuracy;Filtering;Machine learning algorithms;Mathematical model;Sentiment analysis;Uniform resource locators;Web pages","classification;information filtering;social networking (online);text analysis","Internet;URL blocking;Web sites;feature engineering;keyword matching;sentiment analysis;social platform;text classification;text-based intelligent content filtering;user generated content","","1","","26","","","17-19 Dec. 2014","","IEEE","IEEE Conference Publications"
"Enhancing Sentiment Analysis of Financial News by Detecting Negation Scopes","N. Pröllochs; S. Feuerriegel; D. Neumann","Univ. of Freiburg, Freiburg, Germany","2015 48th Hawaii International Conference on System Sciences","20150330","2015","","","959","968","Sentiment analysis refers to the extraction of the polarity of source materials, such as financial news. However, measuring positive tone requires the correct classification of sentences that are negated, i.e. The negation scopes. For example, around 4.74% of all sentences in German ad hoc announcements contain negations. To predict the corresponding negation scope, related literature commonly utilizes two approaches, namely, rule-based algorithms and machine learning. Nevertheless, a thorough comparison is missing, especially for the sentiment analysis of financial news. To close this gap, this paper uses German ad hoc announcements as a common example of financial news in order to pursue a two-sided evaluation. First, we compare the predictive performance using a manually-labeled dataset. Second, we examine how detecting negation scopes can improve the accuracy of sentiment analysis. In this instance, rule-based algorithms produce superior results, resulting in an improvement of up to 9.80% in the correlation between news sentiment and stock market returns.","1530-1605;15301605","Electronic:978-1-4799-7367-5; POD:978-1-4799-7368-2","10.1109/HICSS.2015.119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7069923","","Algorithm design and analysis;Computational modeling;Hidden Markov models;Machine learning algorithms;Prediction algorithms;Sentiment analysis;Speech","knowledge based systems;learning (artificial intelligence);pattern classification;text analysis","German ad-hoc announcements;financial news sentiment analysis enhancement;machine learning;manually-labeled dataset;negated sentences;negation scope detection;positive tone measurement;predictive performance;rule-based algorithms;sentence classification;sentiment analysis accuracy improvement;source material polarity extraction;stock market returns;two-sided evaluation","","4","","28","","","5-8 Jan. 2015","","IEEE","IEEE Conference Publications"
"An improved algorithm of OMKC based on the optimized perceptron with the best kernel","B. Cheng; S. Zhong","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China","Proceedings of 2nd International Conference on Information Technology and Electronic Commerce","20150514","2014","","","247","251","Online Multiple Kernel Classification (OMKC) algorithm has been a popular method for exploring effective online combination of multiple kernel classifiers. The framework for OMKC is commonly obtained by learning multiple kernel classifiers and simultaneously their linear combination. However, the traditional perceptron algorithm which OMKC algorithm bases on does not achieve a much smaller mistake rate. In this paper, we put forward a novel algorithm based on OMKC using an improved perceptron algorithm. Our perceptron algorithm is applied with the best kernel. The algorithm produces an online validation procedure to search for the best kernel among the pool of kernels using the first 10% training examples. By using histograms analysis, our proposed algorithm can achieve smaller mistake rate and less time consuming. Extensive experimental results on twelve data sets demonstrate the effectiveness and efficiency of our algorithm.","","CD-ROM:978-1-4799-5298-4; Electronic:978-1-4799-5299-1; POD:978-1-4799-5300-4","10.1109/ICITEC.2014.7105612","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105612","Online learning;a optimized perceptron;classification;multi-kernel;the best kernel","Algorithm design and analysis;Classification algorithms;Histograms;Kernel;Machine learning algorithms;Prediction algorithms;Training","learning (artificial intelligence);pattern classification","OMKC algorithm;histograms analysis;multiple kernel classifier learning;online multiple kernel classification algorithm;optimized perceptron algorithm","","0","","17","","","20-21 Dec. 2014","","IEEE","IEEE Conference Publications"
"Chest pathology detection using deep learning with non-medical training","Y. Bar; I. Diamant; L. Wolf; S. Lieberman; E. Konen; H. Greenspan","The Blavatnik School of Computer Science, Tel-Aviv University, Tel Aviv 69978, Israel","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","294","297","In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163871","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163871","CNN;Chest Radiography;Computer-Aided Diagnosis Disease Categorization;Deep Learning;Deep Networks","Biomedical imaging;Diagnostic radiography;Feature extraction;Machine learning;Pathology;Visualization;X-rays","convolution;diagnostic radiography;diseases;feature extraction;image classification;image representation;learning (artificial intelligence);medical image processing;neural nets","AUC;CNN algorithm;CNN deep architecture classification;CNN learning;GIST feature;ImageNet;area under curve;chest X-ray image dataset;chest pathology detection;chest radiograph;convolutional neural network;deep learning;domain specific representation;general medical image recognition task;high level image representation learning;large scale nonmedical image database;mid level image representation learning;nonmedical learning;nonmedical training;pathology identification;pathology type","","8","","15","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Deep learning of tissue specific speckle representations in optical coherence tomography and deeper exploration for in situ histology","D. Sheet; S. P. K. Karri; A. Katouzian; N. Navab; A. K. Ray; J. Chatterjee","Department of Electrical Engineering, Indian Institute of Technology Kharagpur, India","2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)","20150723","2015","","","777","780","Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.","1945-7928;19457928","Electronic:978-1-4799-2374-8; POD:978-1-4673-9330-0","10.1109/ISBI.2015.7163987","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163987","Representation learning;cutaneous wounds;denoising autoencoders;in situ histology;optical coherence tomography;tissue characterization","Adaptive optics;Biomedical optical imaging;Machine learning;Optical imaging;Skin;Speckle;Wounds","biomedical optical imaging;image coding;image denoising;medical image processing;neural nets;optical tomography;skin;speckle;tissue engineering;unsupervised learning;wounds","OCT;choroid;clinical reporting variability;coherent sensing;conventional histology;coronary vessels;critical organs;deep learning;deep neural network-based architecture;denoising autoencoders;healing wounds;histologically characterizing healthy skin;in situ histology;interobserver reporting variability challenges;intraobserver reporting variability challenges;laser source incident;leveraged rapid clinical translation;live tissues;noninvasive high-resolution in situ imaging;optical coherence tomography;photon diffraction;retina;speckle image formation;speckle representations;stochastic nature;swept-source OCT;tissue specific speckle appearance;tissue specific speckle representations;unsupervised learning;wounded mice skin","","0","","11","","","16-19 April 2015","","IEEE","IEEE Conference Publications"
"Adaptive threshold architecture for spectrum sensing in public safety radio channels","M. Kist; L. R. Faganello; L. Bondan; M. A. Marotta; L. Z. Granville; J. Rochol; C. B. Both","Federal University of Rio Grande do Sul (UFRGS)","2015 IEEE Wireless Communications and Networking Conference (WCNC)","20150618","2015","","","287","292","Cognitive radio make use of spectrum sensing techniques to detect licensed users transmissions and avoid causing interference. The major drawback in current spectrum sensing techniques is the use of static decision thresholds to detect such transmissions, which may be infeasible in public safety radio channels. More precisely, the cognitive radio may find different noise or interference levels when switching among these channels. This can lead to a wrong picture of the channel occupancy status, which in turn can increase the interference caused to licensed users. In this paper we propose an Adaptive Threshold Architecture, which uses machine learning algorithms to dynamically adapt the decision threshold, enabling the detection of licensed users transmissions in public safety radio channels. Results showed that the proposed architecture increased the sensing accuracy up to 2 times, providing results up to 6 times faster when compared to other solutions of the literature.","1525-3511;15253511","Electronic:978-1-4799-8406-0; POD:978-1-4799-8407-7; USB:978-1-4799-8405-3","10.1109/WCNC.2015.7127484","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7127484","","Accuracy;Cognitive radio;Computational fluid dynamics;Machine learning algorithms;Safety;Sensors;Signal to noise ratio","cognitive radio;learning (artificial intelligence);radio networks;radio spectrum management;radiofrequency interference;signal detection;telecommunication computing;wireless channels","adaptive threshold architecture;channel occupancy status;cognitive radio;current spectrum sensing techniques;interference;licensed users transmissions;machine learning algorithms;public safety radio channels;spectrum sensing techniques;static decision thresholds","","2","","11","","","9-12 March 2015","","IEEE","IEEE Conference Publications"
"An evaluation on the efficiency of hybrid feature selection in spam email classification","M. Mohamad; A. Selamat","Software Eng. Res. Group (SERG), Univ. Teknol. Malaysia, Johor Bahru, Malaysia","2015 International Conference on Computer, Communications, and Control Technology (I4CT)","20150827","2015","","","227","231","In this paper, a spam filtering technique, which implement a combination of two types of feature selection methods in its classification task will be discussed. Spam, which is also known as unwanted message always floods our electronic mail boxes, despite a spam filtering system provided by the email service provider. In addition, the issue of spam is always highlighted by Internet users and attracts many researchers to conduct research works on fighting the spam. A number of frameworks, algorithms, toolkits, systems and applications have been proposed, developed and applied by researchers and developers to protect us from spam. Several steps need to be considered in the classification task such as data pre-processing, feature selection, feature extraction, training and testing. One of the main processes in the classification task is called feature selection, which is used to reduce the dimensionality of word frequency without affecting the performance of the classification task. In conjunction with that, we had taken the initiative to conduct an experiment to test the efficiency of the proposed Hybrid Feature Selection, which is a combination of Term Frequency Inverse Document Frequency (TFIDF) with the rough set theory in spam email classification problem. The result shows that the proposed Hybrid Feature Selection return a good result.","","Electronic:978-1-4799-7952-3; POD:978-1-4799-7953-0; USB:978-1-4799-7951-6","10.1109/I4CT.2015.7219571","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219571","Spam;TFIDF;algorithm;feature selection;filtering;rough set theory","Accuracy;Filtering;Machine learning algorithms;Set theory;Testing;Unsolicited electronic mail","Internet;feature selection;information filtering;pattern classification;security of data;unsolicited e-mail","Internet;TFIDF;data preprocessing;electronic mail boxes;email service provider;feature extraction;hybrid feature selection method;spam email classification problem;spam filtering technique;term frequency inverse document frequency","","0","","14","","","21-23 April 2015","","IEEE","IEEE Conference Publications"
"Efficient regression algorithms for classification of social media data","S. Desai; S. T. Patil","Pune University, India","2015 International Conference on Pervasive Computing (ICPC)","20150416","2015","","","1","5","As data is growing rapidly day by day due to wide usage of social media, this social data will help business analyst as well as researchers to get the feedback about any service or product. Analysis of social media data can be used for many purposes like friend recommendation, product or service recommendation etc. There is need of such algorithms which will classify data accurately and will provide analysis results accurately. Also these algorithms need to scale rapidly with data sets. In this paper, Different phases of social media data mining are identified. We have studied algorithms whose training time remains same though the data size increases. We have implemented decision tree algorithm and compared it with other machine learning algorithms like NaiveBayes, Adaboost etc. It is found that decision tree algorithms gives accurate results as compared to other algorithms. In addition to that, out of different decision tree algorithms random decision tree (RDT) algorithm performs best than other decision tree algorithms like C4.5 or ID3.","","Electronic:978-1-4799-6272-3; POD:978-1-4799-6054-5","10.1109/PERVASIVE.2015.7087040","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087040","Decision Trees;Random Decision Trees;Social Media Data","Algorithm design and analysis;Classification algorithms;Data mining;Decision trees;Machine learning algorithms;Media;Social network services","Bayes methods;data mining;decision trees;learning (artificial intelligence);pattern classification;recommender systems;regression analysis;social networking (online)","Adaboost;C4.5;ID3;NaiveBayes;RDT;business analyst;data size;decision tree algorithm;friend recommendation;machine learning algorithms;product recommendation;random decision tree algorithm;regression algorithms;service recommendation;social media data classification;social media data mining","","0","","19","","","8-10 Jan. 2015","","IEEE","IEEE Conference Publications"
"A suspect point recheck method of fuzzy clustering for robot self-position estimation","Y. Zonglin; C. Hui; Z. Yanbin; J. Lixin; S. Gangquan","State Key Laboratory of Electrical Insulation and Power Equipment, School of Electrical Engineering, Xi'an Jiaotong University, 710049, Xi'an, China","2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)","20150323","2014","","","38","41","For autonomous robots, the Fuzzy C-means algorithm (FCM) is used in the tasks like self-position estimation, path planning and environment navigation. This paper proposes a suspect point recheck method for fuzzy clustering algorithm. First, the proposed method works as the typical FCM to obtain an original clustering result. Then the method classifies all the data points into normal points and suspect points according to their memberships of each cluster. Finally, the method redistributes the suspect points according to the information of their nearby normal points. Three datasets from UCI Machine Learning Repository are used in the experiments. The experimental results verify that the proposed method has higher clustering capability.","","Electronic:978-1-4799-5199-4; POD:978-1-4799-5200-7; USB:978-1-4799-5198-7","10.1109/ICARCV.2014.7064276","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064276","fuzzy C-means;robot;self-position estimation;suspect point","Breast cancer;Classification algorithms;Clustering algorithms;Estimation;Machine learning algorithms;Partitioning algorithms;Robots","mobile robots;pattern classification;pattern clustering;telerobotics","FCM;UCI Machine Learning Repository;autonomous mobile robots;data point classification;fuzzy c-means algorithm;fuzzy clustering;robot self-position estimation;suspect point recheck method","","0","","7","","","10-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"An Empirical Comparison of Classifiers to Analyze Intrusion Detection","P. Aggarwal; S. K. Sharma","Sch. of Eng. & Technol., Ansal Univ., Gurgaon, India","2015 Fifth International Conference on Advanced Computing & Communication Technologies","20150406","2015","","","446","450","The massive data exchange on the web has deeply increased the risk of malicious activities thereby propelling the research in the area of Intrusion Detection System (IDS). This paper aims to first select ten classification algorithms based on their efficiency in terms of speed, capability to handle large dataset and dependency on parameter tuning and then simulates the ten selected existing classifiers on a data mining tool Weka for KDD'99 dataset. The simulation results are evaluated and benchmarked based on the generic evaluation metrics for IDS like F-score and accuracy.","2327-0632;23270632","Electronic:978-1-4799-8488-6; POD:978-1-4799-8489-3","10.1109/ACCT.2015.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079125","Classification algorithm;Intrusion detection syste;NSL-KDD","Accuracy;Classification algorithms;Intrusion detection;Machine learning algorithms;Mathematical model;Measurement;Vegetation","Internet;data mining;electronic data interchange;pattern classification;security of data","F-score;IDS;Web;Weka;classification algorithms;data classifiers;data mining tool;generic evaluation metrics;intrusion detection system;malicious activities;massive data exchange;parameter tuning","","2","","17","","","21-22 Feb. 2015","","IEEE","IEEE Conference Publications"
"Classifying bug severity using dictionary based approach","S. Gujral; G. Sharma; S. Sharma; Diksha","Department Of CSE, CU, Gharuan, India","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","20150713","2015","","","599","602","Bug tracking system allows user to report bugs that they encounter while operating the software. These bugs are then received by developers and they resolve these bugs according to their severity level. This task of assigning severity level is manual task that need expertise of assessing the severity level of reported bug. But if these reported bugs are large in number then manual process to assess severity level of bugs becomes very hectic. So there should be an automatic process to classify it so that bugs that need immediate fixation get resolved early. A few attempts have been made by researchers to automate the task. The approach followed in this paper has made an attempt to automate the bug severity classification using text mining technique and Naïve Bayes Multinomial classifier. This paper further proposes an approach of making this task more efficient using dictionary of bug terms.","","CD-ROM:978-1-4799-8432-9; Electronic:978-1-4799-8433-6; POD:978-1-4799-8434-3","10.1109/ABLAZE.2015.7154933","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154933","Bug Tracking system;Bug severity;Dictionary;software bug classification","Accuracy;Classification algorithms;Computer bugs;Dictionaries;Machine learning algorithms;Software;Text mining","Bayes methods;data mining;pattern classification;program debugging;text analysis","bug severity classification;bug term dictionary;dictionary based approach;naive Bayes multinomial classifier;text mining technique","","0","","18","","","25-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"Intelligent feature selection method rooted in Binary Bat Algorithm for intrusion detection","A. C. Enache; V. Sgârciu; A. Petrescu-Niţă","University Politehnica of Bucharest, Faculty of Automatic Control and Computer Science, Bucharest, Romania","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","20150820","2015","","","517","521","The multitude of hardware and software applications generate a lot of data and burden security solutions that must acquire informations from all these heterogenous systems. Adding the current dynamic and complex cyber threats in this context, make it clear that new security solutions are needed. In this paper we propose a wrapper feature selection approach that combines two machine learning algorithms with an improved version of the Binary Bat Algorithm. Tests on the NSL-KDD dataset empirically prove that our proposed method can reduce the number of features with almost 60% and obtains good results in terms of attack detection rate and false alarm rate, even for unknown attacks.","","Electronic:978-1-4799-9911-8; POD:978-1-4799-9912-5; USB:978-1-4799-9910-1","10.1109/SACI.2015.7208259","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208259","Feature selection;Naïve Bayes and BBA;SVM","Feature extraction;Intrusion detection;Machine learning algorithms;Niobium;Silicon;Support vector machines;Training","learning (artificial intelligence);security of data","NSL-KDD dataset;binary bat algorithm;complex cyber threats;current dynamic threats;intelligent feature selection method;intrusion detection;machine learning algorithms","","0","","19","","","21-23 May 2015","","IEEE","IEEE Conference Publications"
"Accelerating classification time in Hyperspectral Images","K. G. Toker; S. E. Yüksel","Elektrik ve Elektron. Muhendisligi Bolumu, Hacettepe Univ., Ankara, Turkey","2015 23nd Signal Processing and Communications Applications Conference (SIU)","20150622","2015","","","2126","2129","K-nearest neighbour (K-NN) is a supervised classification technique that is widely used in many fields of study to classify unknown queries based on some known information about the dataset. K-NN is known to be robust and simple to implement when dealing with data of small size. However its performance is slow when data is large and has high dimensions. Hyperspectral images, often collected from high altitudes, cover very large areas and consist of a large number of pixels, each having hundreds of spectral dimensions. We focus on one of the most popular algorithms for performing approximate search for large datasets based on the concept of locality-sensitive hashing (LSH) for Hyperspectral Image Processing, that allows us to quickly find similar entries in large databases. Our experiments show that LSH accelerates the classification time significantly without effecting the classification rates.","2165-0608;21650608","Electronic:978-1-4673-7386-9; POD:978-1-4673-7387-6","10.1109/SIU.2015.7130292","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130292","hyperspectral imaging;k nearest neighbour method;locality Sensitive Hashing","Approximation algorithms;Hyperspectral imaging;Machine learning algorithms;Signal processing algorithms;Streaming media","file organisation;hyperspectral imaging;image classification;learning (artificial intelligence)","K-NN;K-nearest neighbour;LSH;hyperspectral image processing;locality-sensitive hashing;supervised classification technique","","0","","17","","","16-19 May 2015","","IEEE","IEEE Conference Publications"
"Novel VLSI Architectures for Real-World Intelligent Systems","M. Kameyama","Grad. Sch. of Inf. Sci., Tohoku Univ., Sendai, Japan","2015 IEEE International Symposium on Multiple-Valued Logic","20150903","2015","","","132","132","A real-world intelligent systems platform based on novel architectures is presented in this article. As real-world applications, we consider advanced intelligent systems such as a highly-safe system and an intelligent robot system which make our daily life safe and comfortable.","0195-623X;0195623X","Electronic:978-1-4799-1777-8; POD:978-1-4799-1778-5","10.1109/ISMVL.2015.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238146","","Algorithm design and analysis;Computer architecture;Hardware;Intelligent systems;Machine learning algorithms;Software;Very large scale integration","artificial intelligence;integrated circuits;intelligent robots","VLSI architectures;advanced intelligent systems;highly-safe system;intelligent robot system;real-world intelligent systems","","0","","","","","18-20 May 2015","","IEEE","IEEE Conference Publications"
"""Hey #311, Come Clean My Street!"": A Spatio-temporal Sentiment Analysis of Twitter Data and 311 Civil Complaints","R. Eshleman; H. Yang","Dept. of Comput. Sci., San Francisco State Univ., San Francisco, CA, USA","2014 IEEE Fourth International Conference on Big Data and Cloud Computing","20150209","2014","","","477","484","Twitter data has been applied to address a wide range of applications (e.g., Political election prediction and disease tracking), however, no studies have been conducted to explore the interactions and potential relationships between twitter data and social events available from government entities. In this paper, we introduce a novel approach to investigate the spatio-temporal relationships between the sentiment aspects of tweets and 311 civil complaints recorded in the 311 Case Database, which is freely available from the City of San Francisco. We also present results from two supporting tasks: (1) We apply sentiment analysis techniques to model the emotional characteristics of five metropolitan areas around the globe, allowing one to gain insight into the relative happiness across cities and neighborhoods within a city, and (2) we quantify the performance of several open-source machine learning algorithms for sentiment analysis by applying them to large volume of twitter data, thereby providing empirical guidelines for practitioners. Major contributions and findings include (1) We have developed a system for the relative ranking of happiness of a geographical area. Our results show that Sydney, Australia is the happiest of the five cities under study, (2) We have found a counterintuitive positive correlation between 311-report frequency and local sentiment, and (3) When performing sentiment analysis of tweets, the inclusion of emoticons in the training dataset can lead to model over fitting, whereas NLP-based features seem to have a great potential to improve the classification accuracy.","","Electronic:978-1-4799-6719-3; POD:978-1-4799-6720-9","10.1109/BDCloud.2014.106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034832","311 civil complaints;happiness index;online social networks;sentiment analysis;spatio-temporal analysis;twitter data","Accuracy;Cities and towns;Indexes;Machine learning algorithms;Sentiment analysis;Twitter","emotion recognition;learning (artificial intelligence);natural language processing;public domain software;social networking (online)","311 case database;311-report frequency;Australia;NLP- based features;San Francisco City;Sydney;Twitter data;civil complaints;disease tracking;emotional characteristics;government entities;open-source machine learning algorithms;political election prediction;sentiment analysis techniques;sentiment aspects;social events;spatio-temporal sentiment analysis;tweet sentiment analysis;twitter data","","0","","27","","","3-5 Dec. 2014","","IEEE","IEEE Conference Publications"
"Study of dimension reduction methodologies in data mining","N. Sharma; K. Saroha","School of IT, Center for Development of Advanced Computing, Noida, India","International Conference on Computing, Communication & Automation","20150706","2015","","","133","137","The data mining applications such as bioinformatics, risk management, forensics etc., involves very high dimensional dataset. Due to large number of dimensions, a well known problem of “Curse of Dimensionality” occurs. This problem leads to lower accuracy of machine learning classifiers due to involvement of many insignificant and irrelevant dimensions or features in the dataset. There are many methodologies that are being used to find the Critical Dimensions for a dataset that significantly reduces the number of dimensions. These feature reduction and subset selection methods reduce feature set, that eventually results in high classification accuracy and lower computation cost of machine learning algorithms. This paper surveys the schemes that are majorly used for Dimensionality Reduction mainly focusing Bioinformatics, Agricultural, Gene and Protein Expression datasets. A comparative analysis of surveyed methodologies is also done, based on which, best methodology for a certain type of dataset can be chosen.","","Electronic:978-1-4799-8890-7; POD:978-1-4799-8891-4","10.1109/CCAA.2015.7148359","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148359","Data mining;Principal Component Analysis;critical dimension;curse of dimensionality;dimensionality reduction;feature selection","Accuracy;Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Machine learning algorithms;Principal component analysis","data mining;pattern classification","agricultural dataset;bioinformatics dataset;curse-of-dimensionality;data mining;dimension reduction methodologies;feature reduction method;gene dataset;machine learning classifiers;protein expression dataset;subset selection method","","0","","20","","","15-16 May 2015","","IEEE","IEEE Conference Publications"
"Cognitive Computing, Analytics, and Personalization","S. Earley","Earley Information Science","IT Professional","20150716","2015","17","4","12","18","This article discusses the increasing sophistication of search technology and its role in cognitive computing. It provides a concrete example of a personal assistant that is context-aware and can retrieve the information most likely to solve users' problems. It also addresses how search engines predict what information users are likely to need, and how the software retrieves and presents the information. Finally, the article discusses personalization and how machine learning can contribute to this process<i>.</i>","1520-9202;15209202","","10.1109/MITP.2015.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160901","artificial intelligence;cognitive computing;data analysis;information retrieval;search","Cognition;Context modeling;Information retrieval;Machine learning algorithms;Ontologies;Search methods;Semantics","information retrieval;learning (artificial intelligence);search engines","cognitive computing;context-aware personal attendant;information retrieval;machine learning;search engines;search technology","","0","","8","","","July-Aug. 2015","","IEEE","IEEE Journals & Magazines"
"IMMIX-intrusion detection and prevention system","S. H. Vasudeo; P. Patil; R. V. Kumar","M. Tech. Wireless and Network Security, Center for Development of Advance Computing, Pune, India","2015 International Conference on Smart Technologies and Management for Computing, Communication, Controls, Energy and Materials (ICSTM)","20150827","2015","","","96","101","Computer security has become a major problem in our society. Specifically, computer network security is concerned with preventing the intrusion of an unauthorized person into a network of computers. An intrusion detection system (IDS) is a tool to monitor the network traffic and users activity with the aim of distinguishing between hostile and non-hostile traffic. Most of current networks implement Misuse detection or Anomaly detection techniques for Intrusion detection. By deploying misuse based IDS it cannot detect unknown intrusions and anomaly based IDS have high false positive rate for detection. To overcome this, proposed system uses combination of both network based and host based IDPS as Hybrid Intrusion Detection and Prevention System which will be helpful for detecting maximum attacks on networks.","","CD:978-1-4799-9853-1; Electronic:978-1-4799-9855-5; Paper:978-1-4799-9854-8","10.1109/ICSTM.2015.7225396","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225396","anomaly based;attacks;classification;intrusion detection;intrusion prevention;misuse based","Classification algorithms;Clustering algorithms;Computers;Intrusion detection;Machine learning algorithms;Monitoring","computer network security;telecommunication traffic","IDS;IMMIX-intrusion detection system;IMMIX-intrusion prevention system;anomaly detection;computer network security;misuse detection;non-hostile traffic","","0","","12","","","6-8 May 2015","","IEEE","IEEE Conference Publications"
"Deep learning for pattern learning and recognition","C. L. P. Chen","Fac. of Sci. & Technol., Univ. of Macau, Macau, China","2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics","20150820","2015","","","17","17","Summary form only given. Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It is typically used to abstract useful information from data. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lower level concepts can help to define many higher-level concepts. Alternatively, the main advantage of deep learning is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text. This talk is to overview the foundationa, data representation capability of deep networks, and to investigate efficient deep learning algorithms, and meaningful applications.","","Electronic:978-1-4799-9911-8; POD:978-1-4799-9912-5; USB:978-1-4799-9910-1","10.1109/SACI.2015.7208200","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208200","","Algorithm design and analysis;Computational intelligence;Informatics;Machine learning;Machine learning algorithms;Pattern recognition","data structures;learning (artificial intelligence);statistical analysis","data representation capability;deep learning algorithms;deep networks;higher-level concepts;machine learning;pattern learning;pattern recognition;statistical models","","0","","","","","21-23 May 2015","","IEEE","IEEE Conference Publications"
"Big Data Open Source Platforms","P. D. C. d. Almeida; J. Bernardino","Dept. of Comput. Eng. & Syst., ISEC - Coimbra Inst. of Eng., Coimbra, Portugal","2015 IEEE International Congress on Big Data","20150820","2015","","","268","275","In a global market the capacity to mine and analyze user data is one way for companies to be as close in time and accuracy to the needs of their users. Big Data Platforms are one solution for companies to solve the necessary challenges to accomplish these capacities. Unfortunately the number of challenges that need to be addressed, allied with the high number of different solutions proposed, has led to the creation of a high number of different platforms making it hard to name one definitive and adequate platform for companies. In this paper we compare six of the most important Big Data Open Source Platforms to help companies or organizations choose the most adequate one to their needs. We analyze the following open source platforms - Apache Mahout, MOA, R Project, Vow pal Wabbit, PEGASUS and Graph Lab Create TM.","2379-7703;23797703","Electronic:978-1-4673-7278-7; POD:978-1-4673-7279-4","10.1109/BigDataCongress.2015.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207229","Big Data;Data Analysis;Data Mining;Open Source","Algorithm design and analysis;Big data;Companies;Data mining;Distributed databases;Machine learning algorithms","Big Data;data analysis;data mining;public domain software","Apache Mahout;Graph Lab Create TM;MOA;PEGASUS;R Project;Vow pal Wabbit;big data open source platforms;user data analysis;user data mining","","0","","34","","","June 27 2015-July 2 2015","","IEEE","IEEE Conference Publications"
"Accelerating Big Data Analytics Using FPGAs","K. Neshatpour; M. Malik; M. A. Ghodrat; H. Homayoun","Dept. of Electr. & Comput. Eng., George Mason Univ., Fairfax, VA, USA","2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines","20150716","2015","","","164","164","Emerging big data analytics applications require a significant amount of server computational power. As chips are hitting power limits, computing systems are moving away from general-purpose designs and toward greater specialization. Hardware acceleration through specialization has received renewed interest in recent years, mainly due to the dark silicon challenge. To address the computing requirements of big data, and based on the benchmarking and characterization results, we envision a data-driven heterogeneous architecture for next generation big data server platforms that leverage the power of field-programmable gate array (FPGA) to build custom accelerators in a Hadoop MapReduce framework. Unlike a full and dedicated implementation of Hadoop MapReduce algorithm on FPGA, we propose the hardware/software (HW/SW) co-design of the algorithm, which trades some speedup at a benefit of less hardware. Considering communication overhead with FPGA and other overheads involved in Hadoop MapReduce environment such as compression and decompression, shuffling and sorting, our experimental results show significant potential for accelerating Hadoop MapReduce machine learning kernels using HW/SW co-design methodology.","","Electronic:978-1-4799-9969-9; POD:978-1-4799-9970-5","10.1109/FCCM.2015.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160063","Big-data;FPGA;MapReduce;acceleration","Acceleration;Big data;Field programmable gate arrays;Hardware;Kernel;Machine learning algorithms;Servers","Big Data;benchmark testing;data analysis;field programmable gate arrays;hardware-software codesign;learning (artificial intelligence);parallel processing","FPGA;HW-SW codesign methodology;Hadoop MapReduce framework;Hadoop MapReduce machine learning kernels;big data analytics;big data computing requirements;computing systems;data-driven heterogeneous architecture;field-programmable gate array;general-purpose designs;hardware acceleration;hardware-software codesign;next generation big data server platforms;server computational power","","4","","","","","2-6 May 2015","","IEEE","IEEE Conference Publications"
"K-means clustering based SVM ensemble methods for imbalanced data problem","J. Lee; J. H. Lee","Dept. of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Republic of Korea","2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)","20150219","2014","","","614","617","When the number of data in one class is significantly larger or less than the data in other class, under machine learning algorithm for classification, a problem of learning generalization occurs to the specific class and this is called imbalanced data problem. In this paper, we propose a novel method to solve the imbalanced data problem. We first divide data into clusters using K-means clustering algorithm and create classifier using the Support Vector Machine (SVM) method on each cluster. Before making classifier for each cluster, we are balancing the data for each cluster using data sampling techniques. After all classifiers are made for each cluster, we validate each classifier's performance using validation data. Final classification result would be calculated using the test data by aggregating all the cluster's classification results. We are using not only the results from the classifiers in each clusters, but also the credit of each classifier and data membership to each cluster. We have verified that the proposed classification method shows better performance than the existing machine learning algorithms for imbalanced data classification problem.","","Electronic:978-1-4799-5955-6; POD:978-1-4799-5956-3; USB:978-1-4799-5954-9","10.1109/SCIS-ISIS.2014.7044861","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7044861","SVM ensemble method;data membership;imbalanced data;k-means clustering","Clustering algorithms;Machine learning algorithms;Pattern recognition;Rain;Support vector machines;Training data","learning (artificial intelligence);pattern classification;pattern clustering;support vector machines","SVM ensemble method;cluster classification;imbalanced data classification problem;k-means clustering;machine learning algorithm;support vector machine","","0","","16","","","3-6 Dec. 2014","","IEEE","IEEE Conference Publications"
"An improving online accuracy updated ensemble method in learning from evolving data streams","X. F. Gu; J. W. Xu; S. J. Huang; L. M. Wang","International Centre for Wavelet Analysis and Its Applications, School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu 611731","2014 11th International Computer Conference on Wavelet Actiev Media Technology and Information Processing(ICCWAMTIP)","20150402","2014","","","430","433","Most stream classifiers need to detect and react to concept drifts, as traditional machine learning goes to big data machine learning. The most popular ways to adaptive to concept drifts are incrementally learning and classifier dynamic ensemble. Recent years, ensemble classifiers have become an established research line in this field, mainly due to their modularity which offers a natural way of adapting to changes. However, many ensembles which process instances in blocks do not react to sudden changes sufficiently quickly, and which process streams incrementally do not offer accurate reactions to gradual and incremental changes. Fortunately, an Online Accuracy Updated Ensemble (OAUE) algorithm was presented by Brzezinski and Stefanowski. OAUE algorithm has been proven to be an effective ensemble to deal with drifting data stream. But, it has a potentially weakness to adaptive to sudden changes as it uses a fixed window. Therefore, we put forward a Window-Adaptive Online Accuracy Updated Ensemble (WAOAUE) algorithm, which is based on OAUE, and a change detector is added to the ensemble for deciding the window size of each candidate classifier. The proposed algorithm was experimentally compared with four state-of-the-art online ensembles, include OAUE, and provided best practice for big data stream mining.","","CD-ROM:978-1-4799-7206-7; Electronic:978-1-4799-7208-1; POD:978-1-4799-7209-8","10.1109/ICCWAMTIP.2014.7073443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073443","Concept drift;OAUE;data stream;ensemble","Accuracy;Big data;Classification algorithms;Data mining;Knowledge discovery;Light emitting diodes;Machine learning algorithms","data handling;learning (artificial intelligence)","WAOAUE;data machine learning;ensemble method;evolving data streams;improving online accuracy;process streams;stream classifiers;window adaptive online accuracy updated ensemble","","1","","16","","","19-21 Dec. 2014","","IEEE","IEEE Conference Publications"
"Applicability of linear and nonlinear principal component analysis for damage detection","A. D. F. Santos; M. F. M. Silva; C. S. Sales; J. C. W. A. Costa; E. Figueiredo","Applied Electromagnetism Laboratory, Universidade Federal do Par&#x00E1;, R. Augusto Corr&#x00EA;a 01, Bel&#x00E9;m, Brazil 66075-110","2015 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) Proceedings","20150709","2015","","","869","874","The goal of this work is to detect structural damage using vibration-based damage identification approaches even when the damage-sensitive features are camouflaged by the presence of operational and environmental conditions. For feature classification purposes, four machine learning algorithms are applied based on the principal component analysis (PCA), nonlinear PCA, kernel PCA and greedy kernel PCA. Time-series data from an array of accelerometers under several structural state conditions were obtained from a well-known base-excited three-story frame structure. The main contribution of this work is the applicability of those PCA-based algorithms, for damage detection, in the presence of operational and environmental effects. For these specific data sets, one can infer that the greedy kernel PCA algorithm is more appropriate when one wants to minimize false-positive indications of damage without increasing, significantly, the false-negative indications of damage.","1091-5281;10915281","Electronic:978-1-4799-6114-6; POD:978-1-4799-6115-3; USB:978-1-4799-6113-9","10.1109/I2MTC.2015.7151383","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7151383","PCA;SHM;damage detection","Data models;Feature extraction;Kernel;Machine learning algorithms;Mathematical model;Principal component analysis;Training","accelerometers;failure analysis;greedy algorithms;learning (artificial intelligence);pattern classification;principal component analysis;structural engineering computing;time series;vibrations","accelerometers;base-excited three-story frame structure;damage-sensitive features;environmental conditions;environmental effects;false-negative indications;false-positive indications;greedy kernel PCA-based algorithms;linear principal component analysis;machine learning algorithms;nonlinear PCA;nonlinear principal component analysis;operational conditions;operational effects;structural damage detection;structural state conditions;time-series data;vibration-based damage identification approaches","","1","","19","","","11-14 May 2015","","IEEE","IEEE Conference Publications"
"Research of Batch Scheduling with Arrival Time Based on Estimation of Distribution Algorithm","D. Li; F. Peng; X. Zhou; C. Liu","Shenyang Inst. of Autom., Shenyang, China","2014 Seventh International Symposium on Computational Intelligence and Design","20150409","2014","2","","125","130","Estimation of distribution has been used to solve the batch scheduling problem with job release problem, which minimizing the make span as the objective function. According to the characteristic of the batch scheduling problem with job release time and the estimation of distribution algorithm, this paper builds the probabilistic model based on the characteristic of batching process and designs the mechanism of personal sampling and probability update, then proposes a new estimation of distribution algorithm to solve the batch scheduling problem with job release time. The mechanism of population generation and probability updating has been improved in the standard compact genetic algorithm (a kind of EDA) which accelerate the convergence rate of algorithm. Moreover, the influence of parameter setting is investigated based on design of experiment and suitable parameter values are suggested. Simulation results based on some instances and comparisons with some exiting algorithms demonstrate the effectiveness and robustness of the proposed algorithm.","","Electronic:978-1-4799-7005-6; POD:978-1-4799-7006-3","10.1109/ISCID.2014.279","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081953","Batch Scheduling;Estimation of distribution algorithm (EDA);Probability Model","Algorithm design and analysis;Genetic algorithms;Job shop scheduling;Machine learning algorithms;Probability;Standards","design of experiments;minimisation;sampling methods;scheduling","EDA;batch scheduling problem;batching process characteristic;design of experiment;estimation-of-distribution algorithm;genetic algorithm;job release problem;job release time;makespan minimization;objective function;personal sampling mechanism;population generation mechanism;probabilistic model;probability update","","0","","14","","","13-14 Dec. 2014","","IEEE","IEEE Conference Publications"
"Modified Apriori Approach for Evade Network Intrusion Detection System","L. Lahoti; C. Chandankhede; D. Mukhopadhyay","Dept. of Inf. Technol., MIT, Pune, India","2014 International Conference on Information Technology","20150209","2014","","","374","378","Intrusion Detection System (IDS) is a software or hardware tool that repeatedly scans and monitors events that took place in a computer or a network. A set of rules are used by Signature based Network Intrusion Detection Systems (NIDS) to detect hostile traffic in network segments or packets, which are so important in detecting malicious and anomalous behavior over the network like known attacks that hackers look for new techniques to go unseen. Sometime, a single failure at any layer will cause the NIDS to miss that attack. To overcome this problem, a technique is used that will trigger a failure in that layer. Such technique is known as Evasive technique. An Evasion can be defined as any technique that modifies a visible attack into any other form in order to stay away from being detect. The proposed system is used for detecting attacks which are going on the network and also gives actual categorization of attacks. The proposed system has advantage of getting low false alarm rate and high detection rate. So that leads into decrease in complexity and overhead on the system. The paper presents the Evasion technique for customized apriori algorithm. The paper aims to make a new functional structure to evade NIDS. This framework can be used to audit NIDS. This framework shows that a proof of concept showing how to evade a self-built NIDS considering two publicly available datasets.","","Electronic:978-1-4799-8084-0; POD:978-1-4799-8133-5","10.1109/ICIT.2014.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033353","Evasion;Intrusion detection;Network intrusion detection system;Network security","Classification algorithms;Computers;Information technology;Intrusion detection;Machine learning algorithms;Monitoring","computer crime;computer network security","actual attack categorization;anomalous behavior;customized apriori algorithm;evade network intrusion detection system;evasion technique;hackers;hardware tool;hostile traffic;malicious behavior;modified apriori approach;network packets;network segments;self-built NIDS;software tool;visible attack","","0","","11","","","22-24 Dec. 2014","","IEEE","IEEE Conference Publications"
"A refinement algorithm for rank aggregation over crowdsourced comparison data","N. Hwang; S. Lee","School of Computer Science, Kookmin University, Seoul, Korea","2015 International Conference on Information Networking (ICOIN)","20150312","2015","","","430","432","Extracting ranking from pairwise comparison data has been very popular these days especially due to the huge source of comparison data available in the Internet. One of the many ways to collect a large amount of data from ordinary users is crowd sourcing. One example is reCaptcha, which converts scanned text images into text by using human recognition capability of a huge number of people.With the comparison data, there have been many algorithms proposed to extract ranking. Since the problem of extracting ranking from comparison data is NP-hard, the proposed algorithms are not guaranteed to be optimal. Thus, in this paper, we propose a simple refinement algorithm called “PM” to make the ranking results of the existing algorithms better. Basically, we check every item in the ranking whether moving the item into other ranking position can reduce the errors of the ranking results. Our refinement algorithm can be used in conjunction with other algorithms. We show that our refinement algorithm can effectively reduce the errors of the original algorithms.","1550-445X;1550445X","Electronic:978-1-4799-8342-1; POD:978-1-4799-8343-8; USB:978-1-4799-8341-4","10.1109/ICOIN.2015.7057937","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057937","","Data mining;Educational institutions;Heuristic algorithms;Internet;Machine learning algorithms;Performance evaluation;Sorting","Internet;computational complexity;pattern classification","Internet;NP-hard problem;PM algorithm;crowdsourced comparison data;position move algorithm;rank aggregation;refinement algorithm","","0","","6","","","12-14 Jan. 2015","","IEEE","IEEE Conference Publications"
"SVM for sketch recognition: Which hyperparameter interval to try?","K. T. Yesilbek; C. Sen; S. Cakmak; T. M. Sezgin","Bilgisayar M&#x00FC;hendisli&#x011F;i B&#x00F6;l&#x00FC;m&#x00FC;, Ko&#x00E7; &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye","2015 23nd Signal Processing and Communications Applications Conference (SIU)","20150622","2015","","","943","946","Hyperparameters are among the most crucial factors that effect the performance of machine learning algorithms. Since there is not a common ground on which hyperparameter combinations give the highest performance in terms of prediction accuracy, hyperparameter search needs to be conducted each time a model is to be trained. In this work, we analyzed how similar hyperparemeters perform on various datasets from sketch recognition domain. Results have shown that hyperparameter search space can be reduced to a subspace despite differences in dataset characteristics.","2165-0608;21650608","Electronic:978-1-4673-7386-9; POD:978-1-4673-7387-6","10.1109/SIU.2015.7129986","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7129986","Hyperparameter search;Support Vector Machines;cross validation;grid search;sketch data","Accuracy;Deformable models;Machine learning algorithms;Predictive models;Reactive power;Support vector machine classification","image recognition;learning (artificial intelligence);prediction theory;support vector machines","SVM;dataset characteristics;hyperparameter search space interval;machine learning algorithm;sketch recognition domain;training","","0","","15","","","16-19 May 2015","","IEEE","IEEE Conference Publications"
"Investigating sample selection bias in the relevance feedback algorithm of the vector space model for Information Retrieval","M. Melucci","Department of Information Engineering, University of Padua, Italy","2014 International Conference on Data Science and Advanced Analytics (DSAA)","20150312","2014","","","83","89","Information Retrieval (IR) is concerned with indexing and retrieving documents including information relevant to a user's information need. Relevance Feedback (RF) is an effective technique for improving IR and it consists of gathering further data representing the user's information need and automatically creating a new query. As RF relies on the ability of an IR system to learn new queries and is mostly based on statistical methods, a parallel between RF and statistical Machine Learning (ML) can be drawn. However, the effectiveness of RF is due to the biased selection of the sample data, thus contradicting the requirement that effective statistical learning is based on unbiased sample data. This paper studies this contradiction and suggests that RF cannot be straightforwardly studied within statistical ML without considering the intrinsic nature of the data managed by an IR system and of the user's information need. In particular, the paper reports that an RF algorithm is mostly influenced by the distance between the informative content of the training set and the informative content of the test set and is not influenced by sample selection bias.","","Electronic:978-1-4799-6991-3; POD:978-1-4799-6982-1","10.1109/DSAA.2014.7058056","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058056","","Algorithm design and analysis;Convergence;Machine learning algorithms;Probability distribution;Radio frequency;Training;Vectors","information retrieval;learning (artificial intelligence);statistical analysis","IR system;ML;RF algorithm;information retrieval;informative content;relevance feedback algorithm;sample selection bias;statistical machine learning;training set;vector space model","","0","","11","","","Oct. 30 2014-Nov. 1 2014","","IEEE","IEEE Conference Publications"
"Human and machine annotation in the Orchive, a large scale bioacoustic archive","S. Ness; G. Tzanetakis","Department of Computer Science, University of Victoria, Canada","2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20150209","2014","","","1136","1140","Advances in computer technology have enabled the collection, digitization, and automated processing of huge archives of bioacoustic sound. Many of the tools previously used in bioacoustics research work well with small to medium-sized audio collections, but are challenged when processing large collections ranging from tens of terabytes to petabyte size. The Orchive is a system that assists researchers to listen to, view, annotate and run advanced audio feature extraction and machine learning algorithms on large bioacoustic archives. Annotation is one of the biggest challenges in our work. In this paper, we describe our efforts to utilize experts as well as citizen scientists to participate in the process of annotating recordings. The Orchive contains over 23,000 hours of orca vocalizations collected over the course of 30 years, and represents one of the largest continuous collections of bioacoustic recordings in the world. Manual annotation is practically impossible and therefore we investigate the effectiveness of a semi-automatic approach for extracting information from these recordings, and show various experimental results. Finally we have been able to apply our automatic analysis over the a large portion of the archive and describe the computational resources required. To the best of our knowledge this is the largest archive of bioacoustic data that has even been automatically analyzed.","","Electronic:978-1-4799-7088-9; POD:978-1-4799-7089-6; USB:978-1-4799-7087-2","10.1109/GlobalSIP.2014.7032299","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032299","","Accuracy;Kernel;Logistics;Machine learning algorithms;Support vector machines;Whales","audio signal processing;bioacoustics;data analysis","Orchive;advanced audio feature extraction algorithm;annotating recordings;automatic analysis;bioacoustic data;bioacoustic recordings;bioacoustic sound;human annotation;large scale bioacoustic archive;machine annotation;machine learning algorithm;manual annotation;medium-sized audio collections;orca vocalization","","0","","23","","","3-5 Dec. 2014","","IEEE","IEEE Conference Publications"
"Pronominal anaphora resolution using salience score for Malayalam","S. Athira; T. S. Lekshmi; R. R. Rajeev; E. Sherly; P. C. Reghuraj","Govt. Engineering College, Sreekrishnapuram, Palakkad","2014 First International Conference on Computational Systems and Communications (ICCSC)","20150216","2014","","","47","51","Anaphora resolution (AR) is the process of resolving references to an entity in the discourse. The paper presents an algorithm to identify the pronominals and its antecedents in the Malayalam text input. Anaphora resolution is achieved by employing a hybrid of statistical machine learning and rule based approaches. The system is implemented by exploiting the morphological richness of the language and it makes use of parts of speech tagging, subject-object identification and person-number-gender of the NPs. We outline a simple, efficient but a naive algorithm for anaphora resolution, which computes the salience value score for each antecedents. The system performance is evaluated with precision, recall measures which produced promising results. The anaphora resolution system itself can improve the performance of many NLP applications such as text summarisation, text categorisation and term extraction.","","CD-ROM:978-1-4799-6012-5; Electronic:978-1-4799-6013-2; POD:978-1-4799-6014-9","10.1109/COMPSC.2014.7032619","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032619","Anaphora resolution;antecedents;discourse;pronominals;salience value","Accuracy;Decision trees;Hidden Markov models;Machine learning algorithms;Speech;Tagging;Training","natural language processing;text analysis","AR;Malayalam text input;NLP applications;person-number-gender;pronominal anaphora resolution system;salience score;salience value score;speech tagging;statistical machine learning;subject-object identification;term extraction;text categorisation;text summarisation","","0","","15","","","17-18 Dec. 2014","","IEEE","IEEE Conference Publications"
"Probabilistic nodes for modelling classification uncertainty for random forest","F. Baumann; K. Vogt; A. Ehlers; B. Rosenhahn","Institut f&#x00FC;r Informationsverarbeitung, Appelstra&#x00DF;e 9a, 30167 Hannover","2015 14th IAPR International Conference on Machine Vision Applications (MVA)","20150713","2015","","","510","513","In this paper, we propose to enhance the original Random Forest algorithm by introducing probabilistic nodes. Platt Scaling is used to interpret the decision of each node as a probability and was initially developed for calibrating Support Vector Machines. Nowadays it is used to calibrate the output probabilities of decision trees, boosted trees or Random Forest classifiers. In comparison to these approaches, we integrate the Platt Scaling calibration method into the decision process of every node within the ensemble of decision trees. Regarding the original Random Forest, the nodes serve as a guide to predict the path through the tree until reaching a leaf node. In this paper, we interpret the decision as a probability and incorporate more information into the decision process. The proposed approach is evaluated using two well-known machine learning datasets as well as object recognition datasets.","","Electronic:978-4-9011-2214-6; POD:978-1-4799-8247-9; USB:978-4-9011-2215-3","10.1109/MVA.2015.7153242","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153242","","Decision trees;Handwriting recognition;Machine learning algorithms;Probabilistic logic;Standards;Training;Vegetation","decision trees;image classification;learning (artificial intelligence);object recognition;support vector machines","boosted trees;classification uncertainty;decision trees;machine learning datasets;object recognition datasets;platt scaling;probabilistic nodes;random forest classifiers;support vector machines","","0","","21","","","18-22 May 2015","","IEEE","IEEE Conference Publications"
"An Intrusion Detection Model Based on Deep Belief Networks","N. Gao; L. Gao; Q. Gao; H. Wang","Dept. of Inf. Sci. & Technol., Northwest Univ., Xi'an, China","2014 Second International Conference on Advanced Cloud and Big Data","20150813","2014","","","247","252","This paper focuses on an important research problem of Big Data classification in intrusion detection system. Deep Belief Networks is introduced to the field of intrusion detection, and an intrusion detection model based on Deep Belief Networks is proposed to apply in intrusion recognition domain. The deep hierarchical model is a deep neural network classifier of a combination of multilayer unsupervised learning networks, which is called as Restricted Boltzmann Machine, and a supervised learning network, which is called as Back-propagation network. The experimental results on KDD CUP 1999 dataset demonstrate that the performance of Deep Belief Networks model is better than that of SVM and ANN.","","Electronic:978-1-4799-8085-7; POD:978-1-4799-8067-3","10.1109/CBD.2014.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176101","Deep Belief Networks;Intrusion Detection;Restricted Boltzmann Machine","Artificial neural networks;Data models;Hidden Markov models;Intrusion detection;Machine learning;Support vector machines;Training","Big Data;Boltzmann machines;backpropagation;belief networks;pattern classification;security of data","ANN;Big Data classification;KDD CUP 1999 dataset;SVM;artificial neural networks;backpropagation network;deep belief networks;deep hierarchical model;deep neural network classifier;intrusion detection model;intrusion recognition domain;multilayer unsupervised learning networks;restricted Boltzmann machine;supervised learning network;support vector machines","","1","","13","","","20-22 Nov. 2014","","IEEE","IEEE Conference Publications"
"CLOM: Counting label occurrence matrix for feature extraction in MR images","J. Nagpal; A. Vidyarthi; N. Mittal","Department of Computer Engineering, Banasthali vidyapeeth University, Jaipur, India","2015 International Conference on Signal Processing and Communication (ICSC)","20150709","2015","","","216","221","The general phenomenon for Image Classification is based on the Feature extraction mechanism. In every domain of image analysis, the classification accuracy is dependent on how better the feature set is generated which helps the machine to learn and predict the unknown sample class label. In this paper, a novel feature extraction mechanism is proposed and named as Counting Label Occurrence Matrix (CLOM). CLOM is based on the counting label of the gray level intensity values of an image and used to extract the textural features of an image for image classification. Four different orientations are used in CLOM for extracting features based on anticipated algorithm. Proposed feature extraction mechanism is dynamic in nature and is used in any domain of image processing and machine learning. CLOM algorithm is compared with some feature extraction algorithms like GLCM (gray level co-occurrence matrix) and Run length Matrix when CLOM is tested in medical domain for classification of brain tumor images. The experimental result shows that the proposed algorithm gives better accuracy than past algorithms when tested with classifiers like KNN and BPNN.","","Electronic:978-1-4799-6761-2; POD:978-1-4799-6762-9","10.1109/ICSPCom.2015.7150650","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150650","BPNN;CLOM;Feature Extraction;GLCM;Image Classification;KNN;Run length Matrix","Accuracy;Biological neural networks;Entropy;Feature extraction;Machine learning algorithms;Training;Tumors","backpropagation;biomedical MRI;brain;cancer;feature extraction;image classification;image texture;learning (artificial intelligence);matrix algebra;medical image processing;neural nets;tumours","BPNN classifiers;CLOM;GLCM;KNN classifiers;MR images;anticipated algorithm;brain tumor images;counting label occurrence matrix;feature extraction;gray level co-occurrence matrix;gray level intensity values;image analysis;image classification;image processing;machine learning;run length matrix;textural features","","0","","20","","","16-18 March 2015","","IEEE","IEEE Conference Publications"
"Fuzzy Classification With Restricted Boltzman Machines and Echo-State Networks for Predicting Potential Railway Door System Failures","O. Fink; E. Zio; U. Weidmann","Institute for Data Analysis and Process Design, Zurich University of Applied Sciences, Switzerland","IEEE Transactions on Reliability","20150828","2015","64","3","861","868","In this paper, a fuzzy classification approach applying a combination of Echo-State Networks (ESNs) and a Restricted Boltzmann Machine (RBM) is proposed for predicting potential railway rolling stock system failures using discrete-event diagnostic data. The approach is demonstrated on a case study of a railway door system with real data. Fuzzy classification enables the use of linguistic variables for the definition of the time intervals in which the failures are predicted to occur. It provides a more intuitive way to handle the predictions by the users, and increases the acceptance of the proposed approach. The research results confirm the suitability of the proposed combination of algorithms for use in predicting railway rolling stock system failures. The proposed combination of algorithms shows good performance in terms of prediction accuracy on the railway door system case study.","0018-9529;00189529","","10.1109/TR.2015.2424213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7098448","Discrete-event diagnostic data;Echo-state networks;failure prediction;fuzzy sets;railway door system failures;restricted Boltzman machines","Feature extraction;Machine learning algorithms;Monitoring;Pragmatics;Prediction algorithms;Rail transportation;Reservoirs","Boltzmann machines;doors;failure analysis;fuzzy set theory;pattern classification;railway engineering;railway rolling stock","ESN;RBM;discrete-event diagnostic data;echo-state networks;fuzzy classification approach;linguistic variables;railway door system failure prediction;railway rolling stock system failures;restricted Boltzman machines","","1","","39","","20150430","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"The Generalization Ability of Online SVM Classification Based on Markov Sampling","J. Xu; Y. Yan Tang; B. Zou; Z. Xu; L. Li; Y. Lu","Faculty of Computer Science and Information Engineering, Hubei University, Wuhan, China","IEEE Transactions on Neural Networks and Learning Systems","20150216","2015","26","3","628","639","In this paper, we consider online support vector machine (SVM) classification learning algorithms with uniformly ergodic Markov chain (u.e.M.c.) samples. We establish the bound on the misclassification error of an online SVM classification algorithm with u.e.M.c. samples based on reproducing kernel Hilbert spaces and obtain a satisfactory convergence rate. We also introduce a novel online SVM classification algorithm based on Markov sampling, and present the numerical studies on the learning ability of online SVM classification based on Markov sampling for benchmark repository. The numerical studies show that the learning performance of the online SVM classification algorithm based on Markov sampling is better than that of classical online SVM classification based on random sampling as the size of training samples is larger.","2162-237X;2162237X","","10.1109/TNNLS.2014.2361026","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926850","Generalization ability;Markov sampling;online support vector machine (SVM) classification;uniformly ergodic Markov chain (u.e.M.c.);uniformly ergodic Markov chain (u.e.M.c.).","Algorithm design and analysis;Classification algorithms;Educational institutions;Machine learning algorithms;Markov processes;Support vector machines;Training","Markov processes;generalisation (artificial intelligence);pattern classification;sampling methods;support vector machines","Markov sampling;classification learning algorithm;generalization ability;kernel Hilbert space;online SVM classification;random sampling;support vector machines;uniformly ergodic Markov chain","","5","","35","","20141016","March 2015","","IEEE","IEEE Journals & Magazines"
"Premonition of storage response class using Skyline ranked Ensemble method","K. Dheenadayalan; V. N. Muralidhara; P. Datla; G. Srinivasaraghavan; M. Shah","Qualcomm India Pvt Ltd. Bangalore, India","2014 21st International Conference on High Performance Computing (HiPC)","20150604","2014","","","1","10","Tertiary storage areas are integral parts of compute environment and are primarily used to store vast amount of data that is generated from any scientific/industry workload. Modelling the possible pattern of usage of storage area helps the administrators to take preventive actions and guide users on how to use the storage areas which are tending towards slower to unresponsive state. Treating the storage performance parameters as a time series data helps to predict the possible values for the next `n' intervals using forecasting models like ARIMA. These predicted performance parameters are used to classify if the entire storage area or a logical component is tending towards unresponsiveness. Classification is performed using the proposed Skyline ranked Ensemble model with two possible classes, i.e. high response state and low response state. Heavy load scenarios were simulated and close to 95% of the behaviour were explained using the proposed model.","1094-7256;10947256","Electronic:978-1-4799-5976-1; POD:978-1-4799-5977-8","10.1109/HiPC.2014.7116886","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116886","ARIMA;Ensemble method;Random Forest;SOM;SVM;Skyline query;storage response time","Adaptation models;Biological system modeling;Computer architecture;Machine learning algorithms;Predictive models;Sensitivity;Time factors","storage management;time series","ARIMA;forecasting models;skyline ranked ensemble method;storage response class premonition;tertiary storage areas;time series data","","0","","27","","","17-20 Dec. 2014","","IEEE","IEEE Conference Publications"
"Convergence of a Fixed-Point Algorithm under Maximum Correntropy Criterion","B. Chen; J. Wang; H. Zhao; N. Zheng; J. C. Príncipe","Institute of Artificial Intelligence and Robotics, Xi&#x2019;an Jiaotong University, Xi&#x2019;an, China","IEEE Signal Processing Letters","20150507","2015","22","10","1723","1727","The maximum correntropy criterion (MCC) has received increasing attention in signal processing and machine learning due to its robustness against outliers (or impulsive noises). Some gradient based adaptive filtering algorithms under MCC have been developed and available for practical use. The fixed-point algorithms under MCC are, however, seldom studied. In particular, too little attention has been paid to the convergence issue of the fixed-point MCC algorithms. In this letter, we will study this problem and give a sufficient condition to guarantee the convergence of a fixed-point MCC algorithm.","1070-9908;10709908","","10.1109/LSP.2015.2428713","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100862","Fixed-point algorithm;maximum correntropy criterion;robust estimation","Adaptive filters;Algorithm design and analysis;Convergence;Machine learning algorithms;Robustness;Signal processing algorithms;Sufficient conditions","adaptive filters;entropy;fixed point arithmetic;gradient methods","fixed-point MCC algorithms;fixed-point algorithm;gradient based adaptive filtering algorithms;machine learning;maximum correntropy criterion;signal processing;sufficient condition","","31","","16","","20150501","Oct. 2015","","IEEE","IEEE Journals & Magazines"
"Polarity detection of Kannada documents","Deepamala N.; Ramakanth Kumar P.","Dept. of Computer Science, R. V. College of Engineering, Bangalore, India","2015 IEEE International Advance Computing Conference (IACC)","20150713","2015","","","764","767","Document polarity detection is a part of sentiment analysis where a document is classified as a positive polarity document or a negative polarity document. The applications of polarity detection are content filtering and opinion mining. Content filtering of negative polarity documents is an important application to protect children from negativity and can be used in security filters of organizations. In this paper, dictionary based method using polarity lexicon and machine learning algorithms are applied for polarity detection of Kannada language documents. In dictionary method, a manually created polarity lexicon of 5043 Kannada words is used and compared with machine learning algorithms like Naïve Bayes and Maximum Entropy. It is observed that performance of Naïve Bayes and Maximum Entropy is better than dictionary based method with accuracy of 0.90, 0.93 and 0.78 respectively.","","CD-ROM:978-1-4799-8046-8; Electronic:978-1-4799-8047-5; POD:978-1-4799-8048-2","10.1109/IADCC.2015.7154810","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154810","Kannada language;Maximum Entropy;Naïve Bayes;Natural language processing;polarity detection;sentiment analysis","Accuracy;Dictionaries;Entropy;Machine learning algorithms;Sentiment analysis;Training","Bayes methods;entropy;learning (artificial intelligence);natural language processing;text analysis","Kannada language documents;content filtering;dictionary based method;document polarity detection;machine learning algorithms;maximum entropy;naïve Bayes;negative polarity document;opinion mining;organizations;polarity lexicon;positive polarity document;security filters;sentiment analysis","","0","","14","","","12-13 June 2015","","IEEE","IEEE Conference Publications"
"Noninvasive Brain-Computer Interfaces Based on Sensorimotor Rhythms","B. He; B. Baxter; B. J. Edelman; C. C. Cline; W. W. Ye","Dept. of Biomed. Eng., Univ. of Minnesota, Minneapolis, MN, USA","Proceedings of the IEEE","20150601","2015","103","6","907","925","Brain-computer interfaces (BCIs) have been explored in the field of neuroengineering to investigate how the brain can use these systems to control external devices. We review the principles and approaches we have taken to develop a sensorimotor rhythm electroencephalography (EEG)-based brain-computer interface (BCI). The methods include developing BCI systems incorporating the control of physical devices to increase user engagement, improving BCI systems by inversely mapping scalp-recorded EEG signals to the cortical source domain, integrating BCI with noninvasive neuromodulation strategies to improve learning, and incorporating mind-body awareness training to enhance BCI learning and performance. The challenges and merits of these strategies are discussed, together with recent findings. Our work indicates that the sensorimotor-rhythm-based noninvasive BCI has the potential to provide communication and control capabilities as an alternative to physiological motor pathways.","0018-9219;00189219","","10.1109/JPROC.2015.2407272","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7110511","Brain–computer interface (BCI);Brain???computer interface (BCI);brain–machine interface (BMI);brain???machine interface (BMI);electroencephalography (EEG);motor imagery;neural interface;sensorimotor rhythm","Brain-computer interfaces;Electroencephalography;Machine learning;Neural engineering;Performance evaluation;Wheelchairs","brain-computer interfaces;electroencephalography","BCIs;cortical source domain;electroencephalography;external device control;mind-body awareness training;neuroengineering;noninvasive brain-computer interfaces;noninvasive neuromodulation strategies;physical device control;scalp-recorded EEG signals;sensorimotor rhythms;user engagement","","5","","169","","20150520","June 2015","","IEEE","IEEE Journals & Magazines"
"Review on 2D-to-3D Image and Video Conversion Methods","S. Patil; P. Charles","Dept. of Electron. & Tele-Commun., D.Y. Patil Coll. of Eng., Pune, India","2015 International Conference on Computing Communication Control and Automation","20150716","2015","","","728","732","We present a review paper on state of the art methods of 2D to 3D image and/or video conversion. In this modern era popularity of 3D hardware is increased but, 3D contents are still dominated by its 2D counterpart. Until now many researchers have proposed different methods to close this gap. Mainly, these conversion methods are categorized in an automatic method and semi-automatic method. In automatic method human intervention is not involved, where as in semi-automatic method human operator is involved. There are distinct attributes that can be considered during conversion, like for video conversion motion and optical flow are mostly considered parameter, while for image conversion local attributes of images are considered. Computational time and design cost are the main design metrics that should be considered while designing algorithm. Each method is having its own pros and cons, depending on specification which method will be suitable for conversion is decided.","","Electronic:978-1-4799-6892-3; POD:978-1-4799-6893-0","10.1109/ICCUBEA.2015.192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155943","2D-to-3D;automatic;image conversion;semi-automatic;video conversion","Cameras;Estimation;Geometry;Machine learning algorithms;Motion segmentation;Three-dimensional displays;Visualization","image sequences;video signal processing","2D-to-3D image conversion method;2D-to-3D video conversion method;3D contents;3D hardware;human intervention;human operator;image local attributes;optical flow;semiautomatic method;video conversion motion","","0","","20","","","26-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"Greedy MaxCut algorithms and their information content","Y. Bian; A. Gronskiy; J. M. Buhmann","Department of Computer Science, ETH Zurich, Switzerland","2015 IEEE Information Theory Workshop (ITW)","20150625","2015","","","1","5","MAXCUT defines a classical NP-hard problem for graph partitioning and it serves as a typical case of the symmetric non-monotone Unconstrained Submodular Maximization (USM) problem. Applications of MAXCUT are abundant in machine learning, computer vision and statistical physics. Greedy algorithms to approximately solve MAXCUT rely on greedy vertex labelling or on an edge contraction strategy. These algorithms have been studied by measuring their approximation ratios in the worst case setting but very little is known to characterize their robustness to noise contaminations of the input data in the average case. Adapting the framework of Approximation Set Coding, we present a method to exactly measure the cardinality of the algorithmic approximation sets of five greedy MAXCUT algorithms. Their information contents are explored for graph instances generated by two different noise models: the edge reversal model and Gaussian edge weights model. The results provide insights into the robustness of different greedy heuristics and techniques for MAXCUT, which can be used for algorithm design of general USM problems.","","Electronic:978-1-4799-5526-8; POD:978-1-4799-5527-5; USB:978-1-4799-5525-1","10.1109/ITW.2015.7133122","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133122","","Algorithm design and analysis;Approximation algorithms;Approximation methods;Greedy algorithms;Machine learning algorithms;Noise;Noise measurement","Gaussian processes;computational complexity;graph theory;greedy algorithms;optimisation","Gaussian edge weights model;NP-hard problem;algorithmic approximation sets cardinality;approximation set coding;edge reversal model;graph partitioning;greedy MAXCUT algorithms;information content;noise models;symmetric nonmonotone USM problems;unconstrained submodular maximization","","0","","16","","","April 26 2015-May 1 2015","","IEEE","IEEE Conference Publications"
"Data mining in functional test content optimization","L. C. Wang","University of California at Santa Barbara, USA","The 20th Asia and South Pacific Design Automation Conference","20150312","2015","","","308","315","This paper reviews the data mining methodologies proposed for functional test content optimization where tests are sequences of instructions or transactions. Basic machine learning concepts and the key ideas of these methodologies are explained. Challenges for implementing these methodologies in practice are illustrated. Promises are demonstrated through experimental results based on industrial verification settings.","2153-6961;21536961","Electronic:978-1-4799-7792-5; POD:978-1-4799-7793-2","10.1109/ASPDAC.2015.7059023","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7059023","","Assembly;Context;Data mining;Generators;Kernel;Machine learning algorithms;Optimization","data mining;learning (artificial intelligence);optimisation","data mining;functional test content optimization;industrial verification settings;machine learning","","0","1","29","","","19-22 Jan. 2015","","IEEE","IEEE Conference Publications"
"Smart Cache: An Optimized MapReduce Implementation of Frequent Itemset Mining","D. Huang; Y. Song; R. Routray; F. Qin","","2015 IEEE International Conference on Cloud Engineering","20150423","2015","","","16","25","Frequent Item set Mining (FIM) is a classic data mining topic with many real world applications such as market basket analysis. Many algorithms including Apriori, FP-Growth, and Eclat were proposed in the FIM field. As the dataset size grows, researchers have proposed MapReduce version of FIM algorithms to meet the big data challenge. This paper proposes new improvements to the MapReduce implementation of FIM algorithm by introducing a cache layer and a selective online analyzer. We have evaluated the effectiveness and efficiency of Smart Cache via extensive experiments on four public datasets. Smart Cache can reduce on average 45.4%, and up to 97.0% of the total execution time compared with the state-of-the-art solution.","","Electronic:978-1-4799-8218-9; POD:978-1-4799-8219-6","10.1109/IC2E.2015.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092894","","Algorithm design and analysis;Data mining;Itemsets;Libraries;Linear regression;Machine learning algorithms;Turning","Big Data;cache storage;data mining","Big Data challenge;FIM;cache layer;data mining;frequent itemset mining;optimized MapReduce implementation;selective online analyzer;smart cache","","2","","35","","","9-13 March 2015","","IEEE","IEEE Conference Publications"
"Manilyzer: Automated Android Malware Detection through Manifest Analysis","S. Feldman; D. Stadther; B. Wang","Univ. of Virginia, Charlottesville, VA, USA","2014 IEEE 11th International Conference on Mobile Ad Hoc and Sensor Systems","20150209","2014","","","767","772","As the world's most popular mobile operating system, Google's Android OS is the principal target of an ever increasing mobile malware threat. To counter this emerging menace, many malware detection techniques have been proposed. A key aspect of many static detection techniques is their reliance on the permissions requested in the AndroidManifest.xml file. Although these permissions are very important, the manifest also contains additional information that can be valuable in identifying malware, which, however, has not been fully utilized by existing studies. In this paper we present Manilyzer, a system that exploits the rich information in the manifest files, produces feature vectors automatically, and uses state-of-the-art machine learning algorithms to classify applications as malicious or benign. We apply Manilyzer to 617 applications (307 malicious, 310 benign) and find that it is very effective: the accuracy is up to 90%, while the false positives and false negatives are both around 10%. In addition to classifying applications, Manilyzer is used to study the trends of permission requests in malicious applications. Through this evaluation and further analysis, it is clear that malware has evolved over time, and not all malware can be detected through static analysis of manifest files. To address this issue, we briefly explore a dynamic analysis technique that monitors network traffic using a packet sniffer.","2155-6806;21556806","Electronic:978-1-4799-6036-1; POD:978-1-4799-6037-8","10.1109/MASS.2014.65","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035780","Android security;Manilyzer;data mining;malware detection","Androids;Humanoid robots;Machine learning algorithms;Malware;Mobile communication;Receivers","Android (operating system);data mining;invasive software;learning (artificial intelligence);program diagnostics","AndroidManifest.xml file;Google Android OS;Manilyzer;automated Android malware detection technique;dynamic analysis technique;feature vectors;machine learning algorithms;manifest analysis;mobile malware threat;mobile operating system;network traffic monitors;packet sniffer;static analysis technique;static detection techniques","","1","","19","","","28-30 Oct. 2014","","IEEE","IEEE Conference Publications"
