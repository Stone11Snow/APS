"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7822674,7824133,7820689,7824242,7823861,7822012,7818560,7823893,7819675,7821661,7824836,7819134,7823232,7822748,7822289,7822720,7814611,7811562,7816924,7813733,7813651,7816541,7813730,7478419,7817112,7813447,7802746,7814825,7816493,7813749,7815484,7813191,7808102,7807872,7811252,7807785,7807857,7807889,7808014,7804021,7804195,7805862,7807300,7806606,7804998,7805855,7758098,7758096,7802887,7798209,7799873,7797722,7798690,7798968,7799295,7798940,7800528,7801541,7470473,7796165,7795360,7796454,7795321,7796524,7793911,7793838,7793751,7790325,7795833,7793141,7794909,7792490,7792849,7793871,7789485,7790221,7790307,7785327,7790075,7789454,7789906,7785021,7785785,7789481,7785783,7785727,4462545,7783243,7783235,7783208,7784645,7783594,7781850,7782062,7764563,7398015,7373625,7782857,7782754,7778637",2017/05/04 23:23:06
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Computational prediction of bacterial type IV-B effectors using C-terminal signals and machine learning algorithms","L. Zou; K. Chen","Bioinformatics Center, Department of Microbiology, College of Basic Medical Sciences, Third Military Medical University, Chongqing, China","2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)","20170102","2016","","","1","5","Many species of bacteria inject effector proteins to host cells by their type IV secretion systems(T4SS). Two main kinds of T4SS subtypes, IVA and IVB, are well studied in recent years. IVB effectors have been confirmed to be involved in the pathogenicity of various human pathogens. Discriminating these proteins in bacterial genomes are very helpful for identifying their functional roles in hosts. However, there are few effective computational methods can achieve these goals. In this study, the C-terminal sequence features were analyzed, furthermore, a novel algorithm based on machine learning was developed to predict IVB effectors in genomic proteins. Tests on datasets showed that this method can discriminate IVB effectors from non-effectors with over 94.4% accuracy and 81.6% true positive rate. Genome-wide tests in Coxiella burnetii also showed this algorithm is highly sensitive to recognize effector proteins. As a whole, this method is very helpful for new IVB effector identification and other relevant biological studies.","","Electronic:978-1-4673-9472-7; POD:978-1-5090-0012-8","10.1109/CIBCB.2016.7758098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758098","computational prediction;feature calculation;machine learning;type IVB effector","Amino acids;Bioinformatics;Genomics;Hidden Markov models;Machine learning algorithms;Microorganisms;Proteins","genomics;learning (artificial intelligence);medical signal processing;microorganisms;molecular biophysics;proteins","C-terminal sequence feature;C-terminal signal;Coxiella burnetii;bacteria inject effector protein;bacterial genome;bacterial type IV-B effector prediction;genome-wide test;genomic protein;human pathogen;machine learning algorithms;type IV secretion system","","","","","","","5-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"NMF-Based Image Quality Assessment Using Extreme Learning Machine","S. Wang; C. Deng; W. Lin; G. B. Huang; B. Zhao","School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Cybernetics","20161214","2017","47","1","232","243","Numerous state-of-the-art perceptual image quality assessment (IQA) algorithms share a common two-stage process: distortion description followed by distortion effects pooling. As for the first stage, the distortion descriptors or measurements are expected to be effective representatives of human visual variations, while the second stage should well express the relationship among quality descriptors and the perceptual visual quality. However, most of the existing quality descriptors (e.g., luminance, contrast, and gradient) do not seem to be consistent with human perception, and the effects pooling is often done in ad-hoc ways. In this paper, we propose a novel full-reference IQA metric. It applies non-negative matrix factorization (NMF) to measure image degradations by making use of the parts-based representation of NMF. On the other hand, a new machine learning technique [extreme learning machine (ELM)] is employed to address the limitations of the existing pooling techniques. Compared with neural networks and support vector regression, ELM can achieve higher learning accuracy with faster learning speed. Extensive experimental results demonstrate that the proposed metric has better performance and lower computational complexity in comparison with the relevant state-of-the-art approaches.","2168-2267;21682267","","10.1109/TCYB.2015.2512852","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398015","Extreme learning machine (ELM);human visual system (HVS);image quality assessment (IQA);non-negative matrix factorization (NMF)","Artificial neural networks;Distortion measurement;Image quality;Nonlinear distortion;Visualization","image representation;learning (artificial intelligence);matrix decomposition","ELM;NMF-based image quality assessment algorithm;extreme learning machine;image degradations;machine learning technique;nonnegative matrix factorization;novel full-reference IQA metric;parts-based representations","","1","","","","20160203","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Multilayer machine learning algorithm to classify diabetic type on knee dataset","L. H. Anjaneya; M. S. Holi","Dept. of Biomedical Engineering, Bapuji Institute of Engineering & Technology, Davangere, Karnataka, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","584","587","Since last decade, the diabetes risks are increasing in children and adults. Various approaches have been proposed for early detection of the diabetes and prevention on it. Some methods use EMG signals for diabetes classification, due to motion artifacts in the EMG signals during acquisition of signal, these approaches are not able to classify the signal efficiently. To overcome this we propose anew method by considering time domain and frequency domain features of the EMG signals and to perform the classification we use neural network. This method is executed using MATLAB tool and simulation study shows the accuracy of proposed approach is 97.05%.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7807889","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807889","Diabetes;EMG signal;classification;frequency domain;neural network;time domain feature","Biological neural networks;Conferences;Diabetes;Electromyography;Feature extraction;Insulin;Muscles","diseases;electromyography;learning (artificial intelligence);medical signal processing;neural nets;signal classification;signal detection;time-frequency analysis","EMG signals;MATLAB tool;diabetes classification;diabetes detection;diabetes prevention;diabetes risks;diabetic type classification;frequency domain;knee dataset;motion artifacts;multilayer machine learning algorithm;neural network;signal acquisition;signal classification;time domain","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"Protein fold identification using machine learning methods on contact maps","K. S. Vani; K. P. Kumar","Department of Computer Science and Engineering, V.R.Siddhartha Engineering College, Andhra Pradesh, India","2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)","20170102","2016","","","1","6","Proteins can be classified among the four structural classes of All-Alpha, All-Beta, Alpha+Beta and Alpha/Beta which are further subdivided into 27 folds. Protein fold classification problem is cited in the literature as a challenging unbalanced classification problem with the accuracy results being as low as 51.1% on the bench mark data set of Ding et al. and highest accuracy at 60.5% using 2500 features. We represent the proteins as 11-length feature vectors and adopt Synthetic Minority Over-sampling Technique (SMOTE) based boosting approach to balance the data to address the 27-way fold classification problem. We build C4.5 decision tree classifier in combination with SMOTE boosting algorithm using the novel contact map features and show that the prediction accuracy is enhanced to 64%. An additional advantage of our approach is the reduced dimensionality of the feature vector which is 11 whereas literature uses more than 100 features on average. Further, we propose an algorithm ExtractPatterns that extracts (non)rectangular 2D regions of contacts from the off-diagonal region in linear time. A feature vector of length 11 is formed using this study constituting diagonal and off-diagonal statistical features.","","Electronic:978-1-4673-9472-7; POD:978-1-5090-0012-8","10.1109/CIBCB.2016.7758096","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758096","Contact map;Decision Tree;Machine Learning;SMOTE","Amino acids;Boosting;Decision trees;Feature extraction;Prediction algorithms;Proteins;Support vector machines","bioinformatics;learning (artificial intelligence);proteins;proteomics","ExtractPattern;SMOTE-based boosting approach;contact map feature;machine learning method;protein fold classification;protein fold identification;synthetic minority over-sampling technique","","","","","","","5-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Relative entropy normalized Gaussian supervector for speech emotion recognition using kernel extreme learning machine","R. Li; D. Yang; X. Li; R. Wang; M. Xu; T. F. Zheng","School of Computer Science, Beijing Information Science & Technology University, China","2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","20170119","2016","","","1","5","Speech emotion recognition is a challenging and significant task. On the one hand, the emotion features need to be robust enough to capture the emotion information, and while on the other, machine learning algorithms need to be insensitive to model the utterance. In this paper, we presented a novel framework of speech emotion recognition to address the two above-mentioned challenges. Relative Entropy based Normalization (REN) was proposed to normalize the supervectors of Gaussian Mixture Model-Universal Background Model (GMM-UBM) as the features to emotions. The Kernel Extreme Learning Machine (KELM) was adopted as the classifier to identify the emotion represented by the normalized supervectors. Experimental results on the EMR_1309 corpus showed the proposed framework outperformed the state-of-the-art i-vector based systems.","","Electronic:978-9-8814-7682-1; POD:978-1-5090-2401-8","10.1109/APSIPA.2016.7820689","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820689","","Covariance matrices;Emotion recognition;Entropy;Feature extraction;Kernel;Speech;Speech recognition","Gaussian processes;emotion recognition;entropy;learning (artificial intelligence);mixture models;operating system kernels;speech recognition","EMR_1309 corpus;GMM-UBM supervector normalization;KELM;REN;emotion information capture;kernel extreme learning machine;relative entropy normalized Gaussian supervector;speech emotion recognition","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning for predictive modeling in management of operations of EDM equipment product","I. Ghosh; M. K. Sanyal; R. K. Jana; P. K. Dan","Department of Operations Management, Calcutta Business School, Kolkata, India","2016 Second International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)","20170116","2016","","","169","174","To sustain and excel in competitive global market, organizations often bank on high productivity and world class quality. Endeavor of this research is to comprehend and model the manufacturing process of Electrical Discharge Machining (EDM) equipment product in order to increase productivity. Outcome of EDM operation is strongly influenced by various process parameters. The paper presents a framework based on machine learning algorithms to analyze the relationship between input process parameters and EDM response to build a predictive model of EDM operations. Physical experimentations have conducted considering Discharge Current, Pulse Duration, Duty Cycle and Discharge Voltage as independent variables while Material Removal Rate has been used as target variable. Four different machine learning algorithms namely Random Forest, Support Vector Regression, Elastic Net and Bagging have been adopted as applied predictive modeling tools. Results justify the usage of machine learning methods to deal with the research problem. Statistical analysis has been conducted as well for comparative performance analysis. Further correlation based supervised feature selection methodology has been applied to identify the key predictors.","","CD:978-1-5090-1045-5; Electronic:978-1-5090-1047-9; POD:978-1-5090-1048-6; USB:978-1-5090-1046-2","10.1109/ICRCICN.2016.7813651","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813651","Bagging;Discharge Current;Discharge Voltage;Duty Cycle;Elastic Net;Electrical Discharge Machining;Pulse Duration;Random Forest;Support Vector Regression","Bagging;Discharges (electric);Kernel;Machining;Mathematical model;Predictive models;Radio frequency","electrical discharge machining;feature selection;learning (artificial intelligence);production engineering computing;production equipment;productivity;regression analysis;support vector machines","EDM equipment product;EDM operations management;bagging;competitive global market;discharge current;discharge voltage;duty cycle;elastic net;electrical discharge machining equipment product;machine learning;manufacturing process;material removal rate;organizations;predictive modeling tools;process parameters;productivity;pulse duration;random forest;statistical analysis;supervised feature selection;support vector regression","","","","","","","23-25 Sept. 2016","","IEEE","IEEE Conference Publications"
"BIST-RM: BIST-assisted reliability management of SoCs using on-chip clock sweeping and machine learning","M. Sadi; G. Contreras; D. Tran; J. Chen; L. Winemberg; M. Tehranipoor","Dept. of Electrical & Computer Engineering, University of Florida, Gainesville, USA","2016 IEEE International Test Conference (ITC)","20170105","2016","","","1","10","In this paper, we present a novel methodology, BIST-RM, to accurately predict the degradation due to aging mechanisms in a SoC at run-time by utilizing the existing LBIST hardware and software implemented Machine Learning classifier. Using an innovative method, we convert ATPG-generated transition delay patterns into LBIST patterns, and the corresponding responses are utilized in developing the predictor. A gate-overlap and path delay-aware pattern selection algorithm selects the features for the classier. Using clock sweeping, LBIST is able to capture the aging effect on targeted paths. The result of machine learning is then utilized to activate countermeasures to remedy the degradation in the field. The area and test time overhead are very low. We implemented our proposed flow on SoC benchmark circuits, and the results demonstrated worst-case prediction accuracy of 94% to 97%.","","Electronic:978-1-4673-8773-6; POD:978-1-4673-8774-3","10.1109/TEST.2016.7805862","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805862","","Aging;Clocks;Degradation;Delays;Instruction sets;Mathematical model","built-in self test;integrated circuit reliability;learning (artificial intelligence);system-on-chip","ATPG-generated transition delay patterns;BIST-RM;LBIST hardware;LBIST software;SoC;aging mechanisms;built-in-self-test-assisted reliability management;gate-overlap;logic-built-in-self-test;machine learning classifier;on-chip clock sweeping;path delay-aware pattern selection algorithm","","","","","","","15-17 Nov. 2016","","IEEE","IEEE Conference Publications"
"Comparative study of machine learning algorithms for breast cancer detection and diagnosis","D. Bazazeh; R. Shubair","Electrical & Computer Engineering Department, Khalifa University, UAE","2016 5th International Conference on Electronic Devices, Systems and Applications (ICEDSA)","20170119","2016","","","1","4","Breast cancer is one of the most widespread diseases among women in the UAE and worldwide. Correct and early diagnosis is an extremely important step in rehabilitation and treatment. However, it is not an easy one due to several uncertainties in detection using mammograms. Machine Learning (ML) techniques can be used to develop tools for physicians that can be used as an effective mechanism for early detection and diagnosis of breast cancer which will greatly enhance the survival rate of patients. This paper compares three of the most popular ML techniques commonly used for breast cancer detection and diagnosis, namely Support Vector Machine (SVM), Random Forest (RF) and Bayesian Networks (BN). The Wisconsin original breast cancer data set was used as a training set to evaluate and compare the performance of the three ML classifiers in terms of key parameters such as accuracy, recall, precision and area of ROC. The results obtained in this paper provide an overview of the state of art ML techniques for breast cancer detection.","","Electronic:978-1-5090-5306-3; POD:978-1-5090-5307-0","10.1109/ICEDSA.2016.7818560","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7818560","","Bayes methods;Breast cancer;Medical diagnostic imaging;Radio frequency;Support vector machines;Training","belief networks;cancer;learning (artificial intelligence);mammography;medical computing;patient diagnosis;pattern classification;random processes;support vector machines","BN;Bayesian Networks;ML classifiers;RF;ROC;SVM;UAE;breast cancer detection;breast cancer diagnosis;diseases;machine learning algorithms;mammograms;random forest;support vector machine","","","","","","","6-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Experimenting Machine Learning Techniques to Predict Vulnerabilities","H. Alves; B. Fonseca; N. Antunes","Inst. de Comput., Univ. Fed. de Alagoas, Macei&#x03BF;&#x0301;, Brazil","2016 Seventh Latin-American Symposium on Dependable Computing (LADC)","20161215","2016","","","151","156","Software metrics can be used as a indicator of the presence of software vulnerabilities. These metrics have been used with machine learning to predict source code prone to contain vulnerabilities. Although it is not possible to find the exact location of the flaws, the models can show which components require more attention during inspections and testing. Each new technique uses his own evaluation dataset, which many times has limited size and representativeness. In this experience report, we use a large and representative dataset to evaluate several state of the art vulnerability prediction techniques. This dataset was built with information of 2186 vulnerabilities from five widely used open source projects. Results show that the dataset can be used to distinguish which are the best techniques. It is also shown that some of the techniques can predict nearly all of the vulnerabilities present in the dataset, although with very low precisions. Finally, accuracy, precision and recall are not the most effective to characterize the effectiveness of this tools.","","Electronic:978-1-5090-5120-5; POD:978-1-5090-5121-2","10.1109/LADC.2016.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7781850","Machine Learning;Software Metrics;Software Security;Vulnerabilities","Decision trees;Kernel;Predictive models;Security;Software metrics","computer evaluation;learning (artificial intelligence);public domain software;software maintenance;software metrics;source code (software)","art vulnerability prediction techniques;evaluation dataset;machine learning;open source projects;software metrics;software vulnerabilities;source code prediction","","","","","","","19-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"An approach to improve flexible manufacturing systems with machine learning algorithms","Hang Li","Institute of Automation and Information System, Technische Universit&#x00E4;t M&#x00FC;nchen, Garching near Munich, Germany","IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society","20161222","2016","","","54","59","The electricity consumption in the industry occupies considerable ratio in the gross electricity consumption compared with the consumption in other sectors e.g. residential, agriculture etc. One crucial solution to this problem is to optimize the production structure. The grand plan “Industry 4.0” provides a more adaptable and flexible perspective for the smart factory. The complexity of a manufacturing system, on the other hand, has been enhanced. Machine learning algorithms are a cluster of excellent approaches to control a complex system and to optimize a stochastic process. In order to improve the performance of a production system, it must be formulated to an executive model at first, then the optional control policies can be selected to cope with it. In this paper, the classification algorithm and the Q-learning algorithm have been implemented to reduce the electricity consumption in an automation system. The simulation results prove that they are capable for manipulating the multi routes transporting system and the system can performance better with the implementation of the machine learning algorithms.","","Electronic:978-1-5090-3474-1; POD:978-1-5090-3475-8","10.1109/IECON.2016.7793838","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7793838","Q-learning;electricity consumption reduction;flexible manufacturing system;machine learning algorithm","Belts;Energy consumption;Flexible manufacturing systems;Industries;Machine learning algorithms;Manufacturing systems","flexible manufacturing systems;learning (artificial intelligence);optimisation;production engineering computing;stochastic processes","Q-learning algorithm;electricity consumption reduction;flexible manufacturing systems;machine learning algorithms;optimization;stochastic process","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Real time driver drowsiness detection using a logistic-regression-based machine learning algorithm","M. Babaeian; N. Bhardwaj; B. Esquivel; M. Mozumdar","Department of Electrical Engineering, California State University, Long Beach, 1250 Bellflower Blvd, Long Beach, CA 90840","2016 IEEE Green Energy and Systems Conference (IGSEC)","20161219","2016","","","1","6","The number of car accidents due to driver drowsiness is very steep. An automated non-contact system that can detect driver's drowsiness early could be lifesaving. Motivated by this dire need, we propose a novel method that can detect driver's drowsiness at an early stage by computing heart rate variation using advanced logistic regression based machine learning algorithm. Our developed technique has been tested with human subjects and it can detect drowsiness in a minimum amount of time, with an accuracy above 90%.","","Electronic:978-1-5090-2294-6; POD:978-1-5090-2295-3","10.1109/IGESC.2016.7790075","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790075","Driver drowsiness detection;electrocardiogram;heart rate variation;machine learning","Adaptive filters;Electrocardiography;Frequency-domain analysis;Hardware;Sensors;Time series analysis;Vehicles","automobiles;cardiology;driver information systems;learning (artificial intelligence);regression analysis;road accidents","advanced logistic regression-based machine learning;automated noncontact system;car accidents;heart rate variation;real time driver drowsiness detection","","","","","","","6-7 Nov. 2016","","IEEE","IEEE Conference Publications"
"An Investigation of Transfer Learning and Traditional Machine Learning Algorithms","K. R. Weiss; T. M. Khoshgoftaar","","2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)","20170116","2016","","","283","290","Previous research focusing on the evaluation of transfer learning algorithms has predominantly used real-world datasets to measure an algorithm's performance. A test with a real-world dataset exposes an algorithm to a single instance of distribution difference between the training (source) and test (target) datasets. These previous works have not measured performance over a wide-range of source and target distribution differences. We propose to use a test framework that creates many source and target datasets from a single base dataset, representing a diverse-range of distribution differences. These datasets will be used as a stress test to measure an algorithm's performance. The stress test process will measure and compare different transfer learning algorithms and traditional learning algorithms. The unique contributions of this paper, with respect to transfer learning, are defining a test framework, defining multiple distortion profiles, defining a stress test suite, and the evaluation and comparison of different transfer learning and traditional machine learning algorithms over a wide-range of distributions.","","Electronic:978-1-5090-4459-7; POD:978-1-5090-4460-3","10.1109/ICTAI.2016.0051","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814611","Distortion profiles;Domain adaptation;Test framework;Traditional machine learning;Transfer learning","Algorithm design and analysis;Distortion;Machine learning algorithms;Stress;Testing;Training;Training data","learning (artificial intelligence)","distribution differences;machine learning algorithms;multiple distortion profiles;real-world datasets;stress test suite;test framework;transfer learning algorithms","","","","","","","6-8 Nov. 2016","","IEEE","IEEE Conference Publications"
"Unsupervised Feature Learning Classification With Radial Basis Function Extreme Learning Machine Using Graphic Processors","D. Lam; D. Wunsch","Department of Electrical and Computer Engineering, Applied Computational Intelligence Laboratory, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Cybernetics","20161214","2017","47","1","224","231","Ever-increasing size and complexity of data sets create challenges and potential tradeoffs of accuracy and speed in learning algorithms. This paper offers progress on both fronts. It presents a mechanism to train the unsupervised learning features learned from only one layer to improve performance in both speed and accuracy. The features are learned by an unsupervised feature learning (UFL) algorithm. Then, those features are trained by a fast radial basis function (RBF) extreme learning machine (ELM). By exploiting the massive parallel computing attribute of modern graphics processing unit, a customized compute unified device architecture (CUDA) kernel is developed to further speed up the computing of the RBF kernel in the ELM. Results tested on Canadian Institute for Advanced Research and Mixed National Institute of Standards and Technology data sets confirm the UFL RBF ELM achieves high accuracy, and the CUDA implementation is up to 20 times faster than CPU and the naive parallel approach.","2168-2267;21682267","","10.1109/TCYB.2015.2511149","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373625","Compute unified device architecture (CUDA);extreme learning machine (ELM);neural network;radial basis function (RBF);support vector machine (SVM)","Graphics processing units;Instruction sets;Kernel;Machine learning;Support vector machines;Training","parallel architectures;parallel processing;radial basis function networks;unsupervised learning","RBF ELM;UFL algorithm;customized compute unified device architecture;graphic processors;massive parallel computing;modern graphics processing unit;radial basis function extreme learning machine;unsupervised feature learning classification algorithm","","","","","","20160106","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Passengers' choices on airport drop-off service: A decision forecast based on social learning and machine learning","Z. Wang; J. Peng; Y. Chen; L. Ning","College of Computer Science, Sichuan University, Chengdu, 610065","2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)","20161219","2016","","","485","489","Airport drop-off service provided by airlines is a chauffeur-driven service (i.e. Uber and DiDi) as an emerging travel choice for travelers. More and more passenger enjoy the drop-off service. In practice, we find an interesting question: if a passenger has ever choice the drop-off service, whether they are willing to recommend this service to other traveler? Although the acknowledgment that social learning is related to travel decision is promoted, quantitative analysis about how social learning shape and impact the decision of passengers is still limited. We study and estimate a diffusion probability between different passengers by proposing a CCM (Co-travel Link Cascade Model) based on a modified EM iterative algorithm. Then, we segment passengers into three types (Influenced, Unchecked and Immune). The three types of passengers are predicated by approaches of IC-like model, Random Forest model and probabilistic model, respectively. In addition, we also design a parallel implementation of our proposed algorithm in the Apache Spark distributed data processing environment. Experimental results on a real aviation data set demonstrate that CCM can efficiently infer the decision of travelers.","","CD:978-1-5090-1254-1; Electronic:978-1-5090-1256-5; POD:978-1-5090-1257-2","10.1109/CCIS.2016.7790307","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790307","Airport drop-off service;EM algorithm;Machine learning;Parallel algorithm;Social influence","Airports;Algorithm design and analysis;Atmospheric modeling;Decision support systems;Machine learning algorithms;Sparks","airports;iterative methods;learning (artificial intelligence);parallel algorithms;probability;social sciences computing;travel industry","Apache Spark distributed data processing environment;CCM;airlines;airport drop-off service;chauffeur-driven service;co-travel link cascade model;decision forecasting;diffusion probability;machine learning;modified EM iterative algorithm;parallel algorithm;random forest model;social learning","","","","","","","17-19 Aug. 2016","","IEEE","IEEE Conference Publications"
"A robust low cost approach for real time car positioning in a smart city using Extended Kalman Filter and evolutionary machine learning","I. Belhajem; Y. Ben Maissa; A. Tamtaoui","Laboratory of Telecommunications, Networks and Service Systems, National Institute of Posts and Telecommunications, Rabat, Morocco","2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)","20170105","2016","","","806","811","A smart city is emerging as an application of information and communication technologies to mitigate the problems generated by the urban population growth. One of the smart city solutions is to establish an efficient fleet management relating to the use of a fleet of vehicles (e.g., ambulances and police vehicles). The most basic function in a fleet management system is the real time vehicle tracking component. This component is usually the Differential Global Positioning System (DGPS) or the integration of Global Positioning System (GPS) and Inertial Navigation Systems (INS). To predict the position, the Extended Kalman Filter (EKF) is generally applied using the sensor's measures and the GPS position as a helper. However, the DGPS high cost solution still suffers from GPS satellite signals loss due to multipath errors and the INS require more complex computing. Furthermore, the EKF performance depends on the vehicle dynamic variations and may quickly diverge because of environment changes (i.e, GPS failures by obstructions from building and trees). In this paper, we present a robust low cost approach using EKF and neural networks (NN) with Genetic Algorithm (GA) to reliably estimate the real time vehicle position using GPS enhanced with low cost Dead Reckoning (DR) sensors. While GPS signals are available, we train the NN with GA on different dynamics and outage times to learn the position errors so we can correct the future EKF predictions during GPS signal outages. We obtain empirically an improvement of up to 95% over the simple EKF predictions in case of GPS failures.","","Electronic:978-1-5090-0751-6; POD:978-1-5090-0752-3","10.1109/CIST.2016.7804998","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804998","Dead Reckoning;Extended Kalman Filter;Genetic Algorithm;Global Positioning System;data fusion;intelligent transportation systems;low cost;neural networks;smart cities","Artificial neural networks;Genetic algorithms;Global Positioning System;Kalman filters;Sensors;Vehicle dynamics;Vehicles","Global Positioning System;Kalman filters;genetic algorithms;intelligent transportation systems;learning (artificial intelligence);neural nets;nonlinear filters;smart cities","EKF;EKF predictions;GPS failures;GPS position;GPS signal outages;NN training;evolutionary machine learning;extended Kalman filter;fleet management;genetic algorithm;global positioning system;information and communication technologies;low cost dead reckoning sensors;neural networks;position errors;real time car positioning;smart city;urban population growth;vehicle tracking component","","","","","","","24-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Evolutionary Online Machine Learning from Imbalanced Data","A. Stein","Org. Comput. Group, Univ. of Augsburg, Augsburg, Germany","2016 IEEE 1st International Workshops on Foundations and Applications of Self* Systems (FAS*W)","20161219","2016","","","281","286","The discipline of machine learning has raised plenty of well-understood and partially well-studied challenges. Research has been concerned with issues such as incompletely labeled or missing data, dataset imbalances regarding the distributions of the target values, as well as the non-deterministic and unpredictable behavior of non-stationary environments. In this article, one particular challenge will be reviewed and motivated - the challenge of online learning from imbalanced data common in real world environments. It is hypothesized how interpolation between already gained knowledge and a proactive exploration of the input space may lead to beneficial effects when learning from data streams exhibiting imbalances. After the definition of this doctoral study's objectives, a reference evolutionary online machine learning technique is briefly introduced. On this basis, all aspects that will be thoroughly investigated are sketched and finally integrated into a research schedule.","","Electronic:978-1-5090-3651-6; POD:978-1-5090-3652-3","10.1109/FAS-W.2016.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789485","Evolutionary Machine Learning;Imbalanced Data;Interpolation;Online Machine Learning;Proactive Problem Space Exploration","Adaptive systems;Approximation algorithms;Data models;Machine learning algorithms;Medical services;Training;Training data","interpolation;learning (artificial intelligence)","imbalanced data;interpolation;proactive exploration;reference evolutionary online machine learning technique","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"Hyperspectral image classification using spatial spectral features and machine learning approach","J. K. Dhandhalya; S. K. Parmar","EC Department GCET, Gujarat, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","1161","1165","Hyperspectral imaging is a technique which gathers large number of images at various wavelength for the same area of Earth. Once Hyperspectral Image (HSI) has been acquired, a meaningful information can be obtained by further processing it. Processing of HSI must aim at achieving of given goals: detect and classify the elementary materials for each pixel. Hyperspectral image classification is an active area to allocate distinct label to each pixel vector so that it is well defined by a given class. For classification of HSI, Support Vector Machine (SVM) is extensively used. To improve the classification accuracy, spectral-spatial preprocessing technique called Multi hypothesis (MH) prediction has been used prior to SVM classifier. This processed HSI will results in less intraclass inconsistency compared to original image and it gives robust classification in the existence of noise. Major contribution of this work is an improvement of classification accuracy and to achieve it, a post processing step after classification has been applied. Spatial domain filter called, Median filter has been used to smooth out wrong classified samples and hence further improvement in classification accuracy has been achieved.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7808014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808014","Hyperspectral Image (HSI);Median filter;Multi Hypothesis (MH) prediction;Support Vector Machine (SVM)","Conferences;Hyperspectral imaging;Market research;Reflectivity;Support vector machines;Training","hyperspectral imaging;image classification;image filtering;learning (artificial intelligence);median filters;spatial filters;support vector machines","HSI;MH prediction;SVM;hyperspectral image classification;hyperspectral imaging;intraclass inconsistency;machine learning;median filter;multihypothesis prediction;spatial domain filter;spatial spectral features;spectral-spatial preprocessing;support vector machine","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"Stock market prediction using machine learning techniques","M. Usmani; S. H. Adil; K. Raza; S. S. A. Ali","Department of Computer Science, Iqra University, Karachi, Pakistan","2016 3rd International Conference on Computer and Information Sciences (ICCOINS)","20161215","2016","","","322","327","The main objective of this research is to predict the market performance of Karachi Stock Exchange (KSE) on day closing using different machine learning techniques. The prediction model uses different attributes as an input and predicts market as Positive & Negative. The attributes used in the model includes Oil rates, Gold & Silver rates, Interest rate, Foreign Exchange (FEX) rate, NEWS and social media feed. The old statistical techniques including Simple Moving Average (SMA) and Autoregressive Integrated Moving Average (ARIMA) are also used as input. The machine learning techniques including Single Layer Perceptron (SLP), Multi-Layer Perceptron (MLP), Radial Basis Function (RBF) and Support Vector Machine (SVM) are compared. All these attributes are studied separately also. The algorithm MLP performed best as compared to other techniques. The oil rate attribute was found to be most relevant to market performance. The results suggest that performance of KSE-100 index can be predicted with machine learning techniques.","","Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1","10.1109/ICCOINS.2016.7783235","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783235","KSE-100 Index;Neural Networks;Stock Prediction;Support Vector Machine","Computational modeling;Computer science;Data models;Neurons;Predictive models;Stock markets;Support vector machines","autoregressive moving average processes;learning (artificial intelligence);multilayer perceptrons;radial basis function networks;stock markets;support vector machines","ARIMA;FEX rate;Foreign Exchange rate;Gold rates;Interest rate;KSE-100 index;Karachi Stock Exchange;MLP;NEWS;Oil rates;RBF;SLP;SVM;Silver rates;autoregressive integrated moving average;machine learning;multilayer perceptron;radial basis function;simple moving average;single layer perceptron;social media feed;statistical techniques;stock market prediction;support vector machine","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"A Stackelberg game perspective on the conflict between machine learning and data obfuscation","J. Pawlick; Q. Zhu","New York University, Tandon School of Engineering, Department of Electrical and Computer Engineering","2016 IEEE International Workshop on Information Forensics and Security (WIFS)","20170119","2016","","","1","6","Data is the new oil; this refrain is repeated extensively in the age of internet tracking, machine learning, and data analytics. As data collection becomes more personal and pervasive, however, public pressure is mounting for privacy protection. In this atmosphere, developers have created applications to add noise to user attributes visible to tracking algorithms. This creates a strategic interaction between trackers and users when incentives to maintain privacy and improve accuracy are misaligned. In this paper, we conceptualize this conflict through an N + 1-player, augmented Stackelberg game. First a machine learner declares a privacy protection level, and then users respond by choosing their own perturbation amounts. We use the general frameworks of differential privacy and empirical risk minimization to quantify the utility components due to privacy and accuracy, respectively. In equilibrium, each user perturbs her data independently, which leads to a high net loss in accuracy. To remedy this scenario, we show that the learner improves his utility by proactively perturbing the data himself. While other work in this area has studied privacy markets and mechanism design for truthful reporting of user information, we take a different viewpoint by considering both user and learnerperturbation.","","Electronic:978-1-5090-1138-4; POD:978-1-5090-1139-1","10.1109/WIFS.2016.7823893","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823893","","","data analysis;data protection;game theory;learning (artificial intelligence);minimisation;risk management","Internet tracking;N + 1-player augmented Stackelberg game;data analytics;data collection;data obfuscation;differential privacy;empirical risk minimization;machine learning;perturbation amounts;privacy protection level;user information","","","","","","","4-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Distributed Machine Learning with Self-Organizing Mobile Agents for Earthquake Monitoring","S. Bosse","Dept. of Math. & Comput. Sci., Univ. of Bremen, Bremen, Germany","2016 IEEE 1st International Workshops on Foundations and Applications of Self* Systems (FAS*W)","20161219","2016","","","126","132","Ubiquitous computing and The Internet-of-Things (IoT) raises rapidly in today's life and is becoming part of self-organizing systems (SoS). A unified and scalable information processing and communication methodology using mobile agents is presented to merge the IoT with Mobile and Cloud environments seamless. A portable and scalable Agent Processing Platform (APP) is an enabling technology that is central for the deployment of Multi-agent Systems (MAS) in strong heterogeneous networks including the Internet. A large-scale distributed heterogeneous seismic sensor and geodetic network used for earthquake analysis is one example, which can be extended by ubiquitous sensing devices like smart phones. To simplify the development and deployment of MAS in the Internet domain agents are directly implemented in JavaScript (JS). The proposed JS Agent Machine (JAM) is an enabling technology. It is capable to execute AgentJS agents in a sandbox environment with full run-time protection, low-resource requirements, and Machine Learning as a service. A simulation of a seismic network and real earthquake data demonstrates the deployment of the JAM platform. Different (mobile) agents perform sensor sensing, aggregation, local learning and prediction, global voting, and the application.","","Electronic:978-1-5090-3651-6; POD:978-1-5090-3652-3","10.1109/FAS-W.2016.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789454","Agent Platforms;Distributed Learning;Earthquake Monitoring;Self-organizing Systems","Smart phones","Internet of Things;Java;authoring languages;cloud computing;computerised monitoring;distributed sensors;earthquakes;geodesy;geophysics computing;learning (artificial intelligence);mobile agents;mobile computing;multi-agent systems;seismology","APP;AgentJS agents;Internet domain agents;Internet-of-Things;IoT;JAM platform;JS agent machine;JavaScript;MAS;SoS;agent processing platform;cloud environments;communication methodology;distributed machine learning;earthquake monitoring;full run-time protection;geodetic network;heterogeneous networks;large-scale distributed heterogeneous seismic sensor;mobile environments;multiagent systems;sandbox environment;scalable information processing;seismic network;self-organizing mobile agents;smart phones;ubiquitous computing;ubiquitous sensing devices","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for recognising woody plants on railway trackbeds","R. G. Nyberg","Dalarna University, Dept. of Informatics, Borlange, Sweden","The International Conference on Railway Engineering (ICRE) 2016","20170116","2016","","","1","5","The purpose of this work in progress study was to test the concept of recognising plants using images acquired by image sensors in a controlled noise-free environment. The presence of vegetation on railway trackbeds and embankments presents potential problems. Woody plants (e.g. Scots pine, Norway spruce and birch) often establish themselves on railway trackbeds. This may cause problems because legal herbicides are not effective in controlling them; this is particularly the case for conifers. Thus, if maintenance administrators knew the spatial position of plants along the railway system, it may be feasible to mechanically harvest them. Primary data were collected outdoors comprising around 700 leaves and conifer seedlings from 11 species. These were then photographed in a laboratory environment. In order to classify the species in the acquired image set, a machine learning approach known as Bag-of-Features (BoF) was chosen. Irrespective of the chosen type of feature extraction and classifier, the ability to classify a previously unseen plant correctly was greater than 85%. The maintenance planning of vegetation control could be improved if plants were recognised and localised. It may be feasible to mechanically harvest them (in particular, woody plants). In addition, listed endangered species growing on the trackbeds can be avoided. Both cases are likely to reduce the amount of herbicides, which often is in the interest of public opinion. Bearing in mind that natural objects like plants are often more heterogeneous within their own class rather than outside it, the results do indeed present a stable classification performance, which is a sound prerequisite in order to later take the next step to include a natural background. Where relevant, species can also be listed under the Endangered Species Act.","","Online|Paper:978-1-78561-293-0|978-1-78561-292-3","10.1049/cp.2016.0513","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816541","Machine learning;computer vision;maintenance planning;railway;weeds","","feature extraction;image classification;learning (artificial intelligence);mechanical engineering computing;railways","BoF;Norway spruce;Scots pine;bag-of-features;birch;conifer seedlings;controlled noise-free environment;embankments;feature extraction;image acquisition;image classifier;image sensors;machine learning approach;maintenance planning;railway system;railway trackbeds;vegetation control;woody plant recognition","","","","","","","12-13 May 2016","","IET","IET Conference Publications"
"JVM characterization framework for workload generated as per machine learning benckmark and spark framework","S. Chidambaram; S. Saraswati; R. Ramachandra; J. B. Huttanagoudar; N. Hema; R. Roopalakshmi","Hewlett Packard (India) Software Operation Private Ltd, Bangalore - 560048, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","1598","1602","Today there are plenty of frameworks to assist the development of Big-data applications. Computation and Storage are two major activities in these applications. Spark framework has replaced Map-Reduce in Hadoop, which is the preferred analytics engine for Big-data applications. Java Virtual Machine (JVM) is used as execution platform irrespective of which framework is used for development. In the production environment it is essential to monitor the health of application to gain better performance. The parameters like memory usage, CPU utilization and frequency of Garbage Collection etc., will help to decide on the health of application. In this paper a framework is proposed to characterize the JVM behavior to monitor the health of application. Workload generated by running Machine Learning algorithms available in Spark Benchmark Suite.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7808102","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808102","Apache Spark;Big-Data;In-Memory Analytics;Java Virtual Machine;Machine Learnig","Benchmark testing;Clustering algorithms;Java;Machine learning algorithms;Monitoring;Sparks","Java;learning (artificial intelligence);virtual machines","JVM characterization framework;Java virtual machine;Spark framework;application health monitoring;machine learning benckmark","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"Tackling the Cloud Adoption Dilemma - A User Centric Concept to Control Cloud Migration Processes by Using Machine Learning Technologies","M. Diener; L. Blessing; N. Rappel","Dept. of Inf. Syst., Univ. of Regensburg, Regensburg, Germany","2016 11th International Conference on Availability, Reliability and Security (ARES)","20161215","2016","","","776","785","Research studies have shown that especially enterprises in European countries are afraid of losing outsourced data or unauthorized access. Despite various existing cloud security mechanisms companies are currently hesitating to adopt cloud resources. This phenomenon is also known as cloud adoption dilemma. We think that data classification is a promising technique that should be considered in the context of cloud security, supporting cloud migration processes. By using classification techniques enterprises are able to control which documents are suited for Cloud Computing and which cloud service providers are sufficient for protecting sensitive documents. In this work we present an efficient concept that involves enterprises' employees and authorities, making it possible to apply powerful security policies in a simple way. We make use of a well-established machine learning algorithm in our developed tool, identifying security levels for different types of documents. Thus, cloud migration processes can become more transparent and enterprises obtain the ability to discuss more openly about adopting innovative cloud services.","","Electronic:978-1-5090-0990-9; POD:978-1-5090-0991-6; USB:978-1-5090-0989-3","10.1109/ARES.2016.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784645","cloud adoption;cloud computing;cloud migration;cloud security;data classification;machine learning;naive bayes classifier;supervised learning;user centric tool","Cloud computing;Companies;Context;Encryption;Machine learning algorithms;Sensitivity","cloud computing;learning (artificial intelligence);pattern classification;security of data","European countries;cloud adoption dilemma;cloud computing;cloud migration processes;cloud resources;cloud security;cloud service providers;data classification;enterprises;machine learning;security policies;sensitive documents protection;user centric concept","","","","","","","Aug. 31 2016-Sept. 2 2016","","IEEE","IEEE Conference Publications"
"Deeper and cheaper machine learning [Top Tech 2017]","D. Schneider","","IEEE Spectrum","20170116","2017","54","1","42","43","Last March, Google's computers roundly beat the world-class Go champion Lee Sedol, marking a milestone in artificial intelligence. The winning computer program, created by researchers at Google DeepMind in London, used an artificial neural network that took advantage of what's known as deep learning, a strategy by which neural networks involving many layers of processing are configured in an automated fashion to solve the problem at hand.","0018-9235;00189235","","10.1109/MSPEC.2017.7802746","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802746","","Computers;Field programmable gate arrays;Google;Hardware;Machine learning;Software;Tensile stress","learning (artificial intelligence);neural nets","Go champion;Google DeepMind;London;artificial intelligence;artificial neural network;computer program;machine learning","","","","","","","January 2017","","IEEE","IEEE Journals & Magazines"
"Critical issues of applying machine learning to condition monitoring for failure diagnosis","F. Q. Yuan","Department of Engineering and Safety, University of Troms&#x00F8;, Norway","2016 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)","20161229","2016","","","1903","1907","Machine learning is a hot topic recently. For condition monitoring, the machine learning is mainly used to improve the failure diagnosis accuracy, as the machine learning can provide flexible decision function. This paper firstly discusses the advantage and disadvantage of the state of art condition monitoring methods. It figures out the machine learning techniques follow a general procedure and can be unified into a general framework. Later on, it figures out some key issues of applying machine learning to condition monitoring. Two examples are given to demonstrate the advantage and disadvantage of machine learning.","","Electronic:978-1-5090-3665-3; POD:978-1-5090-3666-0; USB:978-1-5090-3664-6","10.1109/IEEM.2016.7798209","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798209","Condition Monitoring;Data Driven Model;Feature Extraction;Machine Learning","Analytical models;Art;Condition monitoring;Data models;Feature extraction;Prognostics and health management;Support vector machines","condition monitoring;fault diagnosis;learning (artificial intelligence)","condition monitoring;failure diagnosis;machine learning","","","","","","","4-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Generating Routing-Driven Power Distribution Networks with Machine-Learning Technique","W. H. Chang; C. H. Lin; S. P. Mu; L. D. Chen; C. H. Tsai; Y. C. Chiu; M. C. T. Chao","Dept. of Electronics Engineering & Institute of Electronics, National Chiao-Tung University, Hsinchu, Taiwan. (e-mail: o0000032@yahoo.com.tw).","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2017","PP","99","1","1","As technology node keeps scaling and design complexity keeps increasing, power distribution networks (PDNs) require more routing resource to meet IR-drop and EM constraints. This paper presents a design flow to generate a PDN that can result in near-minimal overhead for the routing of the underlying standard cells while satisfying both IR-drop and EM constraints based on a given cell placement. The design flow relies on a machine-learning model to quickly predict the total wire length of global route associated with a given PDN configuration in order to speed up the search process. The experimental results based on various 28nm industrial block designs have demonstrated the accuracy of the learned model for predicting the routing cost and the effectiveness of the proposed framework for reducing the routing cost of the final PDN.","0278-0070;02780070","","10.1109/TCAD.2017.2648842","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807300","IR drop;electromigration;machine learning;power grid design;routing cost model;routing-driven","Buildings;Indexes;Metals;Predictive models;Routing;Standards;Wires","","","","","","","","20170105","","","IEEE","IEEE Early Access Articles"
"Rotor position estimator based on machine learning","Z. Makni; W. Zine","Valeo Group Electronic Expertise and Development Services (GEEDS), Cr&#x00E9;teil, France","IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society","20161222","2016","","","6687","6692","As considered in this study, machine learning is a field of artificial intelligence which provides computation algorithms without explicit programming. The calculation rules are learned from a set of data that associates the inputs to the outputs. Machine learning based estimator is developed to predict the rotor position of an electrical machine. The estimator requires only the phase currents to be measured. An off-line training makes the model learn to associate rotor position to each instantaneous current measurement. The estimation precision depends on the quality of current measurements and on the effectiveness of the training. Compared to rotor position acquired using mechanical sensor, the estimation error doesn't exceed few percents which is enough to implement a sensorless control or to make safe redundancy of the mechanical sensor. The estimator is developed for a permanent magnets synchronous machine used on the power train of an electric vehicle. Potentially, machine learning based estimator can cover all the speed/torque range of the machine.","","Electronic:978-1-5090-3474-1; POD:978-1-5090-3475-8","10.1109/IECON.2016.7793911","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7793911","automotive power train;electrical drive;machine learning;position estimation;sensorless control","Inverters;Reliability;Rotors","electric current control;electric current measurement;learning systems;machine control;permanent magnet machines;position control;rotors;synchronous machines","artificial intelligence;electric vehicle;electrical machine;estimation precision;instantaneous current measurement;machine learning based estimator;machine speed;machine torque;mechanical sensor;off-line training;permanent magnets synchronous machine;phase currents;power train;rotor position estimator;sensorless control","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Adaptive nonsymmetrical demodulation based on machine learning to mitigate time-varying impairments","J. J. G. Torres; A. Chiuchiarelli; V. A. Thomas; S. E. Ralph; A. M. C. Soto; N. G. González","Universidad de Antioquia, GITA Research Group, Calle 67 No 53-108, Medell&#x00ED;n, Colombia","2016 IEEE Avionics and Vehicle Fiber-Optics and Photonics Conference (AVFOP)","20161219","2016","","","289","290","We proposed and experimentally demonstrated a machine learning-based nonsymmetrical demodulation technique for a DSP-enabled receiver, with the aim of enabling time-varying nonlinear mitigation. Experimental results showed that nonsymmetrical demodulation can reduce the SER by up to 0.7 decades, when assuming time frames consisting of 10 k symbols and fiber transmission of 250 km. The proposed technique is transparent to the specific source of nonlinearity, which makes it simple yet robust. This machine learning method may also allow simplification of the standard demodulation blocks in particular the equalizer. Employing short time windows for demodulation further enables inline optical monitoring, which is a valuable diagnostic tool for future terabit optical communication systems.","","Electronic:978-1-5090-1599-3; POD:978-1-5090-1600-6; USB:978-1-5090-1598-6","10.1109/AVFOP.2016.7789906","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789906","","Clustering algorithms;Demodulation;Optical fiber communication;Optical noise;Receivers;Signal to noise ratio","demodulation;learning (artificial intelligence);optical fibre communication;optical receivers;quadrature amplitude modulation","DSP-enabled receiver;SER;adaptive nonsymmetrical demodulation;distance 250 km;fiber transmission;machine learning;optical monitoring;terabit optical communication systems;time-varying nonlinear mitigation","","","","","","","Oct. 31 2016-Nov. 3 2016","","IEEE","IEEE Conference Publications"
"VLSI Extreme Learning Machine: A Design Space Exploration","E. Yao; A. Basu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","20161226","2017","25","1","60","74","In this paper, we describe a compact low-power high-performance hardware implementation of extreme learning machine for machine learning applications. Mismatches in current mirrors are used to perform the vector-matrix multiplication that forms the first stage of this classifier and is the most computationally intensive. Both regression and classification (on UCI data sets) are demonstrated and a design space tradeoff between speed, power, and accuracy is explored. Our results indicate that for a wide set of problems, σ V<sub>T</sub> in the range of 15-25 mV gives optimal results. An input weight matrix rotation method to extend the input dimension and hidden layer size beyond the physical limits imposed by the chip is also described. This allows us to overcome a major limit imposed on most hardware machine learners. The chip is implemented in a 0.35-μm CMOS process and occupies a die area of around 5 mm × 5 mm. Operating from a 1 V power supply, it achieves an energy efficiency of 0.47 pJ/MAC at a classification rate of 31.6 kHz.","1063-8210;10638210","","10.1109/TVLSI.2016.2558842","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7470473","Classifier;extreme learning machine (ELM);low power;machine learning;neural networks","Hardware;Machine learning algorithms;Mirrors;Neurons;Radiation detectors;Transistors;Very large scale integration","CMOS integrated circuits;VLSI;current mirrors;energy conservation;integrated circuit design;learning (artificial intelligence);low-power electronics;matrix multiplication;neural nets;pattern classification;power aware computing;regression analysis;vectors","CMOS process;VLSI extreme learning machine;classifier;current mirrors;design space exploration;design space tradeoff;energy efficiency;hidden layer size;input weight matrix rotation method;low-power high-performance hardware implementation;regression analysis;size 0.35 mum;vector-matrix multiplication;voltage 1 V","","","","","","20160517","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Machine learning-based defense against process-aware attacks on Industrial Control Systems","A. Keliris; H. Salehghaffari; B. Cairl; P. Krishnamurthy; M. Maniatakos; F. Khorrami","Tandon School of Engineering, New York University, New York, USA","2016 IEEE International Test Conference (ITC)","20170105","2016","","","1","10","The modernization of Industrial Control Systems (ICS), primarily targeting increased efficiency and controllability through integration of Information Technologies (IT), introduced the unwanted side effect of extending the ICS cyber-security threat landscape. ICS are facing new security challenges and are exposed to the same vulnerabilities that plague IT, as demonstrated by the increasing number of incidents targeting ICS. Due to the criticality and unique nature of these systems, it is important to devise novel defense mechanisms that incorporate knowledge of the underlying physical model, and can detect attacks in early phases. To this end, we study a benchmark chemical process, and enumerate the various categories of attack vectors and their practical applicability on hardware controllers in a Hardware-In-The-Loop testbed. Leveraging the observed implications of the categorized attacks on the process, as well as the profile of typical disturbances, we follow a data-driven approach to detect anomalies that are early indicators of malicious activity.","","Electronic:978-1-4673-8773-6; POD:978-1-4673-8774-3","10.1109/TEST.2016.7805855","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805855","","Hardware;Integrated circuits;Mathematical model;Process control;Real-time systems;Security;Software","control engineering computing;industrial control;learning (artificial intelligence);production engineering computing;security of data","ICS cybersecurity threat landscape;IT;Information Technologies;attack vectors;benchmark chemical process;data-driven approach;defense mechanisms;hardware controllers;hardware-in-the-loop testbed;industrial control systems;machine learning;practical applicability;process-aware attacks","","","","","","","15-17 Nov. 2016","","IEEE","IEEE Conference Publications"
"Utilizing Dictionary Learning and Machine Learning for Blind Quality Assessment of 3-D Images","W. Zhou; W. Qiu; M. W. Wu","School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou 310023, China, and is also with the Institute of Information and Communication Engineering, Zhejiang University, Hangzhou 310027, China.","IEEE Transactions on Broadcasting","","2017","PP","99","1","12","In recent years, perceptual objective quality assessment of 3-D images has become an intense focus of research. In this paper, we propose an efficient blind 3-D image quality assessment (IQA) metric that utilizes binocular vision-based dictionary learning (DL) and k-nearest-neighbors (KNN)-based machine learning (ML) to more accurately align with human subjective judgments. More specifically, in the DL stage, histogram representations from the local patterns of simple and complex cells are concatenated to form basic feature vectors. Then, by using a collaborative representation algorithm, the learned binocular quality-aware features of the distorted 3-D image can be efficiently represented by a linear combination of only a few of these basic feature vectors. In the ML stage, we intuitively simulate the complex high-level behaviors of human perceptual activity with KNN-based ML, which transfers the weighted human subjective quality scores from the annotated 3-D images to the query 3-D image. Our results using three standard subject-rated 3-D-IQA databases confirm that the proposed metric consistently aligns with the subjective ratings and outperforms many representative blind metrics.","0018-9316;00189316","","10.1109/TBC.2016.2638620","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811252","3D image quality assessment;binocular vision;dictionary learning;local pattern;machine learning","Dictionaries;Feature extraction;Measurement;Retina;Three-dimensional displays;Visual perception;Visualization","","","","","","","","20170109","","","IEEE","IEEE Early Access Articles"
"Proposed machine learning system to predict and estimate impulse noise in OFDM communication system","A. N. Hasan; T. Shongwe","Department of Electrical and Electronic Engineering Technology, University of Johannesburg, P. O. Box 17011, Doornfontein, 2028, Johannesburg, South Africa","IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society","20161222","2016","","","1016","1020","This paper investigates the use of machine learning (ML) in predicting and estimating the impulse noise. Four ML's algorithms (Multilayer perceptron MLP, support vector machine SVM, k nearest neighbour kNN and naïve Bayesian classifier NBC) were implemented in an OFDM system affected by impulse noise. The impulse noise model used was the Middleton Class A noise model. The ML's were trained with Middleton Class A impulse noise model so that they can be able to predict the presence of impulse noise in the communication system. In terms of prediction accuracy, results showed that kNN slightly outperformed MLP and NBC and accomplished high prediction accuracy of 99.8%. SVM achieved the lowest prediction accuracy among the four used methods. These results indicates that machine learning could be used to estimate impulse noise in OFDM communications system.","","Electronic:978-1-5090-3474-1; POD:978-1-5090-3475-8","10.1109/IECON.2016.7793751","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7793751","OFDM;Support vector machines;impulse noise;multilayer perceptron;naïve Bayesian;prediction","AWGN;Communication systems;Discrete Fourier transforms;Machine learning algorithms;Modulation;OFDM;Support vector machines","OFDM modulation;impulse noise;learning (artificial intelligence);multilayer perceptrons;support vector machines;telecommunication computing","ML algorithms;MLP;Middleton class-A noise model;NBC;SVM;impulse noise estimation;impulse noise prediction;k nearest neighbour;kNN;machine learning system;multilayer perceptron;naive Bayesian classifier;support vector machine","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Data mining and machine learning approaches on engineering materials — A review","P. J. Antony; P. Manujesh; N. A. Jnanesh","Department of Computer Science and Engineering, KVG College of Engineering, Sullia, D.K, 574327","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","69","73","This review paper explores the attempts made by the numerous authors in the field of material selection. There are ample amounts of works were carried out in the field of materials engineering with data mining approaches. From the literature it is revealed that not much of the work is explored on the classification of advanced composite materials using machine learning approaches.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7807785","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807785","Advanced Composite materials;Data mining approach;Machine learning;Material selection","Artificial neural networks;Composite materials;Data mining;Data models;Genetic algorithms;Polymers","composite materials;data mining;learning (artificial intelligence);materials science;pattern classification;production engineering computing","advanced composite material classification;data mining;engineering materials;machine learning;material selection;materials engineering","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning Based Approach for Detecting DRDoS Attacks and Its Performance Evaluation","Y. Gao; Y. Feng; J. Kawamoto; K. Sakurai","Dept. of Inf., Kyushu Univ., Fukuoka, Japan","2016 11th Asia Joint Conference on Information Security (AsiaJCIS)","20161215","2016","","","80","86","DRDoS (Distributed Reflection Denial of Service) attack is a kind of DoS (Denial of Service) attack, in which third-party servers are tricked into sending large amounts of data to the victims. That is, attackers use source address IP spoofing to hide their identity and cause third-parties to send data to the victims as identified by the source address field of the IP packet. This is called reflection because the servers of benign services are tricked into ""reflecting"" attack traffic to the victims. The most typical existing detection methods of such attacks are designed based on known attacks by protocol and are difficult to detect the unknown ones. According to our investigations, one protocol-independent detection method has been existing, which is based on the assumption that a strong linear relationship exists among the abnormal flows from the reflector to the victim. Moreover, the method is assumed that the all packets from reflectors are attack packets when attacked, which is clearly not reasonable. In this study, we found five features are effective for detecting DRDoS attacks, and we proposed a method to detect DRDoS attacks using these features and machine learning algorithms. Its detection performance is experimentally examined and the experimental result indicates that our proposal is of clearly better detection performance.","","Electronic:978-1-5090-2285-4; POD:978-1-5090-2286-1","10.1109/AsiaJCIS.2016.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782062","DRDoS;Machine Learning;attack detection","Bandwidth;Computer crime;Feature extraction;IP networks;Protocols;Reflection;Servers","computer network performance evaluation;computer network security;learning (artificial intelligence);transport protocols","DRDoS;attack detection;distributed reflection denial of service attack;machine learning;performance evaluation;protocol-independent detection;source address IP spoofing","","","","","","","4-5 Aug. 2016","","IEEE","IEEE Conference Publications"
"Auto-Tagging for Massive Online Selection Tests: Machine Learning to the Rescue","S. Krithivasan; S. Gupta; S. Shandilya; K. Arya; K. Lala","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol. Bombay, Mumbai, India","2016 IEEE Eighth International Conference on Technology for Education (T4E)","20170116","2016","","","204","207","Difficulty Level of a question is relative to that of other questions in a test and also to the test takers, hence manually assigning Difficulty Level tags may not be accurate. There is a need to infer them from historical data pertaining to the performance of students in a test. e-Yantra Robotics Competition (eYRC) is an annual competition having around 5000 teams (20,000 students) registering in the latest edition of the competition, eYRC-2015. All four team members take a test simultaneously and each individual gets questions which are different but have a similar Difficulty Level. A Question Bank containing 1800 unique questions from 3 subjects - Aptitude, Electronics, and C-Programming - is used to generate question sets each having 30 questions. It is a challenge to ensure that each set contains questions of similar Difficulty Levels tagged manually as Easy, Medium or Hard. In this paper, we discuss a learning algorithm called Weighted Clustering that can automatically tag questions by analyzing the performance of students. We used this algorithm to analyze the performance data in eYRC-2014 for 614 questions from the Question Bank, we found that Manual Tagging accuracy was 44%. We retagged questions with Suggested Tags resulting from our analysis and used them again in eYRC-2015. When we applied the algorithm to the performance data in eYRC-2015, we found that the accuracy of tagging had significantly improved to 67%.","","Electronic:978-1-5090-6115-0; POD:978-1-5090-6116-7","10.1109/T4E.2016.050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814825","Machine Learning;Online Testing Environment;Robotics Competition;Selection Test;Weighted Clustering;e-Yantra.","Clustering algorithms;Manuals;Measurement;Partitioning algorithms;Robots;Semantics;Tagging","data analysis;educational administrative data processing;learning (artificial intelligence);pattern clustering","Aptitude subject;C-Programming subject;Electronics subject;auto-tagging;difficulty level;e-Yantra Robotics Competition;eYRC-2015;easy level;hard level;machine learning;manual tagging;massive online selection tests;medium level;performance data analysis;question bank;weighted clustering","","","","","","","2-4 Dec. 2016","","IEEE","IEEE Conference Publications"
"Towards an efficient R&D theme prediction with machine learning","M. Shibata; K. Inoue; Y. Ohtsuka; K. Fukuyo; M. Takahashi","Fujitsu Kyushu Network Technologies Limited, Fukuoka Japan","2016 Portland International Conference on Management of Engineering and Technology (PICMET)","20170105","2016","","","1935","1941","This paper proposes an efficient method for R&D theme selection. There are various methods for the R&D theme selection such as patent analysis, paper survey and delphi investigation and so on. Patents and peer reviewed papers are easy to obtain, and are frequently used as materials for the selection method. In addition, making use of the national projects such as the social infrastructures feasibility project is one of the efficient theme selection method. A survey shows that the R&D theme selection is one of the biggest challenge for the private company. Generally, short term R&D theme selection is aiming at implementation within 2 years such as commercialization and sales expansion. Long-term theme is used for the R&D investment budget plan. On the other hand, Medium-term R&D theme is often aimed implementation within 5 years such as an exploratory technology theme. Since it relies on the heuristics knowledge for the theme selection with technology trends, an efficient selection method of the medium-term theme is required. Many technological analyses with the intellectual properties are performed so far. In this paper we propose a method of selecting the R&D theme using a machine learning based on public information.","","POD:978-1-5090-3595-3","10.1109/PICMET.2016.7806606","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806606","","Standardization;Technological innovation;Technology management","investment;learning (artificial intelligence);production engineering computing;research and development","R and D theme prediction;delphi investigation;heuristics knowledge;intellectual properties;investment;machine learning;patent analysis","","","","","","","4-8 Sept. 2016","","IEEE","IEEE Conference Publications"
"Comparing machine learning clustering with latent class analysis on cancer symptoms' data","N. Papachristou; C. Miaskowski; P. Barnaghi; R. Maguire; N. Farajidavar; B. Cooper; X. Hu","University of Surrey, Guildford, GU27XH UK","2016 IEEE Healthcare Innovation Point-Of-Care Technologies Conference (HI-POCT)","20161229","2016","","","162","166","Symptom Cluster Research is a major topic in Cancer Symptom Science. In spite of the several statistical and clinical approaches in this domain, there is not a consensus on which method performs better. Identifying a generally accepted analytical method is important in order to be able to utilize and process all the available data. In this paper we report a secondary analysis on cancer symptom data, comparing the performance of five Machine Learning (ML) clustering algorithms in doing so. Based on how well they separate specific subsets of symptom measurements we select the best of them and proceed to compare its performance with the Latent Class Analysis (LCA) method. This analysis is a part of an ongoing study for identifying suitable Machine Learning algorithms to analyse and predict cancer symptoms in cancer treatment.","","Electronic:978-1-5090-1166-7; POD:978-1-5090-1167-4","10.1109/HIC.2016.7797722","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797722","","Algorithm design and analysis;Cancer;Clustering algorithms;Data mining;Machine learning algorithms;Pain;Psychology","cancer;learning (artificial intelligence);medical computing","cancer symptom data;cancer symptom science;cancer treatment;clinical approach;latent class analysis;machine learning clustering;secondary analysis;statistical approach;symptom cluster research;symptom measurements","","","","","","","9-11 Nov. 2016","","IEEE","IEEE Conference Publications"
"A method to predict diagnostic codes for chronic diseases using machine learning techniques","D. Gupta; S. Khare; A. Aggarwal","Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Bangalore, India","2016 International Conference on Computing, Communication and Automation (ICCCA)","20170116","2016","","","281","287","Healthcare in simplest form is all about diagnosis and prevention of disease or treatment of any injury by a medical practitioner. It plays an important role in providing quality life for the society. The concern is how to provide better service with less expensive therapeutically equivalent alternatives. Machine Learning techniques (ML) help in achieving this goal. Healthcare has various categories of data like clinical data, claims data, drugs data and hospital data. This paper focuses on clinical and claims data for studying 11 chronic diseases such as kidney disease, osteoporosis, arthritis etc. using the claims data. The correlation between the chronic diseases and the corresponding diagnostic tests is analyzed, by using ML techniques. An effective conclusion on various diagnostics for each chronic disease is made, keeping in mind the clinical relevance.","","Electronic:978-1-5090-1666-2; POD:978-1-5090-1667-9","10.1109/CCAA.2016.7813730","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813730","Adaboost;CMS data;Chronic Diseases;ICD9 codes;InfoGain;Machine Learning","Data mining;Diabetes;Diseases;Heart;Medical diagnostic imaging;Predictive models","diseases;health care;learning (artificial intelligence);medical diagnostic computing","arthritis;chronic diseases;claims data;clinical data;clinical relevance;diagnostic codes;diagnostic tests;drugs data;healthcare;hospital data;kidney disease;machine learning;medical practitioner;osteoporosis","","","","","","","29-30 April 2016","","IEEE","IEEE Conference Publications"
"Assessment of defect prediction models using machine learning techniques for object-oriented systems","R. Malhotra; S. Shukla; G. Sawhney","Department of Software Engineering, Delhi Technological University, Delhi-110042, India","2016 5th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","20161219","2016","","","577","583","Software development is an essential field today. The advancement in software systems leads to risk of them being exposed to defects. It is important to predict the defects well in advance in order to help the researchers and developers to build cost effective and reliable software. Defect prediction models extract information about the software from its past releases and predict the occurrence of defects in future releases. A number of Machine Learning (ML) algorithms proposed and used in the literature to efficiently develop defect prediction models. What is required is the comparison of these ML techniques to quantify the advantage in performance of using a particular technique over another. This study scrutinizes and compares the performances of 17 ML techniques on the selected datasets to find the ML technique which gives the best performance for determining defect prone classes in an Object-Oriented(OO) software. Also, the superiority of the best ML technique is statistically evaluated. The result of this study demonstrates the predictive capability of ML techniques and advocates the use of Bagging as the best ML technique for defect prediction.","","Electronic:978-1-5090-1489-7; POD:978-1-5090-1490-3","10.1109/ICRITO.2016.7785021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785021","Defect prediction;Empirical Validation;Machine Learning;Object-oriented metrics;Software Quality","Algorithm design and analysis;Decision trees;Measurement;Object oriented modeling;Predictive models;Reliability;Software","learning (artificial intelligence);object-oriented programming;software engineering","OO software;bagging;defect prediction models;machine learning;object-oriented systems;software development;software systems","","","","","","","7-9 Sept. 2016","","IEEE","IEEE Conference Publications"
"Improving the Performance of Secure Cloud Infrastructure with Machine Learning Techniques","M. S. Sarma; Y. Srinivas; N. Ramesh; M. Abhiram","","2016 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)","20170119","2016","","","78","83","Security is one of the key concerns of the cloud user community. Most important ask from cloud users is to provide quality of services to manage Security end-to-end. The quality of Services (QoS) for securing cloud images were proposed in the earlier papers [2][3]. However, the key concern that still remains is how to balance performance and security. In this paper, a new and an intelligent model of QoS is being proposed which determines the files to be decrypted, without decrypting the entire file list in the secure wallet. Smart QoS is proposed as an extension to the security method proposed and presented in CCEM 2015[5] to improve the performance of Secure Cloud. The Smart QoS is capable of addressing some of the security concerns of cloud user community ensuring security and performance as well. Machine learning techniques have been used to design and develop the Smart Quality of Services. Solutions to ensure end-to-end security in cloud environments were proposed by us in the earlier paper [5]. In this paper, we have extended the security method proposed in [5] to ensure security and performance. Proposed Model is experimented in HPE Helion Cloud on two real time scenarios and results are attached to this paper.","","Electronic:978-1-5090-4573-0; POD:978-1-5090-4574-7","10.1109/CCEM.2016.022","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7819675","","Cloud computing;Cryptography;Filtering;Optimization;Quality of service;Standards","cloud computing;cryptography;learning (artificial intelligence);quality of service","HPE Helion Cloud;QoS intelligent model;cloud environments;cloud image security;cloud user community;end-to-end security;file decryption;machine learning;secure wallet;smart QoS;smart quality of services","","","","","","","19-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"A comparison of GPU execution time prediction using machine learning and analytical modeling","M. Amarís; R. Y. de Camargo; M. Dyab; A. Goldman; D. Trystram","Institute of Mathematics and Statistics, University of S&#x00E3;o Paulo, Brazil","2016 IEEE 15th International Symposium on Network Computing and Applications (NCA)","20161212","2016","","","326","333","Today, most high-performance computing (HPC) platforms have heterogeneous hardware resources (CPUs, GPUs, storage, etc.) A Graphics Processing Unit (GPU) is a parallel computing coprocessor specialized in accelerating vector operations. The prediction of application execution times over these devices is a great challenge and is essential for efficient job scheduling. There are different approaches to do this, such as analytical modeling and machine learning techniques. Analytic predictive models are useful, but require manual inclusion of interactions between architecture and software, and may not capture the complex interactions in GPU architectures. Machine learning techniques can learn to capture these interactions without manual intervention, but may require large training sets. In this paper, we compare three different machine learning approaches: linear regression, support vector machines and random forests with a BSP-based analytical model, to predict the execution time of GPU applications. As input to the machine learning algorithms, we use profiling information from 9 different applications executed over 9 different GPUs. We show that machine learning approaches provide reasonable predictions for different cases. Although the predictions were inferior to the analytical model, they required no detailed knowledge of application code, hardware characteristics or explicit modeling. Consequently, whenever a database with profile information is available or can be generated, machine learning techniques can be useful for deploying automated on-line performance prediction for scheduling applications on heterogeneous architectures containing GPUs.","","Electronic:978-1-5090-3216-7; POD:978-1-5090-3217-4","10.1109/NCA.2016.7778637","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778637","BSP model;CUDA;GPU Architectures;Machine Learning;Performance Prediction","Analytical models;Computational modeling;Computer architecture;Graphics processing units;Mathematical model;Message systems;Predictive models","graphics processing units;learning (artificial intelligence);regression analysis;support vector machines","BSP-based analytical model;GPU architectures;GPU execution time prediction;HPC platform;analytical modeling;application execution times;graphics processing unit;high-performance computing platform;job scheduling;linear regression;machine learning;parallel computing coprocessor;profiling information;random forests;support vector machines;vector operations","","","","","","","Oct. 31 2016-Nov. 2 2016","","IEEE","IEEE Conference Publications"
"A machine learning based framework for parameter based multi-objective optimisation of a H.265 video CODEC","M. Al-Barwani; E. A. Edirisinghe","Department of Computer Science, Loughborough University, Loughborough, UK","2016 Future Technologies Conference (FTC)","20170119","2016","","","553","559","All multimedia devices now incorporate video CODECs that comply with international video coding standards such as H.264 / MPEG4-AVC and the new High Efficiency Video Coding Standard (HEVC) otherwise known as H.265. Although the standard CODECs have been designed to include algorithms with optimal efficiency, large number of coding parameters that can be used to fine tune their operation, within known constraints of for e.g., available computational power, bandwidth, energy consumption, etc. With large number of such parameters involved, determining which parameters will play a significant role in providing optimal quality of service within given constraints is a further challenge that needs to be met. We propose a framework that uses machine learning algorithms to model the performance of a video CODEC based on the significant coding parameters. We define objective functions that can be used to model the video quality, CPU time utilisation and bit-rate. We show that these objective functions can be practically utilised in video Encoder designs, in particular in their performance optimisation within given constraints. A Multi-objective Optimisation framework based on Genetic Algorithms is thus proposed to optimise the performance of a video codec. The framework is designed to jointly minimize the complexity, Bit-rate and to maximize the quality of the compressed video stream.","","Electronic:978-1-5090-4171-8; POD:978-1-5090-4172-5","10.1109/FTC.2016.7821661","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821661","H.264;High Efficiency Video Coding (HEVC);Multi-objective Optimisation","Decoding;Encoding;High efficiency video coding;Optimization;Standards;Streaming media;Video codecs","genetic algorithms;learning (artificial intelligence);video coding","H.265 video codec;HEVC;coding parameter;genetic algorithm;high efficiency video coding;machine learning;multiobjective optimization framework;objective function","","","","","","","6-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"SVM based machine learning approach to identify Parkinson's disease using gait analysis","S. Shetty; Y. S. Rao","Department of Electronics and Telecommunication, Sardar Patel Institute of Technology, Mumbai, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","2","","1","5","Parkinson's Disease (PD) is a neuro-degenerative disease which affects a persons mobility. Tremors, rigidity of the muscles and imprecise gait movements are characteristics of this disease. Past attempts have been made to classify Parkinsons disease from healthy subjects but in this work, effort was made to focus on the specific gait characteristics which would help differentiate Parkinsons Disease from other neurological diseases (Amyotrophic lateral sclerosis (ALS) and Huntingtons Disease) as well as healthy controls. A range of statistical feature vector considered here from the Time-series gait data which are then reduced using correlation matrix. These feature vectors are then individually analysed to extract the best 7 feature vectors which are then classified using a Gaussian radial basis function kernel based Support vector machine (SVM) classifier. Results show that the 7 features selected for SVM achieves good overall accuracy of 83.33%, good detection rate for Parkinsons disease of 75% and low false positive results of 16.67%.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7824836","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824836","Gait analysis;Machine learning;Parkinson's disease;SVM","Correlation;Diseases;Foot;Kernel;Muscles;PD control;Support vector machines","Gaussian processes;diseases;feature extraction;gait analysis;learning (artificial intelligence);matrix algebra;medical computing;muscle;neurophysiology;pattern classification;radial basis function networks;statistical analysis;support vector machines;time series","Gaussian radial basis function;Parkinson disease classification;Parkinson disease identification;SVM based machine learning approach;correlation matrix;feature vector extraction;gait analysis;gait characteristics;kernel-based SVM classifier;kernel-based support vector machine classifier;muscles;neuro-degenerative disease;neurological diseases;person mobility;statistical feature vector;time-series gait data","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning (ML)-based lithography optimizations","S. Shim; S. Choi; Y. Shin","School of Electrical Engineering, KAIST, Daejeon 34141, Korea","2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)","20170105","2016","","","530","533","Recent lithography optimizations demand higher accuracy and cause longer runtime. Optical proximity correction (OPC) and sub-resolution assist feature (SRAF) insertion, for example, take a few days due to lengthy lithography simulations and high pattern density. Etch proximity correction (EPC) is another example of intensive optimization due to a complex physical model of etching process. Machine learning has recently been applied to these lithography optimizations with some success. In this paper, we introduce basic algorithms of machine learning technique, e.g. support vector machine (SVM) and neural networks, and how they are applied to lithography optimization problems. Discussion on learning parameters, preparation of compact learning data set, technique to avoid over-fitting are also provided.","","Electronic:978-1-5090-1570-2; POD:978-1-5090-1571-9; USB:978-1-5090-1569-6","10.1109/APCCAS.2016.7804021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804021","","Adaptive optics;Kernel;Layout;Lithography;Optimization;Predictive models;Support vector machines","electronic engineering computing;etching;feature extraction;learning (artificial intelligence);neural nets;proximity effect (lithography);support vector machines","EPC;ML-based lithography optimizations;OPC;SRAF insertion;SVM;etch proximity correction;learning parameters;lithography simulations;machine learning-based lithography optimizations;neural networks;optical proximity correction;pattern density;sub-resolution assist feature insertion;support vector machine","","","","","","","25-28 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine learning for affective computing and its applications to automated measurement of human facial affect","O. O. Rudovic","MIT Media Lab & Postdoctoral, 75 Amherst St, Cambridge 02139, MA USA","2016 International Symposium on Micro-NanoMechatronics and Human Science (MHS)","20170119","2016","","","1","1","Summary form only given. Automated analysis of human affective behaviour has received a significant research attention over the last decade due to its practical importance in areas such as health, human-computer interaction, social robotics, and marketing, to mention but a few. Traditionally, different behavioural cues are first extracted from the sensory inputs (video, speech and/or physiology). Then, machine learning algorithms are designed to analyse these behavioural cues with the aim of automatically predicting target affective states. However, the modelling of human affect (such as emotion expressions or pain levels) is rather challenging due to the many possible sources of variation in target data, including the target subjects (male vs. female, children vs. adults, etc.), their tasks (human-human or human-robot interaction), culture (eastern vs. western), and so on. All of these make the task of automated estimation of human affect highly context-sensitive. In this talk, I will first provide a general overview of recent trends in the field of affective computing. Then, I will illustrate its application to the domain of facial behaviour analysis by focusing on the most recent advances in estimation of human facial behaviour from static images and video data. To this end, I will describe the state-of-the-art machine learning techniques proposed for context-sensitive modelling of human facial expressions of basic emotions, facial action units and their intensity, as well as the clinical measurement of patient's pain levels. Finally, I will outline the main challenges and provide future directions for applying these approaches `in-the-wild', i.e., naturalistic scenarios such as human-robot interaction and in the context of treatment of neuro developmental disorders (such as autism).","","Electronic:978-1-5090-2785-9; POD:978-1-5090-2786-6","10.1109/MHS.2016.7824242","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824242","","Affective computing;Area measurement;Estimation;Human-robot interaction;Media;Pain;Robot sensing systems","affective computing;face recognition;learning (artificial intelligence);medical computing;medical disorders;medical image processing;neurophysiology","affective computing;autism;automated human facial affect measurement;context-sensitive modelling;emotion expressions;human-computer interaction;human-human interaction;human-robot interaction;machine learning algorithms;neurodevelopmental disorder treatment;patient pain level measurement;physiology;social robotics;speech;static images;video data","","","","","","","28-30 Nov. 2016","","IEEE","IEEE Conference Publications"
"Citizen security using machine learning algorithms through open data","G. B. Rocca; M. Castillo-Cara; R. A. Levano; J. V. Herrera; L. Orozco-Barbosa","Computer Science School - Sciences Faculty, Center of Information and Communication Technologies Universidad Nacional de Ingenier&#x00ED;a, Av. Tupac Amaru, 210, Rimac, Lima, 25, Peru","2016 8th IEEE Latin-American Conference on Communications (LATINCOM)","20170116","2016","","","1","6","The following work is an application proposal based on machine learning algorithms for a possible solution for the public safety problem in a South American city. The aim of this application is to reduce the threat risk of the physical integrity of pedestrians by geolocating, in real-time, safer places to walk. In this context for a city, San Isidro, a business district of Lima, has been established as study case. The district has been divided into map sectors and subsectors, so that by using the GPS location service integrated in mobile devices, it is possible to identify areas that have the highest incidence of different types of incidents. This functionality will allow users to choose safer routes by taking into account the information provided for each sector. The data used in this application has been obtained from an Open Data platform managed by the San Isidro municipality. In this application, we have processed the data enabling the easy and friendly access to the information by the end user. The importance of this work is how we have used the machine learning algorithm for incident rates in real and future time, trying to make predictions that can not only provide safe routes to users, but also predict disasters and allow public authorities to act in advance, thus minimizing the impact of future incidents.","","Electronic:978-1-5090-5137-3; POD:978-1-5090-5138-0; USB:978-1-5090-4758-1","10.1109/LATINCOM.2016.7811562","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811562","Android;Data analytics;Data mining;Machine learning;Open Data;Smart City;citizen security;safe routes","Automobiles;Correlation;Machine learning algorithms;Prediction algorithms;Urban areas;Vehicle crash testing","Global Positioning System;disasters;learning (artificial intelligence);mobile computing;pedestrians;public administration","GPS location service;Open Data platform;San Isidro;South American city;business district of Lima;citizen security;disaster prediction;machine learning algorithm;mobile device;pedestrians physical integrity risk;public safety problem","","","","","","","15-17 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Study on Machine Learning for Imbalanced Datasets with Answer Validation of Question Answering","M. Y. Day; C. C. Tsai","Dept. of Inf. Manage., Tamkang Univ., Taipei, Taiwan","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","20161219","2016","","","513","519","Question Answering is a system that can process and answer a given question. In recent years, an enormous number of studies have been made on question answering, little is known about the effects of imbalanced datasets with answer validation of question answer system. The objective of this paper is to provide a better understanding of the effects of imbalanced datasets model for answer validation in a real world university entrance exam question answering system. In this paper, we proposed a question answer system and provided a comprehensive analysis of imbalanced datasets and balanced datasets model with Answer Validation of Question Answering system using NTCIR-12 QA-Lab2 Japanese university entrance exams English translation development and test datasets. As a result, our system achieved 90% accuracy with imbalanced datasets machine learning model for the NTCIR-12 QA-Lab2 development datasets.","","Electronic:978-1-5090-3207-5; POD:978-1-5090-3208-2","10.1109/IRI.2016.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785785","Answer Validation;Imbalanced Datasets;Machine Learning;QA-Lab;Question Answering;Support Vector Machine","Europe;History;Knowledge discovery;Sensitivity;Systems architecture;Text analysis;XML","learning (artificial intelligence);natural language processing;question answering (information retrieval)","NTCIR-12 QA-Lab2 Japanese university entrance exams;answer validation;imbalanced datasets;machine learning;real world university entrance exam question answering system","","","","","","","28-30 July 2016","","IEEE","IEEE Conference Publications"
"Residential demand response targeting using machine learning with observational data","D. Zhou; M. Balandat; C. Tomlin","Department of Mechanical Engineering, University of California, Berkeley, USA","2016 IEEE 55th Conference on Decision and Control (CDC)","20161229","2016","","","6663","6668","The large-scale deployment of Advanced Metering Infrastructure among residential energy customers has served as a boon for energy systems research relying on granular consumption data. Residential Demand Response aims to utilize the flexibility of consumers to reduce their energy usage during times when the grid is strained. Suitable incentive mechanisms to encourage customers to deviate from their usual behavior have to be implemented to correctly control the bids into the wholesale electricity market as a Demand Response provider. In this paper, we present a framework for shortterm load forecasting on an individual user level, and relate non-experimental estimates of Demand Response efficacy (the estimated reduction of consumption during Demand Response events) to the variability of a user's consumption. We apply our framework on a dataset from a residential Demand Response program in the Western United States. Our results suggest that users with more variable consumption patterns are more likely to reduce their consumption compared to users with a more regular consumption behavior.","","DVD:978-1-5090-1844-4; Electronic:978-1-5090-1837-6; POD:978-1-5090-1838-3","10.1109/CDC.2016.7799295","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7799295","","Forecasting;Load management;Load modeling;Regression tree analysis;Shape;Temperature distribution;Temperature measurement","demand side management;learning (artificial intelligence);load forecasting;metering;power consumption;power engineering computing;power grids;power markets","Western United States;advanced metering infrastructure;consumer flexibility;granular consumption data;machine learning;observational data;regular consumption behavior;residential demand response targeting;residential energy customers;short-term load forecasting;user consumption;wholesale electricity market","","1","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning approach for identification & diagnosing features of Neurodevelopmental disorders using speech and spoken sentences","A. Pahwa; G. Aggarwal; A. Sharma","Computer Science and Engineering, THE NORTHCAP University, Gurgaon, Haryana, India","2016 International Conference on Computing, Communication and Automation (ICCCA)","20170116","2016","","","377","382","Autism Spectrum Disorder is characterised by the defects in communication and social skills. This is the neuro developmental disorder, that is, the disorder in brain and its functioning that affects the emotion, communication, self-control and person's ability to learn by stopping the growth and development of the central nervous system. A number of studies have been made to find the accurate features to diagnose the ASD. This paper presents the review of the robust and accurate feature used for detection of this wide spectrum disease and also reviews the linguistic and acoustic feature for identifying Autism. It also presents the review of a similar disease called Parkinson's disease, which is mainly due to the loss of cells in the various part of the brain, is also reviewed. The purpose of this study is to expose to view the wide range of mechanisms used in the detection of speech related impairments from three similar researches.","","Electronic:978-1-5090-1666-2; POD:978-1-5090-1667-9","10.1109/CCAA.2016.7813749","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813749","Autism Spectrum Disorder;Parkinson's disease;jitter;shimmer","Autism;Diseases;Feature extraction;Harmonic analysis;Jitter;Speech;Standards","brain;diseases;learning (artificial intelligence);medical diagnostic computing;medical disorders;neurophysiology;patient diagnosis;speech processing","ASD;Autism identification;Autism spectrum disorder;Parkinson disease;acoustic feature;central nervous system development;communication skills;diagnosing features;machine learning approach;neurodevelopmental disorders;person ability;social skills;speech related impairments;speech sentences;spoken sentences","","","","","","","29-30 April 2016","","IEEE","IEEE Conference Publications"
"Machine Learning in Software Defined Networks: Data collection and traffic classification","P. Amaral; J. Dinis; P. Pinto; L. Bernardo; J. Tavares; H. S. Mamede","Instituto de Telecomunica&#x00E7;&#x00F5;es, Dep.de Eng. Electrot&#x00E9;cnica, Faculdade de Ci&#x00EA;ncias e Tecnologia, Universidade Nova de Lisboa, Portugal","2016 IEEE 24th International Conference on Network Protocols (ICNP)","20161219","2016","","","1","5","Software Defined Networks (SDNs) provides a separation between the control plane and the forwarding plane of networks. The software implementation of the control plane and the built in data collection mechanisms of the OpenFlow protocol promise to be excellent tools to implement Machine Learning (ML) network control applications. A first step in that direction is to understand the type of data that can be collected in SDNs and how information can be learned from that data. In this work we describe a simple architecture deployed in an enterprise network that gathers traffic data using the OpenFlow protocol. We present the data-sets that can be obtained and show how several ML techniques can be applied to it for traffic classification. The results indicate that high accuracy classification can be obtained with the data-sets using supervised learning.","","Electronic:978-1-5090-3281-5; POD:978-1-5090-3282-2","10.1109/ICNP.2016.7785327","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785327","Data Analysis;Machine Learning;Software defined Networks;Traffic Classification","Conferences;Electronic mail;Ports (Computers);Protocols;Software;Switches","business communication;learning (artificial intelligence);pattern classification;protocols;software defined networking;telecommunication traffic","ML network control application;OpenFlow protocol;SDN;control plane software implementation;data collection;enterprise network;forwarding plane;machine learning network control application;software defined network;supervised learning;traffic classification;traffic data gathering","","","","","","","8-11 Nov. 2016","","IEEE","IEEE Conference Publications"
"Machine learning for source localization in urban environments","D. A. Bibb; Z. Yun; M. F. Iskander","Hawaii Center for Advanced Communications, University of Hawaii at Manoa, Honolulu, 96822, United States","MILCOM 2016 - 2016 IEEE Military Communications Conference","20161226","2016","","","401","405","This paper investigates source localization in urban environments using machine learning methods. Both classification and regression schemes are examined using the random forest algorithm. In both approaches, the localization performance depends mostly on arrival time information of received signals. It is shown that the use of a relative arrival time difference rather than the direct time of arrival (TOA) also provide good performance, which is beneficial for practical application. It is found that with enough number of receivers, the signal power parameter can be omitted, which eliminates frequency dependency of developed models. It is also found that at least three receivers should be used to achieve acceptable prediction performance. Additionally, the number of training examples needed depends on the approach used as well as the desired level of accuracy. This factor is more important in the classification approach, as the amount of training data required closely relates to the number of sectors created by the localization problem. A regression scheme is more natural for predicting spatial coordinate values, and achieved higher localization accuracy with fewer training examples as compared to classification. Ultimately, the regression based localization scheme using the time difference of arrival (TDOA) parameter at three receiver locations achieved an average localization accuracy of 2.8m.","","Electronic:978-1-5090-3781-0; POD:978-1-5090-3782-7","10.1109/MILCOM.2016.7795360","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795360","Machine learning;ray tracing;source localization;urban environments","Data models;Machine learning algorithms;Predictive models;Receivers;Signal processing algorithms;Training;Urban areas","learning (artificial intelligence);regression analysis;signal classification;source separation;time-of-arrival estimation","TDOA;TOA;classification schemes;machine learning methods;random forest algorithm;regression schemes;source localization;time difference of arrival;time of arrival","","","","","","","1-3 Nov. 2016","","IEEE","IEEE Conference Publications"
"WSNs Self-Calibration Approach for Smart City Applications Leveraging Incremental Machine Learning Techniques","R. Rossini; E. Ferrera; D. Conzon; C. Pastrone","Pervasive Technol. (PerT) Res. area, Ist. Superiore Mario Boella, Turin, Italy","2016 8th IFIP International Conference on New Technologies, Mobility and Security (NTMS)","20161222","2016","","","1","7","The diffusion of the Internet of Things paradigm, in the last few years, has led to the need of deploying and managing large-scale Wireless Sensor Networks (WSNs), composed by a multitude of geographically distributed sensors, like the ones needed for Smart City applications. The traditional way to manage WSNs is not suitable for this type of applications, because manually managing and monitoring every single sensor would be too expensive, time consuming and error prone. Moreover, unattended sensors may suffer of several issues that progressively make their measures unreliable and consequently useless. For this reason, several automatically techniques have been studied and implemented for the detection and correction of measurements from sensors which are affected by errors caused by aging and/or drift. These methods are grouped under the name of self-calibration techniques. This paper presents a distributed system, which combines an incremental machine learning technique with a non-linear Kalman Filter estimator, which allows to automatically re-calibrate sensors leveraging the correlation with measurements made by neighbor sensors. After the description of the used model and the system implementation details, the paper describes also the proof-of-concept prototype that has been built for testing the proposed solution.","","Electronic:978-1-5090-2914-3; POD:978-1-5090-2915-0","10.1109/NTMS.2016.7792490","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792490","","Calibration;Estimation;Intelligent sensors;Kalman filters;Sensor phenomena and characterization;Wireless sensor networks","Kalman filters;calibration;learning (artificial intelligence);nonlinear filters;smart cities;wireless sensor networks","Internet of Things;WSNs self-calibration;aging;distributed system;drift;geographically distributed sensors;incremental machine learning techniques;large-scale wireless sensor networks;measurements correlation;nonlinear Kalman filter estimator;sensor measurement correction;sensor measurement detection;smart city","","","","","","","21-23 Nov. 2016","","IEEE","IEEE Conference Publications"
"Cryptographic Algorithm Identification Using Machine Learning and Massive Processing","F. Luis de Mello; J. Antonio Moreira Xexeo","Polytech. Sch., Fed. Univ. of Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Latin America Transactions","20161222","2016","14","11","4585","4590","This paper presents a study on encryption algorithms identification by means of machine learning techniques. Plain text files, written in seven different languages, were encoded by seven cryptographic algorithm under ECB mode. The resulting cryptograms were submitted to a transformation so that it was possible to create metadata files. These files provide information for six data mining algorithms in order to identify the cryptographic algorithm used for encryption. The identification performance was evaluated and the language influence at the procedure was analyzed. The overall experiment involves many cryptograms, a great quantity of metadata, a huge time consuming computation, and therefore, it was employed a high performance computer. The successful identification for each mining algorithm is greater than a probabilistic bid, and there are several scenarios where algorithm identification reaches almost full recognition.","1548-0992;15480992","","10.1109/TLA.2016.7795833","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795833","cryptographic algorithm identification;data mining;machine intelligence;parallel computing","Data mining;Encryption;IEEE transactions;Machine learning algorithms;Metadata;Software","cryptography;data mining;learning (artificial intelligence);meta data","ECB mode;cryptograms;cryptographic algorithm identification;data mining;encoded plain text files;encryption algorithm identification;machine learning;metadata files","","","","","","","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"A Machine-Learning-Driven Sky Model","P. Satýlmýs; T. Bashford-Rogers; A. Chalmers; K. Debattista","University of Warwick","IEEE Computer Graphics and Applications","20170116","2017","37","1","80","91","Sky illumination is responsible for much of the lighting in a virtual environment. A machine-learning-based approach can compactly represent sky illumination from both existing analytic sky models and from captured environment maps. The proposed approach can approximate the captured lighting at a significantly reduced memory cost and enable smooth transitions of sky lighting to be created from a small set of environment maps captured at discrete times of day. The author's results demonstrate accuracy close to the ground truth for both analytical and capture-based methods. The approach has a low runtime overhead, so it can be used as a generic approach for both offline and real-time applications.","0272-1716;02721716","","10.1109/MCG.2016.67","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478419","computer graphics;machine learning;neural network;realistic graphics;sky model","Analytical models;Atmospheric modeling;Computational modeling;Illumination;Lighting;Neural networks;Virtual environments","learning (artificial intelligence);lighting;rendering (computer graphics);sky brightness","machine-learning-driven sky model;sky illumination;virtual environment rendering","","","","","","20160525","Jan.-Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Applications of machine learning in induction cooking","A. Bono-Nuez; C. Bernal-Ruiz; B. Martin-del-Brio; F. J. Perez-Cebolla; A. Martinez-Iturbe","Department of Electronic Engineering and Communications, University of Zaragoza, EINA, Campus R&#x00ED;o Ebro, C. Maria de Luna 1, 50018, Spain","IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society","20161222","2016","","","849","854","In this paper we present a new line of work combining digital signal processing and machine learning algorithms to identify and classify cooking recipients used in domestic induction heating. First, a recipient `signature' is obtained from processing current and voltage waveforms, which includes impedance harmonics and the power factor. Then, the non-linear fitting capabilities of artificial neural networks and other machine learning algorithms allow processing the pot signature. We show two applications of this technique: recipient size estimation and identification of every cooking recipient of a specific user (for instance, for assigning a specific cooking profile to each vessel). Finally, we implement our procedure onto a low-cost electronic circuit such as those included in commercial induction home appliances. This new approach is of interest for developing new applications in the context of automatic cooking.","","Electronic:978-1-5090-3474-1; POD:978-1-5090-3475-8","10.1109/IECON.2016.7793141","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7793141","Induction home appliances;automatic cooking;electronic implementation of neural networks;spectral analysis","Estimation;Harmonic analysis;Impedance;Neurons;Power system harmonics;Reactive power;Spectral analysis","induction heating;learning (artificial intelligence);neural nets;power engineering computing;power factor","ANN;artificial neural networks;commercial induction home appliances;current waveforms;digital signal processing;domestic induction heating;impedance harmonics;induction cooking;low-cost electronic circuit;machine learning algorithms;nonlinear fitting capabilities;pot signature;power factor;recipient signature;voltage waveforms","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"A distributed machine learning approach for the secondary voltage control of an Islanded micro-grid","M. A. Karim; J. Currie; T. T. Lie","","2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)","20161226","2016","","","611","616","Balancing the active and the reactive power in a stand-alone micro-grid is a critical task. A micro-grid without energy storage capability is even more vulnerable to stability issues. This paper investigates a distributed secondary control to maintain the rated voltage in a stand-alone micro-grid. Here multiple machine learning algorithms have been implemented to provide the secondary control where a primary control scheme is insufficient to maintain a stable voltage after a sudden change in the load. The performance of the secondary control is monitored by a centralized system and in most of the cases it does not interfere. Based on different contingencies the proposed method would suggest different machine learning algorithms which are previously trained with similar data. The contingencies are based on an imbalance either in the active or in the reactive power in the system. It is considered that the distributed generators such as the wind and solar plants as well as the residential loads have some degree of randomness. The secondary control is invoked only in the events when primary droop control is insufficient to address the stability issue and maintain a desired voltage in the system.","","Electronic:978-1-5090-4303-3; POD:978-1-5090-5228-8; USB:978-1-5090-4302-6","10.1109/ISGT-Asia.2016.7796454","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796454","Neural network;automatic generation control;distributed generation;load forecasting;solar energy;wind energy","Energy storage;Machine learning algorithms;Power system stability;Synchronous generators;Voltage control;Wind power generation","distributed power generation;energy storage;learning (artificial intelligence);reactive power control;voltage control","distributed secondary control;energy storage capability;machine learning algorithms;primary control scheme;primary droop control;reactive power;residential loads;secondary voltage control;standalone microgrid","","","","","","","Nov. 28 2016-Dec. 1 2016","","IEEE","IEEE Conference Publications"
"Early detection of grapes diseases using machine learning and IoT","S. S. Patil; S. A. Thorat","Department of Computer Science and Engineering, Rajarambapu Institute of Technology, Rajaramnagar, Islampur, Sangli (MS) 415409 India","2016 Second International Conference on Cognitive Computing and Information Processing (CCIP)","20170102","2016","","","1","5","Grape cultivation has social and economic importance in India. In India, Maharashtra ranks first in grapes production. Over the last few years the quality of grapes has degraded because of many reasons. One of the important causes is diseases on grapes. To prevent diseases farmers spray huge amount of pesticides, which result in increasing the cost of production. Also farmers are unable to identify the diseases manually. The diseases are identified only after the infection, but its takes up a lot of time and have adverse effects on vineyard. The proposed work is to develop a monitoring system which will identify the chances of grape diseases in its early stages by using Hidden Markov Model provides alerts via SMS to the farmer and the expert. The system includes temperature, relative humidity, moisture, leaf wetness sensor and Zig-Bee for wireless data transmission.","","Electronic:978-1-5090-1025-7; POD:978-1-5090-1026-4","10.1109/CCIP.2016.7802887","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802887","Hidden Markov Model;Vineyard;Wireless Sensor Network;Zig-Bee","Agriculture;Diseases;Hidden Markov models;Humidity;Pipelines;Statistical analysis;Wireless sensor networks","Internet of Things;Zigbee;agricultural products;computer vision;diseases;electronic messaging;hidden Markov models;learning (artificial intelligence);plant diseases","India;IoT;Maharashtra;SMS;Zig-Bee;grape cultivation;grape disease early detection;hidden Markov model;leaf wetness sensor;machine learning;monitoring system development;pesticides;relative humidity;vineyard;wireless data transmission","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning based estimation of Ozone using spatio-temporal data from air quality monitoring stations","T. M. Chiwewe; J. Ditsela","IBM Research South Africa, IBM, Johannesburg, South Africa","2016 IEEE 14th International Conference on Industrial Informatics (INDIN)","20170119","2016","","","58","63","In this paper, models are created to predict the levels of ground level Ozone at particular locations based on the cross-correlation and spatial-correlation of different air pollutants whose readings are obtained from several different air quality monitoring stations in Gauteng province, South Africa, including the City of Johannesburg which is on the cusp of being one of the world's megacities and is currently the most polluted city in the country. Datasets spanning several years collected from the monitoring stations and transmitted through the Internet-of-Things are used. Big data analytics and cognitive computing is used to get insights on the data and create models that can estimate levels of Ozone without requiring massive computational power or intense numerical analysis.","","Electronic:978-1-5090-2870-2; POD:978-1-5090-2871-9","10.1109/INDIN.2016.7819134","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7819134","analytics;big data;cognitive computing;internet-of-things;machine learning","Air pollution;Atmospheric modeling;Data models;Gases;Monitoring;Predictive models","Big Data;Internet of Things;air pollution;air quality;environmental monitoring (geophysics);environmental science computing;learning (artificial intelligence);ozone","Big Data analytics;Gauteng province;Internet-of-Things;Johannesburg city;South Africa;air pollutants;air quality monitoring stations;cognitive computing;cross-correlation;machine learning based estimation;ozone;spatial-correlation;spatio-temporal data","","","","","","","19-21 July 2016","","IEEE","IEEE Conference Publications"
"Integration of machine learning approach in item bank test system","A. Sangodiah; R. Ahmad; W. F. W. Ahmad","Department of Information System, Universiti Tunku Abdul Rahman, Kampar, Malaysia","2016 3rd International Conference on Computer and Information Sciences (ICCOINS)","20161215","2016","","","164","168","Item test bank system plays very important role in auto generating test or exam paper in assessments in schools and universities. A quite number of researchers have proposed some algorithms in generating test paper based on some well-defined attributes such as time, question type, knowledge point, difficulty level and others. It has always been the aim of these researchers to generate high quality test paper with appropriate level of difficulty in test questions. As a result of this, Bloom taxonomy has been adopted to ensure difficulty level of test questions is appropriate. However, there is no evidence that current test items or questions in the item test bank system are classified in accordance to BT using machine learning approach. Manual classifying is tedious and laborious work and inconsistency in classifying items can take place due to different judgement from instructors. A better approach is to use machine learning namely question classifier such as Support Vector Machine to automate the classification of the test items. Despite some research work has been done on using classifiers to classify questions, there is no evidence that this type of work has been integrated into item bank test system. In view of this, this study proposes a change in existing framework of item test bank system by integrating the facility to automate classifying items in accordance to Bloom taxonomy. With all this in place, the automation of classifying questions or test items in accordance to BT with a reasonable accuracy can be achieved.","","Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1","10.1109/ICCOINS.2016.7783208","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783208","Bloom Taxonomy;Item test bank system;Machine Learning;difficulty level;question classifier","Context;Electronic learning;Feature extraction;Generators;Load modeling;Support vector machines;Taxonomy","computer aided instruction;educational institutions;learning (artificial intelligence);pattern classification","Bloom taxonomy;difficulty level;item bank test system;machine learning;question classifier;schools;universities","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Large-scale strategic games and adversarial machine learning","T. Alpcan; B. I. P. Rubinstein; C. Leckie","Department of Electrical and Electronic Engineering, The University of Melbourne, Australia","2016 IEEE 55th Conference on Decision and Control (CDC)","20161229","2016","","","4420","4426","Decision making in modern large-scale and complex systems such as communication networks, smart electricity grids, and cyber-physical systems motivate novel game-theoretic approaches. This paper investigates big strategic (non-cooperative) games where a finite number of individual players each have a large number of continuous decision variables and input data points. Such high-dimensional decision spaces and big data sets lead to computational challenges, relating to efforts in non-linear optimization scaling up to large systems of variables. In addition to these computational challenges, real-world players often have limited information about their preference parameters due to the prohibitive cost of identifying them or due to operating in dynamic online settings. The challenge of limited information is exacerbated in high dimensions and big data sets. Motivated by both computational and information limitations that constrain the direct solution of big strategic games, our investigation centers around reductions using linear transformations such as random projection methods and their effect on Nash equilibrium solutions. Specific analytical results are presented for quadratic games and approximations. In addition, an adversarial learning game is presented where random projection and sampling schemes are investigated.","","DVD:978-1-5090-1844-4; Electronic:978-1-5090-1837-6; POD:978-1-5090-1838-3","10.1109/CDC.2016.7798940","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798940","","Cost function;Game theory;Games;Kernel;Scalability;Security;Support vector machines","decision making;game theory;learning (artificial intelligence);random processes;sampling methods","Nash equilibrium;adversarial machine learning;decision making;random projection;sampling scheme;strategic game","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Classification of primary biliary cirrhosis using hybridization of dimensionality reduction and machine learning methods","A. Singh; B. Pandey","Computer Science and Engineering, Lovely Professional University, Punjab, India","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","1","","1","5","The key functioning of human body depends on liver health. Liver performs numerous metabolic functions that also enable smooth working of other organs. Any form of illness in liver leads to liver diseases. These diseases are of many types out of which the most commonly occurring are hepatitis A, B, C, D and E, primary biliary cirrhosis (PBC), liver fibrosis, liver tumor, alcoholic liver disease, liver cirrhosis, fatty liver disease and autoimmune hepatitis. Presence of these diseases in various forms indicates the significance of accurate and timely diagnosis. This study accordingly aims to classify PBC stages using individual classifiers and hybrid models. Individual methods include linear discriminant analysis (LDA), diagonal linear discriminant analysis (DLDA), euclidean distance based k-nearest neighbors (KNN), and hybrid models include combination of LDA, DLDA and KNN with dimensionality reduction method. Simulations results showed that hybrid frameworks outperform individual classifiers in terms of classification performance. Furthermore, KNN based hybridization achieved a remarkable accuracy of 91.3%.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7823232","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823232","Primary biliary cirrhosis;classification algorithms;dimensionality reduction;feature selection;feature transformation;k-nearest neighbors;medical decisions support systems","Analytical models;Artificial neural networks;Diseases;Liver diseases;Testing;Training","diseases;learning (artificial intelligence);liver;medical computing;patient diagnosis;pattern classification;regression analysis","DLDA;KNN;PBC;alcoholic liver disease;autoimmune hepatitis;classification performance;diagonal linear discriminant analysis;dimensionality reduction hybridization;euclidean distance based k-nearest neighbors;fatty liver disease;liver fibrosis;liver health;liver tumor;machine learning;metabolic functions;primary biliary cirrhosis classification;timely diagnosis","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"New Ensemble Machine Learning Method for Classification and Prediction on Gene Expression Data","C. W. Wang","Vision and Artificial Intelligence Group, Department of Computing & Informatics, University of Lincoln, Brayford Pool, Lincoln LN6 7TS, United Kingdom. cweiwang@lincoln.ac.uk","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","20161215","2006","","","3478","3481","A reliable and precise classification of tumours is essential for successful treatment of cancer. Recent researches have confirmed the utility of ensemble machine learning algorithms for gene expression data analysis. In this paper, a new ensemble machine learning algorithm is proposed for classification and prediction on gene expression data. The algorithm is tested and compared with three popular adopted ensembles, i.e. bagging, boosting and arcing. The results show that the proposed algorithm greatly outperforms existing methods, achieving high accuracy over 12 gene expression datasets","1557-170X;1557170X","CD:1-4244-003303; Paper:1-4244-0032-5","10.1109/IEMBS.2006.259893","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462545","ensemble machine learning;microarray;pattern recognition","Algorithm design and analysis;Bagging;Boosting;Gene expression;Learning systems;Machine learning;Machine learning algorithms;Testing;Training data;Voting","biology computing;cancer;genetics;learning (artificial intelligence);molecular biophysics;pattern classification;tumours","cancer;ensemble machine learning;gene expression data;microarray;tumour classification","Algorithms;Artificial Intelligence;Biomedical Engineering;Data Interpretation, Statistical;Databases, Genetic;Gene Expression Profiling;Humans;Neoplasms","5","","","","","Aug. 30 2006-Sept. 3 2006","","IEEE","IEEE Conference Publications"
"Improved microarray data analysis using feature selection methods with machine learning methods","Jing Sun; K. Passi; C. K. Jain","Department of Mathematics and Computer Science, Laurentian University, Sudbury, Ontario, Canada","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1527","1534","Microarray data analysis directly relates with the state of disease through gene expression profile, and is based upon several feature extractions to classification methodologies. This paper focuses on the study of 8 different ways of feature selection preprocess methods from 4 different feature selection methods. They are Minimum Redundancy-Maximum Relevance (mRMR), Max Relevance (MaxRel), Quadratic Programming Feature Selection (QPFS) and Partial Least Squared (PLS) methods. In this study, microarray datasets of colon cancer and leukemia cancer were used for implementing and testing four different classifiers i.e. K-Nearest-Neighbor (KNN), Random Forest (RF), Support Vector Machine (SVM) and Neural Network (NN). The performance was measured by accuracy and AUC (area under the curve) value. The experimental results show that discretization can somehow improve performance of microarray data analysis, and mRMR gives the best performance of microarray data analysis on the colon and leukemia datasets. We also list some results on comparative performance of methods for the specific (data-ratio) number of features.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822748","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822748","10-folds Cross Validation;Feature selection;K-Nearest-Neighbor;MaxRel;Neural Network;PLS;QPFS;Random Forest;Support Vector Machine;mRMR","Artificial neural networks;Computational modeling;Radio frequency;Support vector machines","bioinformatics;cancer;data analysis;diseases;feature extraction;feature selection;genetics;genomics;learning (artificial intelligence);least squares approximations;neural nets;quadratic programming;random processes;sensitivity analysis;support vector machines","AUC value;K-Nearest-Neighbor;KNN;Minimum Redundancy-Maximum Relevance;Neural Network;Partial Least Squared methods;QPFS;Quadratic Programming Feature Selection;Random Forest;SVM;Support Vector Machine;area under the curve;classification methodology;colon cancer;colon dataset;disease;feature extraction;feature selection preprocess method;gene expression profile;improved microarray data analysis;leukemia cancer;leukemia dataset;mRMR;machine learning method;microarray dataset","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Achieving secure spectrum sensing in presence of malicious attacks utilizing unsupervised machine learning","Yang Li; Q. Peng","University of Electronic Science and Technology of China, China","MILCOM 2016 - 2016 IEEE Military Communications Conference","20161226","2016","","","174","179","In this paper, we focus on the problem of how to realize secure sensing in a comprehensive hostile cognitive radio network where both PUEA and SSDF are present, while most existing research regarding secure sensing treats Primary User Emulation Attack (PUEA) and Spectrum Sensing Data Falsification (SSDF) separately. The problem arises from the coexistence of PUEA and SSDF that even if PUEA is successfully identified, its neighboring honest users would unintentionally send contaminated sensing reports, and thus leading to unsafe sensing performance. An ideal and straightforward solution is to directly exclude those attacked secondary users from the sensing cooperation process. However, this scheme requires perfect information such as the attacking strength, geographical locations of secondary users, etc. To alleviate this requirement, a secure sensing algorithm is proposed in this paper, which identifies attacked secondary users by incorporating the idea of unsupervised machine learning, without prior information on either the attack or secondary users. Further, to account for identification error and different reliabilities of SUs, an identity value is assigned and adaptively updated for each SU. Simulations show that the proposed algorithm has better performance than conventional secure sensing algorithm in presence of both PUEA and SSDF, and remains robust even in a highly stressed environment where a significant portion of the network is attacked. It is also demonstrated that, when the sensing SNR increases, the performance of the proposed algorithm asymptotically approaches the ideal solution where the attacked users are perfectly removed.","","Electronic:978-1-5090-3781-0; POD:978-1-5090-3782-7","10.1109/MILCOM.2016.7795321","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795321","","Signal processing","cognitive radio;cooperative communication;learning (artificial intelligence);signal detection;telecommunication computing;telecommunication security","PUEA;SSDF;hostile cognitive radio network;malicious attacks;primary user emulation attack;secure spectrum sensing;sensing cooperation;spectrum sensing data falsification;unsupervised machine learning","","","","","","","1-3 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Binary Time Series Model of LTE Scheduling for Machine Learning Prediction","J. A. Sue; R. Hasholzner; J. Brendel; M. Kleinsteuber; J. Teich","Intel Deutschland GmbH, Munich, Germany","2016 IEEE 1st International Workshops on Foundations and Applications of Self* Systems (FAS*W)","20161219","2016","","","269","270","In today's Third-Generation Partnership Project (3GPP) Long-Term Evolution Advanced (LTE-A) cellular radio networks, battery lifetime is critical for mobile devices. During time intervals of no user data transmit or receive activity, energy for receiving and processing irrelevant control information in a mobile device could be saved. Therefore, we propose a binary time series model at 1 ms transmission time interval (TTI) granularity to predict the control channel information. To assess the predictability of the proposed time series, we apply three well-known machine learning (ML) algorithms combined with a non-intrusive cost-sensitive classification (CSC) scheme. Predictions of the proposed time series model successfully reach false negative rates (FNRs) below 2%.","","Electronic:978-1-5090-3651-6; POD:978-1-5090-3652-3","10.1109/FAS-W.2016.64","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789481","LTE;binary;machine learning;prediction;scheduling;time series","Downlink;Long Term Evolution;Mobile handsets;Modems;Prediction algorithms;Time series analysis;Uplink","Long Term Evolution;cellular radio;learning (artificial intelligence);pattern classification;telecommunication computing;telecommunication power management;telecommunication scheduling;time series","3GPP Long-Term Evolution Advanced cellular radio networks;FNR;LTE scheduling;ML algorithms;TTI granularity;Third-Generation Partnership Project;battery lifetime;binary time series model;control channel information prediction;false negative rates;machine learning prediction;mobile devices;nonintrusive CSC;nonintrusive cost-sensitive classification;transmission time interval","","","","","","","12-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"Context Free Frequently Asked Questions Detection Using Machine Learning Techniques","F. Razzaghi; H. Minaee; A. A. Ghorbani","Fac. of Comput. Sci., Univ. of New Brunswick, Fredericton, NB, Canada","2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)","20170116","2016","","","558","561","FAQs are the lists of common questions and answers on particular topics. Today one can find them in almost all web sites on the internet and they can be a great tool to give information to the users. Questions in FAQs are usually identified by the site administrators on the basis of the questions that are asked by their users. While such questions can respond to required information about a service, topic, or particular subject, they can not easily be distinguished from non-FAQ questions. This paper describes machine learning based parsing and question classification for FAQs. We demonstrate that questions for FAQs can be distinguished from other types of questions. Identification of specific features is the key to obtaining an accurate FAQ classifier. We propose a simple yet effective feature set including bag of words, lexical, syntactical, and semantic features. To evaluate our proposed methods, we gathered a large data set of FAQs in three different contexts, which were labeled by humans from real data. We showed that the SVM and Naive Bayes reach the accuracy of 80.3%, which is an outstanding result for the early stage research on FAQ classification. Experimental results show that the proposed approach can be a practical tool for question answering systems. To evaluate the accuracy of our classifier we have conducted an evaluation process and built the questionnaire. Therefore, we compared our classifier ranked questions with user rates and almost 81% similarity of the question ratings gives some confidence.","","Electronic:978-1-5090-4470-2; POD:978-1-5090-4471-9","10.1109/WI.2016.0095","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817112","FAQ Generation;FAQ identification;Frequently Asked Questions;Question Answering;Question classification","Context;Feature extraction;Internet;Knowledge discovery;Niobium;Semantics;Support vector machines","Internet;Web sites;context-free grammars;information retrieval;learning (artificial intelligence);support vector machines","FAQ;Internet;Naive Bayes;SVM;Web sites;answers;bag of words;context free frequently asked questions detection;lexical;machine learning;machine learning techniques;question answering systems;question classification;questions;semantic features;site administrators;syntactical","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"On a class of multi-parametric quadratic programming and its applications to machine learning","Y. Zhou; C. J. Spanos","Department of Electrical Engineering and Computer Science, University of California, Berkeley, 94720, USA","2016 IEEE 55th Conference on Decision and Control (CDC)","20161229","2016","","","2826","2833","We consider a class of multi-parametric quadratic programming (mpQP) that is closely related with large margin learning methods. With a new treatment for two types of constraints, we derive the parametric solution and provide further results on the geometric structure of the optimality. The issue of degeneracy is discussed in some depth: We show that primal degeneracy can be naturally avoided with the new treatment, and that a classification property composes a sufficient condition to rule out dual degeneracy. For general mpQP that are not strictly convex, a decomposition method is proposed based on null space technique. The theoretical analyses are then connected to large margin machine learning: it is shown that the problem of model selection can be directly reduced to the corresponding mpQP. Moreover, we demonstrate that the problem of learning with hidden variables can be transformed into a concave minimization, which admits diverse global optimization algorithms. Finally, experiments are conducted on publicly available datasets to empirically test the two applications.","","DVD:978-1-5090-1844-4; Electronic:978-1-5090-1837-6; POD:978-1-5090-1838-3","10.1109/CDC.2016.7798690","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798690","","Aerospace electronics;Kernel;Machine learning algorithms;Minimization;Quadratic programming;Support vector machines","concave programming;learning (artificial intelligence);mathematics computing;minimisation;quadratic programming","concave minimization;decomposition method;large margin machine learning;mpQP;multiparametric quadratic programming;null space technique","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Timed Dataflow: Reducing Communication Overhead for Distributed Machine Learning Systems","P. Sun; Y. Wen; T. N. B. Duong; S. Yan","Interdiscipl. Grad. Sch., Nanyang Technol. Univ., Singapore, Singapore","2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)","20170119","2016","","","1110","1117","Many distributed machine learning (ML) systems exhibit high communication overhead when dealing with big data sets. Our investigations showed that popular distributed ML systems could spend about an order of magnitude more time on network communication than computation to train ML models containing millions of parameters. Such high communication overhead is mainly caused by two operations: pulling parameters and pushing gradients. In this paper, we propose an approach called Timed Dataflow (TDF) to deal with this problem via reducing network traffic using three techniques: a timed parameter storage system, a hybrid parameter filter and a hybrid gradient filter. In particular, the timed parameter storage technique and the hybrid parameter filter enable servers to discard unchanged parameters during the pull operation, and the hybrid gradient filter allows servers to drop gradients selectively during the push operation. Therefore, TDF could reduce the network traffic and communication time significantly. Extensive performance evaluations in a real testbed showed that TDF could reduce up to 77% and 79% of network traffic for the pull and push operations, respectively. As a result, TDF could speed up model training by a factor of up to 4 without sacrificing much accuracy for some popular ML models, compared to systems not using TDF.","1521-9097;15219097","Electronic:978-1-5090-4457-3; POD:978-1-5090-5382-7","10.1109/ICPADS.2016.0146","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823861","Communication Overhead;Distributed System;Machine Learning;Parameter Server","Bandwidth;Open area test sites;Optimization;Servers;Synchronization;Training;Training data","data flow computing;learning (artificial intelligence)","TDF;communication overhead reduction;communication time reduction;distributed ML systems;distributed machine learning systems;drop gradients;hybrid gradient filter;hybrid parameter filter;network communication;network traffic reduction;performance evaluation;pull operation;pulling parameters;push operation;pushing gradients;timed dataflow;timed parameter storage system","","","","","","","13-16 Dec. 2016","","IEEE","IEEE Conference Publications"
"Domain based sentiment analysis in regional Language-Kannada using machine learning algorithm","V. Rohini; M. Thomas; C. A. Latha","Dept.of CSE, Don Bosco Institute of Technology, VTU, Bengaluru-74","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","503","507","Sentiment analysis (SA) is one of the important fields of Machine learning Language which involves analysis using natural language processing. The main goal of sentiment analysis is to detect and analyze attitude, opinions or sentiments in the text. Sentiment analysis has reach edits popularity by extracting Knowledge from huge amount data present online. The Process of analysis includes selecting features and opinion which is a challenging task in languages other than English. There are very few research works done for determining sentiments in regional languages. This Paper aims on domain based sentiment analysis in Regional language specific to movie susing machine learning algorithm for classification and provide a comparison between analysis using direct Kannada dataset and machine translated English language.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7807872","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807872","decision trees;feature extraction;opinion mining;term frequency formatting","Decision trees;Feature extraction;Frequency measurement;Motion pictures;Sentiment analysis;Speech;Tagging","feature selection;learning (artificial intelligence);sentiment analysis","attitude analysis;attitude detection;direct Kannada dataset;domain based sentiment analysis;feature selection;knowledge extraction;machine learning algorithm;machine translated English language;natural language processing;opinion analysis;regional Language-Kannada languages;sentiment detection;text analysis","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"A demonstration of machine learning for explicit functions for cycle time prediction using MES data","B. Can; C. Heavey","Enterprise Research Centre, University of Limerick, IRELAND","2016 Winter Simulation Conference (WSC)","20170119","2016","","","2500","2511","Cycle time prediction represents a challenging problem in complex manufacturing scenarios. This paper demonstrates an approach that uses genetic programming (GP) and effective process time (EPT) to predict cycle time using a discrete event simulation model of a production line, an approach that could be used in complex manufacturing systems, such as a semiconductor fab. These predictive models could be used to support control and planning of manufacturing systems. GP results in a more explicit function for cycle time prediction. The results of the proposed approach show a difference between 1-6% on the demonstrated production line.","","CD:978-1-5090-4484-9; Electronic:978-1-5090-4486-3; POD:978-1-5090-4487-0; USB:978-1-5090-4485-6","10.1109/WSC.2016.7822289","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822289","","Manufacturing systems;Mathematical model;Measurement;Predictive models;Production facilities;Semiconductor device modeling","data handling;discrete event simulation;genetic algorithms;learning (artificial intelligence);production engineering computing","GP;MES data;complex manufacturing scenarios;cycle time prediction;discrete event simulation model;effective process time;explicit functions;genetic programming;machine learning demonstration;manufacturing system planning","","","","","","","11-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Prediction of the efficacy of Wuji Pills by machine learning methods","Haiqing Li; Guo-Zheng Li; William Yang; Ying Chen; X. Zhu; M. Q. Yang","Department of Control Science and Engineering, Tongji University, Shanghai, China","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1372","1374","Efficacy prediction is an inseparable part of TCM. We firstly analyze the correlation between indicators and efficacy, and max blood-drug concentration(Cmax) is chosen as the target to reflect the efficacy of drugs. Then we apply linear regression(LR), support vector regression(SVR) as well as artificial neural networks(ANNs) to predict the efficacy of Wuji pills. The results of the leave-one-out method show that SVR performs better than other methods for label Cmax, and appears to be a good method for this task. In order to find the relationship between each component of Wuji Pills, several visualization methods are adopted to deal with this problem. The web server of prediction is available at http://data.jindengtai.cn/#/case/drug for public usage.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822720","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822720","ANNs;SVR;drug metabolism;efficacy prediction;leave-one-out","Biochemistry;Neural networks;Optimization;Support vector machines;Visualization;Web servers","blood;drugs;neural nets;neutral currents;regression analysis;support vector machines","Wuji pill efficacy prediction;artificial neural networks;blood-drug concentration;drug efficacy;leave-one-out method;linear regression;machine learning method;support vector regression","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning algorithms in context of intrusion detection","T. Mehmood; H. B. M. Rais","Department of Computer and Information Sciences, University Teknologi Petronas, Tronoh, Malaysia","2016 3rd International Conference on Computer and Information Sciences (ICCOINS)","20161215","2016","","","369","373","Design of efficient, accurate, and low complexity intrusion detection system is a challenging task. Intrusion detection method is a core of intrusion detection system and it can be either signature based or anomaly based. Although, signature based has high detection rate but it cannot detect novel attacks. Asymmetrically, anomaly based detection method can detect novel attacks but it has high false positive rate. Many machine learning techniques have been developed to cope with this problem. These machine learning algorithms develop a detection model in a training phase. This paper compares different supervised algorithms for the anomaly-based detection technique. The algorithms have been applied on the KDD99 dataset, which is the benchmark dataset used for anomaly-based detection technique. The result shows that not a single algorithm has a high detection rate for each class of KDD99 dataset. The performance measures used in this comparison are true positive rate, false positive rate, and precision.","","Electronic:978-1-5090-2549-7; POD:978-1-5090-2550-3; USB:978-1-5090-5144-1","10.1109/ICCOINS.2016.7783243","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783243","anomaly detection;intrusion detection system;machine learning;network-based intrusion detection system;supervised algorithms","Classification algorithms;Computers;Decision trees;Intrusion detection;Machine learning algorithms;Support vector machines;Testing","digital signatures;learning (artificial intelligence)","KDD99 dataset;anomaly based detection method;anomaly based system;benchmark dataset;false positive rate;intrusion detection system;machine learning algorithms;precision;signature based system;training phase;true positive rate","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Compressive Privacy: From Information/Estimation Theory to Machine Learning [Lecture Notes]","S. Y. Kung","EE, Princeton University, Princeton, New Jersey 08540 United States","IEEE Signal Processing Magazine","20170116","2017","34","1","94","112","Most of our daily activities are now moving online in the big data era, with more than 25 billion devices already connected to the Internet, to possibly over a trillion in a decade. However, big data also bears a connotation of “big brother” when personal information (such as sales transactions) is being ubiquitously collected, stored, and circulated around the Internet, often without the data owner's knowledge. Consequently, a new paradigm known as online privacy or Internet privacy is becoming a major concern regarding the privacy of personal and sensitive data.","1053-5888;10535888","","10.1109/MSP.2016.2616720","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815484","","Big data;Cloud computing;Covariance matrices;Data privacy;Entropy;Privacy","Big Data;Internet;data privacy;learning (artificial intelligence)","Big Data;Internet privacy;data owner knowledge;machine learning;online privacy;personal data privacy;personal information;sensitive data privacy","","","","","","","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"A study of spatial machine learning for business behavior prediction in location based social networks","O. Al Sonosy; S. Rady; N. L. Badr; M. Hashem","Information Systems Department, Faculty of Computers and Information Sciences, Ain Shams University, Cairo, Egypt","2016 11th International Conference on Computer Engineering & Systems (ICCES)","20170119","2016","","","266","272","Understanding business behaviors requires acquiring huge amounts of data from diverse field studies. Location Based Social Networks can provide such large amounts of data that can be used in urban analysis to understand business behaviors. Towards more insight for business behavior, a novel analytical prespective that exploits data collected from Location Based Social Networks is introduced to predict business turnouts. Prediction is implemented using machine learning techniques. Spatial regression models are investigated through a comparative study to model the dataset features relationships for business behavior prediction. Geographically Weighted Regression model is found to be the most appropriate in predicting business turnouts of objects provided by Location Based Social Networks. Moreover, a Partitioned Geographically Weighted Regression model is proposed to deal with the data heterogeneity nature pursuing more accurate predictions for the business turnouts. An experimental case study, using data about venues registered in Foursquare is conducted to assess the performance of the proposed methods. The experimental results confirm the best performance by the Geographically Weighted Regression compared to Durbin, Durbin Error, Spatial Lag, Spatial Error, and Spatial Lag X regression models presented in this study. Moreover, the proposed Partitioned Geographically Weighted Regression model experimental results showed better prediction accuracy compared to the classical Geographically Weighted Regression model.","","Electronic:978-1-5090-3267-9; POD:978-1-5090-3268-6; USB:978-1-5090-3266-2","10.1109/ICCES.2016.7822012","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822012","data mining;location based social networks;spatial machine learning","Business;Data analysis;Data mining;Data models;Facebook;Predictive models","business data processing;data mining;learning (artificial intelligence);mobile computing;regression analysis;social networking (online)","Durbin error regression model;Foursquare;business behavior prediction;dataset feature relationships;location based social networks;partitioned geographically weighted regression model;spatial error regression model;spatial lag X regression model;spatial lag regression model;spatial machine learning;spatial regression models;urban analysis","","","","","","","20-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"Research on Extreme Learning Machine Algorithm and Its Application to El-Niño/La-Niña Southern Oscillation Model","D. Xing; W. Zhang; Q. Huang; B. Liu","Acad. of Ocean Sci. & Eng., Nat. Univ. of Defense Technol., Changsha, China","2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","20161215","2016","01","","208","211","Since it has the ability to give a faster result than traditional machine learning algorithms, Extreme Learning Machine (ELM) has become increasingly popular in various research fields recently. However, ELM has been worked on the research of computer science and other related areas except for the atmospheric field. This paper uses the ELM algorithm to simulate the forward integrating process of an ocean-atmosphere oscillator model - El-Niño/La-Niña Southern Oscillation (ENSO). The results show that the ELM algorithm has a good accuracy and efficiency with a quick convergence speed and a strong resistance over observation noises.","","Electronic:978-1-5090-0768-4; POD:978-1-5090-0769-1","10.1109/IHMSC.2016.279","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783594","ENSO;Extreme Learning Machine;ocean–atmospheric oscillator","Algorithm design and analysis;Atmospheric modeling;Feedforward neural networks;Mathematical model;Ocean temperature;Oscillators","El Nino Southern Oscillation;geophysics computing;learning (artificial intelligence)","ELM algorithm;El-Niño/La-Niña Southern Oscillation Model;atmospheric field;convergence speed;extreme learning machine algorithm;forward integrating process;observation noise resistance;ocean-atmosphere oscillator model","","","","","","","27-28 Aug. 2016","","IEEE","IEEE Conference Publications"
"Support system for caregivers with sensor network and machine learning","N. Mitabe; N. Shinomiya","Dept. of Information Systems Science, Soka University, Japan","2016 IEEE 5th Global Conference on Consumer Electronics","20161229","2016","","","1","4","This paper proposes a support system in care facilities for elderly people. The purpose of the system is to monitor the elderly to avoid accidents from happening, and alleviate the burden of caregivers. Also, the system does not use cameras to monitor in order to the privacy of the elderly. Based on the research about the situation of care facilities, the system detects the falls and wanders, which are the main causes of accidents. The hetero-core spliced optical fiber sensors and the RFID sensor tags are used to detect abnormal behaviors of the elderly people. RFID sensor tags are set at various points in the living area to collect a large amount of data, which is analyzed to monitor the elderly. Also optical fiber sensors are used to find out falling. The falling information is transmitted via SNMP, and a cleaning robot confirm the accident to reduce misdirection.","","Electronic:978-1-5090-2333-2; POD:978-1-5090-2334-9","10.1109/GCCE.2016.7800528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7800528","RFID sensor tag;care giving;monitoring system;optical fiber sensor;sensor network","Accidents;Monitoring;Optical sensors;Radiofrequency identification;Robot sensing systems;Senior citizens","computerised instrumentation;fibre optic sensors;geriatrics;health care;learning (artificial intelligence);medical computing;radiofrequency identification","RFID sensor tags;SNMP;abnormal behavior detection;accident;care facilities;caregivers support system;cleaning robot;elderly people;hetero-core spliced optical fiber sensors;machine learning;sensor network","","","","","","","11-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning-Based Framework for Resource Management and Modelling for Video Analytic in Cloud-Based Hadoop Environment","M. Al-Rawahi; E. A. Edirisinghe; T. Jeyarajan","Dept. of Comput. Sci., Loughborough Univ., Loughborough, UK","2016 Intl IEEE Conferences on Ubiquitous Intelligence & Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)","20170116","2016","","","801","807","Hadoop framework has recently been adapted for use by the video analytics community for intensive, distributed video processing, storage. However, the challenge is to estimate the required amount of resources to be used in such an environment to fulfil the requirements of a user with requirements constraints. Therefore, it is important to understand how to model the performance of a Hadoop based implementation of video analytic applications in terms of meeting their performance goals. In this paper we propose the use of machine learning approachs in modelling the execution time based on the given resources. The prediction is based on parameters related to typical video analytic applications such as video file characteristics (e, g, resolution, file size, frame rate. etc.), cluster resource consumption,, Hadoop configuration values (reducer slots, tasks). The investigation carried out compares the use of different machine learning classifiers with regard to their best obtainable performance accuracies, show that a decision based model (M5P) outperforms a Linear Regression model, while the Ensemble Classifier, Bagging, out-performs these standard single classifiers. The research conducted bridges an existing research gap in video analytic-related performance predictions, whereby current research focuses on different application types, is largely limited to using standard learning algorithms such as SVM, Linear Regression, Multilayer Perceptron (MLP).","","Electronic:978-1-5090-2771-2; POD:978-1-5090-2772-9","10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816924","Cloud computing;Hadoop;Machine Learning;MapReduce;performance model;resource allocation;video analytic","Cloud computing;Computational modeling;Data models;Forensics;Predictive models;Quality of service;Resource management","cloud computing;image classification;learning (artificial intelligence);parallel processing;resource allocation;video signal processing","cloud-based Hadoop environment;decision based model;distributed video storage;execution time modelling;intensive distributed video processing;machine learning classifiers;performance accuracies;resource management;video analytic applications;video analytic modelling","","","","","","","18-21 July 2016","","IEEE","IEEE Conference Publications"
"Optimization of decentralized renewable energy system by weather forecasting and deep machine learning techniques","T. Sogabe; H. Ichikawa; T. Sogabe; K. Sakamoto; K. Yamaguchi; M. Sogabe; T. Sato; Y. Suwa","i-Powered Energy Research Center (i-PERC) The University of Electro-Communications, Tokyo, Japan","2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)","20161226","2016","","","1014","1018","Conventional electric energy can be easily adopted to a large scale by providing high quality electricity for wide-area transmissions. However, these energies are usually generated from exhaustible sources such as oil, natural gas, and coal, which are highly expensive in the long run and are the main causes of global warming. Meanwhile a large centralized energy system is more fragile and highly risky in countries like Japan where natural disasters occur frequently. A decentralized renewable energy system containing photovoltaic energy and wind power has been proposed as an alternative energy supply method. Within this system, the photovoltaic energy and wind power are well suited for the “local production and local consumption” with domestic energy transmission and are resilient to the unexpected disasters. The challenge of forming an optimal decentralized renewable energy system is to overcome its intrinsic disadvantages such as the instability and the limit of the power output. The research in this regard has drawn a lot of attention for the past twenty years. A decentralized renewable energy optimization problem is in principle categorized as nonlinear mixed integer programing problem(NMIP). Several challenging issues still remained in finding effective solution to NMIP through mathematical optimization. For instance, there is lack of reliable method to predict the energy generation and consumption; the weak scalability to large scale system is also existed due to the limited computing resource and the algorithm which are intrinsically not suitable for high speed computing. In this work, we report on employing the deep learning artificial intelligence techniques to predict the energy consumption and power generation together with the weather forecasting numerical simulation. The prediction and optimization are further examined by a small scale decentralized verification system (i-REMS) constructed inside the Uni- ersity campus. a novel optimization tool platform using Boltzmann machine algorithm for NMIP problem is also proposed for better computing scalable decentralized renewable energy system.","","Electronic:978-1-5090-4303-3; POD:978-1-5090-5228-8; USB:978-1-5090-4302-6","10.1109/ISGT-Asia.2016.7796524","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796524","Boltzmann machine;LSTM;RNN;artificial intelligence;deep learning;optimization;prediction;renewable energy","Batteries;Optimization;Power generation;Prediction algorithms;Renewable energy sources;Weather forecasting","Boltzmann machines;energy consumption;global warming;integer programming;learning (artificial intelligence);nonlinear programming;numerical analysis;photovoltaic power systems;power engineering computing;weather forecasting;wind power plants","Boltzmann machine algorithm;Japan;NMIP;alternative energy supply method;computing resource;decentralized renewable energy system;decentralized verification system;deep learning artificial intelligence techniques;deep machine learning techniques;domestic energy transmission;electric energy;energy consumption;energy generation;global warming;high speed computing;i-REMS;mathematical optimization;nonlinear mixed integer programming problem;photovoltaic energy;power generation;university campus;weather forecasting numerical simulation;wide-area transmissions;wind power","","","","","","","Nov. 28 2016-Dec. 1 2016","","IEEE","IEEE Conference Publications"
"Crack Detection in ""As-Cast"" Steel Using Laser Triangulation and Machine Learning","J. Veitch-Michaelis; Y. Tao; D. Walton; J. P. Muller; B. Crutchley; J. Storey; C. Paterson; A. Chown","Mullard Space Sci. Lab., Univ. Coll. London, Dorking, UK","2016 13th Conference on Computer and Robot Vision (CRV)","20161229","2016","","","342","349","We describe a high-accuracy inspection system designed to automatically detect cracks in ""as-cast"" steel slabs. Real-time slab inspection requires instrumentation capable of withstanding high temperatures above the steel surface as well as coping with the dirty and dusty environment present in a steel mill. Crack detection is also challenging due to the presence of oxidation scale on the slab surface. A bespoke laser triangulation system has been developed, providing images at 250 fps with a calibrated surface resolution of 97 μm from a 1m standoff distance. Cracks are detected using a combination of morphological detection and SVM classifier. Results are reported from laboratory testing and from extended trials at a production steel mill.","","Electronic:978-1-5090-2491-9; POD:978-1-5090-2492-6","10.1109/CRV.2016.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801541","3D reconstruction;challenging environment;crack detection;laser triangulation","Cameras;Slabs;Steel;Surface cracks;Surface emitting lasers;Surface morphology","crack detection;image classification;inspection;measurement by laser beam;object detection;production engineering computing;slabs;steel;steel manufacture;support vector machines","SVM classifier;as-cast steel slabs;bespoke laser triangulation system;crack detection;distance 1 m;machine learning;morphological detection;oxidation scale;production steel mill;real-time slab inspection;surface resolution","","","","","","","1-3 June 2016","","IEEE","IEEE Conference Publications"
"Early detection of plant faults by using machine learning","T. Henmi; A. Inoue; M. Deng; S. i. Yoshinaga","National Institute of Technology, Kagawa College, Kagawa, Japan","2016 International Conference on Advanced Mechatronic Systems (ICAMechS)","20170116","2016","","","201","205","To detect fault early is very important for safety. This paper proposes a method to detect plant faults early. For the early detections, to classify whether the signals is an early sign of faults or not in subtle signal difference is necessary. To the classification, this paper uses SVM (Support Vector Machine), which is one of a powerful classification technique of machine learning. Data in abnormal state of plants is obtained a few moments later after a fault occurs, hence in early stage of faults, the data of abnormal state is not available. Only measured data at normal state of plants is available in learning stage of the method. For a classification of such cases where only one side of data, which is from normal state, available, one class SVM is useful. In addition, to classify a very subtle signal as a signal of abnormal state, a high generalization ability is necessary for SVM. To get the high ability, a new kernel function, the generalized Gaussian function is used. To show the effectiveness of the method given in this paper, a simulation example of a water level control experimental plant is given.","","Electronic:978-1-5090-5346-9; POD:978-1-5090-5347-6","10.1109/ICAMechS.2016.7813447","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813447","Early fault detection;generalized Gaussian kernel;one class SVM;support vector machine (SVM);water level control experiment system","Electronic mail;Fault detection;Kernel;Level control;Noise measurement;Support vector machines;Temperature measurement","Gaussian processes;fault diagnosis;industrial plants;learning (artificial intelligence);level control;mechanical engineering computing;safety;signal classification;support vector machines","SVM;abnormal state data;early fault detection;generalized Gaussian function;kernel function;machine learning;plant fault detection;signal classification;support vector machine;water level control","","","","","","","Nov. 30 2016-Dec. 3 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach to Improve the Accuracy of GPS-Based Map-Matching Algorithms (Invited Paper)","M. Hashemi; H. A. Karimi","","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","20161219","2016","","","77","86","Advanced map-matching algorithms use location and heading of GPS points along with geometrical and topological features of digital road networks to find the road segment on which the vehicle is moving. However, GPS errors sometimes impede map-matching algorithms in finding the correct segment, especially in dense and complicated parts of the network, such as near intersections with acute angles or on close parallel roads. In this paper an artificial neural network (ANN) approach is explored to improve the segment identification accuracy of map-matching algorithms. The proposed ANN is continuously trained by using the horizontal shift imposed on GPS points and once it is trained, it will be used to correct raw GPS points before inputting them into the map-matching algorithm. Integrating the proposed ANN enabled an existing map-matching algorithm to find the correct segments for some of the GPS points where the original map-matching algorithm had failed to do so.","","Electronic:978-1-5090-3207-5; POD:978-1-5090-3208-2","10.1109/IRI.2016.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785727","GPS error;Machine learning;Map-matching algorithm;Road network;Spatio-temporal pattern","Artificial neural networks;Global Positioning System;Receivers;Roads;Satellites;Training;Vehicles","","","","","","","","","28-30 July 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning-Based Protocol for Efficient Routing in Opportunistic Networks","D. K. Sharma; S. K. Dhurandher; I. Woungang; R. K. Srivastava; A. Mohananey; J. J. P. C. Rodrigues","CAITFS, Division of Information Technology, Netaji Subhas Institute of Technology, University of Delhi, Delhi 110021, India (e-mail: dk.sharma1982@yahoo.com).","IEEE Systems Journal","","2016","PP","99","1","7","This paper proposes a novel routing protocol for OppNets called MLProph, which uses machine learning (ML) algorithms, namely decision tree and neural networks, to determine the probability of successful deliveries. The ML model is trained by using various factors such as the predictability value inherited from the PROPHET routing scheme, node popularity, node’s power consumption, speed, and location. Simulation results show that MLProph outperforms PROPHET+, a probabilistic-based routing protocol for OppNets, in terms of number of successful deliveries, dropped messages, overhead, and hop count, at the cost of small increases in buffer time and buffer occupancy values.","1932-8184;19328184","","10.1109/JSYST.2016.2630923","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782754","Decision tree;PROPHET+;delay-tolerant networks;machine learning (ML);neural networks;opportunistic networks (OppNets)","Artificial neural networks;Routing;Routing protocols;Training;Training data","","","","","","","","20161214","","","IEEE","IEEE Early Access Articles"
"Machine learning approach for classifying the cognitive states of the human brain with functional magnetic resonance imaging (fMRI)","R. F. Ahmad; A. S. Malik; N. Kamel; F. Reza","Department of Electrical and Electronic Engineering, Centre for Intelligent Signal and Imaging Research, (CISIR), Universiti Teknologi PETRONAS, 32610 Bandar Seri Iskandar, Malaysia","2016 6th International Conference on Intelligent and Advanced Systems (ICIAS)","20170119","2016","","","1","4","Cognitive state classification is a challenging task. Many studies were reported using different neuroimaging modalities for classification of the cognitive states of the human brain e.g., EEG, fMRI, MEG etc. However, functional MRI seems to be appropriate for these papers as due to its good spatial resolution and localizing the brain activated regions. In this paper, our objective is to identify the different cognitive brain states. For example, classifying the patterns of high and low cognitive loads. We acquired the fMRI data on the healthy participants. First, data is preprocessed to remove the artifacts and motions corrections. Next, regions of interest were extracted from functional brain volumes of the two states. Data reduction is also performed and data were passed to machine learning classifier i.e., support vector machine. The results showed that high and low cognitive loads were successfully classified with good accuracy.","","Electronic:978-1-5090-0845-2; POD:978-1-5090-0846-9","10.1109/ICIAS.2016.7824133","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824133","","Brain;Data acquisition;Electroencephalography;Magnetic resonance imaging;Principal component analysis;Spatial resolution;Support vector machines","biomedical MRI;brain;data reduction;image classification;learning (artificial intelligence);medical image processing;support vector machines","EEG;MEG;brain activated regions;cognitive brain states;cognitive state classification;data reduction;fMRI data;functional magnetic resonance imaging;human brain;machine learning approach;machine learning classifier;neuroimaging modalities;spatial resolution;support vector machine","","","","","","","15-17 Aug. 2016","","IEEE","IEEE Conference Publications"
"Psychology assisted prediction of academic performance using machine learning","R. R. Halde; A. Deshpande; A. Mahajan","Department of Computer Engineering, Thadomal Shahani Engineering College, Mumbai, IndiaNagpur, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","20170109","2016","","","431","435","The psychological state of the student has deep influence on their academic performance is being proved by various studies. The paper demonstrates the impact of student's psychology and their learning and study skills in the predicting their academic performance. The experiment was performed on the real time data collected from final year students. The matriculate and pre-university examination scores, five semester scores along with data on the motivation level, information processing ability and other learning and study skills were taken as input to the model to predict the Cumulative Grade Point Average(CGPA) of the sixth semester. Two machine learning algorithms were used to test the impact of students' psychology on prediction which includes Neural Network for numeric prediction of sixth semester CGPA and Decision Tree for classification of failures in sixth semester. The performance of the models were evaluated using the coefficient of correlation R and the Mean Squared Error. The accuracy of the prediction increases about 4 to 6%. The study reveals that level of motivation in student's life and the way they perceive the information and use the available study materials for the examination, all counts in prediction of their examination performance.","","Electronic:978-1-5090-0774-5; POD:978-1-5090-0775-2","10.1109/RTEICT.2016.7807857","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807857","Analytics;Decision Tree;Learning and Study Skills;Neural network;Predictive;Psychology","Correlation;Data models;Decision trees;Market research;Neural networks;Predictive models;Psychology","decision trees;educational institutions;learning (artificial intelligence);mean square error methods;neural nets;pattern classification;psychology","CGPA;academic performance;correlation R coefficient;cumulative grade point average;decision tree;failure classification;machine learning;mean squared error;neural network;psychology assisted prediction","","","","","","","20-21 May 2016","","IEEE","IEEE Conference Publications"
"Anomaly Detection in Electrical Substation Circuits via Unsupervised Machine Learning","A. Valdes; R. Macwan; M. Backes","Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","20161219","2016","","","500","505","Cyber-physical systems (CPS), such as smart grids, include cyber assets for monitoring, control, and communication in order to maintain safe and efficient operation of a physical process. We propose that CPS intrusion detection systems (CPS IDS) should seek not just to detect attacks in the host audit logs and network traffic (cyber plane), but should consider how attacks are reflected in measurements from diverse devices at multiple locations (physical plane). In electric grids, voltage and current laws induce physical constraints that can be leveraged in distributed agreement algorithms to detect anomalous conditions. This can be done by explicitly coding the physical constraints into a hybrid CPS IDS, making the detector specific to a particular CPS. We present an alternative approach, along with preliminary results, using machine learning to characterize normal, fault, and attack states in a smart distribution substation CPS, using this as a component of a CPS IDS.","","Electronic:978-1-5090-3207-5; POD:978-1-5090-3208-2","10.1109/IRI.2016.74","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785783","Anomaly detection;Cyber-physical system security;IEC 61850;Machine learning;Smart grid","Circuit faults;Current measurement;Pattern matching;Substations;Training;Voltage measurement","cyber-physical systems;learning (artificial intelligence);power system security;security of data;smart power grids;substations","anomaly detection;cyber-physical systems;electric grids;electrical substation circuits;hybrid CPS IDS;intrusion detection systems;smart distribution substation;smart grids;unsupervised machine learning","","","","","","","28-30 July 2016","","IEEE","IEEE Conference Publications"
"Repairing Intricate Faults in Code Using Machine Learning and Path Exploration","D. Gopinath; K. Wang; J. Hua; S. Khurshid","","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","453","457","Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.75","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816493","JPF;condition faults;data-structures;decisiontree learning;program repair;semi-supervised learning","Data structures;Debugging;Maintenance engineering;Semisupervised learning;Space exploration;Support vector machines;Systematics","data mining;data structures;learning (artificial intelligence);software fault tolerance;source code (software)","MLR;data structure;data-spectra mining;intricate fault repair;machine learning;patch generation;path exploration;search space pruning;source code","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"An ensemble of learning machines for quantitative analysis of bronze alloys","E. D'Andrea; B. Lazzerini","Department of Information Engineering, University of Pisa, Largo Lucio Lazzarino, 56122, Italy","IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society","20161222","2016","","","843","848","We deal with the determination of the composition of bronze alloys measured through Laser-Induced Breakdown Spectroscopy (LIBS) analysis. The relation between LIBS spectra and bronze alloy composition, represented by means of the concentrations of constituting elements, is modeled by adopting an ensemble of learning machines, fed with different inputs. Then, the combiner computes the final response. The results obtained on the test set show that the ensemble model manages to determine the composition of alloy samples with mean squared error of about 6.53 10<sup>-2</sup>.","","Electronic:978-1-5090-3474-1; POD:978-1-5090-3475-8","10.1109/IECON.2016.7793871","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7793871","Laser-Induced Breakdown Spectroscopy;ensemble;feature selection;neural networks","Artificial neural networks;Chemical elements;Genetic algorithms;Lead;Surface emitting lasers;Training","atomic emission spectroscopy;bronze;chemical analysis;chemistry computing;feature selection;mean square error methods;neural nets","LIBS analysis;bronze alloy composition;ensemble;feature selection;laser-induced breakdown spectroscopy analysis;machine learning;mean square error;neural networks;quantitative analysis","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Scalable Feature Selection and Model Updating Approach for Big Data Machine Learning","B. Yang; T. Zhang","Dept. of Comput. & Inf. Technol., Purdue Univ., West Lafayette, IN, USA","2016 IEEE International Conference on Smart Cloud (SmartCloud)","20161226","2016","","","146","151","In this paper, we proposed an innovative approach for feature selection and model updating in big data machine learning. Since hard drive access is the biggest barrier for big data problems, it is therefore nature to reduce disk I/O operations when evaluating different combinations of features, or updating a learning machine. Particularly, we are interested in discovering if small enough matrices exist to represent a system and if the calculation of such matrices can be achieved in a row-by-row fashion to avoid read data from hard drive over and over again. We examined the case of linear regression and proved that arrays of sufficient statistics can be used for feature selection and model updating. Algorithms were designed to compute the arrays in both single processor and MapReduce fashion. The proposed approach can reduce the memory requirement down to O(p<sup>2</sup>), where p is the number of variables in the data set. Simulation results also demonstrated the effectiveness of the algorithms with major computation improvements.","","Electronic:978-1-5090-5263-9; POD:978-1-5090-5264-6","10.1109/SmartCloud.2016.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796165","Big Data;Feature Selection;Machine Learning;Model updating","Arrays;Big data;Computational modeling;Data models;Machine learning algorithms;Testing;Training","Big Data;feature selection;learning (artificial intelligence);regression analysis","Big Data machine learning;MapReduce;linear regression;memory requirement;model updating approach;scalable feature selection;single processor","","","","","","","18-20 Nov. 2016","","IEEE","IEEE Conference Publications"
"Machine learning meets Kalman Filtering","A. Carron; M. Todescato; R. Carli; L. Schenato; G. Pillonetto","Department of Information Engineering of the University of Padova, via Gradenigo, 6/B 35100, Italy","2016 IEEE 55th Conference on Decision and Control (CDC)","20161229","2016","","","4594","4599","In this work we study the problem of efficient non-parametric estimation for non-linear time-space dynamic Gaussian processes (GP). We propose a systematic and explicit procedure to address this problem by pairing GP regression with Kalman Filtering. Under a specific separability assumption of the modeling kernel and periodic sampling on a (possibly non-uniform) space-grid, we show how to build an exact finite dimensional discrete-time state-space representation for the modeled process. The major finding is that the state at instant k of the associated Kalman Filter represents a sufficient statistic to compute the minimum variance prediction of the process at instant k over any arbitrary finite subset of the space. Finally, we compare the proposed strategy with standard approaches.","","DVD:978-1-5090-1844-4; Electronic:978-1-5090-1837-6; POD:978-1-5090-1838-3","10.1109/CDC.2016.7798968","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7798968","Gaussian regression;Kalman filtering;machine learning;spatio-temporal Gaussian processes","Complexity theory;Covariance matrices;Gaussian processes;Kalman filters;Kernel;Space stations","Gaussian processes;Kalman filters;learning (artificial intelligence);regression analysis;signal sampling","GP regression;Kalman filtering;arbitrary finite subset;finite dimensional discrete-time state-space representation;machine learning;minimum variance prediction;modeling kernel;nonlinear time-space dynamic Gaussian processes;nonparametric estimation;periodic sampling;space-grid","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Exploiting distributional semantics to benefit machine learning in automated classification of Chinese clinical text","W. Ning; M. Yu","Department of Industrial Engineering, Tsinghua University, Beijing, China","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","1096","1102","Machine learning has been widely employed for the automated classification of clinical text to enhance the utilization of clinical information and benefit clinical applications. However, the conventional approaches for the vector representations of text in machine learning algorithms cannot model the connections between highly similar words and will also lead to high dimensionality. This study suggests a combination of distributional semantics and supervised classification algorithms in order to tackle these problems. Latent Semantic Analysis, Random Indexing and word2vec are adopted for distributional semantic representations to build classifiers using Support Vector Machines, Naïve Bayes and k-Nearest Neighbors. As an initial study, we adopt Chinese diagnostic phrases as the clinical text to be classified and the 3-digit ICD-10 codes as the class labels. The evaluation results demonstrate that distributional semantic representations can better capture the meanings of text and can improve the accuracy of clinical text classification when the data for training and testing come from different sources and share less consistent language use. Consequently, distributional semantics can enhance the extensibility of the classifiers for clinical text classification.","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822674","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822674","clinical text;distributional semantics;machine learning;text classification;word embedding","Computational modeling;Niobium;Sections;Semantics;Support vector machines","Bayes methods;classification;electronic health records;learning (artificial intelligence);support vector machines;text analysis","3-digit ICD-10 codes;Chinese clinical text;Chinese diagnostic phrases;Naïve Bayes method;automated classification;clinical information;distributional semantics;k-Nearest Neighbors;latent semantic analysis;machine learning;random indexing;supervised classification algorithms;support vector machines;word2vec","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Smartphone wireless gyroscope platform for machine learning classification of hemiplegic patellar tendon reflex pair disparity through a multilayer perceptron neural network","Lemoyne; Mastroianni","Department of Biological Sciences and Center for Bioengineering Innovation, Northern Arizona University, Flagstaff, AZ, USA","2016 IEEE Wireless Health (WH)","20161215","2016","","","1","6","The patellar tendon enables fundamental insight regarding neurological health status. Clinically observed dysfunction may warrant escalation to more advanced and expensive medical diagnostics. Conventionally clinicians apply an ordinal scale to quantify reflex response characteristics. However the reliability of ordinal scales is a subject of debate, and even highly skilled clinicians have disputed the observation of an asymmetric reflex pair. An alternative is the use of the wireless quantified reflex system, which features an impact pendulum attached to a reflex hammer for providing precisely targeted levels of potential energy with a smartphone (iPhone) equipped with software to function as a wireless gyroscope platform that can email a trial sample as an email attachment by wireless connectivity to the Internet. With notable attributes of the gyroscope signal recordings of the reflex response of a hemiplegic patellar tendon reflex pair observed a feature set is developed for machine learning classification. Using the multilayer perceptron neural network considerable classification accuracy is attained. The research implications reveal the potential of integrating machine learning with a wireless reflex quantification system that applies a smartphone (iPhone) as a wireless gyroscope platform.","","Electronic:978-1-5090-3090-3; POD:978-1-5090-3091-0","10.1109/WH.2016.7764563","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7764563","","Accelerometers;Biological neural networks;Gyroscopes;Reliability;Tendons;Wireless communication;Wireless sensor networks","gyroscopes;learning (artificial intelligence);medical disorders;neural nets;neurophysiology;patient diagnosis;smart phones","clinically observed dysfunction;hemiplegic patellar tendon reflex pair disparity;iPhone;machine learning classification;medical diagnostics;multilayer perceptron neural network;neurological health status;smartphone wireless gyroscope platform;wireless quantified reflex system","","","","","","","25-27 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Approach for Predicting Wall Shear Distribution for Abdominal Aortic Aneurysm and Carotid Bifurcation Models","M. Jordanski; M. Radovic; Z. Milosevic; N. Filipovic; Z. Obradovic","","IEEE Journal of Biomedical and Health Informatics","","2016","PP","99","1","1","Computer simulations based on the finite element method (FEM) represent powerful tools for modeling blood flow through arteries. However, due to its computational complexity, this approach may be inappropriate when results are needed quickly. In order to reduce computational time, in this paper we proposed an alternative machine learning based approach for calculation of wall shear stress (WSS) distribution, which may play an important role in mechanisms related to initiation and development of atherosclerosis. In order to capture relationships between geometric parameters, blood density, dynamic viscosity and velocity and WSS distribution of geometrically parameterized abdominal aortic aneurysm (AAA) and carotid bifurcation models, we proposed multivariate linear regression (MLR), multilayer perceptron neural network (MLP) and gaussian conditional random fields (GCRF). Results obtained in this paper show that machine learning approaches can successfully predict WSS distribution at different cardiac cycle time points. Even though all proposed methods showed high potential for WSS prediction, GCRF achieved the highest coefficient of determination (0.930 to 0.948 for AAA model and 0.946 to 0.954 for carotid bifurcation model) demonstrating benefits of accounting for spatial correlation. The proposed approach can be used as an alternative method for real time calculation of wall shear stress distribution.","2168-2194;21682194","","10.1109/JBHI.2016.2639818","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782857","Abdominal aortic aneurysm;carotid bifurcation;finite element modeling;machine learning;wall shear stress","Aneurysm;Bifurcation;Biological system modeling;Blood;Computational modeling;Predictive models;Stress","","","","","","","","20161214","","","IEEE","IEEE Early Access Articles"
"Efficient algorithm selection for packet classification using machine learning","M. Elmahgiubi; O. Ahmed; S. Areibi; G. Grewal","School of Engineering, University of Guelph, Guelph, Canada","2016 IEEE 21st International Workshop on Computer Aided Modelling and Design of Communication Links and Networks (CAMAD)","20161222","2016","","","24","30","Many packet classification algorithms with variable performances and capabilities are available. However, no single algorithm is guaranteed to outperform every other one in every case. Meta-Learning is a subfield in Machine Learning that aims to apply statistical techniques to automate the algorithm selection process. In this work, we propose a novel framework for efficient, automatic packet classification algorithm selection. By utilizing Meta-Learning and Artificial Neural Networks (ANNs) we are able to achieve an average accuracy of 90% when automatically choosing the most appropriate algorithm when applied to over a hundred different rulesets ranging in size from 1K to 5K.","","Electronic:978-1-5090-2558-9; POD:978-1-5090-2559-6","10.1109/CAMAD.2016.7790325","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790325","Algorithms Selection;Artificial Neural Networks;Meta-Learning;Packet Classification;Recommendation Systems","Algorithm design and analysis;Data mining;Feature extraction;Machine learning algorithms;Memory management;Prediction algorithms;Training","computer networks;learning (artificial intelligence);neural nets;pattern classification;statistical analysis","artificial neural networks;automatic packet classification algorithm selection;machine learning;metalearning;statistical techniques","","","","","","","23-25 Oct. 2016","","IEEE","IEEE Conference Publications"
"Code clones detection using machine learning technique: Support vector machine","S. Jadon","Department of Computer Science and Engineering, Motilal Nehru National Institute of Technology, Allahabad, India 211004","2016 International Conference on Computing, Communication and Automation (ICCCA)","20170116","2016","","","399","303","Code clones defined as sequence of source code that occur more than once in the same program or across different programs are undesirable as they increase the size of program and creates the problems of redundancy. Fixing of bugs detected in one clone require detection of all clones. Hence, it is imperative to identify and remove all code clones in a program. The focus of previous research work on the code clone detection was to find identical clones, or clones that are identical up to identifiers and literal values. But, detection of similar clones is often important. In the present paper it is proposed to generate the feature sets after parsing the given C program for code fragments and then match their similarity. On the basis of feature sets the classification of algorithm is being performed by using the Support Vector Machine (SVM) as a machine learning tool. The output of the machine tool would be the similarity ratio with which the two C programs are related to each other and also the class in which they would occur. It was observed that the test results of the tool implementation show detection of code clones in the program and its accuracy increases with the increase in number of instances.","","Electronic:978-1-5090-1666-2; POD:978-1-5090-1667-9","10.1109/CCAA.2016.7813733","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813733","Code clones;classification;feature sets;parsing;support vector machine","Automation;Cloning;Machine learning algorithms;Redundancy;Software;Support vector machines;Syntactics","C language;learning (artificial intelligence);pattern classification;program debugging;source code (software);support vector machines","C program parsing;SVM;bug detection;code clone detection;code fragments;identical clones;identifiers;literal values;machine learning technique;machine tool;similarity ratio;source code;support vector machine","","","","","","","29-30 April 2016","","IEEE","IEEE Conference Publications"
"A machine learning-based tourist path prediction","S. Zheng; Y. Liu; Z. Ouyang","Beihang University, Beijing 100191, China","2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)","20161219","2016","","","38","42","Intelligent recommendations about where to go will be very helpful for personal and commercial travel recommendations. We deal with this kind of recommendations as a prediction problem based on the tourist's historical visiting sequences and supervised machine learning algorithms, namely Random Forests and LambdaMART. We propose a feature set with 56 dimensions from the tourist's historical traveling data. By utilizing the entropy from information theory, all features are ranked. Evaluation results show that when selecting the most important subset of 20 features as our final input of Random Forests, we can get a 4% higher accuracy and a 70% reduction of computation complexity with regard to using the full set of features. A comparison of five different machine learning algorithms, namely Random Forests, LambdaMART, Ranking SVM, ListNet and RankBoost, has been taken on this feature set, results demonstrate that the Random Forests outperforms the other algorithms.","","CD:978-1-5090-1254-1; Electronic:978-1-5090-1256-5; POD:978-1-5090-1257-2","10.1109/CCIS.2016.7790221","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790221","Learning to Rank;Machine Learning;Points of Interest Prediction","Flickr;Global Positioning System;Prediction algorithms;Predictive models;Support vector machines;Urban areas;Vegetation","learning (artificial intelligence);recommender systems;travel industry","LambdaMART;ListNet;RankBoost;SVM;commercial travel recommendations;computation complexity;entropy;historical traveling data;information theory;intelligent recommendations;machine learning-based tourist path prediction;prediction problem;random forests;supervised machine learning algorithms","","","","","","","17-19 Aug. 2016","","IEEE","IEEE Conference Publications"
"Missing data handling using machine learning for human activity recognition on mobile device","O. M. Prabowo; K. Mutijarsa; S. H. Supangkat","School of Electrical Engineering and Informatics - Institut Teknologi Bandung, Bandung, Indonesia","2016 International Conference on ICT For Smart Society (ICISS)","20161222","2016","","","59","62","Human activity recognition is important technology in mobile computing era because it can be applied to many real-life, human-centric problems such as eldercare and healthcare. Successful research has so far focused on recognizing simple human activities. Currently, the smartphone is equipped with various sensors such as an accelerometer, gyroscope, digital compass, microphone, GPS and camera. The sensors have been used in various areas such as human gesture and activity recognition which is opening a new area of research and significantly impact in daily life. Activity recognition between the personal computer and smartphone is different. A mobile device has limited computational and memory capacity which has a chance that some data are missing when limitation of the mobile device is happening. In this research, some algorithms are tested to perform their ability to handling missing data, they are Bayesian Network, Multilayer Perceptron (MLP), C4.5 and k-Nearest Neighbour (k-NN). Missing data are implemented with increment scaling from 5%-40%. Optimal result based on accuracy mean is obtained by kNN with 89,4752%. Based on class, Bayesian Network obtained mean 992 recognized on Sitting class and kNN obtained mean 1010 recognized on Walking class. Multilayer Perceptron is obtained endurance point with decreasing about 9.9109% from normal experiment without missing data.","","Electronic:978-1-5090-1620-4; POD:978-1-5090-1621-1","10.1109/ICTSS.2016.7792849","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792849","Activity Recognition;Classification;Machine Learning;Missing Data;Mobile Computing;Participatory Sensing","Accelerometers;Classification algorithms;Gyroscopes;Mobile handsets;Multilayer perceptrons","belief networks;data handling;gesture recognition;image motion analysis;learning (artificial intelligence);mobile computing;multilayer perceptrons;smart phones","Bayesian network;C4.5;GPS;MLP;accelerometer;camera;digital compass;gyroscope;human activity recognition;human gesture;k-nearest neighbour;kNN;machine learning;microphone;missing data handling;mobile computing;mobile device;multilayer perceptron;smartphone sensors","","","","","","","20-21 July 2016","","IEEE","IEEE Conference Publications"
"Fall detection using supervised machine learning algorithms: A comparative study","N. Zerrouki; F. Harrou; A. Houacine; Y. Sun","","2016 8th International Conference on Modelling, Identification and Control (ICMIC)","20170105","2016","","","665","670","Fall incidents are considered as the leading cause of disability and even mortality among older adults. To address this problem, fall detection and prevention fields receive a lot of intention over the past years and attracted many researcher efforts. We present in the current study an overall performance comparison between fall detection systems using the most popular machine learning approaches which are: Naïve Bayes, K nearest neighbor, neural network, and support vector machine. The analysis of the classification power associated to these most widely utilized algorithms is conducted on two fall detection databases namely FDD and URFD. Since the performance of the classification algorithm is inherently dependent on the features, we extracted and used the same features for all classifiers. The classification evaluation is conducted using different state of the art statistical measures such as the overall accuracy, the F-measure coefficient, and the area under ROC curve (AUC) value.","","Electronic:978-0-9567157-7-7; POD:978-1-5090-1594-8","10.1109/ICMIC.2016.7804195","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804195","Human fall detection;Support vector machine;classification;feature extraction","Cameras;Classification algorithms;Feature extraction;Machine learning algorithms;Support vector machines;Training;Training data","Bayes methods;assisted living;feature extraction;learning (artificial intelligence);neural nets;pattern classification","AUC value;F-measure coefficient;FDD;K nearest neighbor;Naive Bayes method;ROC curve;URFD;fall detection;fall incidents;fall prevention;neural network;older adult disability;older adult mortality;supervised machine learning;support vector machine","","","","","","","15-17 Nov. 2016","","IEEE","IEEE Conference Publications"
"Semi-supervised machine learning for textual anomaly detection","C. Steyn; A. de Waal","Department of Statistics, University of Pretoria","2016 Pattern Recognition Association of South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech)","20170116","2016","","","1","5","Anomaly detection comprises the identification of observations which do not follow the expected patterns of the assumed data set. We attempt to simplify the problem of textual anomaly detection by constructing a Multinomial Naïve Bayes classifier and enhancing it with an augmented Expectation Maximization (EM) algorithm. By doing so, we utilize large amounts of unlabelled data and show how the EM algorithm could increase the accuracy of the Naïve Bayes classifier. The process is applied to a binary classification environment in order to detect anomalies in text.","","Electronic:978-1-5090-3335-5; POD:978-1-5090-3336-2","10.1109/RoboMech.2016.7813191","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813191","","Companies;Convergence;Data models;Mathematical model;Probabilistic logic;Text analysis;Training data","expectation-maximisation algorithm;learning (artificial intelligence);pattern classification;text analysis","EM algorithm;augmented expectation maximization algorithm;binary classification environment;multinomial naive Bayes classifier;semisupervised machine learning;textual anomaly detection;unlabelled data","","","","","","","Nov. 30 2016-Dec. 2 2016","","IEEE","IEEE Conference Publications"
"A machine learning enabled network planning tool","J. Moysen; L. Giupponi; J. Mangues-Bafalluy","Centre Tecnologic de Telecomunicacions de Catalunya-CTTC, Av. Carl Friedrich Gauss 7, 08860 Castelldefels (Spain)","2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)","20161222","2016","","","1","7","In the coming years, planning future mobile networks will be infinitely more complex than nowadays. Future networks are expected to present multiple Network Management (NM) challenges to operators, such as managing network complexity in terms of densification of scenarios, heterogeneous nodes, applications, Radio Access Technologies (RAT), among others. In this context, the exploitation of past information gathered by the network is highly relevant when planning future deployments. In this paper we present a network planning tool based on Machine Learning (ML). In particular, we propose an approach which allows to predict Quality of Service (QoS) offered to end-users, based on data collected by the Minimization of Drive Tests (MDT) function. As a QoS indicator, we focus on Physical Resource Block (PRB) per Megabit (Mb) in an arbitrary point of the network. Minimizing this metric allows serving users with the same QoS by consuming less resources, and therefore, being more cost-effective. The proposed network planning tool considers a Genetic Algorithm (GA), which tries to reach the operator targets. The network parameters we desire to optimise are set as the input to the algorithm. Then, we predict the QoS of the network by means of ML techniques. By integrating these techniques in a network planning tool, operators would be able to find the most appropriate deployment layout, by minimizing the resources (i.e., the cost) they need to deploy to offer a given QoS in a newly planned deployment.","","Electronic:978-1-5090-3254-9; POD:978-1-5090-3255-6","10.1109/PIMRC.2016.7794909","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7794909","Genetic Algorithm;Machine Learning;Minimization of Drive Test;Network planning;Quality of Service","Face;Mobile communication;Mobile computing;Optimization;Planning;Quality of service;Training","genetic algorithms;learning (artificial intelligence);mobile radio;quality of service;telecommunication computing;telecommunication network planning","MDT function;ML techniques;QoS indicator;data collection;genetic algorithm;heterogeneous nodes;machine learning enabled network planning tool;minimization of drive test function;mobile networks;multiple network management;physical resource block per megabit;quality of service;radio access technology","","","","","","","4-8 Sept. 2016","","IEEE","IEEE Conference Publications"
"A study on improvement of recognition accuracy by applying machine learning algorithms to the vision-based traffic condition analysis system","K. Lee; H. Ju; Y. M. Jeong; S. Y. Min","Embedded.SW Research Center, Korea Electronics Technology Institute, Seongnam-si, Republic of Korea","2016 International SoC Design Conference (ISOCC)","20161229","2016","","","233","234","This paper proposes a method of applying a machine learning algorithm in order to improve the recognition rate of the video-based traffic information system. After applying the Error Backpropagation learning neural network algorithm to the traffic information it will be used for image recognition results. The training data is generated from the traffic information system, the noise of the generated data is removed by Gaussian smoothing. In this paper, we develop a machine learning based Traffic Condition analysis system was able to get an improved recognition rate than conventional vision-based system.","","Electronic:978-1-5090-3219-8; POD:978-1-5090-3220-4; USB:978-1-5090-3218-1","10.1109/ISOCC.2016.7799873","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7799873","Error Backpropagation;Traffic condition recognition;machine learning","Backpropagation;Biological neural networks;Image recognition;Machine learning algorithms;Mathematical model;Roads;Support vector machines","Gaussian processes;backpropagation;computer vision;image recognition;neural nets;traffic engineering computing;video signal processing","Gaussian smoothing;error backpropagation learning neural network;image recognition;machine learning;video-based traffic information system;vision-based traffic condition analysis system","","","","","","","23-26 Oct. 2016","","IEEE","IEEE Conference Publications"
