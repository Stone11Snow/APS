"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7456411,7455893,7456965,7455925,7152975,7456903,7453917,7453188,7115113,7448033,7448025,7448189,7446050,7445413,7446049,7445385,7443752,7443788,7348708,7444931,7442356,7347425,7440588,7439360,7439256,7440525,7438442,7440538,7435682,7435686,7436414,7436282,7435789,7435444,7434224,7430958,7430979,7433262,7429309,7401030,7430361,7428567,7358076,7348721,7424415,7424481,7424435,7424037,7424346,7424446,7424086,7424344,7424304,7424301,7423015,7424422,7424323,7424434,7359099,7424426,7423530,7424485,7424324,7424309,7424158,7423529,7424470,7421361,7418921,7404123,7415180,7418412,7418290,7414773,7414202,7417388,7418858,7417113,7400989,7408909,7408329,7359437,7408647,7410129,7411124,7408642,7411015,7410475,7408644,7411268,7406296,7406264,7406330,7407091,7404607,7404935,7064791,7396825,7395609,7395731",2017/05/04 23:27:32
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Driver Distraction Detection Using Semi-Supervised Machine Learning","T. Liu; Y. Yang; G. B. Huang; Y. K. Yeo; Z. Lin","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","IEEE Transactions on Intelligent Transportation Systems","20160325","2016","17","4","1108","1120","Real-time driver distraction detection is the core to many distraction countermeasures and fundamental for constructing a driver-centered driver assistance system. While data-driven methods demonstrate promising detection performance, a particular challenge is how to reduce the considerable cost for collecting labeled data. This paper explored semi-supervised methods for driver distraction detection in real driving conditions to alleviate the cost of labeling training data. Laplacian support vector machine and semi-supervised extreme learning machine were evaluated using eye and head movements to classify two driver states: attentive and cognitively distracted. With the additional unlabeled data, the semi-supervised learning methods improved the detection performance (G-mean) by 0.0245, on average, over all subjects, as compared with the traditional supervised methods. As unlabeled training data can be collected from drivers' naturalistic driving records with little extra resource, semi-supervised methods, which utilize both labeled and unlabeled data, can enhance the efficiency of model development in terms of time and cost.","1524-9050;15249050","","10.1109/TITS.2015.2496157","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347425","Advanced driver assistance system;driver distraction;eye movement;machine learning;semi-supervised learning","Labeling;Position measurement;Real-time systems;Semisupervised learning;Support vector machines;Training data;Vehicles","behavioural sciences computing;data handling;learning (artificial intelligence);support vector machines;traffic engineering computing","Laplacian support vector machine;data driven methods;driver centered driver assistance system;eye movements;head movements;labeling training data;real-time driver distraction detection;semi-supervised extreme learning machine;semisupervised machine learning;semisupervised methods","","1","","52","","20151204","April 2016","","IEEE","IEEE Journals & Magazines"
"Botnet Domain Name Detection based on machine learning","J. Jin; Z. Yan; G. Geng; B. Yan","China Internet Network Information Center, Beijing 100190, China","6th International Conference on Wireless, Mobile and Multi-Media (ICWMMN 2015)","20160419","2015","","","273","276","Domain Name System (DNS) is a fundamental component of today's Internet: it provides mappings between domain names used by people and the corresponding IP addresses required by network protocols. However, the open and fundamental characteristics of DNS are recently used by the botnet for the communication between bots and C&C. In this paper, we select six kinds of special features of botnet domain querying traffic based on the deep studies of the DNS log. Then three popular classifiers are adopted in order to pick the malicious domains out from the DNS traffic using those features.","","Electronic|Paper:978-1-78561-047-9|978-1-78561-046-2","10.1049/cp.2015.0953","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453917","Botnet;Classifier;DNS;FFSN","","IP networks;Internet;invasive software;learning (artificial intelligence);pattern classification;protocols;telecommunication traffic","C and C;DNS;DNS log;DNS traffic;IP addresses;Internet;botnet domain name detection;botnet domain querying traffic;classifiers;fundamental component;machine learning;network protocols;open fundamental characteristics","","","","","","","20-23 Nov. 2015","","IET","IET Conference Publications"
"HadoopCL2: Motivating the Design of a Distributed, Heterogeneous Programming System With Machine-Learning Applications","M. Grossman; M. Breternitz; V. Sarkar","Department of Computer Science, 6100 Main St., Rice University, Houston, TX","IEEE Transactions on Parallel and Distributed Systems","20160211","2016","27","3","762","775","Machine learning (ML) algorithms have garnered increased interest as they demonstrate improved ability to extract meaningful trends from large, diverse, and noisy data sets. While research is advancing the state-of-the-art in ML algorithms, it is difficult to drastically improve the real-world performance of these algorithms. Porting new and existing algorithms from single-node systems to multi-node clusters, or from architecturally homogeneous systems to heterogeneous systems, is a promising optimization technique. However, performing optimized ports is challenging for domain experts who may lack experience in distributed and heterogeneous software development. This work explores how challenges in ML application development on heterogeneous, distributed systems shaped the development of the HadoopCL2 (HCL2) programming system. ML applications guide this work because they exhibit features that make application development difficult: large & diverse datasets, complex algorithms, and the need for domain-specific knowledge. The goal of this work is a general, MapReduce programming system that outperforms existing programming systems. This work evaluates the performance and portability of HCL2 against five ML applications from the Mahout ML framework on two hardware platforms. HCL2 demonstrates speedups of greater than 20x relative to Mahout for three computationally heavy algorithms and maintains minor performance improvements for two I/O bound algorithms.","1045-9219;10459219","","10.1109/TPDS.2015.2414943","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064791","GPU;MapReduce;auto-scheduling;distributed;heterogeneous;programming model","Computational modeling;Java;Kernel;Object oriented modeling;Performance evaluation;Programming;Vectors","data handling;distributed programming;learning (artificial intelligence);optimisation;parallel processing;software engineering","HCL2 programming system;HadoopCL2 programming system;I/O bound algorithms;ML application development;Mahout ML framework;MapReduce programming system;distributed programming system;distributed software development;domain-specific knowledge;heterogeneous programming system;heterogeneous software development;machine-learning applications;optimization technique","","0","","16","","20150320","March 1 2016","","IEEE","IEEE Journals & Magazines"
"Measuring and Modelling Delays in Robot Manipulators for Temporally Precise Control Using Machine Learning","T. T. Andersen; H. B. Amor; N. A. Andersen; O. Ravn","Dept. of Autom. & Control, DTU Electr. Eng. Tech. Univ. of Denmark, Lyngby, Denmark","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","168","175","Latencies and delays play an important role in temporally precise robot control. During dynamic tasks in particular, a robot has to account for inherent delays to reach manipulated objects in time. The different types of occurring delays are typically convoluted and thereby hard to measure and separate. In this paper, we present a data-driven methodology for separating and modelling inherent delays during robot control. We show how both actuation and response delays can be modelled using modern machine learning methods. The resulting models can be used to predict the delays as well as the uncertainty of the prediction. Experiments on two widely used robot platforms show significant actuation and response delays in standard control loops. Predictive models can, therefore, be used to reason about expected delays and improve temporal accuracy during control. The approach can easily be used on different robot platforms.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.98","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424304","Automation;Machine learning algorithms;Robot control","Delays;Predictive models;Robot control;Robot sensing systems;Service robots;Solid modeling","control engineering computing;delays;learning (artificial intelligence);manipulators","actuation delays;control loops;delay measurement;delay modelling;machine learning methods;predictive models;response delays;robot manipulators;temporal accuracy;temporally precise robot control","","","","18","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning based acoustic sensing for indoor room localisation using mobile phones","L. Phillips; C. B. Porter; N. Kottege; M. D'Souza; M. Ros","School of ITEE, The University of Queensland, Brisbane, Australia","2015 9th International Conference on Sensing Technology (ICST)","20160324","2015","","","456","460","We present a novel indoor localisation system that used acoustic sensing. We developed the Acoustic Landmark Locator to determine a person's current room location, within a building. Indoor environments tend to have distinct acoustic properties due to physical structure. Hence rooms in a building can have distinctive acoustic signatures. We found that these acoustic signatures can determine the position of a person. We attempted to identify location based on acoustic sensing of the surrounding indoor environment. We developed a mobile phone application that determined a person's location by measuring the acoustic levels of the surrounding environment. We used a machine learning artificial neural network based algorithm to classify the location of the person, within proximity to a landmark or room. We tested the Acoustic Landmark Locator in an indoor environment. Our tests show that the Acoustic Landmark Locator mobile phone app was able to successfully determine the location of the person carrying the mobile phone, in all test areas. It was also found that background noise caused by the presence of people does distort the landmark acoustic profiles but the artificial neural network based classifier was able to reliably determine the person's room location. Further work will involve investigating how other machine learning approaches can be used to better improve position accuracy.","","Electronic:978-1-4799-6314-0; POD:978-1-4799-6315-7","10.1109/ICSensT.2015.7438442","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438442","","Acoustic measurements;Acoustics;Indoor environments;Mobile handsets;Neural networks;Radio frequency;Sensors","Global Positioning System;acoustic transducers;acoustic variables measurement;computerised instrumentation;indoor environment;learning (artificial intelligence);level measurement;mobile handsets;neural nets;sensors","GPS;Global Positioning System;acoustic landmark locator;acoustic level measurement;acoustic sensing;acoustic signature;artificial neural network based algorithm;indoor environment;indoor room localisation system;machine learning approach;mobile phone application","","","","13","","","8-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning approach for the identification of diabetes retinopathy and its stages","P. Nijalingappa; B. Sandeep","Department of Computer Science and Engineering, Bapuji Institute of Engineering Technology, Davanagere, India","2015 International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","20160421","2015","","","653","658","The effects of the eye abnormalities are mostly gradual in nature which shows the necessity for an accurate abnormality identification system. Abnormality in retina is one among them. Diabetic Retinopathy (DR) is a disease that causes damage to the retina of human eye, which is caused by complications of diabetes. DR is one of the main causes of vision loss and its prevalence keeps rising. Diabetic Retinopathy, a frequent diabetic retinal disease is caused due to the blood vessels in the retina get changes from its original shape. Diabetic Retinopathy generally affects both the human eyes. Most of the ophthalmologists depend on the visual interpretation for the identification of the types of diseases. But, inaccurate diagnosis will change the course of treatment planning which leads to fatal results. Hence, there is a requirement for a bias free automated system which yields highly accurate results. In this paper, we are classifying the various stages of DR. We first present a summary of diabetic retinopathy and its causes. Then, a literature review of the automatic detection of diabetic retinopathy techniques is presented. Explanation and restrictions of retina databases which are used to test the performance of these detection algorithms are given.","","Electronic:978-1-4673-9223-5; POD:978-1-4673-9224-2; USB:978-1-4673-9222-8","10.1109/ICATCCT.2015.7456965","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456965","Diabetic Retinopathy;Exudates;Hemorrhage;Image processing;Medical Imaging;Retinal Image","Diabetes;Diseases;Feature extraction;Image segmentation;Optical imaging;Retina;Retinopathy","blood vessels;diseases;eye;learning (artificial intelligence);medical image processing","bias free automated system;blood vessels;diabetes retinopathy identification;diabetic retinal disease;human eye retina;machine learning approach;ophthalmologists;retina databases;treatment planning course","","","","22","","","29-31 Oct. 2015","","IEEE","IEEE Conference Publications"
"BSSync: Processing Near Memory for Machine Learning Workloads with Bounded Staleness Consistency Models","J. H. Lee; J. Sim; H. Kim","Sch. of Comput. Sci., Georgia Inst. of Technol., Atlanta, GA, USA","2015 International Conference on Parallel Architecture and Compilation (PACT)","20160310","2015","","","241","252","Parallel machine learning workloads have become prevalent in numerous application domains. Many of these workloads are iterative convergent, allowing different threads to compute in an asynchronous manner, relaxing certain read-after-write data dependencies to use stale values. While considerable effort has been devoted to reducing the communication latency between nodes by utilizing asynchronous parallelism, inefficient utilization of relaxed consistency models within a single node have caused parallel implementations to have low execution efficiency. The long latency and serialization caused by atomic operations have a significant impact on performance. The data communication is not overlapped with the main computation, which reduces execution efficiency. The inefficiency comes from the data movement between where they are stored and where they are processed. In this work, we propose Bounded Staled Sync (BSSync), a hardware support for the bounded staleness consistency model, which accompanies simple logic layers in the memory hierarchy. BSSync overlaps the long latency atomic operation with the main computation, targeting iterative convergent machine learning workloads. Compared to previous work that allows staleness for read operations, BSSync utilizes staleness for write operations, allowing stale-writes. We demonstrate the benefit of the proposed scheme for representative machine learning workloads. On average, our approach outperforms the baseline asynchronous parallel implementation by 1.33x times.","1089-795X;1089795X","Electronic:978-1-4673-9524-3","10.1109/PACT.2015.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429309","Asynchronous Parallelism;Atomic Operation;Bounded Staleness Consistency Model;Iterative Convergent Machine Learning Workloads","Atomic layer deposition;Computational modeling;Convergence;Hardware;Instruction sets;Parallel processing;Synchronization","learning (artificial intelligence);parallel processing;storage management","BSSync;asynchronous parallelism;bounded staled sync;bounded staleness consistency models;iterative convergent machine learning workloads;long latency atomic operation;memory hierarchy;near memory processing;parallel machine learning workloads;read operations;read-after-write data dependencies;relaxed consistency models;representative machine learning workloads;stale-writes;write operations","","3","","31","","","18-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Hybrid UCB-HMM: A Machine Learning Strategy for Cognitive Radio in HF Band","L. Melián-Gutiérrez; N. Modi; C. Moy; F. Bader; I. Pérez-Álvarez; S. Zazo","IDeTIC, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran, Canaria, Spain","IEEE Transactions on Cognitive Communications and Networking","20160310","2015","1","3","347","358","Multiple users transmit in the HF band with worldwide coverage but collide with other HF users. New techniques based on cognitive radio principles are discussed to reduce the inefficient use of this band. In this paper, we show the feasibility of the Upper Confidence Bound (UCB) algorithm, based on reinforcement learning, for an opportunistic access to the HF band. The exploration vs. exploitation dilemma is evaluated in single-channel and multi-channel UCB algorithms in order to obtain their best performance in the HF environment. Furthermore, we propose a new hybrid system, which combines two types of machine learning techniques based on reinforcement learning and learning with Hidden Markov Models. This system can be understood as a metacognitive engine that automatically adapts its data transmission strategy according to HF environment's behaviour to efficiently use spectrum holes. The proposed hybrid UCB-HMM system increases the duration of data transmission's slots when conditions are favourable, and is also able to reduce the required signalling transmissions between transmitter and receiver to inform which channels have been selected for data transmission. This reduction can be as high as 61% with respect to the signalling required by multi-channel UCB.","","","10.1109/TCCN.2016.2527021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401030","Cognitive Radio;HF;Hidden Markov Model;Opportunistic Spectrum Access;Upper Confidence Bound","Algorithm design and analysis;Cognitive radio;Data communication;Hidden Markov models;Indexes;Predictive models;Proposals","cognitive radio;hidden Markov models;learning (artificial intelligence);radio spectrum management;telecommunication computing;wireless channels","HF band;cognitive radio principles;data transmission slot duration;exploitation dilemma;exploration dilemma;hidden Markov models;hybrid UCB-HMM system;machine learning strategy;metacognitive engine;multichannel UCB algorithm;opportunistic access;reinforcement learning;single-channel UCB algorithm;spectrum holes;upper confidence bound algorithm","","","","27","","20160208","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Clustering based feature selection using Extreme Learning Machines for text classification","R. K. Roul; S. Gugnani; S. M. Kalpeshbhai","Department of Computer Science, BITS-Pilani K.K. Birla Goa Campus, India - 403726","2015 Annual IEEE India Conference (INDICON)","20160331","2015","","","1","6","The expansion of the dynamic Web increases the digital documents, which has attracted many researchers to work in the field of text classification. It is an important and well studied area of machine learning with a variety of modern applications. A good feature selection is of paramount importance to increase the efficiency of the classifiers working on text data. Choosing the most relevant features out of what can be an incredibly large set of data, is particularly important for accurate text classification. This paper is a motivation in that direction where we propose a new clustering based feature selection technique that reduces the feature size. Traditional k-means clustering technique along with TF-IDF and Wordnet helps us to form a quality and reduced feature vector to train the Extreme Learning Machine (ELM) and Multi-layer ELM (ML-ELM) which have been used as the classifiers for text classification. The experimental work has been carried out on 20-Newsgroups and DMOZ datasets. Results on these two standard datasets demonstrate the efficiency of our approach using ELM and ML-ELM as the classifiers over the state-of-the-art classifiers.","","Electronic:978-1-4673-7399-9; POD:978-1-4673-7400-2","10.1109/INDICON.2015.7443788","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443788","Classification;ELM;K-Means;ML-ELM;SVM;Wordnet","Clustering algorithms;Computational modeling;Computer science;Frequency measurement;Neural networks;Support vector machines;Training","Internet;feature selection;learning (artificial intelligence);pattern clustering;text analysis;vectors","DMOZ datasets;ML-ELM;TF-IDF;Wordnet;clustering based feature selection;digital documents;dynamic Web;extreme learning machines;feature vector;k-means clustering technique;machine learning;multilayer ELM;text data classification","","","","16","","","17-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Learning Convex Piecewise Linear Machine for Data-Driven Optimal Control","Y. Zhou; B. Jin; C. J. Spanos","Dept. of EECS, UC Berkeley, Berkeley, CA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","966","972","In a data-driven Optimal Control (OP) scheme, one or more involved components, such as objective function, system dynamics, or operation constraints, are described with statistical models and learned from data. In this work, we focus on the machine learning of operation constraints which is rarely addressed in previous research. Although a rich collection of supervised learning methods exist in literature, most of them are not suitable for modeling operation constraints, because their decision rules usually induce undesirable non-linear couplings in system variables. In order to surpass simple linear models while at the same time maintaining compatibility with downstream control applications, we propose to describe system operation requirement by convex piecewise linear machine (CPLM), which does not incur any difficulties in optimization and is directly pluggable. The generalization performance of the proposed classifier is analyzed through bounding its VC-dimension, and a large margin cost sensitive learning objective is formulated with Bayes consistent hinge loss. We solve the training problem by online stochastic gradient descent and propose a mixed integer based initialization method. A case study on Heating, Ventilation and Air Conditioning (HVAC) systems control with comfort requirement is conducted and the results show that CPLM is not only a promising candidate for cost sensitive learning in general, but also enables much better description and exploitation of the system operation region for optimal control purpose.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424446","data-driven;learning convex function;optimal control;piecewise linear","Complexity theory;Optimal control;Optimization;Stochastic processes;System dynamics;Training;Upper bound","Bayes methods;convex programming;gradient methods;integer programming;learning (artificial intelligence);learning systems;optimal control;pattern classification;piecewise linear techniques;statistical analysis;stochastic processes","Bayes consistent hinge loss;CPLM;HVAC systems;VC-dimension;classifier;comfort requirement;convex piecewise linear machine;cost sensitive learning;data-driven optimal control;decision rules;heating ventilation and air conditioning systems;linear models;machine learning;mixed integer based initialization method;nonlinear couplings;objective function;online stochastic gradient descent;operation constraints;optimization;statistical models;supervised learning;system dynamics;system operation requirement;system variables","","","","15","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Conformal Prediction in Spark: Large-Scale Machine Learning with Confidence","M. Capuccini; L. Carlsson; U. Norinder; O. Spjuth","Dept. of Pharm. Biosci., Uppsala Univ., Uppsala, Sweden","2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC)","20160215","2015","","","61","67","Increasing size of datasets is challenging for machine learning, and Big Data frameworks, such as Apache Spark, have shown promise for facilitating model building on distributed resources. Conformal prediction is a mathematical framework that allows to assign valid confidence levels to object-specific predictions. This contrasts to current best-practices where the overall confidence level for predictions on unseen objects is estimated based on previous performance, assuming exchangeability. Here we report a Spark-based distributed implementation of conformal prediction, which introduces valid confidence estimation in predictive modeling for Big Data analytics. Experimental results on two large-scale datasets show the validity and the scalabilty of the method, which is freely available as open source.","","Electronic:978-0-7695-5696-3; POD:978-1-5090-0340-2","10.1109/BDC.2015.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406330","Big Data;Conformal Prediction;Machine Learning;Spark","Big data;Calibration;Computational modeling;Prediction algorithms;Predictive models;Sparks;Training","Big Data;data analysis;distributed processing;estimation theory;learning (artificial intelligence)","Apache Spark;Big Data analytics;confidence estimation;conformal prediction;distributed resource;machine learning;predictive modeling","","","","23","","","7-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Learning Concept Embeddings with Combined Human-Machine Expertise","M. J. Wilber; I. S. Kwak; D. Kriegman; S. Belongie","","2015 IEEE International Conference on Computer Vision (ICCV)","20160218","2015","","","981","989","This paper presents our work on ""SNaCK,"" a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.","","Electronic:978-1-4673-8391-2; POD:978-1-4673-8392-9","10.1109/ICCV.2015.118","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410475","","Birds;Computer vision;Crowdsourcing;Kernel;Labeling;Machine learning;Visualization","feature extraction;image classification;learning (artificial intelligence)","CUB dataset;Caltech UCSD Birds;MNIST;SNaCK embeddings;automatic machine similarity kernels;bird classifiers;deep-learned features;human insight;human-machine expertise;labeling mistakes discovery;low-dimensional concept embedding algorithm;nonprime numbers;object visual similarity;pictographic characters;supervised learning","","1","","42","","","7-13 Dec. 2015","","IEEE","IEEE Conference Publications"
"Spam filtering of bi-lingual tweets using machine learning","H. Afzal; K. Mehmood","National University of Sciences and Technology, Islamabad, Pakistan","2016 18th International Conference on Advanced Communication Technology (ICACT)","20160303","2016","","","710","714","During recent years, usage of social media has increased enormously. Billions of users use Twitter, Youtube etc which has resulted in the increase in spams as well. Spammers use spam accounts and target users on online social media. Whether a user accesses this social media through smart-phone or web, he/she is prone to the spammers on social media websites. This paper analyses different classification techniques that are currently being used in spam filtering in the context of social media. The contents of tweets are unique in nature, and are different from emails due to their less content so some techniques used in emails might be effective while some might not be effective. Moreover, the conversations on social media often comprises of short-forms/slangs and incorrect spellings. Usage of social media has also become popular in local/regional languages. One such language is Urdu which is common in subcontinent Indo-Pak and is written using English alphabets. We have performed spam classification for Roman Urdu tweets, collected from five major cities of Pakistan. Some of the most commonly used algorithms and techniques for spam classification are discussed and evaluated on English and Roman Urdu tweets from Pakistan in this paper.","","CD:978-8-9968-6507-0; Electronic:978-8-9968-6506-3","10.1109/ICACT.2016.7423530","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423530","Roman Urdu;machine learning techniques;spam filtering;tweets","Classification algorithms;Filtering;Man machine systems;Media;Support vector machines;Tagging;Twitter","Internet;information filtering;learning (artificial intelligence);natural language processing;pattern classification;smart phones;social networking (online);unsolicited e-mail","English alphabet;Roman Urdu tweet;Twitter;Urdu language;Worl Wide Web;Youtube;bilingual tweet;classification technique;local language;machine learning;online social media;regional language;smart-phone;social media Web site;spam account;spam classification;spam filtering","","1","","16","","","Jan. 31 2016-Feb. 3 2016","","IEEE","IEEE Conference Publications"
"Predicting hardware failure using machine learning","A. Chigurupati; R. Thibaux; N. Lassar","Google[x]","2016 Annual Reliability and Maintainability Symposium (RAMS)","20160407","2016","","","1","6","The Weibull distribution has historically been the Reliability Engineer's best tool for describing the probability of failures over time [1]. While this technique is very accurate at describing failure distributions for large populations of components, it works very poorly at predicting the time until failure of an individual component. The mean time until failure is often used to predict times until failure of individual components, but this value may vary greatly with actual times until failure. With the advent of machine learning techniques, the ability to learn from past behavior in order to predict future behavior makes it possible to predict an individual component's time until failure much more accurately. In this paper, we explore the predictive abilities of a machine learning technique to improve upon our ability to predict individual component times until failure in advance of actual failure. Once failure is predicted, an impending problem can be fixed before it actually occurs. This paper brings to light a machine learning approach for predicting individual component times until failure that we will show is far more accurate than the traditional MTBF approach. The algorithm built was able to monitor the health of 14 hardware samples and notify us of an impending failure well ahead of actual failure, providing adequate time to fix the problem before actual failure occurred.","","Electronic:978-1-5090-0249-8; POD:978-1-5090-0250-4","10.1109/RAMS.2016.7448033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448033","Data Loss;Failure Prediction;Machine Learning","Classification algorithms;Data models;Hardware;Machine learning algorithms;Prediction algorithms;Support vector machines;Training","Weibull distribution;condition monitoring;failure analysis;fault diagnosis;learning (artificial intelligence);performance evaluation","Weibull distribution;failure distribution;failure probability;hardware failure prediction;hardware samples;health monitoring;individual component time prediction;machine learning techniques","","1","","8","","","25-28 Jan. 2016","","IEEE","IEEE Conference Publications"
"Human learning and machine learning: Building bridges or integration?","S. Russ","Department of Computer Science, University of Warwick, UK","2016 8th International Conference on Knowledge and Smart Technology (KST)","20160324","2016","","","319","319","Summary form only given. At the core of Empirical Modelling is an activity we call `making construals'. A construal is a software artefact that embodies how we think about something, or make sense of something. For example, it might be a visualisation of a car engine with gears and controls that behaves - through interaction - like the physical car. We shall show a construal of MENACE : an early example of a simple machine (made with matchboxes) that learns to improve its own performance at playing noughts and crosses. Some experts in machine learning contrast the `big data' methods of training networks with the use of explanatory models. It is proving difficult, but desirable, to integrate these approaches. We'll suggest why Empirical Modelling might offer some useful insights into this problem.","","Electronic:978-1-4673-8139-0; POD:978-1-4673-8140-6","10.1109/KST.2016.7440538","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440538","","Biological system modeling;Buildings;Computational modeling;Computer science;History;Mathematical model","learning (artificial intelligence)","MENACE;construal;empirical modelling;human learning;machine learning;software artefact","","","","","","","3-6 Feb. 2016","","IEEE","IEEE Conference Publications"
"Applied Machine Learning to Identify Alzheimer's Disease through the Analysis of Magnetic Resonance Imaging","E. M. Novoa-Del-Toro; H. G. Acosta-Mesa; J. Fernández-Ruiz; N. Cruz-Ramírez","Univ. Veracruzana, Xalapa, Mexico","2015 International Conference on Computational Science and Computational Intelligence (CSCI)","20160303","2015","","","577","582","Alzheimer's disease is among the most common neurodegenerative diseases [1], doubling the number of patients every 5-year interval beyond age 65 [2]. Different investigations have proven that patients with Alzheimer's disease, show volume reduction at specific areas of the brain [1, 3-11]. Some of these areas, like the precuneus, start showing atrophy since early stages of the disease [1, 3, 6, 12-14], as measured through the use of Magnetic Resonance Imaging [9]. Considering this, we studied the possible use of the precuneus as a biomarker to identify such disease. Our results suggest that the precuneus is a potential biomarker to detect Alzheimer's disease, since 7 out of 10 patients (73.33% of accuracy) can be correctly classified.","","Electronic:978-1-4673-9795-7; POD:978-1-4673-9796-4; USB:978-1-4673-9794-0","10.1109/CSCI.2015.143","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424158","Alzheimer's disease;Magnetic Resonance Imaging;biomarker;classification;precuneus","Computational intelligence;Scientific computing","biomedical MRI;diseases;image classification;neurophysiology","Alzheimer disease;applied machine learning;atrophy;biomarker;magnetic resonance imaging analysis;neurodegenerative diseases;precuneus;volume reduction","","","","33","","","7-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning for inferring phase connectivity in distribution networks","S. Bandyopadhyay; R. Kota; R. Mitra; V. Arya; B. Sullivan; R. Mueller; H. Storey; G. Labut","IBM Research","2015 IEEE International Conference on Smart Grid Communications (SmartGridComm)","20160321","2015","","","91","96","The connectivity model of a power distribution network can easily become outdated due to system changes occurring in the field. Maintaining and sustaining an accurate connectivity model is a key challenge for distribution utilities worldwide. This work focuses on inferring customer to phase connectivity using machine learning techniques. Using voltage time series measurements collected from customer smart meters as the feature set for training classifiers, we study the performance of supervised, semi-supervised and unsupervised techniques. We report analysis and field validation results based on real smart meter measurements collected from three feeder circuits of a large distribution network in North America.","","Electronic:978-1-4673-8289-2; POD:978-1-4673-8290-8","10.1109/SmartGridComm.2015.7436282","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436282","","Manuals;Smart grids;Smart meters;Substations;Support vector machines;Training;Voltage measurement","distribution networks;inference mechanisms;learning (artificial intelligence);power engineering computing;smart meters;time measurement;time series;voltage measurement","North America;feeder circuit;inferring phase connectivity model;machine learning technique;power distribution network;semisupervised technique;smart meter measurement;supervised technique;unsupervised technique;voltage time series measurement","","1","","18","","","2-5 Nov. 2015","","IEEE","IEEE Conference Publications"
"Sentiment Classification Using Machine Learning Techniques with Syntax Features","H. Zou; X. Tang; B. Xie; B. Liu","Sch. of Software Eng., Shanghai Jiao Tong Univ., Shanghai, China","2015 International Conference on Computational Science and Computational Intelligence (CSCI)","20160303","2015","","","175","179","Sentiment classification has adopted machine learning techniques to improve its precision and efficiency. However, the features are always produced by basic words-bag methods without much consideration for words' syntactic properties, which could play an important role in the judgment of sentiment meanings. To remedy this, we firstly generate syntax trees of the sentences, with the analysis of syntactic features of the sentences. Then we introduce multiple sentiment features into the basic words-bag features. Such features were trained on movie reviews as data, with machine learning methods (Naive Bayes and support vector machines). The features and factors introduced by syntax tree were examined to generate a more accurate solution for sentiment classification.","","Electronic:978-1-4673-9795-7; POD:978-1-4673-9796-4; USB:978-1-4673-9794-0","10.1109/CSCI.2015.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424086","Machine learning;POS features;Sentiment classification;Syntax tree","Feature extraction;Hidden Markov models;Learning systems;Motion pictures;Software engineering;Support vector machines;Syntactics","Bayes methods;feature extraction;learning (artificial intelligence);pattern classification;reviews;sentiment analysis;support vector machines;trees (mathematics)","Naive Bayes methods;machine learning methods;machine learning techniques;movie reviews;sentence syntactic feature analysis;sentiment classification;sentiment meaning judgment;support vector machine methods;syntax features;syntax trees;word syntactic properties;words-bag methods","","","","16","","","7-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"The Role of Machine Learning in Finding Chimeric RNAs","S. Beaumeunier; J. Audoux; A. Boureux; T. Commes; N. Philippe; R. Alves","","2015 26th International Workshop on Database and Expert Systems Applications (DEXA)","20160215","2015","","","26","30","High-throughput sequencing technology and bioinformatics have identified chimeric RNAs (chRNAs), raising the possibility of chRNAs expressing particularly in diseases can be used as potential biomarkers in both diagnosis and prognosis. The task of discriminating true chRNA from the false ones poses an interesting Machine Learning (ML) challenge. First of all, the sequencing data may contain false reads due to technical artefacts and during the analysis process, bioinformatics tools may generate false positives due to methodological biases. Thus predicting the real signal from the noise can be a hard task. Furthermore, even if we succeed to have a proper set of observations (enough sequencing data) about true chRNAs, chances are that the devised model can not be able to generalize beyond it. Like any other machine learning problem, the first big issue is finding the good data, observations, to build the prediction model. Unfortunately, as far as we were concerned, there is no common benchmark data available for chRNAs. And, the definition of a classification baseline is lacking in the related literature. In this work we are moving towards a benchmark data and a fair comparison analysis unraveling the role of ML techniques in finding chRNAs. We have developed a benchmark pipeline incorporating a mutated genome process and simulated RNA-seq data by Flux Simulator. These sequencing reads were aligned and annotated by CRAC. CRAC offers a new way to analyze the RNA-seq data by integrating genomic location and local coverage, allowing biological predictions in one step. The resulting data were used as a benchmark for our comparison analysis. We have observed that the no free lunch theorem do not hold for ensemble classifiers. Ensemble learning strategies demonstrated to be more robust to this classification problem, providing an average AUC performance of 95% (ACC=94%, Kappa=0.87%).","1529-4188;15294188","Electronic:978-1-4673-7582-5; POD:978-1-4673-7583-2","10.1109/DEXA.2015.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406264","Chimeric RNAs;Classification;Ensemble Learning;High-throughput Sequencing;RNA-seq Data Analysis","Benchmark testing;Bioinformatics;Buildings;Data models;Genomics;RNA;Sequential analysis","RNA;bioinformatics;genomics;learning (artificial intelligence)","benchmark pipeline;bioinformatics;biomarkers;chRNA;chimeric RNA;data sequencing;flux simulator;machine learning;mutated genome process;sequencing technology","","","","14","","","1-4 Sept. 2015","","IEEE","IEEE Conference Publications"
"Structured Machine Learning for Data Analytics and Modeling: Intelligent Security as an Example","Y. J. Hu; W. Y. Liu; W. N. Wu","Dept. of Comput. Sci., NCCU, Taipei, Taiwan","2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)","20160204","2015","1","","325","332","Structured machine learning refers to learning a structured hypothesis from data with rich internal structure. We apply semantics-enabled (semi-)supervised learning for perfect and imperfect domain knowledge to fulfill the vision of structured machine learning for big data analytics and modeling. First, domain knowledge is modeled as RDF(S) ontologies, and SPARQL enables approximate queries for a type-labeled training dataset from ontologies to exploit a feature combination of a machine learning for hypothesis testing. Then, the existing type-labeled instances are used for classifying type-unlabeled new instances with the validation of testing dataset errors. Finally, these newly type-labeled instances are further forwarded to the structured ontologies to empower the ontology and rule learning. The proposed concepts have been tested and verified for intelligent security with the real KDD CUP 1999 datasets.","","Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6","10.1109/WI-IAT.2015.190","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396825","SPARQL;Structured machine learning;big data analytics;inductive logic programming (ILP);intelligent security;ontology learning;rule learning RDF(S);semantics-enabled (semi-)supervised learning","Analytical models;Big data;Data models;Intrusion detection;Machine learning algorithms;Ontologies","Big Data;data analysis;knowledge based systems;learning (artificial intelligence);ontologies (artificial intelligence);query processing;security of data","KDD CUP 1999 datasets;RDFS ontologies;SPARQL;big data analytics;big data modeling;dataset error testing;hypothesis testing;intelligent security;rule learning;semantics-enabled semisupervised learning;structured hypothesis;structured machine learning;type-labeled instances;type-labeled training dataset","","","","23","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Spam filtering of bi-lingual tweets using machine learning","H. Afzal; K. Mehmood","National University of Sciences and Technology, Islamabad, Pakistan","2016 18th International Conference on Advanced Communication Technology (ICACT)","20160303","2016","","","1","1","During recent years, usage of social media has increased enormously. Billions of users use Twitter, Youtube etc which has resulted in the increase in spams as well. Spammers use spam accounts and target users on online social media. Whether a user accesses this social media through smart-phone or web, he/she is prone to the spammers on social media websites. This paper analyses different classification techniques that are currently being used in spam filtering in the context of social media. The contents of tweets are unique in nature, and are different from emails due to their less content so some techniques used in emails might be effective while some might not be effective. Moreover, the conversations on social media often comprises of short-forms/slangs and incorrect spellings. Usage of social media has also become popular in local/regional languages. One such language is Urdu which is common in subcontinent Indo-Pak and is written using English alphabets. We have performed spam classification for Roman Urdu tweets, collected from five major cities of Pakistan. Some of the most commonly used algorithms and techniques for spam classification are discussed and evaluated on English and Roman Urdu tweets from Pakistan in this paper.","","CD:978-8-9968-6507-0; Electronic:978-8-9968-6506-3","10.1109/ICACT.2016.7423529","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423529","Roman Urdu;machine learning techniques;spam filtering;tweets","","","","","","","","","","Jan. 31 2016-Feb. 3 2016","","IEEE","IEEE Conference Publications"
"A Review of Machine Learning Solutions to Denial-of-Services Attacks in Wireless Sensor Networks","S. Gunduz; B. Arslan; M. Demirci","Dept. of Comput. Eng., Gazi Univ., Ankara, Turkey","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","150","155","Wireless sensor networks (WSNs) are used in various fields where remote data collection is necessary, such as environment and habitat monitoring, military applications, smart homes, traffic control, and health monitoring etc. Since WSNs play a crucial role in various domains and the sensors are constrained by resources, they are vulnerable to different types of attacks. One of the main attack types that threaten WSNs is Denial-of-Service (DoS) attacks. DoS attacks can be carried out at various layers of the network architecture. In this paper, we review the DoS attacks at each layer of TCP/IP protocol stack. Among them we focus on the network layer attacks because they are more diverse than other layer attacks. We review a number of studies proposing machine learning solutions pertaining to network layer DoS attacks in WSNs. We also provide some comparative conclusions to aid researchers studying in this field.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424301","Denial-of-Service (DoS);Machine Learning;Network Layer;TCP/IP Model;Wireless Sensor Networks (WSNs)","Base stations;Computer crime;Monitoring;Routing;Sensors;Support vector machines;Wireless sensor networks","computer network security;learning (artificial intelligence);transport protocols;wireless sensor networks","TCP/IP protocol stack;denial-of-service attacks;machine learning solutions;network layer DoS attacks;remote data collection;wireless sensor networks","","","","48","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Enabling antenna design with nano-magnetic materials using machine learning","C. Gianfagna; M. Swaminathan; P. M. Raj; R. Tummala; G. Antonini","Interconnect and Packaging Center, School of Electrical and Computer Engineering, Georgia Tech, USA","2015 IEEE Nanotechnology Materials and Devices Conference (NMDC)","20160324","2015","","","1","5","A machine learning approach to design with magneto dielectric nano-composite (MDNC) substrate for planar inverted-F antenna (PIFA) is presented. A new mixing rule model has been developed. A database of material properties has been created using several particle radius and volume fraction. A second database built with antenna simulations has been developed to complete the machine learning dataset. It is shown that, starting from particle radius and volume fraction of the nano-magnetic material, it is possible to calculate the antenna parameters like gain, bandwidth, radiation efficiency, resonant frequency, and viceversa with good precision by using machine learning techniques.","","Electronic:978-1-4673-9362-1; POD:978-1-4673-9363-8","10.1109/NMDC.2015.7439256","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439256","Antenna;machine learning;magneto-dielectric nanomaterial","Antennas;Databases;Magnetic resonance;Mathematical model;Permeability;Permittivity","antenna radiation patterns;electrical engineering computing;learning (artificial intelligence);nanocomposites;nanomagnetics;planar inverted-F antennas","antenna bandwidth;antenna gain;antenna parameters;machine learning dataset;magnetodielectric nanocomposite substrate;mixing rule model;nanomagnetic materials;particle radius;planar inverted-F antenna design;radiation efficiency;resonant frequency;volume fraction","","","","12","","","13-16 Sept. 2015","","IEEE","IEEE Conference Publications"
"Studying combined breast cancer biomarkers using machine learning techniques","D. T. Saleh; A. Attia; O. Shaker","Faculty of Engineering, Cairo University, Egypt","2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI)","20160303","2016","","","247","251","The spread of breast cancer and its high fatality has spurred a lot of research for studying its causes and treatments. Since the discovery of gene extraction methods, many biomarkers have been investigated and related to cancer. The large number of genes and their intertwining relations necessitates advanced machine learning models, rather than simple statistical and correlation analysis. Having the goal to advance the current state of knowledge concerning early diagnosis of breast cancer, we used decision trees, random forest, K-nearest neighbor, SVM, and Gaussian process classifiers, combined with testing different and novel biomarkers. The study showed that the LAPTM4B expression level is more indicative than its counter alleles. Moreover, the combination of biomarkers and machine learning led to enhancement in accuracy over single marker with at least 10%. By measuring the markers' importance, we found that LAPTM4B and OPG combined with age has shown a significant increase in the diagnosis accuracy.","","Electronic:978-1-4673-8740-8; POD:978-1-4673-8741-5; USB:978-1-4673-8739-2","10.1109/SAMI.2016.7423015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423015","Biomarkers;Breast Cancer;Decision Trees;Gaussian Process;K nearest neighbor;LAPTM4B;OPG;RANKL;Random Forest;SVM;YKL-40","Biomarkers;Breast cancer;Decision trees;Proteins;Support vector machines;Tumors","Gaussian processes;cancer;decision trees;genetics;learning (artificial intelligence);medical diagnostic computing;patient diagnosis;pattern classification;proteins;random processes;statistical analysis;support vector machines","Gaussian process classifier;LAPTM4B expression level;OPG;SVM;breast cancer biomarker;breast cancer diagnosis;correlation analysis;decision tree;diagnosis accuracy;gene extraction method;intertwining relation;k-nearest neighbor;machine learning model;machine learning technique;random forest;statistical analysis","","","","27","","","21-23 Jan. 2016","","IEEE","IEEE Conference Publications"
"A coefficient comparison of weighted similarity extreme learning machine for drug screening","W. Kudisthalert; K. Pasupa","Faculty of Information Technology, King Mongkuts Institute of Technology Ladkrabang, Bangkok, Thailand","2016 8th International Conference on Knowledge and Smart Technology (KST)","20160324","2016","","","43","48","Machine learning techniques are becoming popular in drug discovery process. It can be used to predict the biological activities of compounds. This paper focuses on virtual screening task. We proposed the Weighted Similarity Extreme Learning Machine algorithm (WELM). It is based on Single Layer Feedforward Neural Network. The algorithm is powerful, iteratively free, and easy to program. In this work, we compared the performance of 17 different types of coefficients with WELM on a well-known dataset in the area of virtual screening named Maximum Unbiased Validation dataset. Moreover, the WELM with different types of coefficients were also compared with the conventional technique-similarity searching. WELM together with Jaccard/Tanimoto were able to achieve the best results on average in most of the activity classes.","","Electronic:978-1-4673-8139-0; POD:978-1-4673-8140-6","10.1109/KST.2016.7440525","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440525","Chemoinformatics;Extreme Learning Machine;Similarity Coefficient;Virtual Screening","Biological information theory;Compounds;Databases;Drugs;Fingerprint recognition;Support vector machines;Training","bioinformatics;drugs;feedforward neural nets;learning (artificial intelligence)","WELM;coefficient comparison;drug discovery process;drug screening;maximum unbiased validation dataset;single layer feedforward neural network;virtual screening task;weighted similarity extreme learning machine","","1","","8","","","3-6 Feb. 2016","","IEEE","IEEE Conference Publications"
"A Generic Platform to Automate Legal Knowledge Work Process Using Machine Learning","K. M. Annervaz; J. George; S. Sengupta","","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","396","401","Management of legal contracts in various business domains such as Real Estate are examples of typical business process outsourcing activity. One of such process is Lease Abstraction, where largely manual inspection and validation of large commercial lease documents made for real estate deals is done by offshore experts and relevant information from the documents is extracted into a structured form. This structured information is further used for aggregate analytics and decision making by large real estate firms. We propose a system based on machine learning techniques to semi automate this process, essentially leading to 50% human effort savings. Our approach weaves together state-of-the-art machine learning techniques like supervised classifier models, sequence modeling techniques and various semi-supervised approaches. We articulate the effectiveness of our solution using the results from the experiments. Our platform is being used in production environment by Accenture Operations and the initial results and user feedback are encouraging.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424344","Information Extraction;Knowledge Work Automation;Machine Learning;Natural Language Processing;Process Automation;Supervised and Semi Supervised Learning","Data mining;Data models;Law;Manuals;Support vector machines;Training","contracts;decision making;document handling;law administration;learning (artificial intelligence);leasing;outsourcing","Accenture Operations;business process outsourcing activity;commercial lease document validation;decision making;lease abstraction;legal contract management;legal knowledge work process automation;machine learning;real estate;semisupervised approaches;sequence modeling techniques;supervised classifier models","","","","21","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning based parametric image estimation for Analyzer-based phase contrast imaging","O. Caudevilla; J. G. Brankov","Department of Electrical Engineering, Illinois Institute of Technology, Chicago, 60616 USA","2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","20160314","2014","","","1","4","An X-ray beam passing through biological tissue is deflected (i.e., refracted) by a small angle typically <;10 μrad. Analyzer-based phase contrast imaging (ABI) systems are capable of measuring this tinny refraction by sampling the intensity of the beam at different propagation directions. An Analyzer crystal is the key element for this task as it acts as a narrow angular filter. Since refraction effects are highly dependent of the radiation wavelength, X-ray beam must be quasi-monochromatic. Therefore the amount of photons that reach the object and detector is much lower then that in traditional radiography. Using a reasonable exposure time, noisy reconstructions of refraction images are obtained. In this manuscript, we present a machine learning parametric image estimation approach to obtain accurate refraction images from noisy raw data.","","Electronic:978-1-4799-6097-2; POD:978-1-4799-6098-9","10.1109/NSSMIC.2014.7430958","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430958","Analyzer-Based Imaging;Diffraction-Enhanced Imaging;Gaussian Process;Image Reconstruction;Machine Learning;Multiple-Image Radiography;Phase Sensitive Imaging","Estimation;Filtering;Gaussian processes;Image reconstruction;Radiography;X-ray imaging","diagnostic radiography;image reconstruction;learning (artificial intelligence);medical image processing","ABI systems;X-ray beam;analyzer crystal;analyzer-based phase contrast imaging;biological tissue;machine learning based parametric image estimation;narrow angular filter;noisy raw data;noisy reconstructions;quasimonochromatic;radiation wavelength;refraction effects;traditional radiography","","","","10","","","8-15 Nov. 2014","","IEEE","IEEE Conference Publications"
"A hybrid ensemble of machine and statistical learning using confidence-based boosting","N. Chairatanasongporn; S. Jaiyen","Department of Computer Science, Faculty of Science, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand","2015 7th International Conference on Information Technology and Electrical Engineering (ICITEE)","20160218","2015","","","41","45","Nowadays, the classification problems have become more challenging due to the various types of data set. Some data are appropriated for machine learning techniques and some data are appropriated for statistical leaning techniques. This work proposes a new hybrid ensemble of machine and statistical learning models using confidence-based boosting. The proposed method which uses variants of based classifiers can solve classification problems in variant data set. Moreover, combining the confidence value to the current boosting method can improve the performance of classification. The performance of proposed method is compared to the ensemble of decision trees and MRN created by Adaboost.M1 on data sets from UCI. The experimental results show that the proposed method can improve the accuracy in both binary and multiclass classification problems.","","Electronic:978-1-4673-7863-5; POD:978-1-4673-7864-2; USB:978-1-4673-7862-8","10.1109/ICITEED.2015.7408909","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408909","Adaboost;Decision Tree;Discriminant;Machine learning;Multilayer Perceptron Neural Network","Algorithm design and analysis;Boosting;Classification algorithms;Decision trees;Information technology;Statistical learning;Training","decision trees;learning (artificial intelligence);pattern classification","Adaboost.M1;UCI;boosting method;classifiers;confidence-based boosting;data sets;decision trees;hybrid ensemble;machine learning techniques;multiclass classification problems;statistical leaning techniques;statistical learning models;variant data set","","","","17","","","29-30 Oct. 2015","","IEEE","IEEE Conference Publications"
"MLM-rank: A Ranking Algorithm Based on the Minimal Learning Machine","A. S. C. Alencar; W. L. Caldas; J. P. P. Gomes; A. H. d. Souza; P. A. C. Aguilar; C. Rodrigues; W. Franco; M. F. d. Castro; R. M. C. Andrade","Comput. Sci. Dept., Fed. Univ. of Ceara, Fortaleza, Brazil","2015 Brazilian Conference on Intelligent Systems (BRACIS)","20160303","2015","","","305","309","Ranking is an important task in information retrieval and has gained much attention in recent years. Among the most used strategies, machine learning has achieved important results. The current work proposes a new machine learning based ranking algorithm, the MLM-RANK. MLM-RANK is based on the recently proposed Minimal Learning Machine (MLM). MLM is a supervised learning method that requires the adjustment of a single hyper parameter. The proposed method was evaluated against Prank and ELM Rank, both state of the art point wise ranking methods. In these tests MLM-RANK achieved promising results.","","Electronic:978-1-5090-0016-6; POD:978-1-5090-0017-3","10.1109/BRACIS.2015.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424037","minimal learning machine;ranking;regession","Cost function;Estimation;Mathematical model;Supervised learning;Support vector machines;Training;Training data","information retrieval;learning (artificial intelligence)","ELM Rank algorithm;MLM-Rank algorithm;Prank algorithm;information retrieval;minimal learning machine;pointwise ranking methods;ranking algorithm;supervised learning method","","","","15","","","4-7 Nov. 2015","","IEEE","IEEE Conference Publications"
"Mixed-signal circuits for embedded machine-learning applications","B. Murmann; D. Bankman; E. Chai; D. Miyashita; L. Yang","Stanford University, Stanford, CA, USA","2015 49th Asilomar Conference on Signals, Systems and Computers","20160229","2015","","","1341","1345","Machine learning algorithms are attractive solutions for a number of problems in data analytics and sensor signal classification. However, to enable the deployment of such algorithms in embedded hardware, significant progress must be made to reduce the large power dissipation of current GPU and FPGA-based implementations. Our work studies the trade-off between energy and accuracy in neural networks, and looks to incorporate mixed-signal design techniques to achieve low power dissipation in a semi-programmable ASIC implementation.","","CD-ROM:978-1-4673-8574-9; Electronic:978-1-4673-8576-3; POD:978-1-4673-8577-0","10.1109/ACSSC.2015.7421361","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421361","","Signal to noise ratio","electronic engineering computing;field programmable gate arrays;graphics processing units;learning (artificial intelligence);mixed analogue-digital integrated circuits;neural nets","FPGA-based implementation;GPU-based implementation;data analytics;embedded hardware;embedded machine-learning applications;machine learning algorithms;mixed-signal circuits;mixed-signal design;neural networks;power dissipation;semiprogrammable ASIC;sensor signal classification","","1","","30","","","8-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"Identification of respiratory phases using seismocardiogram: A machine learning approach","V. Zakeri; K. Tavakolian","Heart Force Medical Inc., Vancouver, BC, Canada","2015 Computing in Cardiology Conference (CinC)","20160218","2015","","","305","308","This study was aimed at developing an algorithm that could identify the respiratory phases, i.e. inspiration (I) or expiration (E), by analysing seismocardiogram (SCG) cycles. In order to better assess SCG cycles, it is needed to discriminate the cycles based on their position in the respiratory phases. The total 2146 SCG cycles obtained from 45 subjects were studied, in which 1109 cycles were in phase I, and the rest in phase E. Support vector machine (SVM), a powerful machine learning algorithm, was employed to identify the respiratory phase of SCG cycles. The systolic interval of each SCG cycle was divided to 32 equal bins, and the averages of these bins obtained the feature vector associated with each cycle. The SVM model was trained using half the data, and then was tested on the other half. The developed model could correctly identify 88% of the testing data. The obtained results are promising and can establish a solid ground for further analysis.","2325-8861;23258861","Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1","10.1109/CIC.2015.7408647","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408647","","Algorithm design and analysis;Feature extraction;Heart;Machine learning algorithms;Support vector machines;Testing;Training","biomedical measurement;cardiology;haemodynamics;learning (artificial intelligence);medical computing;support vector machines","SCG cycles;SVM model;feature vector;machine learning approach;respiratory phases;seismocardiogram cycles;support vector machine","","1","","11","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"Respiratory Artefact Removal in Forced Oscillation Measurements: A Machine Learning Approach","T. T. Pham; C. Thamrin; P. D. Robinson; A. McEwan; P. H. W. Leong","Thuy T. Pham is with the Dept. of Electrical and Information Engineering, The University of Sydney, NSW, Australia. (e-mail:: thuy.pham@sydney.edu.au).","IEEE Transactions on Biomedical Engineering","","2016","PP","99","1","1","Goal: Respiratory artefact removal for the forced oscillation technique can be treated as an anomaly detection problem. Manual removal is currently considered the gold standard but this approach is laborious and subjective. Most existing automated techniques used simple statistics and/or rejected anomalous data points. Unfortunately, simple statistics are insensitive to numerous artefacts, leading to low reproducibility of results. Furthermore, rejecting anomalous data points causes an imbalance between the inspiratory and expiratory contributions. Methods: From a machine learning perspective, such methods are unsupervised and can be considered simple feature extraction. We hypothesize that supervised techniques can be used to find improved features that are more discriminative and more highly correlated with the desired output. Features thus found are then used for anomaly detection by applying quartile thresholding which rejects complete breaths if one of its features is out of range. The thresholds are determined by both saliency and performance metrics rather than qualitative assumptions as in previous works. Results: Feature ranking indicates that our new landmark features are among the highest scoring candidates regardless of age across saliency criteria. F1-scores, receiver operating characteristic, and variability of the mean resistance metrics show that the proposed scheme outperforms previous simple feature extraction approaches. Our subject-independent detector, 1IQR-SU, demonstrated approval rates of 80:6% for adults and 98% for children, higher than existing methods. Conclusion: Our new features are more relevant. Our removal is objective and comparable to the manual method. Significance: This is a critical work to automate FOT quality control.","0018-9294;00189294","","10.1109/TBME.2016.2554599","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453188","Respiratory artefacts;lung;machine learning","Australia;Feature extraction;Lungs;Measurement;Oscillators;Pediatrics;Standards","","","","","","","","20160415","","","IEEE","IEEE Early Access Articles"
"Smartwatch-based activity recognition: A machine learning approach","G. M. Weiss; J. L. Timko; C. M. Gallagher; K. Yoneda; A. J. Schreiber","Department of Computer and Information Science, Fordham University, Bronx, NY 10458 USA","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","426","429","Smartwatches and smartphones contain accelerometers and gyroscopes that sense a user's movements, and can help identify the activity a user is performing. Research into smartphone-based activity recognition has exploded over the past few years, but research into smartwatch-based activity recognition is still in its infancy. In this paper we compare smartwatch and smartphone-based activity recognition, and smartwatches are shown to be capable of identifying specialized hand-based activities, such as eating activities, which cannot be effectively recognized using a smartphone (e.g., smartwatches can identify the ""drinking"" activity with 93.3% accuracy while smartphones achieve an accuracy of only 77.3%). Smartwatch-based activity recognition can form the basis of new biomedical and health applications, including applications that automatically track a user's eating habits.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455925","","Accelerometers;Data models;Gyroscopes;Radio frequency;Sensors;Smart phones;Training data","accelerometers;biomechanics;biomedical engineering;gyroscopes;learning (artificial intelligence);smart phones;watches","accelerometer;biomedical application;drinking activity;eating activity;gyroscope;hand-based activities;health application;machine learning approach;smartphone-based activity recognition;smartwatch-based activity recognition;user eating habit;user movement","","3","","14","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques for robust classification of partial discharges in oil–paper insulation systems","W. L. Woon; A. El-Hag; M. Harbaji","Masdar Institute of Science and Technology, United Arab Emirates","IET Science, Measurement & Technology","20160421","2016","10","3","221","227","Ageing power systems infrastructure and concerns about climate change have increased interest in the next generation of grid infrastructure, known as the smart grid (SG). This study studies a particularly critical SG application: intelligent monitoring of power transformers for the early detection of insulation failure. Specifically, the focus is on the use of machine learning algorithms to distinguish between different types of partial discharges, which are closely correlated with insulation failure. Measurements made using acoustic emission sensors are used to train and test different classification algorithms. In an earlier study, high classification accuracies were achieved using training and test datasets collected under similar measurement conditions. However, under different conditions, classification accuracy was greatly reduced. Experiments using the latest classification techniques were performed, producing significant improvements in classification accuracy. A possible reason for these results could be a form of overfitting, and further experiments were conducted to test this hypothesis.","1751-8822;17518822","","10.1049/iet-smt.2015.0076","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456411","","","acoustic emission testing;learning (artificial intelligence);partial discharges;power transformer insulation;power transformer testing;smart power grids","acoustic emission sensors;classification algorithms;insulation failure;machine learning algorithms;oil-paper insulation systems;partial discharges;power transformers;smart grid","","","","","","","5 2016","","IET","IET Journals & Magazines"
"Ankle Rehabilitation System with Feedback from a Smartphone Wireless Gyroscope Platform and Machine Learning Classification","R. LeMoyne; T. Mastroianni; A. Hessel; K. Nishikawa","Dept. of Biol. Sci., Northern Arizona Univ., Flagstaff, AZ, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","406","409","With the prevalence of traumatic brain injury and associated motor function impairment, an advance in the capacity to measure the efficacy of a rehabilitation strategy is a topic of considerable interest. For example, the development of a rehabilitation system that can quantify the efficacy to an ankle dorsiflexion therapy prescription would be beneficial. An ankle rehabilitation system is presented that amalgamates multiple technologies, such as a smartphone (iPhone) wireless gyroscope platform, machine learning, and 3D printing. The ankle rehabilitation system is produced by mostly 3D printing. A smartphone wireless gyroscope platform records the ankle rehabilitation system's therapy usage with wireless transmission to the Internet as an email attachment. The gyroscope signal data is processed for machine learning. A support vector machine attains 97% classification between a hemiplegic affected ankle and unaffected ankle feature set while using the ankle rehabilitation system. The application can be readily applied to a homebound setting of the subject's convenience.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424346","3D Printing;Ankle Rehabilitation;Dorsiflexion;Gyroscope;Machine Learning;Smartphone;Support Vector Machine;Therapy;Therapy Quantification;Wireless Gyroscope;Wireless Sensor","Communication system security;Electronic mail;Gyroscopes;Medical treatment;Support vector machines;Wireless communication;Wireless sensor networks","gyroscopes;injuries;learning (artificial intelligence);medical signal processing;patient rehabilitation;signal classification;smart phones;support vector machines;telemedicine;three-dimensional printing;wireless sensor networks","3D printing;Internet;ankle dorsiflexion therapy prescription;ankle feature set;ankle rehabilitation system;email attachment;gyroscope signal data;iPhone;machine learning classification;motor function impairment;rehabilitation strategy;smartphone wireless gyroscope platform;support vector machine;traumatic brain injury;wireless transmission","","2","","20","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Hybrid Machine Learning Approaches: A Method to Improve Expected Output of Semi-structured Sequential Data","M. Abdelrahim; C. Merlosy; T. Wang","Dept. of Comput. Sci., California State Univ., Northridge, CA, USA","2016 IEEE Tenth International Conference on Semantic Computing (ICSC)","20160324","2016","","","342","345","This paper proposes an intuitive yet simple machine learning (ML) approach that consist of two generic algorithms augmenting one another to solve problems they are not designed to solve. Since most machine learning algorithms are designed for a particular dataset or task, combining multiple ML algorithms can greatly improve the overall result by either helping tune one another, generalize, or adapt to unknown tasks. In this paper, we attempt to augment the architecture of traditional Artificial Neural Network (ANN) with a state machine acting as a form of short term memory in addition to help divide the work amongst multiple modular ANNs through transitioning from state to state. The result is a larger non-stochastic network that is able to self adjust as it is fed input. We train and test the work on data that is outside either an Artificial Neural Network or a state-machine's normal capability with simplified music notation extracted from midi files. The extracted data are used to simulate inherently sequential data to test the principle. Finally, while we find many large improvements in the augmentation of the ANN's architecture, but discuss further approaches to the system to improve generalization for new data.","","Electronic:978-1-5090-0662-5; POD:978-1-5090-0663-2","10.1109/ICSC.2016.72","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7439360","Adaption;Artificial Neural Network;Machine Learning;Sequence-based features;State-machines","Algorithm design and analysis;Artificial neural networks;Complexity theory;Data mining;Feature extraction;Machine learning algorithms;Prediction algorithms","data mining;data structures;finite state machines;generalisation (artificial intelligence);learning (artificial intelligence);neural nets","ANN architecture augmentation;ML algorithms;ML approach;artificial neural network;data mining;generic algorithms;hybrid machine learning approach;machine learning algorithms;midi files;music notation extraction;nonstochastic network;semistructured sequential data;state-machine normal capability","","","","14","","","4-6 Feb. 2016","","IEEE","IEEE Conference Publications"
"From probabilistic computing approach to probabilistic rough set for solving problem related to uncertainty under machine learning","S. Paul; A. Mitra; K. G. Rajulu","Department of CSE, VITAM, Berhampur, Odisha, India","2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","20160321","2015","","","1","6","Box and Tiao suggested about the prior distribution, which according to them is hypothetically representing the knowledge about anonymous constraints prior to the availability of data. It acts as a productive role in Bayesian analysis. Further, allotments of such kind also represent former knowledge or relative ignorance [4]. The chance of occurrence or predictability is defined by the term Probability. During the availability of partial information related to the result, the calculation becomes more challenging. Even the partial results are also not available in some real world scenario. Several literatures are available in this direction. Pawlak's Rough sets, decision algorithms and Bayes Theorem is in the used to analyze the result in same direction. In our paper, we have extended our work where we have thoroughly studied and tried to create a relationship from probabilistic computing and Rough sets. We have further extended our study the importance of decision making by the concept of probabilistic rough set. Generally, the paper presents a kind of survey, where we intend to model a decision based system which can work efficiently under uncertainty.","","CD-ROM:978-1-4799-7847-2; Electronic:978-1-4799-7849-6; POD:978-1-4799-7850-2","10.1109/ICCIC.2015.7435789","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435789","Bayesian Analysis;Probabilistic Computing;Probabilistic Rough Set;Probability;Rough Sets","Bayes methods;Clustering algorithms;Computational modeling;Computers;Probabilistic logic;Rough sets;Uncertainty","Bayes methods;decision making;decision theory;learning (artificial intelligence);mathematics computing;rough set theory","Bayes theorem;Bayesian analysis;Pawlak rough sets;anonymous constraints;decision algorithms;decision based system;decision making;knowledge representing;machine learning;partial information availability;probabilistic computing approach;probabilistic rough set","","","","32","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"Identification of ovarian mass through ultrasound images using machine learning techniques","H. Pathak; V. Kulkarni","Dept of Computer Engineering, MAEER's MIT, Pune, India","2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)","20160317","2015","","","137","140","Today ovarian cancer is second most perilous cause of cancer deaths in women after breast cancer. In this work, we have developed system which acquires ultrasound images and using image processing and machine learning algorithms accurately classify benign and malignant tumors in ovarian cancer. This technique denoise image using wavelet transform, grey level texture features extracted using GLCM (grey level co-occurrence algorithm), extracted features will be trained through SVM (Support vector machine) and selected non-redundant features selected through Relief-F will be further train and test through SVM for output. Proposed technique was validated by 60 malignant and 60 benign images of patients. On evaluating classifier for 14-texture descriptors give 74% and relief-F gives 82% accuracy. After selecting 6 features from 14 features it will give accuracy 86% and relief-F gives 92% accuracy. Thus, the features are significant for result and preliminary results depict that the proposed technique can be reliable for ovarian tumor classification as this system is fully automated, advantageous and cost-effective too.","","CD-ROM:978-1-4673-6734-9; Electronic:978-1-4673-6735-6; POD:978-1-4673-6736-3","10.1109/ICRCICN.2015.7434224","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434224","Ultrasound images;classification;feature extraction;feature selection;pre-processing","Cancer;Classification algorithms;Entropy;Feature extraction;Ultrasonic imaging;Wavelet transforms","biomedical ultrasonics;cancer;feature extraction;image denoising;learning (artificial intelligence);medical image processing;support vector machines","SVM;breast cancer;feature extraction;grey level cooccurrence algorithm;grey level texture;image denoising;image processing;machine learning technique;ovarian cancer;ovarian mass identification;support vector machine;ultrasound images;wavelet transform","","","","19","","","20-22 Nov. 2015","","IEEE","IEEE Conference Publications"
"Hardware/software implementation of an on-line machine learning algorithm","C. Quintero; L. García; F. Lozano; M. Guerrero","","2010 First IEEE Latin American Symposium on Circuits and Systems (LASCAS)","20160218","2010","","","17","20","Online learning is a machine learning paradigm that is useful when data is not available all at once. In this paper we focus in real time applications for which data is being collected as the algorithm executes. The forgetron Algorithm [1] is an online learning algorithm that works under a limited memory constraint while guaranteeing a bound on the number of total mistakes. We have proposed a specific architecture for the forgetron algorithm using hardware/software based design in order to improve computation time in the training process. Experiments on real world data show the advantages of this implementation compared to an exclusive software implementation using a Xilinx Virtex II Pro FPGA with an embedded Power PC. These experiments validate the performance of the proposed architecture.","","Electronic:978-1-5090-2076-8; POD:978-1-5090-2077-5","10.1109/LASCAS.2010.7410129","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410129","Forgetron;Hardware/Software co-design;Kernel Functions;Online Learning","Algorithm design and analysis;Hardware;Kernel;Memory management;Prediction algorithms;Software algorithms","field programmable gate arrays;hardware-software codesign;learning (artificial intelligence);operating system kernels","Xilinx Virtex II Pro FPGA;computation time improvement;embedded Power PC;forgetron algorithm;hardware based design;kernel functions;limited memory constraint;online machine learning algorithm;software based design;training process","","","","7","","","24-26 Feb. 2010","","IEEE","IEEE Conference Publications"
"A Performance Evaluation of Machine Learning-Based Streaming Spam Tweets Detection","C. Chen; J. Zhang; Y. Xie; Y. Xiang; W. Zhou; M. M. Hassan; A. AlElaiwi; M. Alrubaian","School of Information Technology, Deakin University, Melbourne, Vic., Australia","IEEE Transactions on Computational Social Systems","20160224","2015","2","3","65","76","The popularity of Twitter attracts more and more spammers. Spammers send unwanted tweets to Twitter users to promote websites or services, which are harmful to normal users. In order to stop spammers, researchers have proposed a number of mechanisms. The focus of recent works is on the application of machine learning techniques into Twitter spam detection. However, tweets are retrieved in a streaming way, and Twitter provides the Streaming API for developers and researchers to access public tweets in real time. There lacks a performance evaluation of existing machine learning-based streaming spam detection methods. In this paper, we bridged the gap by carrying out a performance evaluation, which was from three different aspects of data, feature, and model. A big ground-truth of over 600 million public tweets was created by using a commercial URL-based security tool. For real-time spam detection, we further extracted 12 lightweight features for tweet representation. Spam detection was then transformed to a binary classification problem in the feature space and can be solved by conventional machine learning algorithms. We evaluated the impact of different factors to the spam detection performance, which included spam to nonspam ratio, feature discretization, training data size, data sampling, time-related data, and machine learning algorithms. The results show the streaming spam tweet detection is still a big challenge and a robust detection technique should take into account the three aspects of data, feature, and model.","2329-924X;2329924X","","10.1109/TCSS.2016.2516039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400989","","Application programming interfaces;Feature extraction;Machine learning algorithms;Performance evaluation;Real-time systems;Twitter;Uniform resource locators","data handling;feature extraction;learning (artificial intelligence);pattern classification;security of data;social networking (online);unsolicited e-mail","Twitter;binary classification problem;commercial URL-based security tool;data sampling;feature discretization;feature extraction;machine learning-based streaming spam detection methods;performance evaluation;real-time spam detection;spam-to-nonspam ratio;streaming spam tweet detection;time-related data;training data size;tweet representation","","2","","38","","20160208","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"Machine Learning Methods for Binary and Multiclass Classification of Melanoma Thickness From Dermoscopic Images","A. Sáez; J. Sánchez-Monedero; P. A. Gutiérrez; C. Hervás-Martínez","Signal Theory and Communications Department, University of Seville, Spain","IEEE Transactions on Medical Imaging","20160331","2016","35","4","1036","1045","Thickness of the melanoma is the most important factor associated with survival in patients with melanoma. It is most commonly reported as a measurement of depth given in millimeters (mm) and computed by means of pathological examination after a biopsy of the suspected lesion. In order to avoid the use of an invasive method in the estimation of the thickness of melanoma before surgery, we propose a computational image analysis system from dermoscopic images. The proposed feature extraction is based on the clinical findings that correlate certain characteristics present in dermoscopic images and tumor depth. Two supervised classification schemes are proposed: a binary classification in which melanomas are classified into thin or thick, and a three-class scheme (thin, intermediate, and thick). The performance of several nominal classification methods, including a recent interpretable method combining logistic regression with artificial neural networks (Logistic regression using Initial variables and Product Units, LIPU), is compared. For the three-class problem, a set of ordinal classification methods (considering ordering relation between the three classes) is included. For the binary case, LIPU outperforms all the other methods with an accuracy of 77.6%, while, for the second scheme, although LIPU reports the highest overall accuracy, the ordinal classification methods achieve a better balance between the performances of all classes.","0278-0062;02780062","","10.1109/TMI.2015.2506270","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348708","Dermoscopic image;machine learning;melanoma thickness;nominal classification;ordinal classification","Color;Feature extraction;Image color analysis;Lesions;Malignant tumors;Surgery","biomedical optical imaging;cancer;feature extraction;image classification;learning (artificial intelligence);medical image processing;regression analysis;skin;thickness measurement;tumours","LIPU reports;artificial neural networks;binary classification;biopsy;computational image analysis system;depth measurement;dermoscopic images;feature extraction;logistic regression;logistic regression-using-initial variables-and-product units;machine learning methods;melanoma thickness;multiclass classification;nominal classification methods;ordinal classification methods;pathological examination;supervised classification schemes;surgery;suspected lesion;three-class scheme;tumor depth","","","","51","","20151207","April 2016","","IEEE","IEEE Journals & Magazines"
"Using Machine Learning to Understand and Mitigate Model Form Uncertainty in Turbulence Models","J. Ling","Sandia Nat. Labs., Livermore, CA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","813","818","The question of how to accurately model turbulent flows is one of the most long-standing open problems in physics. Advances in high performance computing have enabled direct numerical simulations of increasingly complex flows. Nevertheless, for most flows of engineering relevance, the computational cost of these direct simulations is prohibitive, necessitating empirical model closures for the turbulent transport. These empirical models are prone to ""model form uncertainty"" when their underlying assumptions are violated. Understanding, quantifying, and mitigating this model form uncertainty has become a critical challenge in the turbulence modeling community. This paper will discuss strategies for using machine learning to understand the root causes of the model form error and to develop model corrections to mitigate this error. Rule extraction techniques are used to derive simple rules for when a critical model assumption is violated. The physical intuition gained from these simple rules is then used to construct a linear correction term for the turbulence model which shows improvement over naive linear fits.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424422","Machine learning;rule extraction;turbulence modeling","Analytical models;Anisotropic magnetoresistance;Computational modeling;Decision trees;Predictive models;Uncertainty;Vegetation","learning (artificial intelligence);mechanical engineering computing;parallel processing;turbulence","critical model assumption;direct numerical simulations;empirical model closures;high performance computing;linear correction term;machine learning;model form uncertainty mitigation;rule extraction technique;turbulent flow modelling;turbulent transport","","","","20","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Indonesian-Japanese term extraction from bilingual corpora using machine learning","M. Nassirudin; A. Purwarianti","Department of Informatics Engineering, Bandung Institute of Technology, Bandung, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20160225","2015","","","111","116","As bilateral relation between Indonesia and Japan strengthens, the need of consistent term usage for both languages becomes important. In this paper, a new method for Indonesian-Japanese term extraction is presented. In general, this is done in 3 steps: (1) n-gram extraction for each language, (2) n-gram cross-pairing between both languages, and (3) classification. This method is aimed to be able to handle term extraction from both parallel corpora and comparable corpora. In order to use this method, we have to build a classification model first using machine learning. There are 4 types of feature we take into consideration. They are dictionary based features, cognate based features, combined features, and statistic features. The first three features are linguistic features. Dictionary based features consider word-pair existence in a predefined dictionary, cognate based features consider morpheme level similarity, combined features consider both dictionary and cognate based features altogether, and statistic features is used in case the first 3 features fail. The only statistic feature we use is context heterogeneity similarity, which consider the variety of words that can precede or follow a term. For learning algorithm, we use SVM (Support Vector Machine). In the experiment, we compared several scenarios: only linguistic features, only statistic features, or both features combined. The classification model was built from parallel corpora since plenty of term pairs can be extracted from parallel corpora. The size of training data was 5,000 term pairs. The best result was achieved by using only linguistic features and without the preprocessing step. The accuracy was up to 90.98% and recall 92.14%. A testing from comparable corpora was also done with size of 37,392 term pairs where 94 were equivalent translation and 37,298 were not. Evaluation using test set gave accuracy of 98.63% precision, but with low recall score of 24.47%.","","Electronic:978-1-5090-0363-1; POD:978-1-5090-0364-8; USB:978-1-5090-0362-4","10.1109/ICACSIS.2015.7415180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415180","SVM;comparable;linguistic;machine learning;parallel;statistic;term extraction","Classification algorithms;Context;Dictionaries;Feature extraction;Measurement;Pragmatics;Support vector machines","learning (artificial intelligence);linguistics;natural language processing;pattern classification;support vector machines;text analysis;vocabulary","Indonesian-Japanese term extraction;SVM;bilateral relation;bilingual corpora;classification model;cognate based features;comparable corpora;context heterogeneity similarity;dictionary based features;learning algorithm;linguistic features;machine learning;morpheme level similarity;n-gram cross-pairing;n-gram extraction;parallel corpora;recall score;statistic features;support vector machine;term usage;word-pair existence","","","","9","","","10-11 Oct. 2015","","IEEE","IEEE Conference Publications"
"Dynamic gesture recognition with Wi-Fi based on signal processing and machine learning","G. Zhou; T. Jiang; Y. Liu; W. Liu","Key Laboratory of Universal Wireless Communication, Beijing University of Posts and Telecommunications","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20160225","2015","","","717","721","Wi-Fi signals have been typically acting as information carriers in modern communication system, but recent research has revealed their powerful capability in detecting and identifying various targets. With Wi-Fi, we can now ""see"" people's location, activity, and even hand gestures. In this paper, a new method of dynamic gesture recognition using Wi-Fi based on signal processing and machine learning is proposed. In our work, power profiles of received Wi-Fi signals are acquired for signal processing. The discrete wavelet transform (DWT) is applied to extract features and eliminate noise. And a support vector machine (SVM) improved by dynamic time warping (DTW) algorithm is built to classify and recognize different gestures. The experimental result shows that, by applying the method, nine predefined dynamic gestures can be effectively recognized, with an average recognition rate up to 94.8%, using only a small amount of training samples.","","Electronic:978-1-4799-7591-4; POD:978-1-4799-7592-1","10.1109/GlobalSIP.2015.7418290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418290","Wi-Fi;discrete wavelet transform (DWT);dynamic gesture recognition;dynamic time warping (DTW);machine learning;signal processing;support vector machine (SVM)","Conferences;Decision support systems;Discrete wavelet transforms;Gesture recognition;Information processing;Signal processing;Support vector machines","discrete wavelet transforms;gesture recognition;learning (artificial intelligence);signal processing;support vector machines;ubiquitous computing;wireless LAN","DTW;DWT;SVM;Wi-Fi signals;discrete wavelet transform;dynamic gesture recognition;dynamic time warping algorithm;hand gestures;information carriers;machine learning;modern communication system;signal processing;support vector machine","","","","10","","","14-16 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning-based signal processing using physiological signals for stress detection","A. Ghaderi; J. Frounchi; A. Farnam","Electrical and Computer Engineering Department, University of Tabriz, Tabriz, Iran","2015 22nd Iranian Conference on Biomedical Engineering (ICBME)","20160225","2015","","","93","98","Stress is a common part of daily life which most people struggle in different occasions. However, having stress for a long time, or a high level of stress will jeopardize our safety, and will disrupt our normal life. Consequently, performance and management ability in critical situations degrade significantly. Therefore, it is necessary to have information in stress cognition and design systems with the ability of stress cognition. In this paper a signal processing approach is introduced based on machine learning algorithms. We used collected biological data such as Respiration, GSR Hand, GSR Foot, Heart Rate and EMG, from different subjects in different situations and places, while they were driving. Then, data segmentation for various time intervals such 100, 200 and 300 seconds is performed for different stress level. We extracted statistical features from the segmented data, and feed this features to the available classifier. We used KNN, K-nearest neighbor, and support vector machine which are the most common classifiers. We classified the stress into three levels: low, medium, and high. Our results show that the stress level can be detected by accuracy of 98.41% for 100 seconds and 200 seconds time intervals and 99% for 300 seconds time intervals.","","Electronic:978-1-4673-9351-5; POD:978-1-4673-9352-2","10.1109/ICBME.2015.7404123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404123","EMG;GSR;HR;KNN;Machine Learning;Respiration;SVM;Signal Processing","Electromyography;Feature extraction;Physiology;Sensor phenomena and characterization;Stress;Support vector machines","electrocardiography;electromyography;learning (artificial intelligence);medical signal processing;signal classification;support vector machines","EMG;GSR foot;GSR hand;data segmentation;electromyography;galvanic skin response;heart rate;k-nearest neighbor;machine learning algorithm;machine learning-based signal processing;physiological signal;respiration;statistical feature;stress cognition;stress detection;stress level;support vector machine","","","","15","","","25-27 Nov. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning Techniques in Optical Communication","D. Zibar; M. Piels; R. Jones; C. G. Schäeffer","DTU&#160;Fotonik, Department of Photonics Engineering, Technical University of Denmark, Kgs., Lyngby, Denmark","Journal of Lightwave Technology","20160303","2016","34","6","1442","1452","Machine learning techniques relevant for nonlinearity mitigation, carrier recovery, and nanoscale device characterization are reviewed and employed. Markov Chain Monte Carlo in combination with Bayesian filtering is employed within the nonlinear state-space framework and demonstrated for parameter estimation. It is shown that the time-varying effects of cross-phase modulation (XPM) induced polarization scattering and phase noise can be formulated within the nonlinear state-space model (SSM). This allows for tracking and compensation of the XPM induced impairments by employing approximate stochastic filtering methods such as extended Kalman or particle filtering. The achievable gains are dependent on the autocorrelation (AC) function properties of the impairments under consideration which is strongly dependent on the transmissions scenario. The gain of the compensation method are therefore investigated by varying the parameters of the AC function describing XPM-induced polarization scattering and phase noise. It is shown that an increase in the nonlinear tolerance of more than 2 dB is achievable for 32 Gbaud QPSK and 16-quadratic-amplitude modulation (QAM). It is also reviewed how laser rate equations can be formulated within the nonlinear state-space framework which allows for tracking of nonLorentzian laser phase noise lineshapes. It is experimentally demonstrated for 28 Gbaud 16-QAM signals that if the laser phase noise shape strongly deviates from the Lorentzian, phase noise tracking algorithms employing rate equation-based SSM result in a significant performance improvement (>8 dB) compared to traditional approaches using digital phase-locked loop. Finally, Gaussian mixture model is reviewed and employed for nonlinear phase noise compensation and characterization of nanoscale devices structure variations.","0733-8724;07338724","","10.1109/JLT.2015.2508502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359099","Bayesian filtering;Monte Carlo methods;expectation maximization;expectation maximization (EM);machine learning;optical communication","Kalman filters;Mathematical model;Modulation;Nonlinear optics;Optical noise;Phase noise;State-space methods","Bayes methods;Kalman filters;Markov processes;Monte Carlo methods;laser noise;learning (artificial intelligence);nanophotonics;optical communication equipment;optical filters;optical phase locked loops;parameter estimation;phase modulation;phase noise;quadrature amplitude modulation;quadrature phase shift keying","16-quadratic amplitude modulation;Bayesian filtering;Gaussian mixture model;Markov chain Monte Carlo;QAM;QPSK;SSM;XPM;approximate stochastic filtering methods;autocorrelation function;carrier recovery;cross-phase modulation;digital phase-locked loop;extended Kalman filtering;laser rate equations;machine learning;nanoscale device characterization;nonLorentzian laser phase noise lineshapes;nonlinear phase noise compensation;nonlinear state-space framework;nonlinear state-space model;nonlinear tolerance;nonlinearity mitigation;optical communication;parameter estimation;particle filtering;phase noise tracking algorithms;polarization scattering;time-varying effects","","2","","25","","20151217","March15, 15 2016","","IEEE","IEEE Journals & Magazines"
"Machine learning-based metamodels for sawing simulation","M. Morin; F. Paradis; A. Rolland; J. Wery; F. Laviolette; F. Laviolette","FORAC Research Consortium / Department of Computer Science and Software Engineering, 1065, av. de la M&#233;decine, Universit&#233; Laval, Qu&#233;bec, QC, G1V 0A6, CANADA","2015 Winter Simulation Conference (WSC)","20160218","2015","","","2160","2171","We use machine learning to generate metamodels for sawing simulation. Simulation is widely used in the wood industry for decision making. These simulators are particular since their response for a given input is a structured object, i.e., a basket of lumbers. We demonstrate how we use simple machine learning algorithms (e.g., a tree) to obtain a good approximation of the simulator's response. The generated metamodels are guaranteed to output physically realistic baskets (i.e., there exists at least one log that can produce the basket). We also propose to use kernel ridge regression. While having the power to exploit the structure of a basket, it can predict previously unseen baskets. We finally evaluate the impact of possibly predicting unrealistic baskets using ridge regression jointly with a nearest neighbor approach in the output space. All metamodels are evaluated using standard machine learning metrics and novel metrics especially designed for the problem.","","CD-ROM:978-1-4673-9741-4; Electronic:978-1-4673-9743-8; POD:978-1-4673-9973-9","10.1109/WSC.2015.7408329","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408329","","","decision making;learning (artificial intelligence);production engineering computing;regression analysis;wood processing","decision making;kernel ridge regression;machine learning-based metamodels;nearest neighbor approach;sawing simulation;wood industry","","","","31","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Variants of heuristic rule generation from multiple patterns in Michigan-style fuzzy genetics-based machine learning","Y. Nojima; K. Watanabe; H. Ishibuchi","Department of Computer Science and Intelligent Systems, Graduate School of Engineering, Osaka Prefecture University, Sakai, 99-8531, Japan","2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)","20160215","2015","","","427","432","In the design of rule-based classifiers, a single rule is often generated from a single pattern in a heuristic manner. Since the generated rule is likely to be over-specialized to the pattern, its conditions are often randomly replaced with don't care. However, the generalized rule with don't care conditions does not always have high classification ability. This is because the replacement is randomly performed without utilizing any information about other patterns. In our previous studies, we proposed an idea of generating a fuzzy classification rule from multiple patterns. In this paper, we propose its six variants. Each variant has a different criterion for choosing multiple patterns from which a single rule is generated. The proposed variants are used to generate fuzzy classification rules in Michigan-style fuzzy genetics-based machine learning. The usefulness of each variant is evaluated as a heuristic fuzzy rule generation method through computational experiments on 20 benchmark data sets.","","Electronic:978-1-4673-9606-6; POD:978-1-4673-9607-3","10.1109/TAAI.2015.7407091","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407091","Heuristic rule generation;fuzzy classifier design;fuzzy genetics-based machine learning","Glass;Heart;Ionosphere;Sonar;Vehicles","fuzzy set theory;genetic algorithms;learning (artificial intelligence);pattern classification","Michigan-style fuzzy genetics-based machine learning;fuzzy classification rules;heuristic fuzzy rule generation method;rule-based classifiers","","","","17","","","20-22 Nov. 2015","","IEEE","IEEE Conference Publications"
"Robotic grasp detection using extreme learning machine","C. Sun; Y. Yu; H. Liu; J. Gu","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, Fujian, 350116, China","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20160225","2015","","","1115","1120","Object grasping using vision is one of the important functions of manipulators. Machine learning based methods have been proposed for grasp detection. However, due to the variety of grasps and 3D shapes of objects, how to effectively find the best grasp is still a challenging issue. Thus this paper presents an extreme learning machine (ELM) based method to cope with this issue. This proposed method consists of three successive modules, including candidate object detection, estimation of object's major orientations and grasp detection. In the first module, candidate object region is extracted based on depth information. In the second module, object's major orientations guide the directions for sliding windows. In the third module, a cascaded classifier is trained to identify the right grasp. ELM is used as the base classifier in the cascade. Histograms of oriented gradients (HOG) are used as features. Experimental results in benchmark dataset and real manipulators have shown that this proposed method outperforms other methods in terms of accuracy and computational efficiency.","","Electronic:978-1-4673-9675-2; USB:978-1-4673-9674-5","10.1109/ROBIO.2015.7418921","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418921","Extreme Learning Machine;Histograms of Oriented Gradients;Machine Learning;Robotic Grasping","Feature extraction;Grasping;Histograms;Object detection;Robots;Support vector machines;Training","feature extraction;grippers;image classification;image segmentation;learning (artificial intelligence);learning systems;manipulators;object detection;robot vision","ELM based method;HOG features;candidate object detection;candidate object region extraction;cascaded classifier training;depth information;extreme learning machine;grasp identification;histograms of oriented gradients;machine learning based method;manipulator;object 3D shape;object grasping;object major orientation estimation;robot vision;robotic grasp detection;sliding windows","","","","15","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Run-Time Machine Learning for HEVC/H.265 Fast Partitioning Decision","S. Momcilovic; N. Roma; L. Sousa; I. Milentijevic","INESC-ID, Univ. Lisboa, Lisbon, Portugal","2015 IEEE International Symposium on Multimedia (ISM)","20160328","2015","","","347","350","A novel fast Coding Tree Unit partitioning for HEVC/H.265 encoder is proposed in this paper. This method relies on run-time trained neural networks for fast Coding Units splitting decisions. Contrasting to state-of-the-art solutions, this method does not require any pre-training and provides a high adaptivity to the dynamic changes in video contents. By an efficient sampling strategy and a multi-thread implementation, the presented technique successfully mitigates the computational overhead inherent to the training process on both the overall processing performance and on the initial encoding delay. The experiments show that the proposed method successfully reduces the HEVC/H.265 encoding time for up to 65% with negligible rate-distortion penalties.","","Electronic:978-1-5090-0379-2; POD:978-1-5090-0380-8; USB:978-1-5090-0378-5","10.1109/ISM.2015.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442356","HEVC/H.265;machine learning;multi-threading;neural networks;video coding","Artificial neural networks;Encoding;Input variables;Instruction sets;Training;Video coding","neural nets;sampling methods;trees (mathematics);video coding","HEVC-H.265 fast partitioning decision;multi-thread implementation;novel fast coding tree unit partitioning;run-time machine learning;run-time trained neural networks;sampling strategy","","","","","","","14-16 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Machine-Learning Based Approach for Measuring the Completeness of Online Privacy Policies","N. Guntamukkala; R. Dara; G. Grewal","Sch. of Comput. Sci., Univ. of Guelph, Guelph, ON, Canada","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","289","294","Web site privacy policies are often long, difficult to understand, and contain incomplete information. Consequently, users tend not to read the privacy policies, thus putting their privacy at risk. This paper describes an automated approach for assisting users to evaluate online privacy policies based on completeness. The term completeness refers to the presence of 8 sections in an online privacy policy that have been recognized as helpful in establishing the transparency of a privacy policy. Given a new online privacy policy, the proposed system employs a machine-learning based approach to predict a completeness score for the privacy policy. This score can then be used by the user to assess the risk to their privacy.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.143","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424323","completeness;machine learning;privacy policy;text mining;transparency","Companies;Data privacy;Law;Privacy;Security;Web sites","Web sites;data mining;data privacy;learning (artificial intelligence);text analysis","Web site privacy policies;automated user assistance approach;completeness measurement;completeness score prediction;machine-learning based approach;online privacy policies;risk assessment;text mining","","","","17","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"MLaaS: Machine Learning as a Service","M. Ribeiro; K. Grolinger; M. A. M. Capretz","Electr. & Comput. Eng., Western Univ., London, ON, Canada","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","896","902","The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.152","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424435","Machine Learning as a Service;Platform as a Service;Prediction;Regression;Service Component Architecture;Service Oriented Architecture;Supervised Learning","Adaptation models;Computer architecture;Data models;Machine learning algorithms;Prediction algorithms;Predictive models;Training","cloud computing;knowledge acquisition;learning (artificial intelligence);load forecasting;power engineering computing","MLaaS;context-specific data;electricity demand forecast;global data sources;knowledge extraction;machine learning as a service;real-world sensor data;weather data","","","","12","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"On the feasibility of machine learning as a tool for automatic security classification: A position paper","A. Yazidi; H. Hammer; A. Bai; P. E. Engelstad","Oslo and Akershus University College of Applied Sciences (HiOA), Oslo, Norway","2016 International Conference on Computing, Networking and Communications (ICNC)","20160324","2016","","","1","6","With the proliferation of threats of leakage of sensitive information such as military classified documents, information guards have recently gained increased interest. An information guard is merely a filter than controls the content of the exchanged information between two domains where one of them has a higher confidentiality level than the other one. The main role of an information guard is to block leakage of the sensitive information from the higher confidentiality domain to the lower confidentiality domain. An example of a higher confidentiality domain is a military network while a subcontractor network is an example of a lower confidentiality domain. The common practice is to use an automatic information guard based on predefined list of words that is called ""dirty word list"" in order to decide the security level of a document and consequently release it to the lower confidentially domain or block it. Traditional information guards are configured manually based on the notion of ""Dirty Lists"". The classification logic of traditional information guards uses the occurrence of words from the ""Dirty Lists"". In this paper, we advocate the use of machine learning as a corner stone for building advanced information guards. Machine learning can also be used as a supplement to the decision obtained based on ""Dirty Lists"" classification. Machine learning has hardly been analysed for this problem, and the analysis on topical classification presented here provides new knowledge and a basis for further work within this area. Ten different machine learning algorithms were applied on real life data from a military context. Presented results are promising and demonstrates that machine learning can become a useful tool to assist humans in determining the appropriate security label of an information object.","","Electronic:978-1-4673-8579-4; USB:978-1-4673-8578-7","10.1109/ICCNC.2016.7440588","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440588","Security;classification;cross-domain information exchange;labelling;machine learning","Buildings;Context;Digital signatures;Information exchange;Labeling;Machine learning algorithms","information filters;learning (artificial intelligence);pattern classification;security of data","automatic security classification;dirty lists classification;filter;information guards;machine learning algorithms;sensitive information leakage blocking;topical classification","","","","20","","","15-18 Feb. 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach for exploring rock arts through the cloud infrastructure","R. S. Ponmagal; N. Srinivasan","Dept. of Computer Science and Engineering, Dr. M.G.R. Educational and Research Institute University, Chennai, India","2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","20160321","2015","","","1","6","This paper is aimed at proposing a machine learning approach to analyze and make sense out of the ancient rock arts by exploring them through cloud infrastructure. The visual language of the rock art is proposed to be interpreted and transformed into the current language of human cognition. The rock arts can be captured as 3D motion pictures; ultrasonically detected images; pictures captured using laser sensors and thermography techniques. Since the countries across the Globe are rich in culture and also diverse in nature, rock arts have been explored and keeping on exploring more in quantity, the rock arts information collected through the above said methods can be represented and processed using cloud infrastructure. Further, using machine learning algorithms in the cloud is proposed, to arrive at definite, meaningful information from rock arts. Through the machine learning approach, the symbols represented by rock arts could be matched with the twenty six English alphabets. The proposed work is the interpretation of the olden rock art scripts and hence to predict the meaning that they wish to convey.","","CD-ROM:978-1-4799-7847-2; Electronic:978-1-4799-7849-6; POD:978-1-4799-7850-2","10.1109/ICCIC.2015.7435682","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435682","cloud;interpretation;machine learning;rock art;symbol;visual language","Art;Cloud computing;Computer architecture;Machine learning algorithms;Rocks;Three-dimensional displays;Visualization","art;cloud computing;image capture;image classification;image matching;learning (artificial intelligence);motion estimation;rocks;ultrasonic imaging","3D motion pictures;English alphabets;ancient rock arts;cloud infrastructure;human cognition;laser sensors;machine learning approach;picture capture;rock art information collection;rock art scripts;symbol matching;thermography techniques;ultrasonic image detection;visual language","","","","17","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"Non-Intrusive Load Monitoring Using Semi-Supervised Machine Learning and Wavelet Design","J. M. Gillis; W. G. Morsi","Department of Electrical, Computer, and Software Engineering, Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), Oshawa, ON L1H7K4, Canada.","IEEE Transactions on Smart Grid","","2016","PP","99","1","8","This paper presents a new approach based on semi-supervised machine learning and wavelet design applied to non-intrusive load monitoring. Co-training of two machine learning classifiers is used to automate the process of learning the load pattern after designing new wavelets. The numerical results demonstrating the effectiveness of the proposed approach are discussed and conclusions are drawn.","1949-3053;19493053","","10.1109/TSG.2016.2532885","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430361","Monte Carlo methods;non-intrusive load monitoring;semi-supervised machine learning;wavelet design","Feature extraction;Home appliances;Monitoring;Multiresolution analysis;Training;Wavelet transforms","","","","","","","","20160310","","","IEEE","IEEE Early Access Articles"
"A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots","A. Giusti; J. Guzzi; D. C. Cireşan; F. L. He; J. P. Rodríguez; F. Fontana; M. Faessler; C. Forster; J. Schmidhuber; G. D. Caro; D. Scaramuzza; L. M. Gambardella","Dalle Molle Institute for Artificial Intelligence (IDSIA), USI-SUPSI, Lugano, Switzerland","IEEE Robotics and Automation Letters","20160308","2016","1","2","661","667","We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.","2377-3766;23773766","","10.1109/LRA.2015.2509024","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358076","Aerial Robotics;Deep Learning;Machine Learning;Visual-Based Navigation","Cameras;Image segmentation;Mobile robots;Roads;Robot vision systems;Visual perception","autonomous aerial vehicles;helicopters;image classification;learning (artificial intelligence);microrobots;neural nets;robot vision","deep-neural network;forest trails;machine learning approach;mobile robots;monocular image;quadrotor microaerial vehicle control;qualitative analysis;quantitative analysis;supervised image classifier;viewing direction;visual perception","","2","","35","","20151217","July 2016","","IEEE","IEEE Journals & Magazines"
"A Machine Learning Approach to Meter Placement for Power Quality Estimation in Smart Grid","S. Ali; K. Wu; K. Weston; D. Marinakis","Department of Computer Science, University of Victoria, Victoria, BC, Canada","IEEE Transactions on Smart Grid","20160421","2016","7","3","1552","1561","Due to the high-measuring cost, the monitoring of power quality (PQ) is nontrivial. This paper is aimed at reducing the cost of PQ monitoring in power network. Using a real-world PQ dataset, this paper adopts a learn-from-data approach to obtain a device latent feature model, which captures the device behavior as a PQ transition function. With the latent feature model, the power network could be modeled, in analogy, as a data-driven network, which presents the opportunity to use the well-investigated network monitoring and data estimation algorithms to solve the network quality monitoring problem in power grid. Based on this network model, algorithms are proposed to intelligently place measurement devices on suitable power links to reduce the uncertainty of PQ estimation on unmonitored power links. The meter placement algorithms use entropy-based measurements and Bayesian network models to identify the most suitable power links for PQ meter placement. Evaluation results on various simulated networks including IEEE distribution test feeder system show that the meter placement solution is efficient, and has the potential to significantly reduce the uncertainty of PQ values on unmonitored power links.","1949-3053;19493053","","10.1109/TSG.2015.2442837","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152975","Bayesian networks (BNs);Monte Carlo (MC) simulations;conditional entropy (CE);power quality (PQ) monitoring","Estimation;Monitoring;Phasor measurement units;Power grids;Power quality;Reliability;Voltage fluctuations","belief networks;learning (artificial intelligence);power engineering computing;power supply quality;power system measurement;smart power grids","Bayesian network models;data estimation algorithm;device latent feature model;entropy based measurements;machine learning;meter placement;network monitoring algorithm;power quality estimation;power quality monitoring;smart grid","","2","","20","","20150708","May 2016","","IEEE","IEEE Journals & Magazines"
"Machine Learning for Resources Prediction in Multihoming Scenarios","N. Capela; S. Sargento","Inst. de Telecomun., Univ. de Aveiro, Aveiro, Portugal","2015 IEEE Globecom Workshops (GC Wkshps)","20160225","2015","","","1","7","Nowadays, mobile terminals have the ability to connect to several access networks at the same time. In an IoT scenario, it is important that all technologies be available simultaneously to transfer the information between all (users, sensors, vehicles) and the infrastructure. This paper proposes the use of machine learning to gather accurate network information in a real multihoming environment. It predicts the network information and, consequently, reduces the overhead of intrusive measurement processes. We propose a learning mechanism that extracts the required information, creates its own database in a dynamic way, and identifies when the existing information is enough to perform a ""good"" prediction. Due to its characteristics, this can be seamlessly adapted to different scenarios. The results obtained in a real scenario demonstrate that this approach significantly reduces the use of an intrusive measurement approach (by around 80%), while keeping the accuracy of the information and the performance of multihoming.","","Electronic:978-1-4673-9526-7; POD:978-1-4673-9527-4","10.1109/GLOCOMW.2015.7414202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414202","","Databases;Packet loss;Prediction algorithms;Support vector machines;Throughput;Wireless communication","Internet of Things;learning (artificial intelligence);mobile computing;resource allocation;telecommunication computing;telecommunication traffic","Internet of Things;information extraction;intrusive measurement process;machine learning;network information prediction;real multihoming environment;resources prediction","","","","14","","","6-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Keynote 3: Interplay between Machine Learning and Artificial Intelligence by Yixin Chen","","","2015 Third International Conference on Advanced Cloud and Big Data","20160321","2015","","","xviii","xviii","Provides an abstract of the keynote presentation and a brief professional biography of the presenter. The complete presentation was not made available for publication as part of the conference proceedings.","","Electronic:978-1-4673-8537-4; POD:978-1-4673-8538-1","10.1109/CBD.2015.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435444","","","","","","","","","","","Oct. 30 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"The scheduling based on machine learning for heterogeneous CPU/GPU systems","D. A. Shulga; A. A. Kapustin; A. A. Kozlov; A. A. Kozyrev; M. M. Rovnyagin","National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Moscow, Russian Federation","2016 IEEE NW Russia Young Researchers in Electrical and Electronic Engineering Conference (EIConRusNW)","20160407","2016","","","345","348","Efficient use all of the available computing devices is an important issue for heterogeneous computing systems. The ability to choose a CPU or GPU processor for a specific task has a positive impact on the performance of GPGPU-systems. It helps to reduce the total processing time and to achieve the uniform system utilization. In this paper, we propose a scheduler that selects the executing device after prior training, based on the size of the input data. The article also contains the plots and time characteristics that demonstrate improvement in overall execution time, depending on the input data. The program modules were developed in C++ using CUDA libraries.","","CD:978-1-5090-0444-7; Electronic:978-1-5090-0445-4","10.1109/EIConRusNW.2016.7448189","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448189","GPGPU;Graphics processing units;Heterogeneous System;NVIDIA CUDA;Scheduling;Simple vector machine","Central Processing Unit;Dynamic scheduling;Graphics processing units;Heuristic algorithms;Processor scheduling;Runtime;Support vector machines","C++ language;graphics processing units;learning (artificial intelligence);parallel architectures;processor scheduling","C++;CPU-GPU processor;CUDA libraries;GPGPU-systems;heterogeneous CPU-GPU systems;heterogeneous computing systems;machine learning based scheduling;plot characteristics;time characteristics","","","","16","","","2-3 Feb. 2016","","IEEE","IEEE Conference Publications"
"Towards Sleep Apnea Screening with an Under-the-Mattress IR-UWB Radar Using Machine Learning","A. Q. Javaid; C. M. Noble; R. Rosenberg; M. A. Weitnauer","Georgia Inst. of Technol., Atlanta, GA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","837","842","In this work, we apply machine learning to investigate the effectiveness of an Impulse Radio Ultra-Wide Band (IR-UWB) radar panel, in an under-the-mattress configuration, for detecting apnea events in subjects known to have obstructive sleep apnea (OSA). We consider a collection of features, some novel and some inspired by features that worked well for sleep apnea detection using other types of sensors (i.e., not IR-UWB). To extract the features, we collected a total of 25 hours of data from four subjects as they slept through the night. The data included digitized samples of the IR-UWB radar return signal and the scored polysomnograph (PSG), which is the gold standard and measures a large number of physiological parameters in a well-equipped sleep laboratory. Normal and apnea epochs were extracted from the IR-UWB data corresponding to normal and apnea epochs in the PSG data. Statistical features were derived from these extracted epochs and a Linear Discriminant classifier was trained. Using cross-validation, we found that the classifier had an accuracy of around 70% in detection of apnea and normal epochs. The novel aspect of this project involves processing and investigation of different methods for feature extraction on data obtained from real apnea subjects and suggests that the radar, when paired with other under-the-mattress sensors might provide an effective screening device in a convenient form factor.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.79","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424426","Obstructive apnea;hypopnea;ultra-wide band radar","Biomedical monitoring;Data mining;Doppler radar;Feature extraction;Sensors;Sleep apnea","learning (artificial intelligence);medical signal processing;radar signal processing;sleep;statistical analysis;ultra wideband radar","IR-UWB radar return signal;impulse radio ultrawide band radar;linear discriminant classifier;machine learning;obstructive sleep apnea;scored polysomnograph;sleep apnea screening;statistical feature;under-the-mattress configuration","","2","","26","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Secure Collaborative Machine Learning Framework Based on Data Locality","K. Xu; H. Ding; L. Guo; Y. Fang","Dept. of Electr. & Comput. Eng., Univ. of Florida, Gainesville, FL, USA","2015 IEEE Global Communications Conference (GLOBECOM)","20160225","2015","","","1","5","Advancements in big data analysis offer cost-effective opportunities to improve decision-making in numerous areas such as health care, economic productivity, crime, and resource management. Nowadays, data holders are tending to sharing their data for better outcomes from their aggregated data. However, the current tools and technologies developed to manage big data are often not designed to incorporate adequate security or privacy measures during data sharing. In this paper, we consider a scenario where multiple data holders intend to find predictive models from their joint data without revealing their own data to each other. Data locality property is used as an alternative to multi-party computation (SMC) techniques. Specifically, we distribute the centralized learning task to each data holder as local learning tasks in a way that local learning is only related to local data. Along with that, we propose an efficient and secure protocol to reassemble local results to get the final result. Correctness of our scheme is proved theoretically and numerically. Security analysis is conducted from the aspect of information theory.","","Electronic:978-1-4799-5952-5; POD:978-1-4799-5953-2","10.1109/GLOCOM.2015.7417113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417113","","Big data;Collaboration;Data privacy;Protocols;Security;Training;Training data","data analysis;decision making;learning (artificial intelligence);security of data","big data analysis;collaborative machine learning;data locality;decision-making;predictive models;security analysis","","1","","16","","","6-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Applying Machine Learning Techniques to a Real Cognitive Network: File Transfer ETAs Prediction","D. Del Testa; M. Danieletto; M. Zorzi","Dept. of Inf. Eng., Univ. of Padova, Padua, Italy","2015 IEEE Global Communications Conference (GLOBECOM)","20160225","2015","","","1","7","Cognitive Network (CN) paradigm and Machine Learning (ML) techniques are increasingly becoming popular in Wireless network design. CN allows to efficiently arrange the network stack parameters among the nodes involved in the data transmission. It can be divided into two modules: the former permits to retrieve the ISO/OSI protocol stack parameters and the latter optimizes the transmission through a Cognitive Engine entity. This engine usually adopts ML algorithms to extract important features regarding the network, with the objective of either maximizing the global throughput or predicting relevant network behaviors. This paper analyzes how common ML algorithms are able to model transmissions occurring in a typical wireless network scenario. In particular, we test the algorithms with our CN testbed to predict transmission estimated time of arrivals (ETAs) in different network setups. We compare these results with those obtained via scp, which is the typical Unix/Linux shell program used to exchange files. We show that all learning techniques significantly improve the goodness of ETA prediction, thus suggesting to embed such algorithms in future scp revisions.","","Electronic:978-1-4799-5952-5; POD:978-1-4799-5953-2","10.1109/GLOCOM.2015.7417388","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7417388","","Engines;Input variables;Linear regression;Optimization;Prediction algorithms;Support vector machines;Training","ISO standards;cognitive radio;learning (artificial intelligence);telecommunication computing;time-of-arrival estimation","CN paradigm;CN testbed;ISO protocol stack parameter;Linux shell program;ML technique;OSI protocol stack parameter;Unix shell program;cognitive engine entity;data transmission;file transfer ETA prediction;machine learning technique;real cognitive network;transmission estimated ETA prediction;transmission estimated time of arrival prediction;wireless network design","","","","27","","","6-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Personal Name Extraction from Japanese Historical Documents Using Machine Learning","N. Nagai; F. Kimura; A. Maeda; R. Akama","Grad. Sch. of Inf. Sci. & Eng., Ritsumeikan Univ., Kusatsu, Japan","2015 International Conference on Culture and Computing (Culture Computing)","20160314","2015","","","207","208","In this poster, we propose a method for extracting persons' real names and aliases from Japanese historical documents. In this method, we extract personal names and aliases by applying a named entity extraction technique based on machine learning using characters as the unit of analysis. One of the features of this method is that it uses already attached annotations to named entities in order to find undiscovered ones. Experimental results showed that our proposed method was able to extract personal names and aliases from ""Yakusha-Hyoban-Ki"", a collection of review documents of Kabuki actors in Edo Era (1603-1868) in Japan, with approximately 0.91 in F-measure.","","CD-ROM:978-1-4673-8231-1; Electronic:978-1-4673-8232-8; POD:978-1-4673-8233-5","10.1109/Culture.and.Computing.2015.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433262","Support Vector Machine (SVM);annotation;named entity recognition","Data mining;Dictionaries;Feature extraction;Information science;Support vector machines;Training data;Vocabulary","character recognition;document image processing;feature extraction;learning (artificial intelligence)","Japanese historical documents;Kabuki actors;Yakusha-Hyoban-Ki;machine learning;named entity extraction technique;personal name extraction","","","","4","","","17-19 Oct. 2015","","IEEE","IEEE Conference Publications"
"Quantifying California current plankton samples with efficient machine learning techniques","J. Ellen; Hongyu Li; M. D. Ohman","Department of Computer Science and Engineering, University of California, San Diego, La Jolla, 92093-0404, USA","OCEANS 2015 - MTS/IEEE Washington","20160211","2015","","","1","9","This paper improves on the accuracy of other published machine learning results for quantifying plankton samples. The contributions of this work are: (1) Clarifying the number of expertly labeled images required for machine learning results. (2) Providing guidance as to what algorithms provide the best performance, and how to tune them. (3) Leveraging an ensemble of models to achieve recall rates beyond any single algorithm. (4) Investigating the applicability of abstaining. (5) Using size fractionation to learn more efficiently. (6) Analysis of efficacy of simple geometric features for plankton identification.","","Electronic:978-0-9339-5743-5","10.23919/OCEANS.2015.7404607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404607","Zooscan;image analysis;machine learning;zooplankton","Algorithm design and analysis;Classification algorithms;Machine learning algorithms;Radio frequency;Shape;Support vector machines;Training","geophysical image processing;image processing;learning (artificial intelligence);oceanographic techniques","California Cooperative Oceanic Fisheries Investigation;Zooscan;image analysis;machine learning technique;zooplankton","","","","13","","","19-22 Oct. 2015","","IEEE","IEEE Conference Publications"
"Probabilistic layer identification in a multi-layer fast timing detector for time-of-flight PET using machine learning","M. G. Ertosun; J. W. Cates; C. S. Levin","Molecular Imaging Program at Stanfard University, Department of Radiology, Stanford University, CA, United States of America","2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","20160314","2014","","","1","2","This work presents an effective algorithm to identify crystal element positions for the design and operation of practical PET imaging detectors capable of achieving both excellent time resolution required for time-of-flight (ToF) and depth-of-Interaction (DoI) information. The detector unit consists of a dual layer (LYSO:Ce and LSO:Ce,Ca(0.4%)) stack of two 3×3×10 mm<sup>3</sup> crystals, 1:1 coupled to SiPM arrays. Features of energy, crossover time metric, and a probability density estimation based unsupervised machine learning approach have been used for identification of in which layer a 511 KeV photon interacts. A global coincidence time resolution of 224 ps and a 91% layer identification accuracy has been achieved.","","Electronic:978-1-4799-6097-2; POD:978-1-4799-6098-9","10.1109/NSSMIC.2014.7430979","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430979","","Crystals;Detectors;Image resolution;Photonics;Positron emission tomography;Timing","positron emission tomography;probability;unsupervised learning","SiPM arrays;crossover time metric;crystal element positions;depth-of-Interaction information;detector unit;effective algorithm;electron volt energy 511 keV;global coincidence time resolution;layer identification accuracy;machine learning;multilayer fast timing detector;photon interaction;positions;practical PET imaging detectors;probabilistic layer identification;probability density estimation;time 224 ps;time resolution;time-of-flight PET;unsupervised machine learning","","","","6","","","8-15 Nov. 2014","","IEEE","IEEE Conference Publications"
"Memristive Boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning","M. N. Bojnordi; E. Ipek","University of Rochester, Rochester, NY 14627 USA","2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)","20160404","2016","","","1","13","The Boltzmann machine is a massively parallel computational model capable of solving a broad class of combinatorial optimization problems. In recent years, it has been successfully applied to training deep machine learning models on massive datasets. High performance implementations of the Boltzmann machine using GPUs, MPI-based HPC clusters, and FPGAs have been proposed in the literature. Regrettably, the required all-to-all communication among the processing units limits the performance of these efforts. This paper examines a new class of hardware accelerators for large-scale combinatorial optimization and deep learning based on memristive Boltzmann machines. A massively parallel, memory-centric hardware accelerator is proposed based on recently developed resistive RAM (RRAM) technology. The proposed accelerator exploits the electrical properties of RRAm to realize in situ, fine-grained parallel computation within memory arrays, thereby eliminating the need for exchanging data between the memory cells and the computational units. Two classical optimization problems, graph partitioning and boolean satisfiability, and a deep belief network application are mapped onto the proposed hardware. As compared to a multicore system, the proposed accelerator achieves 57x higher performance and 25x lower energy with virtually no loss in the quality of the solution to the optimization problems. The memristive accelerator is also compared against an RRAM based processing-in-memory (PIM) system, with respective performance and energy improvements of 6.89x and 5.2x.","","Electronic:978-1-4673-9211-2; POD:978-1-4673-9212-9","10.1109/HPCA.2016.7446049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446049","","Arrays;Computational modeling;Hardware;Machine learning;Simulated annealing;Training","Boltzmann machines;Boolean algebra;application program interfaces;computability;field programmable gate arrays;graph theory;graphics processing units;learning (artificial intelligence);mathematics computing;message passing;multiprocessing systems;optimisation;parallel processing;resistive RAM","FPGAs;GPU;MPI-based HPC clusters;RRAM PIM system;RRAM based processing-in-memory system;RRAM technology;all-to-all communication;boolean satisfiability;combinatorial optimization problems;deep belief network application;deep machine learning models;electrical properties;fine-grained parallel computation;graph partitioning;hardware accelerators;high performance implementations;large-scale combinatorial optimization;massively parallel memory-centric hardware accelerator;memory arrays;memristive Boltzmann machines;memristive accelerator;multicore system;parallel computational model;resistive RAM technology","","4","","84","","","12-16 March 2016","","IEEE","IEEE Conference Publications"
"Transactional Memory Scheduling Using Machine Learning Techniques","B. Assiri; C. Busch","Louisana State Univ., Baton Rouge, LA, USA","2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)","20160404","2016","","","718","725","Current shared memory multi-core systems require powerful software and hardware techniques to support the performance parallel computation and consistency simultaneously. The use of transactional memory results in significant improvement of performance by avoiding thread synchronization and locks overhead. Also, transactions scheduling apparently influences the performance of transactional memory. In this paper, we study the fairness of transactions' scheduling using Lazy Snapshot Algorithm. The fairness of transactions' scheduling aims to balance between transactions types which are read-only and update transactions. Indeed, we support the fairness of the scheduling procedure by a machine learning technique. The machine learning techniques improve the fairness decisions according to transactions' history. The experiments in this paper show that the throughput of the Lazy Snapshot Algorithm is improved with a machine learning support. Indeed, our experiments show that the learning significantly affects the performance if the durations of update transactions are much longer than read-only ones. We also study several machine learning techniques to investigate the fairness decisions accuracy. In fact, K-Nearest Neighbor machine learning technique shows more accuracy and more suitability, for our problem, than Support Vector Machine Model and Hidden Markov Model.","","Electronic:978-1-4673-8776-7; POD:978-1-4673-8777-4","10.1109/PDP.2016.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445413","Fairness Values;Hidden Markov Model;K-Nearest Neighbor;Lazy Snapshot Algorithm;Support Vector Machine;Transactional Memory","Hidden Markov models;History;Machine learning algorithms;Scheduling;Support vector machines;Throughput;Training","hidden Markov models;learning (artificial intelligence);pattern classification;shared memory systems;support vector machines;synchronisation","K-nearest neighbor machine learning technique;fairness decisions;fairness decisions accuracy;hardware techniques;hidden Markov model;lazy snapshot algorithm;locks overhead;performance parallel computation;shared memory multicore systems;software techniques;support vector machine model;thread synchronization;transactional memory scheduling","","","","18","","","17-19 Feb. 2016","","IEEE","IEEE Conference Publications"
"Automated Detection of Adenoviral Conjunctivitis Disease from Facial Images using Machine Learning","M. Gunay; E. Goceri; T. Danisman","Dept. of Comput. Eng., Akdeniz Univ., Antalya, Turkey","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","1204","1209","Nowadays scientists are focusing on diagnosing certain eye diseases using image processing. Among these diseases, Adenoviral conjunctivitis is a key eye infection to be observed and diagnosed. In this paper, digital image processing (DIP) is applied for an automated, fast and cost-effective diagnosis of conjunctivitis by physicians. In our study, we measure the vascularization and intensity of redness in pink eyes after segmenting the region of infection in corneal images to diagnose the conjunctivitis. Corneal images captured using our simple setup and processed through the proposed DIP approach successfully detects eye infections and isolates potentially contagious patients correctly 93% of the time. We were able to achieve this rate by isolating the sclera region using the automated GrabCut method that identifies the seed region from the image itself. Such adaptive isolation of region of interest overcomes challenges presented by the lightning and resolution. During this study, we evaluated the performance of known DIP methods and incorporated them in eye disease diagnosis.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.232","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424485","Adenoviral conjunctivitis (Pink Eye);GrabCut;Gray-Level Co-occurrence Matrix (GLCM);Sclera Segmentation;Vascularization","Biomedical imaging;Blood vessels;Diseases;Face;Feature extraction;Image segmentation;Iris","diseases;eye;face recognition;graph theory;image resolution;image segmentation;learning (artificial intelligence);medical image processing;object detection","Adenoviral conjunctivitis disease;DIP approach;adaptive isolation;automated GrabCut method;automated detection;automated diagnosis;contagious patients;corneal image segmentation;cost-effective diagnosis;digital image processing;eye disease diagnosis;eye infection detection;facial images;image lightning;image resolution;machine learning;pink eyes;redness intensity;sclera region;seed region;vascularization","","","","33","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"A 128-Channel Extreme Learning Machine-Based Neural Decoder for Brain Machine Interfaces","Y. Chen; E. Yao; A. Basu","Centre of Excellence in IC Design (VIRTUS), School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Biomedical Circuits and Systems","20160304","2016","10","3","679","692","Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning coprocessor in 0.35- μm CMOS for the motor intention decoding in the brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 3.45 pJ/MAC at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same coprocessor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by ≈ 2X.","1932-4545;19324545","","10.1109/TBCAS.2015.2483618","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348721","Brain-machine interfaces;extreme learning machine;implant;machine learning;motor intention;neural decoding;neural network;portable;very large scale integration (VLSI)","Decoding;Digital signal processing;Implants;Machine learning algorithms;Neurons;Sorting;Training","CMOS integrated circuits;biomechanics;biomedical electronics;biomedical equipment;brain-computer interfaces;learning (artificial intelligence);low-power electronics;medical computing;neurophysiology","128-channel extreme learning machine algorithm;CMOS;PC;asynchronous neural spikes;brain-machine interfaces;classification accuracy;core analog processor;digitally stored coefficients;frequency 50 Hz;low-power analog processing;machine learning coprocessor;monkey finger movements;neural decoder;pJ-MAC;sparsity promoting training scheme;state-of-the-art motor intention decoding algorithms;time-delayed feature dimension enhancement","","4","","43","","20151207","June 2016","","IEEE","IEEE Journals & Magazines"
"Reduction of false cardiac arrhythmia alarms through the use of machine learning techniques","M. Caballero; G. M. Mirsky","Benedictine University, Lisle, IL, USA","2015 Computing in Cardiology Conference (CinC)","20160218","2015","","","1169","1172","Due to the so-called “crying wolf” effect, frequent false cardiac arrhythmia alarms have been shown to diminish staff attentiveness and thus reduce the quality of care patients receive in the ICU. The PhysioNet/Computing in Cardiology 2015 Challenge seeks to improve patient care by decreasing the number of these false cardiac arrhythmia alarms. Using a training set of 750 multi-parameter recordings organized by type of arrhythmia alarm, we developed a decision tree for each arrhythmia category. We derived the features utilized in the decision tree from the arterial blood pressure (ABP) waveform and the photoplethysmogram (PPG). For Phase 1 of the challenge, our score for the realtime test set = 57.64 and retrospective test set = 61.15, resulting in an overall score of 59.39. For Phase 11, our score for the real-time test set = 65.19 and retrospective test set = 72.19. In conclusion, decision trees have been shown to generate reasonable results in reducing false cardiac arrhythmia alarms; future work will involve more sophisticated machine learning algorithms to improve performance.","2325-8861;23258861","Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1","10.1109/CIC.2015.7411124","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411124","","Glass;Instruments;Training","cardiology;decision trees;learning (artificial intelligence);medical computing;medical disorders;patient care;photoplethysmography","PhysioNet;arrhythmia category;arterial blood pressure waveform;crying wolf effect;decision tree;false cardiac arrhythmia alarms;machine learning techniques;multiparameter recordings;patient care;photoplethysmogram","","1","","12","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"Combating Comment Spam with Machine Learning Approaches","M. Alsaleh; A. Alarifi; F. Al-Quayed; A. Al-Salman","King Abdulaziz City for Sci. & Technol., Riyadh, Saudi Arabia","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","295","300","The feature of posting comments enables websites visitors (e.g., Youtube and Amazon) to interact and contribute to the posted content by adding comments. The fact that such comments are becoming part of the website content so that many visitors read them and that such comments are usually unvetted make them attractive to spammers for the purposes of advertising, spreading malware, phishing attacks, or spreading political or religious views. Due to large volume of comment spam, using manual filtration and vetting is unpractical and hence automatic spam detection techniques play a de-facto role in fighting spam content. In this paper, we propose and develop a comment spam detection mechanism that can be deployed as a browser plugin for inspecting the Document Object Model (DOM) of the web page in question and remove comments with spam content. We examine most detection features in the literature along with proposing new features to build a comment spam classifier. In order to test the accuracy of our classifier, we manually label a new corpus of blogs comments. We encourage other researchers to build upon our work and we hope that our corpus will benefit the research community in this area.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424324","Blog Spam;Comment Features;Comment Spam Detection;Spam","Blogs;Decision trees;Feature extraction;IP networks;Java;Web pages;XML","Web sites;information filtering;pattern classification;text analysis;unsolicited e-mail","DOM;Web page;Web site content;Web site visitors;advertising;blog comment corpus;browser plugin;comment spam classifier;comment spam detection mechanism;document object model;machine learning approach;malware;phishing attacks;political views;religious views;spam content","","","","27","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Suspicious electric consumption detection based on multi-profiling using live machine learning","T. Hartmann; A. Moawad; F. Fouquet; Y. Reckinger; T. Mouelhi; J. Klein; Y. Le Traon","Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg","2015 IEEE International Conference on Smart Grid Communications (SmartGridComm)","20160321","2015","","","891","896","The transition from today's electricity grid to the so-called smart grid relies heavily on the usage of modern information and communication technology to enable advanced features like two-way communication, an automated control of devices, and automated meter reading. The digital backbone of the smart grid opens the door for advanced collecting, monitoring, and processing of customers' energy consumption data. One promising approach is the automatic detection of suspicious consumption values, e.g., due to physically or digitally manipulated data or damaged devices. However, detecting suspicious values in the amount of meter data is challenging, especially because electric consumption heavily depends on the context. For instance, a customers energy consumption profile may change during vacation or weekends compared to normal working days. In this paper we present an advanced software monitoring and alerting system for suspicious consumption value detection based on live machine learning techniques. Our proposed system continuously learns context-dependent consumption profiles of customers, e.g., daily, weekly, and monthly profiles, classifies them and selects the most appropriate one according to the context, like date and weather. By learning not just one but several profiles per customer and in addition taking context parameters into account, our approach can minimize false alerts (low false positive rate). We evaluate our approach in terms of performance (live detection) and accuracy based on a data set from our partner, Creos Luxembourg S.A., the electricity grid operator in Luxembourg.","","Electronic:978-1-4673-8289-2; POD:978-1-4673-8290-8","10.1109/SmartGridComm.2015.7436414","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436414","","Context;Monitoring;Network topology;Smart grids;Smart meters;Topology","energy consumption;smart power grids","Creos Luxembourg S.A;advanced software monitoring;alerting system;automated device control;automated meter reading;communication technology;context-dependent consumption;electricity grid;energy consumption;information technology;live machine learning;smart grid;suspicious consumption value detection;suspicious electric consumption detection;two-way communication","","","","24","","","2-5 Nov. 2015","","IEEE","IEEE Conference Publications"
"Decreasing the false alarm rate of arrhythmias in intensive care using a machine learning approach","L. M. Eerikäinen; J. Vanschoren; M. J. Rooijakkers; R. Vullings; R. M. Aarts","Department of Electrical Engineering, Eindhoven University of Technology, The Netherlands","2015 Computing in Cardiology Conference (CinC)","20160218","2015","","","293","296","We present a novel algorithm for classifying true and false alarms of five life-threatening arrhythmias in intensive care. This algorithm was entered in the PhysioNet/Computing in Cardiology Challenge 2015 Reducing False Arrhythmia Alarms in the ICU. The algorithm performs a binary classification of the alarms for a specified arrhythmia type by combining signal quality information and physiological features from multiple sources, such as electrocardiogram (ECG), photoplethysmogram (PPG), and arterial blood pressure (ABP). Signals were selected for feature computation by first assessing the quality for available signals. Random Forest classifiers were trained separately for every type of arrhythmia with arrhythmia-specific features. Hence, the complete algorithm leverages five different predictive models. Classification sensitivities of true alarms 75-99 % (average 93 %) on the training set with cross-validation and 22-100 %(average 90 %) on the unrevealed test set. Classification specificities on the training and test set were 76-94% (average 80%) and 75-100% (average 82%), respectively. The best performance was for extreme bradycardia, whereas the poorest results were for ventricular arrhythmias. The results are for the real-time category when only information prior to the alarm is considered. The final challenge score was 75.54.","2325-8861;23258861","Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1","10.1109/CIC.2015.7408644","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408644","","Biomedical monitoring;Blood pressure;Detectors;Electrocardiography;Feature extraction;Heart rate;Training","bio-optics;blood vessels;electrocardiography;haemodynamics;learning (artificial intelligence);medical disorders;medical signal processing;patient care;plethysmography;signal classification","ABP;ECG;ICU;PPG;PhysioNet/Computing in Cardiology Challenge 2015;arrhythmias;arterial blood pressure;binary classification;bradycardia;electrocardiogram;false alarm classification;false alarm rate;intensive care unit;machine learning approach;photoplethysmogram;physiological features;random forest classifiers;signal quality information;true alarm classification;ventricular arrhythmias","","6","","14","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"Multi-sensor Visual Analytics Supported by Machine-Learning Models","G. Sharma; G. Shroff; A. Pandey; B. Singh; G. Sehgal; K. Paneri; P. Agarwal","TCS Res., Delhi-NCR, Delhi, India","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","20160204","2015","","","668","674","Machines, such as engines, vehicles, or even aircraft, go through extensive controlled trials during their development. Each machine is typically instrumented with hundreds of sensors that produce voluminous time-series data. Engineers analyze suchdata to improve their understanding of how machines are used in practice, which in turn helps them in taking design decisions. Most often they study operational profiles various sensors fora given day of operation using histograms, or examine time-series from multiple sensors together. However, when confrontedwith data from dozens of sensors, over many years of operation, they are challenged by the large number of histograms toanalyze, and the sheer length of time-series' to explore. Traditional approaches such as hierarchical histograms, time-series semantic zooming etc. often cannot cope with the volume of data encountered in practice. We augment basic data visualizations such as histograms, heat-maps and basic time-series visualizations with machine-learning models that aid in summarizing, querying, searching, and interactively linking visualizations derived fromlarge volumes of multi-sensor data. In this paper we describe our machine-learning augmented approach to visual analytics in thecontext of its actual use in practice for answering questions ofinterest to engineers analyzing large-scale multi-sensor data.","","Electronic:978-1-4673-8493-3; POD:978-1-4673-8494-0","10.1109/ICDMW.2015.190","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395731","Visual analytics;machine learning;multi-dimensional data","Data visualization;Engines;Histograms;Navigation;Semantics;Sensors;Visual analytics","data visualisation;learning (artificial intelligence);sensor fusion;time series","data visualizations;hierarchical histograms;histograms;machine-learning model;multi-sensor visual analytics;time-series data","","","","18","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Extraction of Definitional Contexts through Machine Learning","V. Mijangos","","2015 26th International Workshop on Database and Expert Systems Applications (DEXA)","20160215","2015","","","217","221","Automatic extraction of definitional contexts has been a problem that deserved to be addressed to in different studies by applications demands in the Natural Language Processing. The first approach to the automatic extraction of these resources has been through specific linguistic patterns, but this approach requires previous extensive linguistic knowledge and a thorough previous work. A model machine learning, on the other hand, reduces the work and, as we believe, can improve the results obtained with only one approach based on linguistic rules. Here experiments for extraction/classification of definitional contexts with naive bayes classifier and SVM are presented. We show that through machine learning approaches we can improve the results of this specific task. The highest result was obtained by the naive bayes classifier with back-off as smoothing.","1529-4188;15294188","Electronic:978-1-4673-7582-5; POD:978-1-4673-7583-2","10.1109/DEXA.2015.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406296","Definitional contexts;Support Vector Machine;clasification;machine learning;naive Bayes;terminology extraction","Context;Context modeling;Pragmatics;Subspace constraints;Support vector machines;Syntactics;Training","Bayes methods;computational linguistics;knowledge acquisition;learning (artificial intelligence);natural language processing;support vector machines","SVM;definitional context classification;definitional context extraction;linguistic pattern;machine learning;naive Bayes classifier;natural language processing","","","","17","","","1-4 Sept. 2015","","IEEE","IEEE Conference Publications"
"Pruning Extreme Learning Machines Using the Successive Projections Algorithm","D. Parente Mesquita; J. Gomes; L. Ramos Rodrigues; R. Kawakami Galvao","Univ. Fed. do Ceara, Fortaleza, Brazil","IEEE Latin America Transactions","20160211","2015","13","12","3974","3979","Extreme Learning Machine (ELM) is a recently proposed machine learning method with successful applications in many domains. The key strengths of ELM are its simple formulation and the reduced number of hyper-parameters. Among these hyper-parameters, the number of hidden nodes has significant impact on ELM performance since too few/many hidden nodes may lead to underfitting/overfitting. In this work, we propose a pruning strategy for ELM using the Successive Projections Algorithm (SPA) as an approach to automatically find the number of hidden nodes. SPA was originally proposed for variable selection. In this work, it was adapted in order to be used to prune ELMs. The proposed method was compared to the Optimally Pruned Extreme Learning Machine algorithm (OP-ELM), which is considered as a state of the art method. Real world datasets were used to assess the performance of the proposed method for regression and classification problems. The application of the proposed model resulted in much simpler models with similar performance compared to the OP-ELM. For some classification instances, the performance of the proposed method outperformed the OP-ELM method.","1548-0992;15480992","","10.1109/TLA.2015.7404935","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404935","exterme learning machines;neural networks;prunning techniques","Classification algorithms;Feedforward neural networks;Input variables;Learning systems;Predictive models;Presses;Projection algorithms","data handling;learning (artificial intelligence)","ELM;OP-ELM;SPA;hyper-parameters;optimally pruned extreme learning machine algorithm;pruning extreme learning machines;real world datasets;successive projections algorithm;variable selection","","","","","","","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"Package yield enhancement using machine learning in semiconductor manufacturing","H. Gun Kim; Y. S. Han; J. H. Lee","Sungkyunkwan University, Suwon, Republic of Korea","2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","20160310","2015","","","316","320","The production difficulty of the DRAM family has rapidly increasing. Several factors can be adduced to explain. For example, increasing of step in the production process by the miniaturization of semiconductor process, challenges of Low-power designed due to the growth of mobile family. And the two are connected by a combination of the result. It may increase the difficulty of an analysis of the test area for the yield improvement. This study analyzed the Fab measurement Data, Wafer Test Data, Package Test Data and looked at Package Yield Improvement. It covered Data pre-processing, Feature Selection, and then verify the relevant data, to derive an optimized relationship, and the actual production process applied too.","","CD-ROM:978-1-4799-1978-9; Electronic:978-1-4799-1980-2; POD:978-1-4799-1981-9","10.1109/IAEAC.2015.7428567","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428567","Feature Selection;High Speed Test;Machine Learning;Package Yield;Yield Prediction","Data preprocessing;Databases;Decision support systems;High definition video","DRAM chips;electronic engineering computing;feature selection;learning (artificial intelligence);production engineering computing;semiconductor device manufacture;semiconductor device packaging;wafer level packaging","DRAM family;data preprocessing;fab measurement data;feature selection;machine learning;package test data;package yield enhancement;semiconductor manufacturing;wafer test data","","","","4","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Local path planning based on Ridge Regression Extreme Learning Machines for an outdoor robot","L. Yu; Z. Long; N. Xi; Y. Jia; C. Ding","East Lansing, 48823, USA","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20160225","2015","","","745","750","For mobile robot local path planning under outdoor environment, Ridge Regression Extreme Learning Machines (RRELM) is adopted, it is a fast machine learning classification method to apply in path planning. Firstly, the laser rangefinder data are extracted and marked to describe the outdoor environment. Secondly, ridge regression theory is utilized to improve the generalization ability of ELM for local path planning. Meanwhile, the start-goal point constraint is considered for planning. Additionally, abrupt dynamic obstacle is regarded as a kind of disturbance to plan the path by RRELM. Then the optimal path is estimated by the distance evaluation function among feasible paths. Finally, a great deal of outdoor robot simulation experiments are shown that RRELM find out the safety path for outdoor robot, and the generalization ability, smoothness and rapidity of RRELM for path planning are better than SVM and ELM, furthermore, the performance of RRELM for the dynamic environment is also efficient.","","Electronic:978-1-4673-9675-2; USB:978-1-4673-9674-5","10.1109/ROBIO.2015.7418858","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418858","","Navigation;Path planning;Planning;Robot kinematics;Support vector machines;Vehicle dynamics","learning (artificial intelligence);mobile robots;path planning;regression analysis","RRELM;SVM;distance evaluation function;dynamic environment;dynamic obstacle;fast machine learning classification method;generalization ability;laser rangefinder data;mobile robot local path planning;optimal path;outdoor environment;outdoor robot simulation;ridge regression extreme learning machines;ridge regression theory","","","","25","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Remaining useful life prognostics using pattern-based machine learning","A. Ragab; S. Yacout; M. S. Ouali","&#201;cole Polytechnique de Montr&#233;al","2016 Annual Reliability and Maintainability Symposium (RAMS)","20160407","2016","","","1","7","This paper presents a prognostic methodology that can be implemented in a condition-based maintenance (CBM) program. The methodology estimates the remaining useful life (RUL) of a system by using a pattern-based machine learning and knowledge discovery approach called Logical Analysis of Data (LAD). The LAD approach is based on the exploration of the monitored system's database, and the extraction of useful information which describe the physics that characterize its degradation. The diagnostic information, which is updated each time the new data is gathered, is combined with a non-parametric reliability estimation method, in order to predict the RUL of a monitored system working under different operating conditions. In this paper, the developed methodology is compared to a known CBM prognostic technique; the Cox proportional hazards model (PHM). The methodology has been tested and validated based on the Friedman statistical test. The results of the test indicate that the proposed methodology provides an accurate RUL prediction.","","Electronic:978-1-5090-0249-8; POD:978-1-5090-0250-4","10.1109/RAMS.2016.7448025","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448025","CBM;Logical Analysis of Data;Machine Learning;Pattern Recognition;Prognostics;Remaining Useful Life","Data mining;Estimation;Knowledge discovery;Maintenance engineering;Monitoring;Reliability;Training","condition monitoring;data mining;data preparation;learning (artificial intelligence);maintenance engineering;nonparametric statistics;oil technology;remaining life assessment;statistical testing","CBM prognostic technique;CBM program;Cox proportional hazards model;Friedman statistical test;LAD approach;PHM;RUL prediction;condition-based maintenance program;diagnostic information update;information extraction;knowledge discovery approach;logical analysis-of-data;nonparametric reliability estimation method;operating conditions;pattern-based machine learning;remaining useful life prognostics","","","","26","","","25-28 Jan. 2016","","IEEE","IEEE Conference Publications"
"TABLA: A unified template-based framework for accelerating statistical machine learning","D. Mahajan; J. Park; E. Amaro; H. Sharma; A. Yazdanbakhsh; J. K. Kim; H. Esmaeilzadeh","Alternative Computing Technologies (ACT) Lab, Georgia Institute of Technology","2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)","20160404","2016","","","14","26","A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core G- Us (Tegra K1, GTX 650 Ti, and Tesla K40) using real hardware measurements. TABLA-generated accelerators provide 19.4x and 2.9x average speedup over the ARM and Xeon processors, respectively. These accelerators provide 17.57x, 20.2x, and 33.4x higher Performance-per-Watt in comparison to Tegra, GTX 650 Ti and Tesla, respectively. These benefits are achieved while the programmers write less than 50 lines of code.","","Electronic:978-1-4673-9211-2; POD:978-1-4673-9212-9","10.1109/HPCA.2016.7446050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446050","","Algorithm design and analysis;Data models;Field programmable gate arrays;Hardware;Linear programming;Machine learning algorithms;Stochastic processes","field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;stochastic programming","ASIC;FPGA;ML algorithm;TABLA framework;TABLA-generated accelerators;accelerators generation;application specific integrated circuits;compute-intensive applications;field programmable gate arrays;general-purpose processors;gradient descent solver;machine learning algorithms;many-core GPU;multicore CPU;objective function;statistical machine learning;stochastic optimization problem;unified template-based framework","","3","","58","","","12-16 March 2016","","IEEE","IEEE Conference Publications"
"An E-Learning System with Multifacial Emotion Recognition Using Supervised Machine Learning","A. T. S; J. Jose; R. G; G. R. M. Reddy","Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Surathkal, India","2015 IEEE Seventh International Conference on Technology for Education (T4E)","20160204","2015","","","23","26","E-Learning systems based on Affective computingare popularly used for emotional/behavioral analysis of the users. Emotions expressed by the user is depicted by detecting the facialexpression of the user and accordingly the teaching strategies willbe changed. The present eLearning systems mainly focus on thesingle user face detection. Hence, in this paper, we proposemultiuser face detection based eLearning system using supportvector machine based supervised machine learning technique. Experimental results demonstrate that the proposed systemprovides the accuracy of 89% to 100% w.r.t different datasets(LFW, FDDB, and YFD). Further, to improve the speed ofemotional feature processing, we used GPU along with the CPUand thereby achieve a speedup factor of 2.","","Electronic:978-1-4673-9509-0; POD:978-1-4673-9510-6","10.1109/T4E.2015.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395609","Active Appearance Model;Affective Computing;Facial expression recognition;Local Binary Patterns;Machine Learning;eLearning","Affective computing;Electronic learning;Face;Face recognition;Graphics processing units;Training","computer aided instruction;emotion recognition;face recognition;graphics processing units;learning (artificial intelligence);microprocessor chips;support vector machines;teaching","CPU;GPU;affective computing;emotional feature processing;facial expression detecting;multifacial emotion recognition;multiuser face detection based e-learning system;support vector machine based supervised machine learning technique;teaching strategies;user behavioral analysis;user emotional analysis;user face detection;user facial expression detection","","","","17","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach for the Integration of miRNA-Target Predictions","S. Beretta; M. Castelli; Y. Martínez; L. Muñoz; S. Silva; L. Trujillo; L. Milanesi; I. Merelli","DISCo, Univ. degli Studi di Milano-Bicocca, Milan, Italy","2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)","20160404","2016","","","528","534","Although several computational methods have been developed for predicting interactions between miRNA and target genes, there are substantial differences in the achieved results. For this reason, machine learning approaches are widely used for integrating the predictions obtained from different tools. In this work we adopt a method, called M3GP, which relies on a genetic programming approach, to classify results from three tools: miRanda, TargetScan, and RNAhybrid. Such algorithm is highly parallelizable and its adoption provides great advantages while handling problems involving big datasets, since it is independent from the implementation and from the architecture on which it is executed. More precisely, we apply this technique for the classification of the achieved miRNA target predictions and we compare its results with those obtained with other classifiers.","","Electronic:978-1-4673-8776-7; POD:978-1-4673-8777-4","10.1109/PDP.2016.125","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445385","Evolutionary Algorithm;Genetic Programming;Parallel Computing;miRNA-Target Prediction","Electronic mail;Genetic programming;Genomics;Niobium;RNA;Sociology;Statistics","Big Data;RNA;bioinformatics;genetic algorithms;learning (artificial intelligence)","M3GP method;RNAhybrid;TargetScan;big datasets;genetic programming approach;machine learning approach;miRNA-target predictions;miRanda","","","","25","","","17-19 Feb. 2016","","IEEE","IEEE Conference Publications"
"Bird sounds classification by large scale acoustic features and extreme learning machine","K. Qian; Z. Zhang; F. Ringeval; B. Schuller","MISP group, MMK, Technische Universit&#228;t M&#252;nchen, Germany","2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","20160225","2015","","","1317","1321","Automatically classifying bird species by their sound signals is of crucial relevance for the research of ornithologists and ecologists. In this study, we present a novel framework for bird sounds classification from audio recordings. Firstly, the p-centre is used to detect the `syllables' of bird songs, which are the units for the recognition task; then, we use our openSMILE toolkit to extract large scales of acoustic features from chunked units of analysis (the `syllables'). ReliefF helps to reduce the dimension of the feature space. Lastly, an Extreme Learning Machine (ELM) serves for decision making. Results demonstrate that our system can achieve an excellent and robust performance scalable to different numbers of species (mean unweighted average recall of 93.82%, 89.56%, 85.30%, and 83.12% corresponding to 20, 30, 40, and 50 species of birds, respectively).","","Electronic:978-1-4799-7591-4; POD:978-1-4799-7592-1","10.1109/GlobalSIP.2015.7418412","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418412","Bird Sounds;Extreme Learning Machine;ReliefF;openSMILE;p-centre","Acoustics;Audio recording;Birds;Feature extraction;Hidden Markov models;Support vector machines;Training","acoustic signal processing;acoustics;learning (artificial intelligence)","ELM;audio recordings;bird songs;bird sounds classification;bird species classification;chunked units;decision making;ecologists;extreme learning machine;feature space;large scale acoustic features;openSMILE toolkit;ornithologists;recognition task;sound signals","","","","34","","","14-16 Dec. 2015","","IEEE","IEEE Conference Publications"
"Machine learning-based spectrum decision algorithms for Wireless Sensor Networks","V. F. Silva; D. F. Macedo; J. L. Leoni","Computer Science Department, Universidade Federal de Minas Gerais, Brazil","2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC)","20160331","2016","","","1024","1029","Wireless Sensor Networks (WSNs) employ Industrial, Scientific and Medical (ISM) spectrum bands for communication, which are overloaded due to various technologies such as WLANs and other WSNs. Therefore, such networks must employ intelligent methods such as Cognitive Radio (CR) to coexist with other networks. This study investigates the use of supervised Machine Learning (ML) for channel selection in WSNs. The proposed models were analyzed using ML tools and techniques, and the best algorithms were evaluated on real sensor nodes. The experiments show performance improvements on the delivery rate and delivery delay when the proposed cognitive solutions are employed.","","Electronic:978-1-4673-9292-1; POD:978-1-4673-9293-8","10.1109/CCNC.2016.7444931","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444931","","Algorithm design and analysis;Hardware;IEEE 802.11 Standard;Interference;Prediction algorithms;Training;Wireless sensor networks","cognitive radio;learning (artificial intelligence);spread spectrum communication;wireless LAN;wireless sensor networks","ISM spectrum bands;WLAN;channel selection;cognitive radio;industrial scientific medical spectrum bands;intelligent methods;machine learning;spectrum decision;wireless sensor networks","","","","18","","","9-12 Jan. 2016","","IEEE","IEEE Conference Publications"
"A customer classification prediction model based on machine learning techniques","T. K. Das","School of Information Technology and Engineering, VIT University, Vellore-632014, India","2015 International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)","20160421","2015","","","321","326","Most of the service providers and product based companies while launching brand new products, services or releasing new versions of existent products need to campaign to reach at the potential customers. While doing so they target their already existing customers who are the ambassadors of their company. To address the existing customers, they maintain the detailed customer data at all levels as customer maser data [9]. In this paper, we have built a prediction model to identify the customers who would most likely respond to the prospective offerings of the company basing on their past purchasing trends. Experiments have been conducted using the well known classifiers, viz., Naïve Bayes, KNN and SVM to classify a bank customer data. Subsequently, we have compared the effectiveness of these techniques and found out which one produces the maximum accuracy for the existing data set.","","Electronic:978-1-4673-9223-5; POD:978-1-4673-9224-2; USB:978-1-4673-9222-8","10.1109/ICATCCT.2015.7456903","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456903","Attribute selection;Classification;Cross validation","Algorithm design and analysis;Classification algorithms;Data mining;Decision trees;Error analysis;Prediction algorithms;Support vector machines","Bayes methods;learning (artificial intelligence);marketing data processing;pattern classification;support vector machines","K nearest neighbor;KNN;SVM;bank customer data;customer classification prediction model;machine learning techniques;naive Bayes;purchasing trends;support vector machines","","","","12","","","29-31 Oct. 2015","","IEEE","IEEE Conference Publications"
"Automatic plant species recognition technique using machine learning approaches","S. Purohit; R. Viroja; S. Gandhi; N. Chaudhary","Department of Computer Science, Gujarat University, Ahmedabad, India","2015 International Conference on Computing and Network Communications (CoCoNet)","20160218","2015","","","710","719","Motivated from the need of automation of plant speciess recognition and availability of digital databases of plants,we propose an image based identification of speciess of plant. These images may belong to different organs of the plants such as leaf, stem or bark, flower and fruit. Different methods for recognition of the speciess are used according to the part of the plant to which the image belongs to. For flower category, fusion of shape, color and texture features are used. For other categories like stem, fruit, leaf and leafscan, Sparsely coded SIFT features pooled with Spatial pyramid matching approach is used. To cater the seasonal and topographic influences on the appearance of the plant, our system also uses metadata i.e. content, date, time, latitude, longitude associated with images to aid the identification process and obtain more accurate results. For a given image of plant and associated metadata, the system recognizes the speciess of the given plant image and produces an output that contains the Family, Genus, and Speciess name. The proposed framework is implemented and tested on ImageClef data with 50 different classes of speciess. Maximum accuracy of 98% is attained in leaf scan sub-category whereas minimum accuracy is achieved in fruit sub-category which is 67.3 %.","","Electronic:978-1-4673-7309-8; POD:978-1-4673-7310-4","10.1109/CoCoNet.2015.7411268","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411268","Content based retrievel;HSV color space;Plant Species;SIFT;Sparse Coding;Spatial Pyramid matchin;Texture fetaures extraction","Automation;Feature extraction;Image color analysis;Image recognition;Image segmentation;Metadata;Shape","biology computing;botany;image colour analysis;image matching;image recognition;image texture;learning (artificial intelligence);meta data;shape recognition;transforms;visual databases","ImageClef data;automatic plant species recognition technique;color feature fusion;digital plant databases;family name;genus name;identification process;image based plant species identification;machine learning approach;shape feature fusion;sparsely coded SIFT features;spatial pyramid matching approach;species name;texture feature fusion;topographic influences","","","","40","","","16-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Survey of Machine Learning Applications for Energy-Efficient Resource Management in Cloud Computing Environments","M. Demirci","Dept. of Comput. Eng., Gazi Univ. Ankara, Ankara, Turkey","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","1185","1190","Ensuring energy efficiency in data centers is a crucial objective in modern cloud computing because it reduces operating costs and complies with the goals of green computing. Researchers strive to develop optimal policies for resource management in the cloud, which has many components such as virtual machine placement, task scheduling, workload consolidation, and so on. Machine learning has a major role to play in these efforts. In this paper, we provide a detailed survey of recent works in the literature which have employed machine learning (ML) to offer solutions for energy efficiency in cloud computing environments. We also present a comparative classification of the proposed methods. Furthermore, we enrich this survey by studying non-ML proposals to energy conservation in data centers, and also how ML has been applied towards other objectives in the cloud.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.205","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424481","Cloud Computing;Data Centers;Energy Efficiency;Machine Learning;Resource Management","Cloud computing;Energy consumption;Heuristic algorithms;Machine learning algorithms;Prediction algorithms;Resource management;Servers","cloud computing;computer centres;cost reduction;energy conservation;green computing;learning (artificial intelligence)","cloud computing;data centers;energy-efficient resource management;green computing;machine learning applications;operating cost reduction","","","","41","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Comparison of some machine learning and statistical algorithms for classification and prediction of human cancer type","B. Shamsaei; C. Gao","Department of Computational Engineering, The University of Tennessee, Chattanooga, Chattanooga, Tennessee, USA","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","296","299","Use of gene expression profile of an animal under a certain disease gives pre-clinical insights for the potential efficacy of novel drugs. Selection of an animal model, accurately resembling the human disease, profoundly reduces the research cost in resources and time. Here, a statistical procedure based on analysis of variance (ANOVA) defined in [1] is investigated to select the animal model that most accurately mimics the human disease in terms of genome-wide gene expression. Two other commonly used data fitting algorithms in machine learning, logistic regression and artificial neural networks are examined and analyzed for the same data set. Implementing procedure of each of these algorithms is discussed and computational cost and advantage and drawback of each algorithm is scrutinized for prediction of pediatric Medulloblastoma cancer type.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455893","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455893","Anova;Artificial neural networks;Gene expression value;Human cancer type;Logistic regression;Pediatric medulloblastoma","Analysis of variance;Animals;Cancer;Gene expression;Logistics;Measurement;Neural networks","bioinformatics;brain;cancer;genomics;learning (artificial intelligence);medical computing;neural nets;paediatrics;regression analysis","analysis-of-variance;artificial neural networks;data fitting algorithm;gene expression profile;genome-wide gene expression;human cancer classification;human cancer type prediction;human disease;logistic regression;machine learning;pediatric medulloblastoma cancer type;statistical algorithm;statistical procedure","","","","9","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"Reducing false arrhythmia alarms using robust interval estimation and machine learning","C. Hoog Antink; S. Leonhardt","Medical Information Technology, RWTH Aachen University, Germany","2015 Computing in Cardiology Conference (CinC)","20160218","2015","","","285","288","Reducing false arrhythmia alarms in the intensive care unit is the objective of the PhysioNet/Computing in Cardiology Challenge 2015. In this paper, an approach is presented that analyzes multimodal cardiac signals in terms of their beat-to-beat intervals as well as their average rhythmicity. Based on this analysis, several features in time and frequency domain are extracted and used for subsequent machine learning. Results show that alarm-specific strategies proved optimal for different types of arrhythmia and that obtained scores varied: While the score for reducing false ventricular tachycardia alarms was 68:91, false extreme tachycardia alarms could be suppressed with perfect accuracy. Overall, a top score of 75.55 / 75.18 could be achieved for real-time / retrospective false alarm reduction.","2325-8861;23258861","Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1","10.1109/CIC.2015.7408642","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408642","","Correlation;Electrocardiography;Estimation;Feature extraction;Heart beat;Principal component analysis;Robustness","electrocardiography;feature extraction;learning (artificial intelligence);medical signal processing","ECG;PhysioNet/Computing;beat-to-beat intervals;false arrhythmia alarm reduction;feature extraction;intensive care unit;machine learning;multimodal cardiac signals;rhythmicity","","4","","8","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning Based WSN System for Autism Activity Recognition","S. S. Alwakeel; B. Alhalabi; H. Aggoune; M. Alwakeel","Coll. of Comput. & Inf. Sci, King Saud Univ., Riyadh, Saudi Arabia","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","771","776","Autistic children often develop abnormal habits and in some cases they could be unsafe or even dangerous to themselves and their family members. Because of their limited speech ability, their inexperienced parents may underestimate their physical abilities compared to their intellectual level and may not realize that they could easily hurt themselves. Therefore, the need for an automatic alert system for autistic child parent assistance is great, and it will enhance the life experience for both the autistic child and the family. In this paper, we present a machine learning based electronic system for autism activity recognition using wireless sensor networks (WSNs). The system accurately detects autistic child gesture and motion. The system is named Autistic child Sensor and Assistant System (ACSA), and is comprised of three main components: the ACSA Wearable sensor device, the companion ACSA Parent Application and the machine learning algorithms developed for autistic movement event detection and processing. The paper describes the system concepts, its components and details of its architecture and operation. Individuals and families with Autism Spectrum Disorder children can utilize this system as alarming devices that assist them to protect their autistic child regardless of the environment. The proposed system is expected to enhance the life experience for all aides, the autistic child, the parents, and the autistic child whole family.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424415","Autism Wearable Sensor device;Autistic children support system;Machine learning for Autism activity Sensing;Machine learning for medical WSN applications","Autism;Pediatrics;Technological innovation;Temperature measurement;Temperature sensors;Wearable sensors;Wireless sensor networks","biomechanics;learning (artificial intelligence);medical computing;medical disorders;paediatrics;wearable computers;wireless sensor networks","ACSA wearable sensor device;Autistic child Sensor and Assistant System;autism activity recognition;autism spectrum disorder children;autistic child gesture;autistic child motion;autistic child parent assistance;autistic movement event detection;autistic movement event processing;automatic alert system;companion ACSA Parent Application;machine learning algorithms;machine learning based WSN system;machine learning based electronic system;physical abilities;speech ability;wireless sensor networks","","","","10","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Runtime-behavior based malware classification using online machine learning","A. Pektaş; T. Acarman; Y. Falcone; J. C. Fernandez","Univ. Grenoble Alpes, VERIMAG, F-38000, Grenoble, France","2015 World Congress on Internet Security (WorldCIS)","20160218","2015","","","166","171","Identification of malware's family is an intricate process whose success and accuracy depends on different factors. These factors are mainly related to the process of extracting of meaningful and distinctive features from a set of malware samples, modeling malware via its static or dynamic features and particularly techniques used to classify malware samples. In this paper, we propose a new malware classification method based on behavioral features. File system, network, registry activities observed during the execution traces of the malware samples are used to represent behavior based features. Existing classification schemes apply machine-learning algorithms to the stored data, i.e., they are off-line. In this study, we use on-line machine learning algorithms that can provide instantaneous update about the new malware sample by following its introduction to the classification scheme. To validate the effectiveness and scalability of our method, we have evaluated our method by using 18,000 recent malicious files. Experimental results show that our method classifies malware with an accuracy of 92.","","Electronic:978-1-908320-50-6; POD:978-1-4673-9483-3; USB:978-1-908320-49-0","10.1109/WorldCIS.2015.7359437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359437","dynamic analysis;malware classification;online machine learning","Classification algorithms;Computational modeling;Feature extraction;Internet;Malware;Runtime","invasive software;learning (artificial intelligence);pattern classification;storage allocation;system monitoring","dynamic analysis;malware sample classification;online machine learning algorithm;runtime-behavior-based malware classification;static analysis","","","","20","","","19-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Adaptive OpenMP Task Scheduling Using Runtime APIs and Machine Learning","A. Qawasmeh; A. M. Malik; B. M. Chapman","Dept. of Comput. Sci., Univ. of Houston, Houston, TX, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","889","895","Task-based programming models adopt different scheduling strategies to exploit parallelism in irregular applications. These scheduling strategies differ in terms of exploiting data locality, maintaining load balance, and minimizing overhead. OpenMP tasks allow programmers to express unstructured parallelism at a high level of abstraction and make the runtime responsible about the burden of scheduling parallel execution. For irregular applications, the performance of task scheduling cannot often be predicted due to the nature of application, the used compiler, and the platform/architecture dependencies. In this work, we introduce an automatic, portable, and adaptive runtime feedback-driven framework (APARF) that combines standard low-level tasking runtime APIs, a developed profiling tool, and a hybrid machine learning model. We employ APARF to select the optimum task scheduling scheme of any given application using similarity analysis through the correlation between the captured runtime APIs with low profiling costs. Our hybrid model predicts the best scheduling strategy for a variety of unseen applications with an average accuracy of 93%, while maintaining a 100% training accuracy. An average performance enhancement of 25% was obtained compared with the default configuration, when APARF was applied on different unseen programs. APARF was examined against a real application (Molecular Dynamics), where we achieved up to 31% performance improvement. Compared to Intel, PGI and GNU compilers, our predicted scheme achieved better performance in most cases.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424434","Adaptive scheduling;Collector APIs;Machine learning;OMPT;OpenMP Tasks;Similarity analysis","Benchmark testing;Magnetic heads;Optimal scheduling;Program processors;Runtime;Scheduling;Standards","application program interfaces;learning (artificial intelligence);message passing;processor scheduling;program compilers","APARF;GNU compiler;Intel compiler;PGI compiler;adaptive OpenMP task scheduling;automatic portable and adaptive runtime feedback-driven framework;data locality;default configuration;developed profiling tool;high level of abstraction;hybrid machine learning model;load balance;molecular dynamics;optimum task scheduling scheme;performance enhancement;performance improvement;platform/architecture dependency;profiling cost;scheduling strategy;similarity analysis;standard low-level tasking runtime API;task-based programming model;unseen program;unstructured parallelism","","","","25","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Anomaly Detection in IPv4 and IPv6 networks using machine learning","B. Vrat; N. Aggarwal; S. Venkatesan","Department of Information Technology, Indian Institute of Information Technology, Allahabad, India","2015 Annual IEEE India Conference (INDICON)","20160331","2015","","","1","6","Anomaly Detection is an important requirement to secure a network against the attackers. Detecting attacks within a network by analysing the behaviour pattern has been a significant field of study for several researchers and application systems in IPv4 as well as IPv6 networks. For precise anomaly detection, it is essential to implement and use an efficient data-mining methodology like machine learning. In this paper, we contemplated an anomaly detection model which uses machine learning algorithms for data mining within a network to detect anomalies present at any time. This proposed model is evaluated against Denial of Service (DOS) attacks in both IPv4 and IPv6 networks while selecting the most common and evident features of IPv6 and IPv4 networks for optimizing the detection. The results also show that the proposed system can detect most of the IPv4 and IPv6 attacks in efficient manner.","","Electronic:978-1-4673-7399-9; POD:978-1-4673-7400-2","10.1109/INDICON.2015.7443752","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443752","","Feature extraction;Intrusion detection;Machine learning algorithms;Protocols;Telecommunication traffic;Training","IP networks;computer network security;data mining;learning (artificial intelligence)","DOS attacks;IPv4 network;IPv6 network;anomaly detection model;attack detection;behaviour pattern analysis;data mining methodology;denial of service attacks;machine learning algorithms","","","","28","","","17-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Using Consumer Behavior Data to Reduce Energy Consumption in Smart Homes: Applying Machine Learning to Save Energy without Lowering Comfort of Inhabitants","D. Schweizer; M. Zehnder; H. Wache; H. F. Witschel; D. Zanatta; M. Rodriguez","Inst. of Bus. Inf. Syst., Univ. of Appl. Sci. & Arts Northwestern Switzerland, Olten, Switzerland","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","1123","1129","This paper discusses how usage patterns and preferences of inhabitants can be learned efficiently to allow smart homes to autonomously achieve energy savings. We propose a frequent sequential pattern mining algorithm suitable for real-life smart home event data. The performance of the proposed algorithm is compared to existing algorithms regarding completeness/correctness of the results, run times as well as memory consumption and elaborates on the shortcomings of the different solutions. We also propose a recommender system based on the developed algorithm. This recommender provides recommendations to the users to reduce their energy consumption. The recommender system was deployed to a set of test homes. The test participants rated the impact of the recommendations on their comfort. We used this feedback to adjust the system parameters and make it more accurate during a second test phase. The historical dataset provided by digitalSTROM contained 33 homes with 3521 devices and over 4 million events. The system produced 160 recommendations on the first phase and 120 on the second phase. The ratio of useful recommendations was close to 10%.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424470","association rules;energy saving;internet of things;recommender systems;smart cities;smart homes;unsupervised learning","Algorithm design and analysis;Data mining;Energy consumption;Heuristic algorithms;Memory management;Recommender systems;Smart homes","data mining;energy consumption;home computing;learning (artificial intelligence);power engineering computing;recommender systems","consumer behavior data;digitalSTROM;energy consumption;feedback;pattern mining algorithm;real-life smart home event data;recommender system;smart home","","2","","17","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Detection of forest fires using machine learning technique: A perspective","A. Kansal; Y. Singh; N. Kumar; V. Mohindru","Department of Computer Science & Engineering, Jaypee University of Information Technology, Waknaghat, Solan-173234, (H.P), India","2015 Third International Conference on Image Information Processing (ICIIP)","20160225","2015","","","241","245","Wireless Sensor Networks (WSN) has gained attention as it has been useful in warning about disasters. Predicting natural disasters like hailstorm, fire, rainfall etc. by WSN are infrequent and stochastic. This is an important topic of research. Detection of these disasters should be fast and accurate as they may cause damage and destruction at a large scale. In this paper, comparison of various machine learning techniques such as SVM, regression, decision trees, neural networks etc. has been done for prediction of forest fires. The proposed approach in this paper presents how regression works best for detection of forest fires with high accuracy by dividing the dataset. Fast detection of forest fires is done in this paper by taking less time as compared to other machine learning techniques.","","Electronic:978-1-5090-0148-4; POD:978-1-5090-0149-1","10.1109/ICIIP.2015.7414773","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414773","Classification;Decision Tree;Regression;SVM;Sensor;Wireless Sensor Networks","Wireless sensor networks","decision trees;disasters;environmental management;fires;learning (artificial intelligence);neural nets;rain;regression analysis;storms;support vector machines;wireless sensor networks","SVM;WSN;decision trees;forest fires;hailstorm;machine learning techniques;natural disasters;neural networks;rainfall;regression works;wireless sensor networks","","","","18","","","21-24 Dec. 2015","","IEEE","IEEE Conference Publications"
"Calculating web service similarity using ontology learning with machine learning","R. A. H. M. Rupasingha; I. Paik; B. T. G. S. Kumara","School of Computer Science and Engineering, University of Aizu, Aizu-Wakamatsu, Fukushima, Japan","2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","20160321","2015","","","1","8","The Web is a popular, easy and common way to propagate information today and according to the growth of the Web, Web service discovery has become a challenging task. Clustering Web services into similar clusters through calculating the semantic similarity of Web services is one way for overcome this issue. Several methods are used for current similarity calculation process such as knowledge based, information-retrieval based, text mining, ontology based and context-aware based methods. Through this paper, present a method for calculating Web service similarity using both ontology learning and machine learning that uses a support vector machine for similarity calculation in generated ontology instead of edge count base method. Experimental results show that our hybrid approach of combining ontology learning and machine learning works efficiently and give accurate results than previous two approaches.","","CD-ROM:978-1-4799-7847-2; Electronic:978-1-4799-7849-6; POD:978-1-4799-7850-2","10.1109/ICCIC.2015.7435686","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435686","Context aware service similarity;Information Retrieval;Machine Learning;Ontology Learning;Web Service Clustering","Clustering algorithms;Context;Feature extraction;Ontologies;Quality of service;Semantics;Web services","Web services;learning (artificial intelligence);ontologies (artificial intelligence);pattern clustering;support vector machines","Web service clustering;Web service discovery;Web service similarity calculation;edge count base method;hybrid approach;machine learning;ontology learning;support vector machine","","1","","24","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning Approach to False Alarm Detection for Critical Arrhythmia Alarms","X. Wang; Y. Gao; J. Lin; H. Rangwala; R. Mittu","Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA","2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)","20160303","2015","","","202","207","High false alarm rates in Intensive Care Unit (ICU) is a common problem that leads to alarm desensitization -- a phenomenon called alarm fatigue. Alarm fatigue can cause longer response time or missing of important alarms. In this work, we propose a methodology to identify false alarms generated by ICU bedside monitors. The novelty in our approach lies in the extraction of 216 relevant features to capture the characteristics of all alarms, from both arterial blood pressure (ABP) and electrocardiogram (ECG) signals. Our multivariate approach mitigates the imprecision caused by existing heartbeat/peak detection algorithms. Unlike existing methods on ICU false alarm detection, our approach does not require separate techniques for different types of alarms. The experimental results show that our approach can achieve high accuracy on false alarm detection, and can be generalized for different types of alarms.","","Electronic:978-1-5090-0287-0; POD:978-1-5090-0288-7","10.1109/ICMLA.2015.176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424309","","Databases;Electrocardiography;Feature extraction;Heart rate;MIMICs;Monitoring;Standards","blood vessels;cardiovascular system;electrocardiography;feature extraction;learning (artificial intelligence);medical signal processing","ABP signal;ECG signal;ICU;alarm desensitization;alarm fatigue;arterial blood pressure signal;critical arrhythmia alarms;electrocardiogram signal;false-alarm rate detection;feature extraction;intensive care unit;machine learning approach;multivariate approach","","","","18","","","9-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"Inverse-Free Extreme Learning Machine With Optimal Information Updating","S. Li; Z. H. You; H. Guo; X. Luo; Z. Q. Zhao","Department Computing, Hong Kong Polytechnic University, Hong Kong","IEEE Transactions on Cybernetics","20160413","2016","46","5","1229","1241","The extreme learning machine (ELM) has drawn insensitive research attentions due to its effectiveness in solving many machine learning problems. However, the matrix inversion operation involved in the algorithm is computational prohibitive and limits the wide applications of ELM in many scenarios. To overcome this problem, in this paper, we propose an inverse-free ELM to incrementally increase the number of hidden nodes, and update the connection weights progressively and optimally. Theoretical analysis proves the monotonic decrease of the training error with the proposed updating procedure and also proves the optimality in every updating step. Extensive numerical experiments show the effectiveness and accuracy of the proposed algorithm.","2168-2267;21682267","","10.1109/TCYB.2015.2434841","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7115113","Extreme learning machine (ELM);inverse-free;neural networks;optimal updates","Accuracy;Approximation algorithms;Approximation error;Least squares approximations;Neural networks;Training","approximation theory;learning (artificial intelligence);matrix algebra","ELM;inverse-free extreme learning machine;machine learning problems;matrix inversion operation;neural networks;optimal information updating","","6","","64","","20150601","May 2016","","IEEE","IEEE Journals & Magazines"
"Enhancing accuracy of arrhythmia classification by combining logical and machine learning techniques","V. Kalidas; L. S. Tamil","The University of Texas at Dallas, Richardson, USA","2015 Computing in Cardiology Conference (CinC)","20160218","2015","","","733","736","This paper is a contribution to the Physionet/Computing in Cardiology Challenge 2015. The aim is to reduce the occurrence of false alarms in the ICU during the detection of asystole, extreme bradycardia, extreme tachycardia, ventricular fibrillation and ventricular tachycardia. Robust classification of each arrhythmia is achieved using a combination of logical and SVM-based machine learning techniques. Information from electrocardiogram and photoplethysmogram signals, sampled at 250Hz, is used for logical analysis and to form the feature set. This information includes time-domain and frequency-domain data such as R-R intervals, power spectrum density, autocorrelation plots and standard deviation values. Pan-Tompkins algorithm is applied to ECG signals for QRS complex detection.","2325-8861;23258861","Electronic:978-1-5090-0684-7; POD:978-1-5090-0660-1","10.1109/CIC.2015.7411015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7411015","","Biomedical monitoring;Monitoring;Standards;Support vector machines","electrocardiography;frequency-domain analysis;learning (artificial intelligence);medical disorders;medical signal detection;medical signal processing;photoplethysmography;signal classification;signal sampling;support vector machines;time-domain analysis","ECG signals;ICU;Pan-Tompkins algorithm;QRS complex detection;R-R intervals;SVM-based machine learning techniques;arrhythmia classification;asystole detection;autocorrelation plots;cardiology;electrocardiogram signals;extreme bradycardia detection;extreme tachycardia detection;feature set;frequency 250 Hz;frequency-domain data;logical analysis;photoplethysmogram signals;power spectrum density;standard deviation values;time-domain data;ventricular fibrillation detection;ventricular tachycardia detection","","6","","6","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
