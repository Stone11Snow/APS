"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7879617,7877468,7877583,7879402,7876714,7875982,7875984,7875094,7872953,7873830,7873027,7872724,7873831,7873004,7872992,7872799,7873147,7873292,7871074,7869124,7870913,7529065,7866201,7867660,7868415,7868438,7868369,7868959,7868242,7868949,7866831,7868376,7868701,7868802,7792212,7863129,7862974,7862060,7860040,7860900,7859950,7862067,7862021,7861353,7859944,7860233,7860214,7860043,7862023,7860229,7860414,7859673,7862048,7858439,7858157,7858395,7858402,7852593,7856622,7856730,7857484,7856705,7855962,7853008,7572159,7856509,7856826,7857308,7852915,7431997,7393590,7856983,7849639,7849953,7850050,7850221,7849899,7851530,7850200,7849379,7852288,7849484,7850094,7852248,7849987,7849886,7851957,7849952,7850079,7850943,7844465,7848208,7844258,7845053,7845462,7847994,7844417,7845073,7847089,7847948",2017/05/04 23:21:20
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Sequential Nonlinear Learning for Distributed Multiagent Systems via Extreme Learning Machines","N. D. Vanli; M. O. Sayin; I. Delibalta; S. S. Kozat","Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Neural Networks and Learning Systems","20170215","2017","28","3","546","558","We study online nonlinear learning over distributed multiagent systems, where each agent employs a single hidden layer feedforward neural network (SLFN) structure to sequentially minimize arbitrary loss functions. In particular, each agent trains its own SLFN using only the data that is revealed to itself. On the other hand, the aim of the multiagent system is to train the SLFN at each agent as well as the optimal centralized batch SLFN that has access to all the data, by exchanging information between neighboring agents. We address this problem by introducing a distributed subgradient-based extreme learning machine algorithm. The proposed algorithm provides guaranteed upper bounds on the performance of the SLFN at each agent and shows that each of these individual SLFNs asymptotically achieves the performance of the optimal centralized batch SLFN. Our performance guarantees explicitly distinguish the effects of data- and network-dependent parameters on the convergence rate of the proposed algorithm. The experimental results illustrate that the proposed algorithm achieves the oracle performance significantly faster than the state-of-the-art methods in the machine learning and signal processing literature. Hence, the proposed method is highly appealing for the applications involving big data.","2162-237X;2162237X","","10.1109/TNNLS.2016.2536649","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7431997","Distributed systems;extreme learning machine (ELM);multiagent optimization;sequential learning;single hidden layer feedforward neural networks (SLFNs)","Convergence;Cost function;Learning systems;Machine learning algorithms;Signal processing;Signal processing algorithms","learning (artificial intelligence);multi-agent systems;neural nets","Big Data;SLFN;distributed multiagent systems;distributed subgradient-based extreme learning machine algorithm;neighboring agents;sequential nonlinear learning;single hidden layer feedforward neural network","","","","","","20160311","March 2017","","IEEE","IEEE Journals & Magazines"
"Namatad: Inferring occupancy from building sensors using machine learning","A. Dey; X. Ling; A. Syed; Y. Zheng; B. Landowski; D. Anderson; K. Stuart; M. E. Tolentino","Intelligent Platforms & Architecture Lab, University of Washington, Tacoma, WA, USA","2016 IEEE 3rd World Forum on Internet of Things (WF-IoT)","20170209","2016","","","478","483","Driven by the need to improve efficiency, modern buildings are instrumented with numerous sensors to monitor utilization and regulate environmental conditions. While these sensor systems serve as valuable tools for managing the comfort and health of occupants, there is an increasing need to expand the deployment of sensors to provide additional insights. Because many of these desired insights have high temporal value, such as occupancy during emergency situations, such insights are needed in real time. However, augmenting buildings with new sensors is often expensive and requires a significant capital investment. In this paper, we propose and describe the real-time, streaming system called Namatad that we developed to infer insights from many sensors typical of Internet of Things (IoT) deployments. We evaluate the effectiveness of this platform by leveraging machine learning to infer new insights from environmental sensors within buildings. We describe how we built the components of our system leveraging several open source, streaming frameworks. We also describe how we ingest and aggregate from building sensors and sensing platforms, route data streams to appropriate models, and make predictions using machine learning techniques. Using our system, we have been able to predict the occupancy of rooms within a building on the University of Washington campus over the last three months, in real time, at accuracies of up to 95%.","","Electronic:978-1-5090-4130-5; POD:978-1-5090-4131-2","10.1109/WF-IoT.2016.7845462","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845462","Building;IoT;Machine Learning;Sensors;Server","","Internet of Things;building management systems;control engineering computing;learning (artificial intelligence);public domain software;sensors","Internet of Things;Namatad;University of Washington campus;building sensors;capital investment;data streams;environmental condition regulation;environmental sensors;machine learning techniques;occupancy prediction;open source streaming frameworks;real-time streaming system","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"Using semi-supervised machine learning to address the Big Data problem in DNS networks","L. Watkins; S. Beck; J. Zook; A. Buczak; J. Chavis; W. H. Robinson; J. A. Morales; S. Mishra","Johns Hopkins University, Information Security Institute, Baltimore, MD USA","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","6","The problem of Big Data in cyber security (i.e., too much network data to analyze) compounds itself every day. Our approach is based on a fundamental characteristic of Big Data: an overwhelming majority of the network traffic in a traditionally secured enterprise (i.e., using defense-in-depth) is non-malicious. Therefore, one way of eliminating the Big Data problem in cyber security is to ignore the overwhelming majority of an enterprise's non-malicious network traffic and focus only on the smaller amounts of suspicious or malicious network traffic. Our approach uses simple clustering along with a dataset enriched with known malicious domains (i.e., anchors) to accurately and quickly filter out the non-suspicious network traffic. Our algorithm has demonstrated the predictive ability to accurately filter out approximately 97% (depending on the algorithm used) of the non-malicious data in millions of Domain Name Service (DNS) queries in minutes and identify the small percentage of unseen suspicious network traffic. We demonstrate that the resulting network traffic can be analyzed with traditional reputation systems, blacklists, or in-house threat tracking sources (we used virustotal.com) to identify harmful domains that are being accessed from within the enterprise network. Specifically, our results show that the method can reduce a dataset of 400k query-answer domains (with complete malicious domain ground truth) down to only 3% containing 99% of all malicious domains. Further, we demonstrate that this capability scales to 10 million query-answer pairs, which it can reduce by 97% in less than an hour.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868376","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868376","","Approximation algorithms;Big data;Clustering algorithms;Computer security;Semisupervised learning;Sensitivity;Telecommunication traffic","Big Data;learning (artificial intelligence);query processing;security of data","Big Data problem;DNS networks;cyber security;domain name service queries;enterprise nonmalicious network traffic;in-house threat tracking sources;query-answer domains;semi-supervised machine learning;unseen suspicious network traffic","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Machine-learning approach to analysis of driving simulation data","A. Yoshizawa; H. Nishiyama; H. Iwasaki; F. Mizoguchi","Denso IT Laboratory, Shibuya-ku Tokyo, 150-0002, Japan","2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20170223","2016","","","398","402","In our study, we sought to generate rules for cognitive distractions of car drivers using data from a driving simulation environment. We collected drivers' eye-movement and driving data from 18 research participants using a simulator. Each driver drove the same 15-minute course two times. The first drive was normal driving (no-load driving), and the second drive was driving with a mental arithmetic task (load driving), which we defined as cognitive-distraction driving. To generate rules of distraction driving using a machine-learning tool, we transformed the data at constant time intervals to generate qualitative data for learning. Finally, we generated rules using a Support Vector Machine (SVM).","","CD:978-1-5090-3845-9; Electronic:978-1-5090-3846-6; POD:978-1-5090-3847-3","10.1109/ICCI-CC.2016.7862067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862067","Car Driving Simulation;Eye-Movement Data;Machine Learning;Support Vector Machine","Acceleration;Automobiles;Data models;Safety;Support vector machines;Visualization","automobiles;cognition;data analysis;driver information systems;learning (artificial intelligence);support vector machines","SVM;car drivers;cognitive-distraction driving;drivers eye-movement;driving simulation data analysis;machine learning;mental arithmetic task;no-load driving;support vector machine","","","","","","","22-23 Aug. 2016","","IEEE","IEEE Conference Publications"
"Structured support vector machine learning of conditional random fields","R. P. Rangkuti; A. J. Mantau; V. Dewanto; N. Habibie; W. Jatmiko","Faculty of Computer Science, Universitas Indonesia","2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20170309","2016","","","548","555","This research aims to improve the capability of semantic segmentation through data perspective. This research proposed a parameterized Conditional Random Fields model and learns the model by using Structured Support Vector Machine (SSVM). The SSVM utilizes Hamming loss function for optimizing 1-slack Margin Rescaling formulation. The joint feature vector is derived from energy potentials. Variation of image size produces some missing values in the joint feature vector. This research shows that a zero padding can resolve the missing values. The experiment result shows that prediction with parameterized CRF yields 75.867% global accuracy (GA) and 22.1410 % averaged class accuracy (CA).","","Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7","10.1109/ICACSIS.2016.7872799","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872799","","Image color analysis;Image segmentation;Inference algorithms;Labeling;Optimization;Semantics;Support vector machines","Hamming codes;image segmentation;learning (artificial intelligence);random processes;support vector machines","Hamming loss function;SSVM;energy potentials;image size;joint feature vector;margin rescaling formulation;parameterized conditional random fields model;semantic segmentation;structured support vector machine learning","","","","","","","15-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for Noise Sensor Placement and Full-Chip Voltage Emergency Detection","X. Liu; S. Sun; X. Li; H. Qian; P. Zhou","School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","20170216","2017","36","3","421","434","Power supply fluctuation can be potential threat to the correct operations of processors, in the form of voltage emergency that happens when supply voltage drops below a certain threshold. Noise sensors (with either analog or digital outputs) can be placed in the nonfunction area of processors to detect voltage emergencies by monitoring the runtime voltage fluctuations. Our work addresses two important problems related to building a sensor-based voltage emergency detection system: 1) offline sensor placement, i.e., where to place the noise sensors so that the number and locations of sensors are optimized in order to strike a balance between design cost and chip reliability and 2) online voltage emergency detection, i.e., how to use these placed sensors to detect voltage emergencies in the hotspot locations. In this paper, we propose integrated solutions to these two problems, respectively, for analog and digital (more specifically, binary) sensor outputs, by exploiting the voltage correlation between the sensor candidate locations and the hotspot locations. For the analog case, we use the Group Lasso and an ordinary least squares approach; for the binary case, we integrate the Group Lasso and the SVM approach. Experimental results show that, our approach can achieve 2.3X-2.7X better voltage emergency detection results on average for analog outputs when compared to the state-of-the-art work; and for the binary case, on average our methodology can achieve up to 21% improvement in prediction accuracy compared to an approach called max-probability-no-prediction.","0278-0070;02780070","","10.1109/TCAD.2016.2611502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572159","Group Lasso;linear support vector machine (LSVM);machine learning;noise sensor;sensor placement;voltage emergency","Buildings;Electronic mail;Predictive models;Sun;Support vector machines;System-on-chip;Training","detector circuits;integrated circuit reliability;learning (artificial intelligence);least squares approximations;sensor placement","SVM approach;chip reliability;full-chip voltage emergency detection;group Lasso;hotspot locations;machine learning;max-probability-no-prediction;noise sensor placement;offline sensor placement;online voltage emergency detection;ordinary least squares approach;power supply fluctuation;runtime voltage fluctuations","","","","","","20160920","March 2017","","IEEE","IEEE Journals & Magazines"
"A review of machine learning techniques using decision tree and support vector machine","M. Somvanshi; P. Chavan","Department of Information Technology, Pimpri Chinchwad College of engineering, Pune, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","7","In this paper, the brief survey of data mining classification by using the machine learning techniques is presented. The machine learning techniques like decision tree and support vector machine play the important role in all the applications of artificial intelligence. Decision tree works efficiently with discrete data and SVM is capable of building the nonlinear boundaries among the classes. Both of these techniques have their own set of strengths which makes them suitable in almost all classification tasks.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7860040","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860040","classification;decision tree;id3;kernel;machine learning;support vector machine","Classification algorithms;Data mining;Databases;Decision trees;Machine learning algorithms;Supervised learning;Support vector machines","data mining;decision trees;learning (artificial intelligence);pattern classification;support vector machines","SVM;artificial intelligence;data mining classification;decision tree;discrete data;machine learning;support vector machine","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Dynamic prediction of drivers' personal routes through machine learning","Yue Dai; Yuan Ma; Qianyi Wang; Y. L. Murphey; S. Qiu; J. Kristinsson; J. Meyer; F. Tseng; T. Feldkamp","Intelligent System Laboratory, University of Michigan-Dearborn, USA","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","Personal route prediction (PRP) has attracted much research interest recently because of its technical challenges and broad applications in intelligent vehicle and transportation systems. Traditional navigation systems generate a route for a given origin and destination based on either shortest or fastest route schemes. In practice, different people may very likely take different routes from the same origin to the same destination. Personal route prediction attempts to predict a driver's route based on the knowledge of driver's preferences. In this paper we present an intelligent personal route prediction system, I_PRP, which is built based upon a knowledge base of personal route preference learned from driver's historical trips. The I_PRP contains an intelligent route prediction algorithm based on the first order Markov chain model to predict a driver's intended route for a given pair of origin and destination, and a dynamic route prediction algorithm that has the capability of predicting driver's new route after the driver departs from the predicted route.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850094","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850094","","Global Positioning System;Heuristic algorithms;Knowledge based systems;Machine learning algorithms;Prediction algorithms;Roads;Vehicles","Markov processes;intelligent transportation systems;learning (artificial intelligence)","I_PRP;driver historical trips;driver preferences;dynamic driver personal route prediction;first-order Markov chain model;intelligent personal route prediction system;machine learning;personal route preference","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"A machine learning analysis of Twitter sentiment to the Sandy Hook shootings","N. Wang; B. Varghese; P. D. Donnelly","School of EEECS, Queen's University Belfast, UK","2016 IEEE 12th International Conference on e-Science (e-Science)","20170306","2016","","","303","312","Gun related violence is a complex issue and accounts for a large proportion of violent incidents. In the research reported in this paper, we set out to investigate the pro-gun and anti-gun sentiments expressed on a social media platform, namely Twitter, in response to the 2012 Sandy Hook Elementary School shooting in Connecticut, USA. Machine learning techniques are applied to classify a data corpus of over 700,000 tweets. The sentiments are captured using a public sentiment score that considers the volume of tweets as well as population. A web-based interactive tool is developed to visualise the sentiments and is available at http://www.gunsontwitter.com. The key findings from this research are: (i) There are elevated rates of both pro-gun and anti-gun sentiments on the day of the shooting. Surprisingly, the pro-gun sentiment remains high for a number of days following the event but the anti-gun sentiment quickly falls to pre-event levels. (ii) There is a different public response from each state, with the highest pro-gun sentiment not coming from those with highest gun ownership levels but rather from California, Texas and New York.","","Electronic:978-1-5090-4273-9; POD:978-1-5090-4274-6; USB:978-1-5090-4272-2","10.1109/eScience.2016.7870913","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870913","","Data visualization;Feature extraction;Niobium;Sociology;Statistics;Twitter;Weapons","Internet;educational institutions;learning (artificial intelligence);pattern classification;sentiment analysis;social networking (online)","California;Connecticut;New York;Sandy Hook Elementary School shooting;Texas;Twitter sentiment;USA;Web-based interactive tool;anti-gun sentiments;data corpus classification;gun related violence;machine learning;pro-gun sentiments;public sentiment score;social media platform;violent incidents","","","","","","","23-27 Oct. 2016","","IEEE","IEEE Conference Publications"
"Coke optimization with machine learning method in sinter plant","A. Beşkardeş; S. Çevik","&#x0130;skenderun Demir ve &#x00C7;elik A.&#x015E;.","2016 National Conference on Electrical, Electronics and Biomedical Engineering (ELECO)","20170213","2016","","","170","173","Sintered product which is the largest entry of blast furnace is produced in the sinter plant which is one of the main unit of integrated iron and steel factories. Limestone and dolomite are added to fine ore in the sinter plant, and this mixture is sent to the blast furnace after smelted with coke fuel. The use of coke powder in the correct ratio is very important in terms of maintain thermal equilibrium in sinter plant and controlling raw material costs. In this study, a coke powder ratio prediction model is established with multiple linear regression and support vector machines by using coke analysis information, raw material and process data of İskenderun Iron & Steel Plant (ISDEMIR) sinter plant. Both methods reached 77% accuracy rate in this study that has 70 sample data have been collected for 20 months.","","POD:978-605-01-0923-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851957","","Art;Blast furnaces;Iron;Optimization;Powders;Raw materials;Steel","blast furnaces;coke;learning (artificial intelligence);optimisation;production engineering computing;regression analysis;smelting;steel manufacture;support vector machines","Iskenderun;blast furnaces;coke optimization;dolomite;iron factories;limestone;machine learning;multiple linear regression;sinter plant;smelting;steel factories;support vector machines;thermal equilibrium","","","","","","","1-3 Dec. 2016","","IEEE","IEEE Conference Publications"
"Fine-grained accelerators for sparse machine learning workloads","A. K. Mishra; E. Nurvitadhi; G. Venkatesh; J. Pearce; D. Marr","Intel Corp., U.S.A.","2017 22nd Asia and South Pacific Design Automation Conference (ASP-DAC)","20170220","2017","","","635","640","Text analytics applications using machine learning techniques have grown in importance with ever increasing amount of data being generated from web-scale applications, social media and digital repositories. Apart from being large in size, these generated data are often unstructured and are heavily sparse in nature. The performance of these applications on current systems is hampered by hard to predict branches and low compute-per-byte ratio. This paper proposes a set of fine-grained accelerators that improve the performance and energy-envelope of these applications by an order of magnitude.","","Electronic:978-1-5090-1558-0; POD:978-1-5090-1559-7","10.1109/ASPDAC.2017.7858395","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858395","","Data models;Data structures;Engines;Kernel;Sparse matrices;Support vector machines;System-on-chip","learning (artificial intelligence);text analysis","fine-grained accelerators;sparse machine learning workloads;text analytics applications","","","","","","","16-19 Jan. 2017","","IEEE","IEEE Conference Publications"
"Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches","Z. Du; D. D. B. D. Rubin; Y. Chen; L. Hel; T. Chen; L. Zhang; C. Wu; O. Temam","State Key Laboratory of Computer Architecture, Institute of Computing Technology (ICT), CAS, China","2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)","20170216","2015","","","494","507","A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.","","Electronic:978-1-4503-4034-2; POD:978-1-5090-6601-8","10.1145/2830772.2830789","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856622","Accelerator;Comparison;Neuromorphic","Artificial neural networks;Biological neural networks;Encoding;Handwriting recognition;Hardware;Neurons;Neuroscience","learning (artificial intelligence);neural nets","SNN algorithm;STDP learning algorithm;hardware accelerators;hardware neural network accelerators;machine-learning approaches;neural network algorithms;neuromorphic accelerators;neuroscience;online learning;spike coding","","","","","","","5-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Survey on Software Vulnerability Analysis Method Based on Machine Learning","G. Jie; K. Xiao-Hui; L. Qiang","Nat. Key Lab. of Sci. & Technol. on Inf. Syst. Security, Beijing Inst. of Syst. & Eng., Beijing, China","2016 IEEE First International Conference on Data Science in Cyberspace (DSC)","20170302","2016","","","642","647","With the increasingly rich of vulnerability related data and the extensive application of machine learning methods, software vulnerability analysis methods based on machine learning is becoming an important research area of information security. In this paper, the up-to-date and well-known works in this research area were analyzed deeply. A framework for software vulnerability analysis based on machine learning was proposed. And the existing works were described and compared, the limitations of these works were discussed. The future research directions on software vulnerability analysis based on machine learning were put forward in the end.","","Electronic:978-1-5090-1192-6; POD:978-1-5090-1193-3","10.1109/DSC.2016.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866201","Analysis Framework;Machine Learning;Software Vulnerability Analysis","Computer bugs;Feature extraction;Information systems;Learning systems;Security;Software systems","learning (artificial intelligence);security of data;software engineering","information security;machine learning;software vulnerability analysis","","","","","","","13-16 June 2016","","IEEE","IEEE Conference Publications"
"Software design pattern recognition using machine learning techniques","A. K. Dwivedi; A. Tirkey; R. B. Ray; S. K. Rath","Department of Computer Science and Engineering, National Institute of Technology, Rourkela, 769008, Odisha, India","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","222","227","Design patterns helpful for software development are the reusable abstract documents which provide acceptable solutions for the recurring design problems. But in the process of reverse engineering, it is often desired to identify as well as recognize design pattern from source code, as it improves maintainability and documentation of the source code. In this study, the process of software design pattern recognition is presented which is based on machine learning techniques. Firstly, a training dataset is developed which is based on software metrics. Subsequently, machine learning algorithms such as Layer Recurrent Neural Network and Decision Tree are applied for patterns detection process. In order to evaluate the proposed study, an open source software i.e., JHotDraw 7.0.6 has been used for the recognition of design patterns.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7847994","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847994","Design Pattern Recognition;Machine Learning;Object-Oriented Metrics;Software Design Patterns","Decision trees;Measurement;Pattern recognition;Production facilities;Software design;Training","learning (artificial intelligence);pattern recognition;public domain software;reverse engineering;software architecture;software metrics;source code (software)","JHotDraw 7.0.6;decision tree;documentation improvement;layer recurrent neural network;machine learning techniques;maintainability improvement;open source software;reusable abstract documents;reverse engineering;software design pattern recognition;software development;software metrics;source code;training dataset","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Comparative analysis of machine learning KNN, SVM, and random forests algorithm for facial expression classification","R. A. Nugrahaeni; K. Mutijarsa","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia","2016 International Seminar on Application for Technology of Information and Communication (ISemantic)","20170309","2016","","","163","168","Human depicts their emotions through facial expression or their way of speech. In order to make this process possible for a machine, a training mechanism is needed to give machine the ability to recognize human expression. This paper compare and analyse the performance of three machine learning algorithm to do the task of classifying human facial expression. The total of 23 variables calculated from the distance of facial features are used as the input for the classification process, with the output of seven categories, such as: angry, disgust, fear, happy, neutral, sad, and surprise. Some test cases were made to test the system, in which each test cases has different amount of data, ranging from 165-520 training data. The result for each algorithm is quite satisfying with the accuracy of 75.15% for K-Nearest Neighbor (KNN), 80% for Support Vector Machine (SVM), and 76.97% for Random Forests algorithm, tested using test case with the smallest amount of data. As for the result using the largest amount of data, the accuracy is 98.85% for KNN, 90% for SVM, and 98.85% for Random Forests algorithm. The training data for each test case was also classified using Discriminant Analysis with the result 97.7% accuracy.","","Electronic:978-1-5090-2326-4; POD:978-1-5090-2327-1","10.1109/ISEMANTIC.2016.7873831","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873831","Emotion;Facial Expression;KNN;Random Forest;SVM","Classification algorithms;Eyebrows;Facial features;Machine learning algorithms;Mouth;Support vector machines;Training","face recognition;image classification;learning (artificial intelligence);support vector machines","KNN;SVM;discriminant analysis;facial expression classification;k-nearest neighbor;machine learning;random forest algorithm;support vector machine;training mechanism","","","","","","","5-6 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning application for refrigeration showcase fault discrimination","A. Santana; Y. Fukuyama; K. Murakami; T. Matsui","Computational Intelligence Lab., Federal University of Para, Belem, Brazil","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","10","13","Open refrigeration showcases are commonly utilized equipment in super markets and convenience stores to maintain the temperature and quality of foods and drinks. Often set in a broader refrigeration arrangement, where a number of showcases and outdoor condensers are connected, it is a system that remains susceptible to fault events, which can lead to financial losses for stores. Therefore, faults and early abnormal behaviors that can lead to future problems should be identified. To classify events as in-control of faulty, samples or patterns for both types of events are needed, however, it is often the case in practical industrial applications where only the in-control type of data is available. This paper assesses the applicability of the machine learning approaches for supervised and unsupervised learning in the task of identifying unusual behavior in real showcase data.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7847948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847948","clustering;machine learning;refrigeration showcase;supervised learning;unary classification","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Fault diagnosis;Monitoring;Process control;Training","fault diagnosis;power engineering computing;refrigeration;unsupervised learning","industrial applications;machine learning application;refrigeration showcase fault discrimination;supervised learning;unsupervised learning","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Combining a rule-based classifier with ensemble of feature sets and machine learning techniques for sentiment analysis on microblog","U. A. Siddiqua; T. Ahsan; A. N. Chy","Department of Computer Science & Engineering, International Islamic University Chittagong, Chittagong-4314, Bangladesh","2016 19th International Conference on Computer and Information Technology (ICCIT)","20170223","2016","","","304","309","Microblog, especially Twitter, have become an integral part of our daily life, where millions of users sharing their thoughts daily because of its short length characteristics and simple manner of expression. Monitoring and analyzing sentiments from such massive Twitter posts provide enormous opportunities for companies and other organizations to estimate the user acceptance of their products and services. But the ever-growing unstructured and informal user-generated posts in Twitter demands sentiment analysis tools that can automatically infer sentiments from Twitter posts. In this paper, we propose an approach for sentiment analysis on Twitter, where we combine a rule-based classifier with a majority voting based ensemble of supervised classifiers. We introduce a set of rules for the rule-based classifier based on the occurrences of emoticons and sentiment-bearing words. To train the supervised classifiers, we extract a set of features grouped into Twitter specific features, textual features, parts-of-speech (POS) features, lexicon based features, and bag-of-words (BoW) feature. A supervised feature selection method based on the chi-square statistics (χ<sup>2</sup>) and information gain (IG) is applied to select the best feature combination. We conducted our experiments on Stanford sentiment140 dataset. Experimental results demonstrate the effectiveness of our method over the baseline and known related work.","","Electronic:978-1-5090-4090-2; POD:978-1-5090-4091-9","10.1109/ICCITECHN.2016.7860214","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860214","Microblog;feature extraction;machine learning;rule-based classifier;sentiment analysis;sentiment lexicon","Computers;Feature extraction;Information technology;Sentiment analysis;Support vector machines;Training;Uniform resource locators","feature extraction;feature selection;knowledge based systems;learning (artificial intelligence);pattern classification;sentiment analysis;social networking (online);text analysis","BoW feature;Stanford sentiment140 dataset;bag-of-words feature;chi-square statistics;emoticon occurrences;feature combination;feature set extraction;information gain;lexicon based features;machine learning techniques;majority voting based ensemble;massive twitter posts;microblog;parts-of-speech features;rule-based classifier;sentiment analysis;sentiment monitoring;sentiment-bearing words;short length characteristics;supervised classifiers;supervised feature selection;textual features;twitter specific features;user acceptance","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Algorithms for determining semantic relations of formal concepts by cognitive machine learning based on concept algebra","M. Valipour; Y. Wang; O. A. Zatarain; M. L. Gavrilova","International Institute of Cognitive Informatics and Cognitive Computing (ICIC), Laboratory for Computational Intelligence, Denotational Mathematics, Software Science, and Cognitive Systems, Dept. of Electrical and Computer Engineering, Schulich School of Engineering and Hotchkiss Brain Institute, University of Calgary, 2500 University Drive NW, Calgary, Alberta, Canada T2N 1N4","2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20170223","2016","","","97","105","It is recognized that the semantic space of knowledge is a hierarchical concept network. This paper presents theories and algorithms of hierarchical concept classification by quantitative semantic relations via machine learning based on concept algebra. The equivalence between formal concepts are analyzed by an Algorithm of Concept Equivalence Analysis (ACEA), which quantitatively determines the semantic similarity of an arbitrary pair of formal concepts. This leads to the development of the Algorithm of Relational Semantic Classification (ARSC) for hierarchically classify any given concept in the semantic space of knowledge. Experiments applying Algorithms ACEA and ARSC on 20 formal concepts are successfully conducted, which encouragingly demonstrate the deep machine understanding of semantic relations and their quantitative weights beyond human perspectives on knowledge learning and natural language processing.","","CD:978-1-5090-3845-9; Electronic:978-1-5090-3846-6; POD:978-1-5090-3847-3","10.1109/ICCI-CC.2016.7862021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862021","Cognitive machine learning;cognitive algorithms;concept algebra;concept classification;concept hierarchy;concept similarity;formal concept;knowledge representation;semantic analysis","Animals;Decision support systems;Handheld computers;Ink;Instruments;Lead;Writing","algebra;formal concept analysis;learning (artificial intelligence);natural language processing","ACEA;ARSC;algorithm of concept equivalence analysis;algorithm of relational semantic classification;cognitive machine learning;concept algebra;formal concepts;hierarchical concept classification;knowledge learning;knowledge semantic space;natural language processing;quantitative semantic relations","","","","","","","22-23 Aug. 2016","","IEEE","IEEE Conference Publications"
"Human gesture classification by brute-force machine learning for exergaming in physiotherapy","F. Deboeverie; S. Roegiers; G. Allebosch; P. Veelaert; W. Philips","Department of Telecommunications and Information Processing, Image Processing and Interpretation, UGent/iMinds, St-Pietersnieuwstraat 41, 9000 Ghent, Belgium","2016 IEEE Conference on Computational Intelligence and Games (CIG)","20170223","2016","","","1","7","In this paper, a novel approach for human gesture classification on skeletal data is proposed for the application of exergaming in physiotherapy. Unlike existing methods, we propose to use a general classifier like Random Forests to recognize dynamic gestures. The temporal dimension is handled afterwards by majority voting in a sliding window over the consecutive predictions of the classifier. The gestures can have partially similar postures, such that the classifier will decide on the dissimilar postures. This brute-force classification strategy is permitted, because dynamic human gestures show sufficient dissimilar postures. Online continuous human gesture recognition can classify dynamic gestures in an early stage, which is a crucial advantage when controlling a game by automatic gesture recognition. Also, ground truth can be easily obtained, since all postures in a gesture get the same label, without any discretization into consecutive postures. This way, new gestures can be easily added, which is advantageous in adaptive game development. We evaluate our strategy by a leave-one-subject-out cross-validation on a self-captured stealth game gesture dataset and the publicly available Microsoft Research Cambridge-12 Kinect (MSRC-12) dataset. On the first dataset we achieve an excellent accuracy rate of 96.72%. Furthermore, we show that Random Forests perform better than Support Vector Machines. On the second dataset we achieve an accuracy rate of 98.37%, which is on average 3.57% better then existing methods.","","Electronic:978-1-5090-1883-3; POD:978-1-5090-1884-0; USB:978-1-5090-1882-6","10.1109/CIG.2016.7860414","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860414","","Decision trees;Games;Gesture recognition;Radio frequency;Skeleton;Training;Vegetation","computer games;gesture recognition;image classification;learning (artificial intelligence);patient treatment;random processes;support vector machines","Microsoft Research Cambridge-12 Kinect dataset;adaptive game development;automatic gesture recognition;brute-force classification strategy;brute-force machine learning;dynamic gesture recognition;exergaming;human gesture classification;leave-one-subject-out cross-validation;online continuous human gesture recognition;physiotherapy;random forests;self-captured stealth game gesture dataset;support vector machines;temporal dimension","","","","","","","20-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"PSN-aware circuit test timing prediction using machine learning","Y. C. Liu; C. Y. Han; S. Y. Lin; J. C. M. Li","Graduate Institute of Electronics Engineering, National Taiwan University, Taiwan","IET Computers & Digital Techniques","20170228","2017","11","2","60","67","Excessive power supply noise (PSN) such as IR drop can cause yield loss when testing very large scale integration chips. However, simulation of circuit timing with PSN is not an easy task. In this study, the authors predict circuit timing for all test patterns using three machine learning techniques, neural network (NN), support vector regression (SVR), and least-square boosting (LSBoost). To reduce the huge dimension of raw data, they propose four feature extractions: <i>input/output transition</i> (IOT), <i>flip-flop transition in window</i> (FFTW), <i>switching activity in window</i> (SAW), and <i>terminal FF transition of long paths</i> (PATH). SAW and FFTW are physical-aware features while PATH is a timing-aware feature. Their experimental results on leon3mp benchmark circuit (638 K gates, 2 K test patterns) show that, compared with the simple IOT method, SAW effectively reduced the dimension by up to 472 times, without significant impact on prediction accuracy [correlation coefficient = 0.79]. Their results show that NN has best prediction accuracy and SVR has the least under-prediction. LSBoost uses the least memory. The proposed method is more than six orders of magnitude faster than traditional circuit simulation tools.","1751-8601;17518601","","10.1049/iet-cdt.2016.0032","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862974","","","VLSI;circuit simulation;data reduction;feature extraction;integrated circuit noise;integrated circuit testing;learning (artificial intelligence);least squares approximations;neural nets;regression analysis;support vector machines","FFTW;IOT method;IR drop;LSBoost;PATH;PSN;PSN-aware circuit test timing prediction;SAW;SVR;circuit simulation tools;circuit timing simulation;feature extractions;flip-flop transition in window;input-output transition;least-square boosting;leon3mp benchmark circuit;machine learning;neural network;physical-aware features;power supply noise;raw data dimension reduction;support vector regression;switching activity in window;terminal FF transition of long paths;timing-aware feature;very large scale integration chip testing;yield loss","","","","","","","3 2017","","IET","IET Journals & Magazines"
"Probability density estimation of photometric redshifts based on machine learning","S. Cavuoti; M. Brescia; C. Vellucci; G. Longo; V. Amaro; C. Tortora","Osservatorio di Capodimonte, INAF - via Moiariello 16, 80131 - Napoli, Italy","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","6","Photometric redshifts (photo-z's) provide an alternative way to estimate the distances of large samples of galaxies and are therefore crucial to a large variety of cosmological problems. Among the various methods proposed over the years, supervised machine learning (ML) methods capable to interpolate the knowledge gained by means of spectroscopical data have proven to be very effective. METAPHOR (Machine-learning Estimation Tool for Accurate PHOtometric Redshifts) is a novel method designed to provide a reliable PDF (Probability density Function) of the error distribution of photometric redshifts predicted by ML methods. The method is implemented as a modular workflow, whose internal engine for photo-z estimation makes use of the MLPQNA neural network (Multi Layer Perceptron with Quasi Newton learning rule), with the possibility to easily replace the specific machine learning model chosen to predict photo-z's. After a short description of the software, we present a summary of results on public galaxy data (Sloan Digital Sky Survey - Data Release 9) and a comparison with a completely different method based on Spectral Energy Distribution (SED) template fitting.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849953","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849953","","Estimation;Knowledge based systems;Neural networks;Photometry;Probability density function;Radio frequency;Training","Galaxy;astronomy computing;cosmology;estimation theory;learning (artificial intelligence);multilayer perceptrons;photometry;probability;red shift","METAPHOR;MLPQNA neural network;PDF;SED template fitting;cosmological problems;error distribution;machine-learning estimation tool;modular workflow;multi layer perceptron;photo-z estimation;photometric redshifts;probability density estimation;probability density function;public galaxy data;quasi Newton learning rule;spectral energy distribution;spectroscopical data;supervised machine learning","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Interpretable machine learning via convex cardinal shape composition","K. R. Varshney","Mathematical Sciences Department, IBM Thomas J. Watson Research Center, Yorktown Heights, New York 10598, United States of America","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","327","330","For safety reasons, the interpretability of models is important in consequential applications of supervised classification in which predictions are used to support human decision makers. In this paper, we extend cardinal shape composition, a new method developed in the image processing and computer vision literature for image segmentation, to general machine learning problems. Our transformation results in a computationally-tractable ℓ<sub>1</sub>-regularized hinge loss optimization over a shape dictionary. This approach yields human-interpretable models with an appropriate choice of atomic shapes in the dictionary used to compose decision boundaries.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852248","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852248","","Bayes methods;Dictionaries;Fasteners;Image segmentation;Kernel;Safety;Shape","computer vision;convex programming;image segmentation;learning (artificial intelligence)","computationally-tractable ℓ<sub>1</sub>-regularized hinge loss optimization;computer vision;convex cardinal shape composition;image processing;image segmentation;interpretable machine learning;supervised classification","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"An innovative delay based algorithm to boost PUF security against machine learning attacks","F. Amsaad; M. Choudhury; C. R. Chaudhuri; M. Niamat","Electrical Engineering and Computer Science Department, University of Toledo, Toledo, OH, USA","2016 Annual Connecticut Conference on Industrial Electronics, Technology & Automation (CT-IETA)","20170302","2016","","","1","6","Physical Unclonable Functions (PUFs) are the state-of-the-art topic in hardware oriented security and trust (HOST). PUFs have recently emerged as a promising solution for cyber security related issues including low-cost chip authentication, hacking of digital systems and tampering of physical systems. In this paper, a dynamic area efficient design along with an algorithm, namely, Optimal Time Delay Algorithm (OTPA) are proposed to improve the ROPUF challenge/response pairs (CRPs space). The generated CRPs space using this algorithm exhibit high PUF performance in terms of uniqueness and reliability. Experimental results demonstrate the competence of this innovative algorithm to enhance the CRP space using the probability of combinational statistic formula for improved security of the ROPUF against modeling attacks.","","Electronic:978-1-5090-0799-8; POD:978-1-5090-0800-1","10.1109/CT-IETA.2016.7868242","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868242","PUFs;hardware security;modeling attacks","Delays;Heuristic algorithms;Machine learning algorithms;Ring oscillators;Security;Silicon;Table lookup","cryptography;delay circuits;learning (artificial intelligence);message authentication;trusted computing","CRP;HOST;OTPA;PUF security;challenge response pairs;cyber security;digital system hacking;hardware oriented security and trust;innovative delay;low-cost chip authentication;machine learning attacks;optimal time delay algorithm;physical system tampering;physical unclonable functions","","","","","","","14-15 Oct. 2016","","IEEE","IEEE Conference Publications"
"Partitioning streaming parallelism for multi-cores: A machine learning based approach","Z. Wang; M. F. P. O'Boyle","Institute for Computing Systems Architecture, School of Informatics, The University of Edinburgh, UK","2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)","20170213","2010","","","307","318","Stream based languages are a popular approach to expressing parallelism in modern applications. The efficient mapping of streaming parallelism to multi-core processors is, however, highly dependent on the program and underlying architecture. We address this by developing a portable and automatic compiler-based approach to partitioning streaming programs using machine learning. Our technique predicts the ideal partition structure for a given streaming application using prior knowledge learned off-line. Using the predictor we rapidly search the program space (without executing any code) to generate and select a good partition. We applied this technique to standard StreamIt applications and compared against existing approaches. On a 4-core platform, our approach achieves 60% of the best performance found by iteratively compiling and executing over 3000 different partitions per program. We obtain, on average, a 1.90x speedup over the already tuned partitioning scheme of the StreamIt compiler. When compared against a state-of-the-art analytical, model-based approach, we achieve, on average, a 1.77x performance improvement. By porting our approach to a 8-core platform, we are able to obtain 1.8x improvement over the StreamIt default scheme, demonstrating the portability of our approach.","","Electronic:978-1-4503-0178-7; POD:978-1-5090-5032-1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851530","Compiler Optimization;Machine Learning;Partitioning Streaming Parallelism","Dynamic programming;Fuses;Hardware;Optimization;Parallel processing;Pipelines;Predictive models","learning (artificial intelligence);multiprocessing systems;parallel programming;program compilers","StreamIt compiler;automatic compiler-based approach;ideal partition structure prediction;machine learning based approach;multicore processors;program space;stream based languages;streaming parallelism partitioning;streaming program partitioning","","","","","","","11-15 Sept. 2010","","IEEE","IEEE Conference Publications"
"A machine learning approach for authorship attribution for Bengali blogs","S. Phani; S. Lahiri; A. Biswas","Information Technology, IIEST, Shibpur, Howrah 711103, West Bengal, India","2016 International Conference on Asian Language Processing (IALP)","20170313","2016","","","271","274","In this paper we described an authorship attribution system for Bengali blog texts. We have presented a new Bengali blog corpus of 3000 passages written by three authors. Our study proposes a text classification system, based on lexical features such as character bigrams and trigrams, word n-grams (n = 1, 2, 3) and stop words, using four classifiers. We achieve best results (more than 99%) on the held-out dataset using Multi layered Perceptrons (MLP) amongst the four classifiers, which indicates MLP can produce very good results for big data sets and lexical n-gram based features can be the best features for any authorship attribution system.","","Electronic:978-1-5090-0922-0; POD:978-1-5090-0923-7; USB:978-1-5090-0921-3","10.1109/IALP.2016.7875984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875984","","Blogs;Electronic mail;Information technology;Niobium;Standards;Training;Writing","Web sites;learning (artificial intelligence);multilayer perceptrons;text analysis","Bengali blog corpus;Bengali blog texts;MLP;authorship attribution system;character bigrams;held-out dataset;lexical n-gram based features;machine learning approach;multilayered perceptrons;stop words;trigrams;word n-grams","","","","","","","21-23 Nov. 2016","","IEEE","IEEE Conference Publications"
"Analysis framework for machine learning experiments based on classifier combination for petrographic images","A. S. Marathe; M. A. Chavhan; V. Vyas","Department of Electronics & Telecommunication College of Engineering, Pune Pune, India","2016 International Conference on Signal and Information Processing (IConSIP)","20170216","2016","","","1","5","The analysis and description of rocks is very well useful in geological industry and also in rock mining. Igneous rock is most abandoned rock in nature. The classification of Igneous rocks in its two types namely Plutonic and Volcanic is a complex and tedious job because of homogeneity between the classes. In geology this classification process has been carried out on the manual basis regularly, requiring high geological expertise. But being done manually can subject to error and leads to unnecessary delay. Hence for automating the process, pattern classification approach is used. In this paper a locally relevant database of igneous rocks involving both the rock types is considered for experimentation and testing. The grain size is considered as a discriminating textural feature for this classification problem followed by identification statistical features for textural discrimination such as Haralick features, and Laws Masks. A Radial basis function support vector machine classifier is used for the classification providing machine learning approach and giving reasonable results. But In Igneous rocks, the image classes are overlapping in the feature space. So the Classification accuracy can be further increased if the hypothesis of the multiple Support vector machine (SVM) image classifiers are combined to give a single hypothesis for the classification of an image. A method is presented for combining different base classifiers i.e. Adaboost technique.","","Electronic:978-1-5090-1522-1; POD:978-1-5090-1523-8","10.1109/ICONSIP.2016.7857484","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857484","Adaboost;Generalization;Haralick features;Laws Masks;Overfitting;Support Vector Machine;Underfitting","Classification algorithms;Databases;Kernel;Rocks;Support vector machines;Training","feature extraction;geophysics computing;image classification;learning (artificial intelligence);petrology;radial basis function networks;support vector machines","classifier combination;geological industry;grain size;igneous rock classification;igneous rock database;image classification;machine learning experiments;pattern classification;petrographic images;plutonic igneous rock;radial basis function support vector machine classifier;rock analysis;rock description;rock mining;textural feature identification;volcanic igneous rock","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Lung cancer detection using Local Energy-based Shape Histogram (LESH) feature extraction and cognitive machine learning techniques","S. K. Wajid; A. Hussain; K. Huang; W. Boulila","Computing Science & Mathematics, School of Natural Sciences, University of Stirling, Stirling, UK","2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20170223","2016","","","359","366","The novel application of Local Energy-based Shape Histogram (LESH) feature extraction technique was recently proposed for breast cancer diagnosis using mammogram images [22]. This paper extends our original work to apply the LESH technique to detect lung cancer. The JSRT Digital Image Database of chest radiographs is selected for research experimentation. Prior to LESH feature extraction, we enhanced the radiograph images using a contrast limited adaptive histogram equalization (CLAHE) approach. Selected state-of-the-art cognitive machine learning classifiers, namely extreme learning machine (ELM), support vector machine (SVM) and echo state network (ESN) are then applied using the LESH extracted features for efficient diagnosis of correct medical state (existence of benign or malignant cancer) in the x-ray images. Comparative simulation results, evaluated using the classification accuracy performance measure, are further bench-marked against state-of-the-art wavelet based features, and authenticate the distinct capability of our proposed framework for enhancing the diagnosis outcome.","","CD:978-1-5090-3845-9; Electronic:978-1-5090-3846-6; POD:978-1-5090-3847-3","10.1109/ICCI-CC.2016.7862060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862060","Clinical Decision Support Systems (CDSSs);Echo State Network (ESN);Extreme Learning Machine (ELM);Local Energy based Shape Histogram (LESH);Support Vector Machine (SVM)","Decision support systems","diagnostic radiography;feature extraction;image enhancement;learning (artificial intelligence);mammography;medical image processing;support vector machines","CLAHE approach;ELM;ESN;JSRT digital image database;LESH;SVM;X-ray images;chest radiographs;cognitive machine learning classifiers;cognitive machine learning techniques;contrast limited adaptive histogram equalization;echo state network;extreme learning machine;feature extraction;local energy-based shape histogram;lung cancer detection;mammogram images;radiograph image enhancement;support vector machine;wavelet based features","","","","","","","22-23 Aug. 2016","","IEEE","IEEE Conference Publications"
"Cross-platform machine learning characterization for task allocation in IoT ecosystems","Wanlin Cui; Y. Kim; T. S. Rosing","University of California San Diego, USA","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","7","With the emergence of the Internet of Things (IoT) and Big Data era, many applications are expected to assimilate a large amount of data collected from environment to extract useful information. However, how heterogeneous computing devices of IoT ecosystems can execute the data processing procedures has not been clearly explored. In this paper, we propose a framework which characterizes energy and performance requirements of the data processing applications across heterogeneous devices, from a server in the cloud and a resource-constrained gateway at edge. We focus on diverse machine learning algorithms which are key procedures for handling the large amount of IoT data. We build analytic models which automatically identify the relationship between requirements and data in a statistical way. The proposed framework also considers network communication cost and increasing processing demand. We evaluate the proposed framework on two heterogenous devices, a Raspberry Pi and a commercial Intel server. We show that the identified models can accurately estimate performance and energy requirements with less than error of 4.8% for both platforms. Based on the models, we also evaluate whether the resource-constrained gateway can process the data more efficiently than the server in the cloud. The results present that the less-powerful device can achieve better energy and performance efficiency for more than 50% of machine learning algorithms.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868438","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868438","","","Big Data;Internet of Things;energy consumption;internetworking;learning (artificial intelligence);network servers;power aware computing","Big Data;Internet of Things;IoT data;IoT ecosystems;Raspberry Pi;commercial Intel server;cross-platform machine learning characterization;data assimilation;data processing;energy requirements;heterogeneous computing devices;heterogenous devices;information extraction;network communication cost;performance estimate;performance requirements;resource-constrained gateway;task allocation","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Simulation of marketplace customer satisfaction analysis based on machine learning algorithms","A. A. Turdjai; K. Mutijarsa","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Ganesha Street 10, Bandung 40132, Indonesia","2016 International Seminar on Application for Technology of Information and Communication (ISemantic)","20170309","2016","","","157","162","Twitter can be a source of public opinion data and sentiment. Such data can be used efficiently for marketing or social studies. In this research addresses this issue by measuring net sentiment based on customer satisfaction through customer's sentiment analysis from Twitter data. Sample model is built and extracted from more than 3.000 raw Twitter messages data from March to April 2016 of top marketplace in Indonesia. We compared several algorithms, and the classification schemes. The sentiments are classified and compared using five different algorithms classification. There are, K-Nearest Neighbor, Logistic Regression, Naïve Bayes, Random Forest, and Support Vector Machine. The five machine learning can be applied to the Indonesian-language sentiment analysis. Preprocessing on the stages of tokenization, parsing, and stop word deletion of word frequency counting. Frequency of the word of the document used weighted by TF-IDF. The Random Forest, Support Vector Machine, and Logistic Regression generate better accuracy and stable compared with the Naïve Bayes and K-Nearest Neighbor. The results showed Support Vector Machine has accuracy 81.82% with 1000 sampling dataset and 85.4% with 2000 sampling dataset. This shows that the more the number of training data will improve the accuracy of the system. The Net Sentiment score for marketplace in Indonesia is 73%. This results also showed that customer satisfaction has average Net Promoter Score (NPS) 3.3%.","","Electronic:978-1-5090-2326-4; POD:978-1-5090-2327-1","10.1109/ISEMANTIC.2016.7873830","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873830","Algorithms Clasification;Customer Satisfaction;Machine Learning;Marketplace","Analytical models;Classification algorithms;Logistics;Machine learning algorithms;Sentiment analysis;Support vector machines;Text mining","Bayes methods;customer satisfaction;grammars;learning (artificial intelligence);pattern classification;regression analysis;sampling methods;sentiment analysis;social networking (online);support vector machines","Indonesian-language sentiment analysis;K-nearest neighbor;NPS;Naive Bayes;TF-IDF;Twitter message data;customer satisfaction;customer sentiment analysis;logistic regression;machine learning;marketplace customer satisfaction analysis simulation;net promoter score;net sentiment;net sentiment score;parsing;public opinion data source;public sentiment;random forest;sampling dataset;stop word deletion;support vector machine;tokenization;word frequency counting","","","","","","","5-6 Aug. 2016","","IEEE","IEEE Conference Publications"
"Detecting malicious URLs using machine learning techniques","F. Vanhoenshoven; G. Nápoles; R. Falcon; K. Vanhoof; M. Köppen","Universiteit Hasselt Campus Diepenbeek, Agoralaan Gebouw D, BE3590, Belgium","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","The World Wide Web supports a wide range of criminal activities such as spam-advertised e-commerce, financial fraud and malware dissemination. Although the precise motivations behind these schemes may differ, the common denominator lies in the fact that unsuspecting users visit their sites. These visits can be driven by email, web search results or links from other web pages. In all cases, however, the user is required to take some action, such as clicking on a desired Uniform Resource Locator (URL). In order to identify these malicious sites, the web security community has developed blacklisting services. These blacklists are in turn constructed by an array of techniques including manual reporting, honeypots, and web crawlers combined with site analysis heuristics. Inevitably, many malicious sites are not blacklisted either because they are too recent or were never or incorrectly evaluated. In this paper, we address the detection of malicious URLs as a binary classification problem and study the performance of several well-known classifiers, namely Naïve Bayes, Support Vector Machines, Multi-Layer Perceptron, Decision Trees, Random Forest and k-Nearest Neighbors. Furthermore, we adopted a public dataset comprising 2.4 million URLs (instances) and 3.2 million features. The numerical simulations have shown that most classification methods achieve acceptable prediction rates without requiring either advanced feature selection techniques or the involvement of a domain expert. In particular, Random Forest and Multi-Layer Perceptron attain the highest accuracy.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850079","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850079","","Bayes methods;Decision trees;Electronic mail;Security;Support vector machines;Uniform resource locators;Vegetation","Bayes methods;Internet;decision trees;learning (artificial intelligence);multilayer perceptrons;numerical analysis;pattern classification;security of data;support vector machines","Naıve Bayes;Uniform Resource Locator;Web crawlers;Web security community;World Wide Web;binary classification;blacklisting services;criminal activities;decision trees;honeypots;k-nearest neighbors;malicious URL detection;manual reporting;multilayer perceptron;numerical simulations;random forest;site analysis heuristics;support vector machines","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"EPOC aware energy expenditure estimation with machine learning","Soljee Kim; Kyoungwoo Lee; Junga Lee; J. Y. Jeon","Department of Computer Science, Yonsei University, Seoul, South Korea","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001585","001590","In 2014, 39 % of adults were overweight, and 13 % were obese. Clearly, knowing exact energy expenditure (EE) is important for sports training and weight control. Furthermore, excess post-exercise oxygen consumption (EPOC) must be included in the total EE. This paper presents a machine learning-based EE estimation approach with EPOC for aerobic exercise using a heart rate sensor. On a dataset acquired from 33 subjects, we apply machine learning algorithms using Weka machine learning toolkit. We could achieve 0.88 correlation and 0.23 kcal/min root mean square error (RMSE) with linear regression. The proposed model could be applied to various wearable devices such as a smartwatch.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844465","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844465","EPOC;application;energy expenditure estimation;exercise;healthcare;machine learning","Correlation;Estimation;Feature extraction;Heart rate;Machine learning algorithms;Standards;Training","estimation theory;health care;learning (artificial intelligence);mean square error methods;regression analysis;weight control","EPOC aware energy expenditure estimation;Weka machine learning toolkit;aerobic exercise;excess post-exercise oxygen consumption;heart rate sensor;linear regression;machine learning-based EE estimation;root mean square error;smart watch;sports training;wearable devices;weight control","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Data preparation step for automated diagnosis based on HRV analysis and machine learning","V. Timothy; A. S. Prihatmanto; K. H. Rhee","Departement of Electrical Engineering, Bandung Institute of Technology, Ganesha Street 10, 40132, Indonesia","2016 6th International Conference on System Engineering and Technology (ICSET)","20170213","2016","","","142","148","This paper describes the data preparation step of a proposed method for automated diagnosis of various diseases based on heart rate variability (HRV) analysis and machine learning. HRV analysis - consisting of time-domain analysis, frequency-domain analysis, and nonlinear analysis - is employed because its resulting parameters are unique for each disease and can be used as the statistical symptoms for each disease, while machine learning techniques are employed to automate the diagnosis process. The input data consist of electrocardiogram (ECG) recordings. The proposed method is divided into three main steps, namely dataset preparation step, machine learning step, and disease classification step. The dataset preparation step aims to prepare the training data for machine learning step from raw ECG signals, and to prepare the test data for disease classification step from raw RRI signals. The machine learning step aims to obtain the classifier model and its performance metric from the prepared dataset. The disease classification step aims to perform disease diagnosis from the prepared dataset and the classifier model. The implementation of data preparation step is subsequently described with satisfactory result.","","Electronic:978-1-5090-5089-5; POD:978-1-5090-5090-1","10.1109/ICSEngT.2016.7849639","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849639","Automated diagnosis;ECG signal;HRV analysis;RRI signal;machine learning","Diseases;Electrocardiography;Frequency-domain analysis;Heart rate variability;Pregnancy;Rail to rail inputs;Time-domain analysis","data preparation;diseases;electrocardiography;learning (artificial intelligence);medical signal processing;patient diagnosis;pattern classification","HRV analysis;automated diagnosis;data preparation step;disease classification step;electrocardiogram recordings;frequency-domain analysis;heart rate variability analysis;machine learning step;nonlinear analysis;raw ECG signals;raw RRI signals;statistical symptoms;time-domain analysis","","","","","","","3-4 Oct. 2016","","IEEE","IEEE Conference Publications"
"Nature and biology inspired approach of classification towards reduction of bias in machine learning","H. Liu; A. Gegov; M. Cocea","School of Computing, University of Portsmouth, Portsmouth, PO1 3HE, United Kingdom","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170309","2016","2","","588","593","Machine learning has become a powerful tool in real applications such as decision making, sentiment prediction and ontology engineering. In the form of learning strategies, machine learning can be specialized into two types: supervised learning and unsupervised learning. Classification is a special type of supervised learning task, which can also be referred to as categorical prediction. In other words, classification tasks involve predictions of the values of discrete attributes. Some popular classification algorithms include Naïve Bayes and K Nearest Neighbour. The above type of classification algorithms generally involves voting towards classifying unseen instances. In traditional ways, the voting is made on the basis of any employed statistical heuristics such as probability. In Naïve Bayes, the voting is made through selecting the class with the highest posterior probability on the basis of the values of all independent attributes. In K Nearest Neighbour, majority voting is usually used towards classifying test instances. This kind of voting is considered to be biased, which may lead to overfitting. In order to avoid such overfitting, this paper proposes to employ a nature and biology inspired approach of voting referred to as probabilistic voting towards reduction of bias. An extended experimental study is reported to show how the probabilistic voting can manage to effectively reduce the bias towards improvement of classification accuracy.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7872953","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872953","Data mining;K Nearest Neighbour;Machine learning;Naïve Bayes;Probabilistic classification","Biology;Classification algorithms;Mathematical model;Probabilistic logic;Probability;Training;Training data","Bayes methods;data reduction;learning (artificial intelligence);pattern classification","bias reduction;biology inspired approach;classification algorithm;machine learning;naïve Bayes;nature inspired approach;posterior probability;probabilistic voting;unsupervised learning","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Recognition of human activities using machine learning methods with wearable sensors","L. Cheng; Y. Guan; Kecheng Zhu; Yiyang Li","Research and Development Department, Kiwii Power Technology Corporation, Troy, NY, USA","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","7","Body activity recognition using wearable sensor technology has drawn more and more attentions over the past few decades. The complexity and variety of body activities makes it difficult to fast, accurately and automatically recognize body activities. To solve this problem, this paper formulates body activity recognition problem as a classification problem using data collected by wearable sensors. And three different machine learning algorithms, support vector machine, hidden markov model and artificial neural network are presented to recognize different body activities. Various numerical experiments on a real-world wearable sensors dataset are designed to verify the effectiveness of these classification algorithms. Finally the results demonstrate that all the three algorithms achieve satisfactory activity recognition performance.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868369","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868369","artificial neural network;body activity recognition;hidden markov model;support vector machine;wearable sensor","Activity recognition;Artificial neural networks;Classification algorithms;Hidden Markov models;Support vector machines;Training;Wearable sensors","hidden Markov models;learning (artificial intelligence);neural nets;object recognition;sensors;signal classification;support vector machines","artificial neural network;body activity recognition problem;classification problem;hidden Markov model;human activity recognition;machine learning methods;real-world wearable sensors dataset;support vector machine;wearable sensor technology","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Supervised machine learning for signals having RRC shaped pulses","M. Bari; H. Taher; S. S. Sherazi; M. DoroslovaCki","Electrical and Computer Engineering, The George Washington University, Washington, DC, USA","2016 50th Asilomar Conference on Signals, Systems and Computers","20170306","2016","","","652","656","Classification performances of the supervised machine learning techniques such as support vector machines, neural networks and logistic regression are compared for modulation recognition purposes. The simple and robust features are used to distinguish continuous-phase FSK from QAM-PSK signals. Signals having root-raised-cosine shaped pulses are simulated in extreme noisy conditions having joint impurities of block fading, lack of symbol and sampling synchronization, carrier offset, and additive white Gaussian noise. The features are based on sample mean and sample variance of the imaginary part of the product of two consecutive complex signal values.","","DVD:978-1-5386-3952-8; Electronic:978-1-5386-3954-2; POD:978-1-5386-3955-9","10.1109/ACSSC.2016.7869124","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869124","Machine learning;block fading;logistic regression;neural networks;support vector machines","Artificial neural networks;Fading channels;Frequency shift keying;Signal to noise ratio;Support vector machines;Training","AWGN;continuous phase modulation;fading;feature extraction;frequency shift keying;learning (artificial intelligence);neural nets;quadrature amplitude modulation;regression analysis;signal classification;support vector machines","QAM-PSK signals;RRC shaped pulse;additive white Gaussian noise;block fading;carrier offset;classification;continuous-phase FSK signals;extreme noisy condition;logistic regression;modulation recognition;neural networks;robust features;root-raised-cosine shaped pulse;sample mean;sample variance;supervised machine learning technique;support vector machines;symbol-sampling synchronization","","","","","","","6-9 Nov. 2016","","IEEE","IEEE Conference Publications"
"A fuzzy-based machine learning model for robot prediction of link quality","C. J. Lowrance; A. P. Lauf; M. Kantardzic","Department of Electrical Engineering and Computer Science, United States Military Academy, West Point, NY USA","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849899","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849899","","Adaptation models;Fuzzy sets;Measurement;Predictive models;Robots;Wireless communication;Wireless sensor networks","fuzzy set theory;learning (artificial intelligence);mobile robots;optimisation","fuzzy-based machine learning model;fuzzy-based prediction model;incremental learning;link quality;offset classifiers;real-world robot network;robot mobility;robot prediction;wireless channel","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Performance analysis of supervised machine learning algorithms for text classification","S. Z. Mishu; S. M. Rafiuddin","Department of Computer Science and Engineering, Rajshahi University of Engineering and Technology, Rajshahi 6204, Bangladesh","2016 19th International Conference on Computer and Information Technology (ICCIT)","20170223","2016","","","409","413","The demand of text classification is growing significantly in web searching, data mining, web ranking, recommendation systems and so many other fields of information and technology. This paper illustrates the text classification process on different dataset using some standard supervised machine learning techniques. Text documents can be classified through various kinds of classifiers. Labeled text documents are used to classify the text in supervised classifications. This paper applied these classifiers on different kinds of labeled documents and measures the accuracy of the classifiers. An Artificial Neural Network (ANN) model using Back Propagation Network (BPN) is used with several other models to create an independent platform for labeled and supervised text classification process. An existing benchmark approach is used to analysis the performance of classification using labeled documents. Experimental analysis on real data reveals which model works well in terms of classification accuracy.","","Electronic:978-1-5090-4090-2; POD:978-1-5090-4091-9","10.1109/ICCITECHN.2016.7860233","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860233","artificial neural network;back propagation network;machine learning;text classification","Artificial neural networks;Logistics;Measurement;Static VAr compensators;Support vector machines;Text categorization;Training","Internet;backpropagation;classification;data mining;neural nets;query formulation;recommender systems;text analysis","ANN;BPN;Web ranking;Web searching;artificial neural network;backpropagation network;data mining;labeled documents;performance analysis;recommendation systems;supervised machine learning;text classification","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Molecular Descriptors Selection and Machine Learning Approaches in Protein-Ligand Binding Affinity with Applications to Molecular Docking","C. E. Hsieh; G. S. Chen; J. S. Yeh; Y. L. Lin","Dept. of Appl. Chem., Providence Univ., Taichung, Taiwan","2016 International Computer Symposium (ICS)","20170220","2016","","","38","43","In this paper, we propose algorithms for biomolecular docking sites selection problem by various machine learning approaches with selective features reduction. The proposed method can reduce the number of various amino acid features before constructing machine learning prediction models. Given frame boxes with features, the proposed method analyzes the important features by correlation coefficients to LE values. The algorithm ranks these possible candidate locations on the receptor before launching AutoDock. Given a small molecular, namely ligand, it is a time-consuming task to compute the molecular docking against a large, relatively stationary molecule, or receptor. Our methods divide the surface area of receptor to several subspaces and evaluate these subspaces before choosing the promising subspaces to speed up the molecular docking simulation. The method is implemented upon the widely employed automated molecular docking simulation software package, AutoDock. The paper examines three different machine learning prediction models including the support vector machines (LIBSVM), deep neural networks (H2O), and the logistic regression model (AOD). The proposed affinity estimation algorithm, incorporated with a ligand-specific SVM prediction model, achieves about 4 folds faster comparing with original Autodock searching the whole surface of the receptor with similar binding energy score (LE, lowest engery) measurement. Furthermore, the proposed method can be easily parallelized in the implementation. Hadoop MapReduce frameworks are used in our experiments to parallelize the underlying massive computation works corresponding to ligand-receptor pairs examined under the experiment.","","Electronic:978-1-5090-3438-3; POD:978-1-5090-3439-0","10.1109/ICS.2016.0017","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858439","AutoDock;Hadoop;MapReduce;SVM;aaindex;algorithm;bioinformatics;deep learning;drug design;logistic regression;machine learning;molecular docking","Computational modeling;Correlation;Machine learning algorithms;Prediction algorithms;Predictive models;Proteins;Support vector machines","bioinformatics;learning (artificial intelligence);neural nets;parallel programming;proteins;regression analysis;software packages;support vector machines","AOD;AutoDock;H2O;Hadoop MapReduce framework;LE values;LIBSVM;affinity estimation algorithm;amino acid features;automated molecular docking simulation software package;binding energy score;biomolecular docking site selection problem;correlation coefficients;deep neural networks;ligand-receptor pairs;ligand-specific SVM prediction model;logistic regression model;machine learning prediction models;molecular descriptor selection;protein-ligand binding affinity;selective feature reduction;support vector machines","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Mining complex hyperspectral ALMA cubes for structure with neural machine learning","E. Merényi; J. Taylor; A. Isella","Department of Statistics, Department of Electrical and Computer Engineering, Rice University, Houston, Texas 77005, USA","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","9","Astronomy is producing the largest “Big Data” sets today and in the near future, with instruments such as the Atacama Large Millimeter and sub-millimeter Array (ALMA), the Large Synoptic Survey Telescope (LSST), and the Square Kilometer Array (SKA). These observations afford a deeper, wider, and more dynamic glimpse into the structure and composition of the universe than ever before. However, in addition to unprecedented volume, the data also exhibit unprecedented complexity, mandating new approaches for extracting and summarizing relevant information. ALMA data, in particular, challenges with very high dimensionality (measurements in a large number of spectral channels) where the dimensions represent both compositional information and velocities, and the high spectral resolution allows detailed interpretation of the kinematic structure of sources such as molecular clouds or protoplanetary disks. Traditional tools like moment maps can no longer fully exploit and visualize the rich information in these data. We present a neural map-based clustering approach that can utilize all spectral channels simultaneously and is capable of finding clusters of widely varying statistical properties, which are expected in these complex data sets. Many clustering methods, including modern graph segmentation algorithms, run into limitations when encountering such data. We demonstrate our tools, collectively named “NeuroScope”, through structure mining from an ALMA image of the protoplanetary disk HD142527. We highlight the advantages for both the emerging details and visualization. In addition, we explore an augmentation of leading graph segmentation algorithms with NeuroScope products, which can lead to efficient full automation of our clustering process for fast distillation of large data sets on-board or in archives.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849952","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849952","","Kinematics;Lattices;Manifolds;Neurons;Prototypes;Spatial resolution","astronomy computing;data mining;learning (artificial intelligence);neural nets;pattern clustering;radiotelescopes","Atacama large millimeter and sub-millimeter array;NeuroScope;graph segmentation algorithms;hyperspectral ALMA cubes;neural machine learning;neural map-based clustering approach;protoplanetary disk HD142527;radio telescope;spectral channels;structure mining","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Multi-objective parameter configuration of machine learning algorithms using model-based optimization","D. Horn; B. Bischl","TU Dortmund, Computational Statistics, 44227, Germany","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","The performance of many machine learning algorithms heavily depends on the setting of their respective hyperparameters. Many different tuning approaches exist, from simple grid or random search approaches to evolutionary algorithms and Bayesian optimization. Often, these algorithms are used to optimize a single performance criterion. But in practical applications, a single criterion may not be sufficient to adequately characterize the behavior of the machine learning method under consideration and the Pareto front of multiple criteria has to be considered. We propose to use model-based multi-objective optimization to efficiently approximate such Pareto fronts.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850221","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850221","","Algorithm design and analysis;Machine learning algorithms;Numerical models;Optimization;Prediction algorithms;Support vector machines;Tuning","Bayes methods;Pareto optimisation;evolutionary computation;learning (artificial intelligence);search problems","Bayesian optimization;Pareto front;evolutionary algorithms;grid search approach;machine learning;model-based optimization;multiobjective parameter configuration;random search approach","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning based performance development for diagnosis of breast cancer","B. Bektaş; S. Babur","Bilgisayar Teknolojileri B&#x00F6;l&#x00FC;m&#x00FC;, &#x0130;stanbul &#x00DC;niversitesi, &#x0130;stanbul, T&#x00FC;rkiye","2016 Medical Technologies National Congress (TIPTEKNO)","20170228","2016","","","1","4","Breast cancer is prevalent among women and develops from breast tissue. Early diagnosis and accurate treatment is vital to increase the rate of survival. Identification of genetic factors with microarray technology can make significant contributions to diagnosis and treatment process. In this study, several machine learning algorithms are used for Diagnosis of Breast Cancer and their classification performances are compared with each other. In addition, the active genes in breast cancer are identified by attribute selection methods and the conducted study show success rate 90,72 % with 139 feature.","","Electronic:978-1-5090-2386-8; POD:978-1-5090-2387-5","10.1109/TIPTEKNO.2016.7863129","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863129","breast cancer diagnosis;feature selection;machine learning;microarray","Biomedical informatics;Breast cancer;Lenses;Machine learning algorithms;Receivers;Support vector machines","cancer;feature selection;genetics;learning (artificial intelligence);medical diagnostic computing;patient diagnosis;pattern classification","active genes;breast cancer diagnosis;classification performances;feature selection;genetic factors;machine learning based performance development","","","","","","","27-29 Oct. 2016","","IEEE","IEEE Conference Publications"
"Increasing accuracy of traffic light color detection and recognition using machine learning","J. L. Binangkit; D. H. Widyantoro","School of Electrical Engineering and Informatics, Institute of Technology Bandung, Indonesia","2016 10th International Conference on Telecommunication Systems Services and Applications (TSSA)","20170306","2016","","","1","5","Traffic light detection is an important system because it can alert driver on upcoming traffic light so that he/she can anticipate a head of time. In this paper we described our work on detecting traffic light color using machine learning approach. Using HSV color representation, our approach is to extract features based on an area of X×X pixels. Traffic light color model is then created by applying a learning algorithm on a set of examples of features representing pixels of traffic and non-traffic light colors. The learned model is then used to classify whether an area of pixels contains traffic light color or not. Evaluation of this approach reveals that it significantly improves the detection performance over the one based on value-range color segmentation technique.","","Electronic:978-1-5090-5170-0; POD:978-1-5090-5171-7","10.1109/TSSA.2016.7871074","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7871074","color model;color segmentation;machine learning;traffic light;transportation","Autonomous vehicles;Classification algorithms;Color;Feature extraction;Image color analysis;Mathematical model;Support vector machines","feature extraction;image colour analysis;image recognition;image segmentation;learning (artificial intelligence)","HSV color representation;driver alert;feature extraction;machine learning approach;nontraffic light color pixel;traffic light color detection;traffic light color pixel;traffic light color recognition;value-range color segmentation technique","","","","","","","6-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Computational intelligence based machine learning methods for rule-based reasoning in computer vision applications","T. T. Dhivyaprabha; P. Subashini; M. Krishnaveni","Department of Computer Science, Avinashilingam Institute for Home Science and Higher Education for Women, Coimbatore, India","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","In robot control, rule discovery for understanding of data is of critical importance. Basically, understanding of data depends upon logical rules, similarity evaluation and graphical methods. The expert system collects training examples separately by exploring an anonymous environment by using machine learning techniques. In dynamic environments, future actions are determined by sequences of perceptions thus encoded as rule base. This paper is focused on demonstrating the extraction and application of logical rules for image understanding, using newly developed Synergistic Fibroblast Optimization (SFO) algorithm with well-known existing artificial learning methods. The SFO algorithm is tested in two modes: Michigan and Pittsburgh approach. Optimal rule discovery is evaluated by describing continuous data and verifying accuracy and error level at optimization phase. In this work, Monk's problem is solved by discovering optimal rules that enhance the generalization and comprehensibility of a robot classification system in classifying the objects from extracted attributes to effectively categorize its domain.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850050","Synergistic Fibroblast Optimization (SFO) algorithm;classification problem;machine learning;robot systems;rule discovery","Algorithm design and analysis;Classification algorithms;Fibroblasts;Machine learning algorithms;Optimization;Sociology;Training","expert systems;feature extraction;image classification;knowledge based systems;learning (artificial intelligence);optimisation;robot vision","Michigan approach;Monk's problem;Pittsburgh approach;SFO algorithm;artificial learning methods;computational intelligence based machine learning methods;computer vision applications;data understanding;error level;expert system;graphical methods;image understanding;logical rules extraction;optimal rule discovery;optimization phase;robot classification system;robot control;rule discovery;rule-based reasoning;similarity evaluation;synergistic fibroblast optimization algorithm","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"A new machine learning approach to select adaptive IMFs of EMD","M. B. Uddin; J. Uddin; R. Sultana; S. Islam","Dept. of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh","2016 2nd International Conference on Electrical, Computer & Telecommunication Engineering (ICECTE)","20170316","2016","","","1","4","An adaptive algorithm for selection of Intrinsic Mode Functions (IMF) of Empirical Mode Decomposition (EMD) is a time demand in the field of signal processing. This paper presents a new model of an effective algorithm for the adaptive selection of IMFs for the EMD. Our proposed model suggests the decomposition of an input signal using EMD, and the resultant IMFs are classified into two categories the relevant noise free IMFs and the irrelevant noise dominant IMFs using a trained Support Vector Machine (SVM). The Pearson Correlation Coefficient (PCC) is used for the supervised training of SVM. Noise dominant IMFs are then de-noised using the Savitzky-Golay filter. The signal is reconstructed using both noise free and de-noised IMFs. Our proposed model makes the selection process of IMFs adaptive and it achieves high Signal to Noise Ratio (SNR) while the Percentage of RMS Difference (PRD) and Max Error values are low. Experimental result attained up to 41.79% SNR value, PRD and Max Error value reduced to 0.814% and 0.081%, respectively compared to other models.","","Electronic:978-1-5090-5785-6; POD:978-1-5090-5786-3","10.1109/ICECTE.2016.7879617","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879617","Pearson's correlation coefficient;empirical mode decomposition;intrinsic mode functions;support vector machine","Computational modeling;Empirical mode decomposition;Mathematical model;Noise measurement;Signal to noise ratio;Support vector machines","adaptive filters;adaptive signal processing;learning (artificial intelligence);signal denoising;signal reconstruction;support vector machines","EMD;IMF adaptive selection process;PCC;PRD;Pearson correlation coefficient;SNR;SVM;Savitzky-Golay filter;empirical mode decomposition;input signal decomposition;intrinsic mode functions;irrelevant noise dominant IMF denoising;machine learning approach;percentage of RMS difference;relevant noise free IMFs;signal processing;signal reconstruction;signal to noise ratio;supervised training;support vector machine","","","","","","","8-10 Dec. 2016","","IEEE","IEEE Conference Publications"
"Supervised Machine Learning for Estimation of Target Aspect Angle From Bistatic Acoustic Scattering","E. M. Fischell; H. Schmidt","Laboratory for Autonomous Marine Sensing Systems, Massachusetts Institute of Technology (MIT), Cambridge, MA 02139 USA&#x00A0;(e-mail: emf43@mit.edu).","IEEE Journal of Oceanic Engineering","","2017","PP","99","1","11","When an aspect-dependent target is insonified by an acoustic source, distinct features are produced in the resulting bistatic scattered field. These features change as the aspect between the source and the target is varied. This paper describes the use of these features for estimation of the target aspect angle using data collected by an autonomous underwater vehicle (AUV). An experiment was conducted in November 2014 in Massachusetts Bay to collect data using a ship-based acoustic source producing 7–9-kHz linear frequency modulation (LFM) chirps insonifying a steel pipe. The true target orientation was unknown, as the target was dropped from the ship with no rotation control. The AUV Unicorn, fitted with a 16-element nose array, was deployed in data collection behaviors around the target, and the ship was moved to create two target aspects. A support vector machine regression model was trained using simulated scattering bistatic field data. This model was then used to estimate the target aspect angle from the data collected during the experiment. The difference between the estimates was consistent with experimental observations of relative source positioning. The simulation-based model appeared successful in estimating the target aspect angle despite uncertainties in target and source location and mismatch between true environment and simulation parameters.","0364-9059;03649059","","10.1109/JOE.2017.2650759","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856983","Machine learning;swimming robots;underwater acoustics","Acoustics;Antenna radiation patterns;Data models;Global Positioning System;Marine vehicles;Scattering;Steel","","","","","","","","20170215","","","IEEE","IEEE Early Access Articles"
"Adaptive Scheme for Caching YouTube Content in a Cellular Network: A Machine Learning Approach","S. M. S. Tanzil; W. Hoiles; V. Krishnamurthy","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver V6T 1Z4, Canada (e-mail: stanzil@ece.ubc.ca)","IEEE Access","","2017","PP","99","1","1","Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users’ quality of experience (QoE) and reduces network traffic. The algorithm accounts for users’ behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.","2169-3536;21693536","","10.1109/ACCESS.2017.2678990","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873292","Cellular Network;Content Caching;Extreme Learning Machines;Feature Selection;Mixed-Integer Linear Programming;YouTube","Adaptive systems;Base stations;Cellular networks;Delays;YouTube","","","","","","","","20170307","","","IEEE","IEEE Early Access Articles"
"From measurement to machine learning: Towards analysing cognition","K. R. Müller","Machine Learning Group, TU Berlin, Marchstr 23, 10587, Germany","2017 5th International Winter Conference on Brain-Computer Interface (BCI)","20170220","2017","","","53","54","This article discusses machine learning and BCI efforts of the BBCI team and co-workers with the general focus on analysing cognition. Due to the fact that many different aspects are reviewed, a high overlap to prior own contributions is not only unavoidable but intentional. When analysing cognition, it is often useful to combine information from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012). In BCI recently muItimodal fusion concepts have received great attention under the label hybrid BCI (Pfurtscheller et al., 2010, Müller-Putz et al. 2015, Dähne et al. 2015, Fazli et al. 2012, 2015) or as data analysis technique for extracting (non-) linear relations between data (see e.g. Biessmann et al., 2011, Fazli et al., 2015, Dähne et al., 2015, Samek et al. 2016a). They are rooted in the modem machine leaming and signal processing techniques that are now available fot, analysing EEG, for decoding mental states etc. (see Müller et al. 2008, Bünau et al. 2009, Tomioka and Müller, 2010, Blankertz et al., 2008, 2011, 2016, Lemm et al., 2011, for recent reviews and contributions to Machine Learning for BCI, see Samek et al. 2014 for, a review on robust methods). Note that fusing information has also been a very common practice in the sciences and engineering (Waltz and Llinas, 1990). The talk will discuss recent technical work from the BBCI group that are intended to broaden the application spectrum of Brain Computer Interfaces. I will first report on a novel simuItaneous measurement method for NIRS and EEG that is miniaturized, wireless and has low noise properties (von Lühmann et al. 2016); it is well suited for performing future out-of-Iab BCI experiments, In addition, I will expand on technical advances in unsupervised ML-based BCI analysis that allows to use of label proportions and can thus help- to avoid the usual calibration phase (Hübner et al. 2016, cf. also Kindermans et al. 2014). Finally, if time permits, an application of the explanation framework fot, deep neural networks (Baehrens et al. 2010, Bach et al. 2015, Lapuschkin et al. 2016a and 2016b, Samek et al. 2016b, Montavon et al. 2016) to BCI data is given (Sturm et al. 2016). This abstract is based on joint work with Wojciech Samek, Benjamin Blankertz, Gabriel Curio, Michael Tangermann, Siamac Fazli, Vadim Nikulin, Gregoire Montavon, Sebastian Bach/Lapuschkin, Irene Sturm, Pieter-Jan Kindermans, Alex von Lühmann and many other members of the Berlin Brain Computer Interface team, the machine learning groups and many more esteemed collaborators, We greatly acknowledge funding by BMBF, EU, DFG and NRF.","","Electronic:978-1-5090-5096-3; POD:978-1-5090-5097-0","10.1109/IWW-BCI.2017.7858157","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858157","","Biological system modeling;Biomedical monitoring;Brain modeling;Computational modeling;Monitoring;Neuroscience","brain-computer interfaces;learning (artificial intelligence)","BCI;EEG;brain-computer interface;cognition analysis;machine learning;muItimodal fusion concepts;unsupervised ML-based BCI analysis","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"An adaptive learning method of Restricted Boltzmann Machine by neuron generation and annihilation algorithm","S. Kamada; T. Ichimura","Graduate School of Information Sciences, Hiroshima City University, 3-4-1, Ozuka-Higashi, Asa-Minami-ku, 731-3194, Japan","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","001273","001278","Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based model of artificial neural network for unsupervised learning. Recently, RBM is well known to be a pre-training method of Deep Learning. In addition to visible and hidden neurons, the structure of RBM has a number of parameters such as the weights between neurons and the coefficients for them. Therefore, we may meet some difficulties to determine an optimal network structure to analyze big data. In order to evade the problem, we investigated the variance of parameters to find an optimal structure during learning. For the reason, we should check the variance of parameters to cause the fluctuation for energy function in RBM model. In this paper, we propose the adaptive learning method of RBM that can discover an optimal number of hidden neurons according to the training situation by applying the neuron generation and annihilation algorithm. In this method, a new hidden neuron is generated if the energy function is not still converged and the variance of the parameters is large. Moreover, the inactivated hidden neuron will be annihilated if the neuron does not affect the learning situation. The experimental results for some benchmark data sets were discussed in this paper.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844417","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844417","","Adaptation models;Biological neural networks;Convergence;Learning systems;Machine learning;Neurons;Training","Boltzmann machines;stochastic processes;unsupervised learning","RBM;adaptive learning;annihilation algorithm;artificial neural network;energy function;generative stochastic energy-based model;hidden neurons;neuron generation;optimal network structure;restricted Boltzmann machine;unsupervised learning","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Analysis of machine learning solutions to detect malware in android","Q. Jamil; M. A. Shah","Department of Computer Science, Bahria Univeristy, Islamabad, Pakistan","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","226","232","The recent use of mobile devices and increase in connectivity technologies(GSM, GPRS, Bluetooth & WiFi enable us to access abundant services. These services and communication channels are exploited by susceptibilities immensely. Hence, for malware writers, mobile devices became ideal target. Applications installed on smartphones request access to the sensitive information which may lead to security vulnerabilities. Different malwares named as Botnet, Backdoor, Rootkits, Virus, Worms, and Trojans can attack android Operating System (OS). Due to these attacks privacy of the users is compromised. This paper surveys the already proposed security solutions by using machine learning approaches especially focused on supervised, semi supervised and unsupervised approaches. We also analyzed the architecture of these approaches and present the taxonomy of Android OS based security solutions. Our aim is to provide the best approach for malware detection in Android OS.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845073","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845073","Analysis;Android;Machine learning;Malware detection","Androids;Computer architecture;Feature extraction;Humanoid robots;Machine learning algorithms;Malware;Smart phones","Android (operating system);data privacy;invasive software;learning (artificial intelligence);smart phones","Android OS;Android operating system;Backdoor;Bluetooth;Botnet;GPRS;GSM;Rootkits;Trojans;Virus;WiFi;Worms;communication channels;connectivity technologies;machine learning solutions;malware detection;malware writers;mobile devices;sensitive information;smartphones;user privacy","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Recent machine learning based approaches for disease detection and classification of agricultural products","M. K. Tripathi; D. D. Maktedar","Department of Computer Engineering, D. Y. Patil College of Engineering, Ambi, Pune, Maharashtra-India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","6","Disease infection to agricultural products like plants, fruits and vegetables, results in degradation of quality and quantity of agriculture products. This directly affects the financial source of agriculturists and the human health. Hence, detection of diseases in plants, fruits and vegetables crops at early stages of development leads to reduce loss of yield and quality. The traditional approaches for disease detection requires continues monitoring and observation of farms either by farmers or by experts. But, it is very costly and time consuming. In last few years, various researchers have focused into this area to provide optimized solution. Popular methods have utilized machine learning, image processing and classification based approaches to identify and detect the diseases on agricultural products. The existing techniques for disease detection have utilized various image processing methods followed by various classification techniques. This paper presents an overview of existing reported techniques useful in detection of diseases of agricultural products. A comparative study of different methods based on the type of agricultural product, methodology and its efficiency together with the advantages and disadvantages is also included in this paper.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7860043","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860043","agricultural product disease detection;agricultural science;classification;image processing","Agricultural products;Diseases;Feature extraction;Image color analysis;Support vector machines","agricultural products;image classification;learning (artificial intelligence);object detection;plant diseases","agricultural product disease classification;agricultural product disease detection;crops;disease identification;disease infection;fruits;image classification;image processing;machine learning based approaches;plants;quality degradation;quantity degradation;vegetables","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Machine learning approaches for supporting patient-specific cardiac rehabilitation programs","D. Lofaro; M. C. Groccia; R. Guido; D. Conforti; S. Caroleo; G. Fragomeni","de-Health Lab, DIMEG, University of Calabria, Rende, Italy","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","149","152","Cardiac rehabilitation is a well-recognised non-pharmacological intervention that prevents the recurrence of cardiovascular events. Previous studies investigated the application of data mining techniques for the prediction of the rehabilitation outcome in terms of physical, but fewer reports are focused on using predictive models to support clinicians in the choice of a patient-specific rehabilitative treatment path. Aim of the work was to derive a prediction model for help clinicians in the prescription of the rehabilitation program. We enrolled 129 patients admitted for cardiac rehabilitation after a major cardiovascular event. Data on anthropometric measures, surgical procedure and complications, comorbidities and physical performance scales were collected at admission. The prediction outcome was the rehabilitation program divided in four different paths. Different algorithms were tested to find the best predictive model. Models performance were measured by prediction accuracy. Mean model accuracy was 0.790 (SD 0.118). Best model selected was Lasso regression showing an average classification accuracy on test set of0.935. Data mining techniques have shown to be a reliable tool for support clinicians in the decision of cardiac rehabilitation treatment path.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868701","","Data mining;Heart;Kernel;Medical services;Predictive models;Support vector machines;Training","cardiovascular system;data mining;learning (artificial intelligence);patient rehabilitation;regression analysis;surgery","Lasso regression;anthropometric measures;cardiac rehabilitation;cardiovascular event;data mining techniques;machine learning approaches;nonpharmacological intervention;patient-specific rehabilitative treatment path;surgical procedure","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Using machine learning to predict if a profiled lay rescuer can successfully deliver a shock using a public access automated external defibrillator?","R. R. Bond; H. Torney; P. O'Hare; L. Davis; B. Delafont; H. McReynolds; A. McLister; B. McCartney; R. Di Maio; D. D. Finlay; D. Guldenring; J. McLaughlin; D. McEneaney","Ulster University, Jordanstown, Northern Ireland, United Kingdom","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","1181","1184","A public access automated external defibrillator (AED) is a device that is intended to be used by lay rescuers in an event where a member of the public experiences a sudden cardiac arrest due to a severe ventricular arrhythmia. Therefore, it is imperative that the human-machine interface of an AED is optimized in terms of its usability and intuitive design. This study involved the recruitment of362 subjects (lay people) in a shopping mall to undertake the task of using an AED in a simulated environment as facilitated by a `sensorised' manikin and an AED that was developed by HeartSine Technologies. We found that a large proportion (91.44%) of lay people can successfully use an AED in a simulated emergency scenario to deliver a successful shock. We also found that CPR training did not provide greater likelihood for shock success whilst those with AED training did. Exploratory data analysis and machine learning were used to determine if demographics and other variables are potential predictors for delivering a successful shock using an AED. We found that user demographics and educational attainment were not predictive for AED `usage' success, which is reassuring since the objective of the medical industry is to develop AEDs that are intuitive to any member of the public.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868959","","Decision trees;Electric shock;Entropy;Predictive models;Training;Usability","cardiovascular system;defibrillators;diseases;learning (artificial intelligence);man-machine systems;medical computing;patient treatment","AED training;AED usage success;CPR training;HeartSine Technologies;demographics;educational attainment;exploratory data analysis;human-machine interface;machine learning;profiled lay rescuer;public access automated external defibrillator;sensorised manikin;severe ventricular arrhythmia;simulated emergency scenario;simulated environment;successful shock;sudden cardiac arrest","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Performance analysis of machine learning and pattern recognition algorithms for Malware classification","B. N. Narayanan; O. Djaneye-Boundjou; T. M. Kebede","Department of Electrical and Computer Engineering, University of Dayton, Dayton, Ohio 45469 - 0232, USA","2016 IEEE National Aerospace and Electronics Conference (NAECON) and Ohio Innovation Summit (OIS)","20170216","2016","","","338","342","Anti-Malware industry faces the challenge of evaluating huge amount of data for potential malicious contents. This is due to the fact that hackers introduce polymorphism to the existing malicious groups/classes. Effective feature extraction and classification of malware data is necessary to tackle such issues. In this paper, we visualize viruses in an image as they capture minor changes while retaining a global structure. Later, we implement Principal Component Analysis (PCA) method for feature extraction. Based on extracted PCA features, we study the performance of various Artificial Neural Network (ANN) algorithms along with K-Nearest Neighbors (kNN) and Support Vector Machine (SVM) classification techniques for identification of malware data into their respective classes. We use k-fold validation to gauge the effectiveness of our approach. The study makes use of the publicly available Kaggle database provided by Microsoft for the Microsoft Malware Classification Challenge (BIG 2015).","","Electronic:978-1-5090-3441-3; POD:978-1-5090-3442-0","10.1109/NAECON.2016.7856826","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856826","K-Nearest Neighbors;Malware Detection;Neural Network;Principle Component Analysis;Support Vector Machine","Artificial neural networks;Data visualization;Feature extraction;Malware;Principal component analysis;Support vector machines;Training","feature extraction;image classification;invasive software;learning (artificial intelligence);neural nets;principal component analysis;support vector machines","ANN algorithm;BIG 2015;Kaggle database;Microsoft Malware Classification Challenge;PCA method;SVM;antimalware industry;artificial neural network algorithm;feature classification;feature extraction;k-nearest neighbor;kNN;machine learning;malware classification;malware data identification;pattern recognition algorithm;polymorphism;principal component analysis method;support vector machine","","","","","","","25-29 July 2016","","IEEE","IEEE Conference Publications"
"Application of Machine Learning for Optimization of 3-D Integrated Circuits and Systems","S. J. Park; B. Bae; J. Kim; M. Swaminathan","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA 30332 USA. He is now with Samsung Electronics, Co., Ltd., Hwaseong 18448, South Korea.","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2017","PP","99","1","10","The 3-D integration helps improve performance and density of electronic systems. However, since electrical and thermal performance for 3-D integration is related to each other, their codesign is required. Machine learning, a promising approach in artificial intelligence, has recently shown promise for addressing engineering optimization problems. In this paper, we apply machine learning for the optimization of 3-D integrated systems where the electrical performance and thermal performance need to be analyzed together for maximizing performance. In such systems, modeling can be challenging due to the multiscale geometries involved, which increases computation time per iteration. In this paper, we show that machine learning can be applied to such systems where multiple parameters can be optimized to achieve the desired performance using the minimum number of iterations. These results have been compared with other promising optimization methods in this paper. The results show that on an average, 4.4%, 31.1%, and 6.9% improvement in temperature gradient, CPU time, and skew are possible using machine learning, as compared with other methods.","1063-8210;10638210","","10.1109/TVLSI.2017.2656843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850943","3-D IC;Bayesian optimization (BO);electrical-thermal simulation;machine learning;temperature gradient;thermal-induced skew.","Clocks;Computational modeling;Integrated circuit modeling;Optimization;Semiconductor device measurement;Temperature distribution;Temperature measurement","","","","","","","","20170210","","","IEEE","IEEE Early Access Articles"
"Machine learning paradigms for weed mapping via unmanned aerial vehicles","M. Pérez-Ortiz; P. A. Gutiérrez; J. M. Peña; J. Torres-Sánchez; F. López-Granados; C. Hervás-Martínez","Dept. of Quantitative Methods, Universidad Loyola Andaluc&#x00ED;a, 14004 C&#x00F3;rdoba, Spain","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","This paper presents a novel strategy for weed monitoring, using images taken with unmanned aerial vehicles (UAVs) and concepts of image analysis and machine learning. Weed control in precision agriculture designs site-specific treatments based on the coverage of weeds, where the key is to provide precise weed maps timely. Most traditional remote platforms, e.g. piloted planes or satellites, are, however, not suitable for early weed monitoring, given their low temporal and spatial resolutions, as opposed to he ultra-high spatial resolution of UAVs. The system here proposed makes use of UAV-imagery and is based on: 1) Divide the image, 2) compute and binarise the vegetation indexes, 3) detect crop rows, 4) optimise the parameters and 4) learn a classification model. Since crops are usually organised in rows, the use of a crop row detection algorithm helps to separate properly weed and crop pixels, which is a common handicap given the spectral similitude of both. Several artificial intelligence paradigms are compared in this paper to identify the most suitable strategy for this topic (i.e. unsupervised, supervised and semi-supervised approaches). Our experiments also study the effect of different parameteres: the flight altitude, the sensor and the use of previously trained models at a different height. Our results show that 1) very promising performance can be obtained, even when using very few labelled data and 2) the classification model can be learnt in a subplot of the experimental field at low altitude and then applied to the whole field at a higher height, which simplifies the whole process. These results motivate the use of this strategy to design weed monitoring strategies for early post-emergence weed control.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849987","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849987","","Agriculture;Cameras;Indexes;Monitoring;Remote sensing;Spatial resolution;Vegetation mapping","autonomous aerial vehicles;crops;learning (artificial intelligence)","UAV-imagery;artificial intelligence paradigms;classification model;crop pixels;crop row detection algorithm;crop rows;flight altitude;image analysis;machine learning paradigms;piloted planes;post-emergence weed control;precision agriculture;remote platforms;satellites;site-specific treatments;spectral similitude;ultra-high spatial resolution;unmanned aerial vehicles;vegetation indexes;weed mapping;weed monitoring","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors","K. Ma; X. Li; S. R. Srinivasa; Y. Liu; J. Sampson; Y. Xie; V. Narayanan","Dept. of Computer Science and Engineering, Pennsylvania State University","2017 22nd Asia and South Pacific Design Automation Conference (ASP-DAC)","20170220","2017","","","678","683","Batteryless energy harvesting systems face a twofold challenge in converting incoming energy into forward progress. Not only must such systems contend with inherently weak and fluctuating power sources, but they have very limited temporal windows for capitalizing on transitory periods of above-average power. To maximize forward progress, such systems should aggressively consume energy when it is available, rather than optimizing for peak averagecase efficiency. However, there are multiple ways that a processor can trade between consumption and performance. In this paper, we examine two approaches, frequency scaling and resource scaling, and develop a predictor-driven scheme for dynamically allocating future power budgets between the two techniques. We show that our solution can achieve forward progress equal to 2.08X of the baseline Out-of-Order (OoO) processor with the best static configuration of frequency and resources. The combined technique outperforms either technique in isolation, with frequency-only and resource-only approaches achieving 1.43X and 1.61X forward progress improvements, respectively.","","Electronic:978-1-5090-1558-0; POD:978-1-5090-1559-7","10.1109/ASPDAC.2017.7858402","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858402","Internet of Things;Nonvolatile processor;energy harvesting;machine learning;power-adaptive microarchitecture","Biological neural networks;Computer architecture;Energy harvesting;Energy storage;Microarchitecture;Power demand;Program processors","energy harvesting;power aware computing;storage management chips","Spendthrift;ambient energy harvesting nonvolatile processors;batteryless energy harvesting system;frequency scaling;machine learning based resource;out-of-order processor;predictor-driven scheme;resource scaling;static configuration","","","","","","","16-19 Jan. 2017","","IEEE","IEEE Conference Publications"
"A reinforcement learning approach for dynamic selection of virtual machines in cloud data centres","M. Duggan; K. Flesk; J. Duggan; E. Howley; E. Barrett","National University of Ireland, Galway","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","92","97","In recent years Machine Learning techniques have proven to reduce energy consumption when applied to cloud computing systems. Reinforcement Learning provides a promising solution for the reduction of energy consumption, while maintaining a high quality of service for customers. We present a novel single agent Reinforcement Learning approach for the selection of virtual machines, creating a new energy efficiency practice for data centres. Our dynamic Reinforcement Learning virtual machine selection policy learns to choose the optimal virtual machine to migrate from an over-utilised host. Our experiment results show that a learning agent has the abilities to reduce energy consumption and decrease the number of migrations when compared to a state-of-the-art approach.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845053","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845053","Energy;Live Migration;Reinforcement Learning","Cloud computing;Energy consumption;Learning (artificial intelligence);Quality of service;Resource management;Servers;Virtual machining","cloud computing;computer centres;energy conservation;learning (artificial intelligence);power aware computing;virtual machines","cloud data centres;dynamic virtual machines selection;energy consumption;energy efficiency practice;learning agent;machine learning techniques;over-utilised host;reinforcement learning approach","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"A novel application of machine learning techniques for activity-based load disaggregation in rural off-grid, isolated solar systems","V. Mehra; R. Ram; C. Vergara","Tata Center for Technology and Design, Massachusetts Institute of Technology","2016 IEEE Global Humanitarian Technology Conference (GHTC)","20170216","2016","","","372","378","In power systems and electricity markets, accurate monitoring and prediction of electricity demand is important in order to manage real-time load balancing and for distribution and transmission planning. In the context of rural electrification, the uncertainty of both supply - often with 100% renewables - and demand is large and the central reason that solar-based micro-grids and home systems can be expensive. Oversizing battery storage and solar panel size or limiting the electricity available to each user can alleviate this uncertainty but increases system costs. Historical data for solar irradiance is available and allows quantification of the uncertainty associated with supply. However, electricity demand in rural households is not well characterized in developing world contexts. In an ideal scenario, the data should be captured on a per-household level and must be measured on a sufficiently fine time resolution to translate the demand onto individual, user activities. This massive quantity of household demand data requires automatic tools to convert time series power usage data into data on the use of individual appliances. This paper presents a methodology, based on data acquired in individual, isolated solar home systems in Jharkhand, India, on utilizing classification and clustering algorithms to create activity-based models that can be used to conduct load forecasts. Additional statistical data analysis can yield insights on users' power consumption behavior in relation to exogenous variables such as time of day and conditioned on ambient air temperature.","","Electronic:978-1-5090-2432-2; POD:978-1-5090-2433-9; USB:978-1-5090-2431-5","10.1109/GHTC.2016.7857308","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857308","","Conferences;Decision support systems;Home appliances;Layout;Voltage measurement","building integrated photovoltaics;distributed power generation;learning (artificial intelligence);load forecasting;pattern classification;pattern clustering;power consumption;power engineering computing;power markets;statistical analysis","India;Jharkhand;activity-based load disaggregation;distribution planning;electricity demand;electricity markets;isolated solar systems;machine learning;real-time load balancing;rural electrification;rural households;rural off-grid;solar home systems;solar panel size;solar-based microgrids;statistical data analysis;transmission planning;user power consumption behavior","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning and Conceptual Reasoning for Inconsistency Detection","J. A. Otaibi; Z. Safi; A. Hassaïne; F. Islam; A. Jaoua","Computer Science and Engineering Department, College of Engineering, Qatar University, Doha, Qatar","IEEE Access","20170301","2017","5","","338","346","This paper focuses on detecting inconsistencies within text corpora. It is a very interesting area with many applications. Most existing methods deal with this problem using complicated textual analysis, which is known for not being accurate enough. We propose a new methodology that consists of two steps, the first one being a machine learning step that performs multilevel text categorization. The second one applies conceptual reasoning on the predicted categories in order to detect inconsistencies. This paper has been validated on a set of Islamic advisory opinions (also known as fatwas). This domain is gaining a large interest with users continuously checking the authenticity and relevance of such content. The results show that our method is very accurate and can complement existing methods using the linguistic analysis.","2169-3536;21693536","","10.1109/ACCESS.2016.2642402","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792212","Information extraction;conceptual reasoning;hyper rectangular decomposition;inconsistency detection;text categorization","Cognition;Context;Density measurement;Natural language processing;Ontologies;Pragmatics;Text categorization","inference mechanisms;learning (artificial intelligence);planning (artificial intelligence);text analysis","Islamic advisory opinions;complicated textual analysis;conceptual reasoning;fatwas;inconsistency detection;linguistic analysis;machine learning;multilevel text categorization;text corpora","","","","","","20161220","2017","","IEEE","IEEE Journals & Magazines"
"Detecting jute plant disease using image processing and machine learning","Z. N. Reza; F. Nuzhat; N. A. Mahsa; M. H. Ali","Department of Computer Science and Engineering, BRAC University, Dhaka, Bangladesh","2016 3rd International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)","20170309","2016","","","1","6","Detecting stem diseases of plants by image analysis are still in an inchoate state in the research field. This research has been conducted on detecting the stem diseases of jute plants which is one of the most important cash crops in some of the Asian countries. An automated system based on an Android application has been implemented to take pictures of the disease affected stems of jute plants and send them to the dedicated server for assaying. On the server side, the affected portion from the image will be segmented using customized thresholding formula based on hue-based segmentation. The consequential feature values will be extracted from the segmented portion for texture analysis using color co-occurrence methodology. The extracted values will be compared with the sample values stored in the pre-defined database which will lead the disease to be identified and classified using Multi-SVM classifier. At the final step, the classification result along with the necessary control measures will be sent back to the farmer within three seconds through the application on their phone.","","Electronic:978-1-5090-2906-8; POD:978-1-5090-2907-5","10.1109/CEEICT.2016.7873147","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873147","Android application;Multi-SVM classifier;color co-occurrence methodology;hue based segmentation;texture analysis","Diseases;Feature extraction;Image color analysis;Image segmentation;Mobile applications;Servers","Android (operating system);biology computing;crops;image segmentation;image texture;learning (artificial intelligence);support vector machines","Android application;Asian countries;Multi-SVM classifier;automated system;cash crops;consequential feature values;customized thresholding formula;dedicated server;detecting jute plant disease;detecting stem diseases;disease affected stems;extracted values;image analysis;image processing;machine learning;research field;stem diseases;texture analysis","","","","","","","22-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"A Machine Learning Based Framework for Verification and Validation of Massive Scale Image Data","J. Ding; X. H. Hu; V. Gudivada","Department of Computer Science, East Carolina University, Greenville, NC, 27858 USA (e-mail: dingj@ecu.ed).","IEEE Transactions on Big Data","","2017","PP","99","1","1","Big data validation and system verification are crucial for ensuring the quality of big data applications. However, a rigorous technique for such tasks is yet to emerge. During the past decade, we have developed a big data system called CMA for investigating the classification of biological cells based on cell morphology that is captured in diffraction images. CMA includes a group of scientific software tools, machine learning algorithms, and a large scale cell image repository. We have also developed a framework for rigorous validation of the massive scale image data and verification of both the software systems and machine learning algorithms. Different machine learning algorithms integrated with image processing techniques were used to automate the selection and validation of the massive scale image data in CMA. An experiment based technique guided by a feature selection algorithm was introduced in the framework to select optimal machine learning features. An iterative metamorphic testing approach is applied for testing the scientific software. Due to the non-testable characteristic of the scientific software, a machine learning approach is introduced for developing test oracles iteratively to ensure the adequacy of the test coverage criteria. Performance of the machine learning algorithms is evaluated with the stratified N-fold cross validation and confusion matrix. We describe the design of the proposed framework with CMA as the case study. The effectiveness of the framework is demonstrated through verifying and validating the data set, software systems and algorithms in CMA.","","","10.1109/TBDATA.2017.2680460","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875094","Big Data;Deep Learning;Diffraction Image;Machine Learning;Metamorphic Testing","Big data;Diffraction;Machine learning algorithms;Morphology;Software;Testing;Three-dimensional displays","","","","","","","","20170309","","","IEEE","IEEE Early Access Articles"
"Stock market sentiment analysis based on machine learning","V. S. Rajput; S. M. Dubey","Department of Computer Science and Engineering, Institute of Technology and Management, Gwalior, India","2016 2nd International Conference on Next Generation Computing Technologies (NGCT)","20170316","2016","","","506","510","Opinion mining is used as scrutiny of public opinions. The growth of social network has put onward the views of the general public on a larger scale and in an open manner. The comments, views and opinions act as deciding factors whether these are positive opinion or negative opinion. Guessing about the opinions' polarity is not a good idea, so, an intelligent system need to be introduced to categorize the views. Sentiment analysis thus emerged as a highlighted area in data mining. The opinions are judged on the basis of unsupervised and supervised learning. Supervised learning has unwavering to be superior to unsupervised mode of view verdict. The proposed paper has given a comparative study of naïve Bayes and SVM on the opinions of the reviewers of the stock market. No system has been created for sentiment analysis in the share market. Thus, new field is chosen and worked upon and its result can helps the user to take better decisions in the field of stock market.","","DVD:978-1-5090-3256-3; Electronic:978-1-5090-3257-0; POD:978-1-5090-3258-7","10.1109/NGCT.2016.7877468","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877468","Opinion Mining;SVM;Sentiment;Stock Market;Supervised Learning","Algorithm design and analysis;Data mining;Motion pictures;Next generation networking;Supervised learning;Support vector machines;Taxonomy","Bayes methods;data mining;sentiment analysis;stock markets;support vector machines;unsupervised learning","SVM;data mining;intelligent system;machine learning;naïve Bayes method;negative opinion;opinion mining;opinion polarity;positive opinion;public opinions;share market;social network;stock market sentiment analysis;supervised learning;unsupervised learning;user comments;user opinions;user views","","","","","","","14-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Machine learning methods for the health-indexing and ranking of underground distribution cables and joints","J. Heres; R. Stijl; F. Reinders","Alliander N.V., the Netherlands","CIRED Workshop 2016","20170223","2016","","","1","4","An aging asset population and a less predictable volatile electricity consumption and production pattern urge DSOs to get insight in the condition of their medium voltage (MV) and low voltage (LV) networks. Because visual inspections of underground networks are impossible and the number of measurements is still very limited, this paper proposes a method to rank underground assets by looking for trends and patterns in historical outages with help of Machine Learning methods. Nine years of outages of MV and LV cables and joints in the network of a large Dutch DSO are analysed. A model is developed that couples each outage to the asset most probable responsible. Twenty-two different datasets are coupled with the asset database, ranging from load estimates of the asset to distance-to-a-railway. Each set could contain data that explains or correlates to some of the outages. Several Machine Learning techniques are benchmarked. The final model, created by the Random Forest algorithm, is applied to rank current assets. It is operational to determine the positioning of an online monitoring system in the DSO's MV network.","","","10.1049/cp.2016.0751","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7861353","CBAM;Condition Based Assetmanagement;Condition management;Machine Learning;Underground Grid","","asset management;learning (artificial intelligence);power distribution faults;power distribution lines;power distribution reliability;power engineering computing;underground cables","DSO MV network;LV cable outage;LV joint outage;MV cable outage;MV joint outage;aging asset;asset database;health-indexing and ranking;load estimation;low-voltage network;machine learning method;medium-voltage network;online monitoring system;production pattern;random forest algorithm;underground distribution cable;underground distribution joint;underground network;volatile electricity consumption","","","","","","","14-15 June 2016","","IET","IET Conference Publications"
"A novel approach for classification of normal/abnormal phonocardiogram recordings using temporal signal analysis and machine learning","S. Vernekar; S. Nair; D. Vijaysenan; R. Ranjan","NIT Surathkal, Surathkal, India","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","1141","1144","This paper discusses a novel approach used for classification of phonocardiogram (PCG) excerpts into normal and abnormal classes as a part of Physionet 2016 challenge [10]. The dataset used for the competition comprises of cardiac abnormalities such as mitral valve prolapse (MVP), benign murmurs, aortic diseases, coronary artery disease, miscellaneous pathological conditions etc. [3], We present the approach used for classification from a general machine learning application standpoint, giving details on feature extraction, type of classifiers used comparing their performances individually and in combination. We propose a technique which leverages previous research on feature extraction with a novel approach to modeling temporal dynamics of the signal using Markov chain analysis [7,9]. These newly introduced Markov features along with other statistical and frequency domain features, trained over an ensemble of artificial neural networks and gradient boosting trees, with bagging, gave us an accuracy of 82% on the validation dataset provided in the competition and was consistent with the test data with the best result of 78%.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868949","","Feature extraction;Frequency-domain analysis;Heart;Markov processes;Phonocardiography;Training;Training data","Markov processes;blood vessels;diseases;feature extraction;learning (artificial intelligence);medical signal processing;neural nets;phonocardiography;signal classification;statistical analysis","Markov chain analysis;aortic diseases;artificial neural networks;benign murmurs;cardiac abnormalities;coronary artery disease;feature extraction;frequency domain features;gradient boosting trees;machine learning;miscellaneous pathological conditions;mitral valve prolapse;phonocardiogram recording classification;statistical domain features;temporal signal analysis","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Techniques for Optical Performance Monitoring From Directly Detected PDM-QAM Signals","J. Thrane; J. Wass; M. Piels; J. C. M. Diniz; R. Jones; D. Zibar","DTU&#160;Fotonik, Department of Photonics Engineering, Technical University of Denmark, Kgs. Lyngby, Denmark","Journal of Lightwave Technology","20170303","2017","35","4","868","875","Linear signal processing algorithms are effective in dealing with linear transmission channel and linear signal detection, whereas the nonlinear signal processing algorithms, from the machine learning community, are effective in dealing with nonlinear transmission channel and nonlinear signal detection. In this paper, a brief overview of the various machine learning methods and their application in optical communication is presented and discussed. Moreover, supervised machine learning methods, such as neural networks and support vector machine, are experimentally demonstrated for in-band optical signal to noise ratio estimation and modulation format classification, respectively. The proposed methods accurately evaluate optical signals employing up to 64 quadrature amplitude modulation, at 32 Gbd, using only directly detected data.","0733-8724;07338724","","10.1109/JLT.2016.2590989","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529065","Machine learning;neural networks;optical communication;performance monitoring;support vector machines","Neural networks;Nonlinear optics;Optical modulation;Optical noise;Optical polarization;Signal to noise ratio","learning (artificial intelligence);neural nets;optical communication;optical signal detection;quadrature amplitude modulation;support vector machines","directly detected PDM-QAM signals;in-band optical signal to noise ratio estimation;machine learning techniques;modulation format classification;neural networks;optical communication;optical performance monitoring;quadrature amplitude modulation;support vector machine","","","","","","20160802","Feb.15, 15 2017","","IEEE","IEEE Journals & Magazines"
"The rise of machine learning for big data analytics","R. Alfred","Faculty of Computing and Informatics, Universiti Malaysia Sabah, Malaysia","2016 2nd International Conference on Science in Information Technology (ICSITech)","20170216","2016","","","1","1","This paper addresses the rise of machine learning for big data analytics. First, machine learning and several terms related to machine learning are defined and explained in details and these terms include artificial intelligence, data mining, data science, data analytics and knowledge discovery, statistics and Business Intelligence. These definitions will show how these terms are inter-related to each other. Then, the definition of big data is outlined based on three terms: Volume, Velocity and Variety. Implementing a good big data strategy is very crucial in order to guarantee the success of applying machine learning for learning big data. As a result, the trending in Big Data is also illustrated and defined based on the landscape of big data; Infrastructure, Analytics, Applications, Cross-Infrastructures/Analytics, Open Sources, Data Sources and API, Incubators and Schools. This paper also addresses some of the open source facilities that are available for public in order to ensure that large scale of machine learning application can be realized. Finally, in conclusion, the big trend over the last few months in Big Data analytics has been the increasing focus on artificial intelligence to help analyze massive amounts of data and derive predictive insights. AI/machine learning is now precipitating a trend towards the emergence of the application layer of Big Data. The combination of Big Data and AI will drive incredible innovation across pretty much every industry. From that perspective, the Big Data opportunity is probably even bigger than people thought.","","Electronic:978-1-5090-1721-8; POD:978-1-5090-1722-5; USB:978-1-5090-1720-1","10.1109/ICSITech.2016.7852593","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852593","artificial intelligence;big data;data analytics;data mining;data science;machine learning","","","","","","","","","","26-27 Oct. 2016","","IEEE","IEEE Conference Publications"
"Classification of tea samples using SVM as machine learning component of E-tongue","P. K. Kundu; M. Kundu","Electrical Engineering, Jadavpur University, Kolkata, India","2016 International Conference on Intelligent Control Power and Instrumentation (ICICPI)","20170223","2016","","","56","60","This article introduces a new approach for identification of tea sample using pulse voltammetry method in an electronic tongue based instrumentation. The classifier system consists of a principle component (PCA) based feature extraction module followed by support vector machine based discrimination. The PCA score of unknown tea sample is undergone through different pair-wise (binary) classification using SVM for repeated times. For six different categories of tea samples in the present case, unknown sample is examined for fifteen times. The result of classification is six membership grades. Finally these membership grades are analyzed by decision directed acrylic graph method (DDAG) for decision making task about the exact authentication of unknown tea sample belonging to six categories. The proposed method could be equally followed for more than six categories.","","Electronic:978-1-5090-2638-8; POD:978-1-5090-2639-5","10.1109/ICICPI.2016.7859673","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859673","DDAG;Electronic tongue (E-Tongue);PCA;SVM","Authentication;Electrodes;Instruments;Kernel;Principal component analysis;Radiation detectors;Support vector machines","beverages;chemical engineering computing;decision making;directed graphs;electronic tongues;feature extraction;learning (artificial intelligence);pattern classification;principal component analysis;support vector machines;voltammetry (chemical analysis)","DDAG;PCA score;SVM;binary classification;classifier system;decision directed acrylic graph method;decision making;e-tongue;electronic tongue based instrumentation;feature extraction;machine learning;membership grades;pair-wise classification;principle component analysis;pulse voltammetry method;support vector machine based discrimination;tea sample authentication;tea sample identification;tea samples classification","","","","","","","21-23 Oct. 2016","","IEEE","IEEE Conference Publications"
"Comparative study of machine learning algorithms for activity recognition with data sequence in home-like environment","X. Fan; H. Zhang; C. Leung; C. Miao","Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Nanyang Technological University, Singapore","2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)","20170213","2016","","","168","173","Activity recognition is a key problem in multisensor systems. With data collected from different sensors, a multi-sensor system identifies activities performed by the inhabitants. Since an activity always lasts a certain duration, it is beneficial to use data sequence for the desired recognition. In this work, we experiment several machine learning techniques, including Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) and Meta-Layer Network for solving this problem. We observe that (1) compare with “single-frame” activity recognition, data sequence based classification gives better performance; and (2) directly using data sequence information with a simple “mete layer” network model yields a better performance than memory based deep learning approaches.","","Electronic:978-1-4673-9708-7; POD:978-1-4673-9709-4","10.1109/MFI.2016.7849484","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849484","","Activity recognition;Computational modeling;Logic gates;Recurrent neural networks;TV;Testing;Training","data handling;learning (artificial intelligence);pattern classification;recurrent neural nets;sensor fusion","GRU;LSTM;RNN;data sequence based classification;gated recurrent unit;home-like environment;long short-term memory;machine learning;metalayer network;mete layer network model;multisensor systems;recurrent neural networks;sensor data;single-frame activity recognition","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Joint network coding and machine learning for error-prone wireless broadcast","D. Nguyen; C. Nguyen; Thuan Duong-Ba; H. Nguyen; A. Nguyen; T. Tran","Saolasoft Inc., 9200 E Mineral Ave, Centennial, CO 880211, USA","2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC)","20170302","2017","","","1","7","Reliable broadcasting data to multiple receivers over lossy wireless channels is challenging due to the heterogeneity of the wireless link conditions. Automatic Repeat-reQuest (ARQ) based retransmission schemes are bandwidth inefficient due to data duplication at receivers. Network coding (NC) has been shown to be a promising technique for improving network bandwidth efficiency by combining multiple lost data packets for retransmission. However, it is challenging to accurately determine which lost packets should be combined together due to disrupted feedback channels. This paper proposes an adaptive data encoding scheme at the transmitter by joining network coding and machine learning (NCML) for retransmission of lost packets. Our proposed NCML extracts the important features from historical feedback signals received by the transmitter to train a classifier. The constructed classifier is then used to predict states of transmitted data packets at different receivers based on their corrupted feedback signals for effective data mixing. We have conducted extensive simulations to collaborate the efficiency of our proposed approach. The simulation results show that our machine learning algorithm can be trained efficiently and accurately. The simulation results show that on average the proposed NCML can correctly classify 90% of the states of transmitted data packets at different receivers. It achieves significant bandwidth gain compared with the ARQ and NC based schemes in different transmission terrains, power levels, and the distances between the transmitter and receivers.","","Electronic:978-1-5090-4228-9; POD:978-1-5090-4229-6","10.1109/CCWC.2017.7868415","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868415","Machine learning;network coding;wireless broadcasting","Bandwidth;Network coding;Receivers;Reliability;Transmitters;Wireless communication;Wireless sensor networks","adaptive codes;automatic repeat request;broadcast communication;learning (artificial intelligence);network coding;pattern classification;radio receivers;radio transmitters;telecommunication computing;wireless channels","ARQ;NCML;adaptive data encoding;automatic repeat-request based retransmission schemes;broadcasting data;corrupted feedback signals;data mixing;disrupted feedback channels;error-prone wireless broadcast;joint network coding;machine learning;network coding;power levels;transmission terrains","","","","","","","9-11 Jan. 2017","","IEEE","IEEE Conference Publications"
"Machine learning classification model for Network based Intrusion Detection System","S. Kumar; A. Viinikainen; T. Hamalainen","Department of Mathematical Information Technology, University of Jyvaskyla, Finland","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","20170216","2016","","","242","249","With an enormous increase in number of mobile users, mobile threats are also growing rapidly. Mobile malwares can lead to several cybersecurity threats i.e. stealing sensitive information, installing backdoors, ransomware attacks and sending premium SMSs etc. Previous studies have shown that due to the sophistication of threats and tailored techniques to avoid detection, not every antivirus system is capable of detecting advance threats. However, an extra layer of security at the network side can protect users from these advanced threats by analyzing the traffic patterns. To detect these threats, this paper proposes and evaluates, a Machine Learning (ML) based model for Network based Intrusion Detection Systems (NIDS). In this research, several supervised ML classifiers were built using data-sets containing labeled instances of network traffic features generated by several malicious and benign applications. The focus of this research is on Android based malwares due to its global share in mobile malware and popularity among users. Based on the evaluation results, the model was able to detect known and unknown threats with the accuracy of up to 99.4%. This ML model can also be integrated with traditional intrusion detection systems in order to detect advanced threats and reduce false positives.","","Electronic:978-1-908320-73-5; POD:978-1-5090-4852-6","10.1109/ICITST.2016.7856705","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856705","Anomaly Detection;Intrusion Detection;Machine Learning;Mobile Malware;Traffic Analysis","Feature extraction;IP networks;Intrusion detection;Malware;Mobile communication;Smart phones","Android (operating system);computer network security;invasive software;learning (artificial intelligence);mobile computing;pattern classification","Android based malwares;NIDS;cybersecurity threats;machine learning classification model;mobile malwares;mobile threats;mobile users;network based intrusion detection system;network security;network traffic features;supervised ML classifiers;threat detection","","","","","","","5-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Extend relation identification in scientific papers based on supervised machine learning","Y. Sibaroni; D. H. Widyantoro; M. L. Khodra","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20170309","2016","","","379","384","This paper discusses the identification of extend relation in scientific papers based on supervised machine learning. Identification of extend relations is conducted by classifying each sentence in scientific papers into extend category. Extend relation is one type of papers' relations that obtained by using the citation context based approach. Citation context is a set of words or phrases in a sentence collection that citing or discussing other papers. Citation context based approach use the semantic of citation sentence to identify the relationship between scientific papers and can identify more varied relationship than two other approaches i.e. content-based and citation analysis. The recently research in papers' relations used a rule-based approach to identify extend relations. In this paper, supervised learning approached with proposed features set was used to identify extend relations. The learning of classifier model is explored by using Naïve Bayes, Decision Tree, Ibk and Logistic Regression. Experimental results show that the performance of extend sentence classification based on supervised machine learning with proposed features is superior compared to the baseline. Feature selection based on correlation value is also effective to improve the performance of the extend sentence classification.","","Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7","10.1109/ICACSIS.2016.7872724","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872724","Citation Context;Extend;Feature;Relations;Rule-based;Supervised machine Learning","Context;Data models;Feature extraction;Predictive models;Supervised learning;Text categorization;Training","Bayes methods;citation analysis;classification;content-based retrieval;decision trees;feature extraction;feature selection;information retrieval;learning (artificial intelligence);regression analysis","Naïve Bayes;citation context based approach;citation sentence;classifier model learning;correlation value;decision tree;extend relation identification;feature extraction;feature selection;ibk;logistic regression;papers relations;rule-based approach;scientific papers;sentence classification;sentence collection;supervised machine learning","","","","","","","15-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Extreme learning machine based on cross entropy","Y. Cui; J. Zhai; X. Wang","College of Mathematics and Information Science, Hebei University, Baoding, 071002, China","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170309","2016","2","","1066","1071","Extreme Learning Machine (ELM) is an algorithm for training single hidden layer feed-forward neural networks (SLFNs). Because ELM does not need the process of iterative learning, it is extremely faster than traditional learning algorithms such as back propagation algorithm and support vector machine. In ELM, the optimal solution with least squares norm is found by calculating the generalized inverse of hidden output matrix. When the order of hidden output matrix is a high, i.e., the number of hidden layer nodes is many, the over-fitting phenomenon will occur. Aiming at solving the over-fitting problem existing in ELM with many hidden layer nodes, this paper proposes a Cross Entropy based ELM (CE-ELM) in which, the mean square error minimization principle is replaced with the cross entropy minimization principle. The experimental results confirmed that the proposed CE-ELM can sufficiently overcome the drawback of overfitting in ELM with many hidden layer nodes.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7873027","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873027","Cross Entropy;Extreme learning machine;Least squares method;Over-fitting","Approximation algorithms;Entropy;Mathematical model;Minimization;Neural networks;Probability distribution;Training","entropy;feedforward neural nets;iterative methods;learning (artificial intelligence);least mean squares methods;minimisation;support vector machines","CE-ELM;SLFN;back propagation algorithm;cross entropy;cross entropy based ELM;cross entropy minimization principle;extreme learning machine;hidden layer nodes;hidden output matrix;iterative learning;least squares norm;mean square error minimization principle;over-fitting phenomenon;single hidden layer feed-forward neural networks;support vector machine","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Deep Learning for Consumer Devices and Services: Pushing the limits for machine learning, artificial intelligence, and computer vision.","J. Lemley; S. Bazrafkan; P. Corcoran","","IEEE Consumer Electronics Magazine","20170315","2017","6","2","48","56","In the last few years, we have witnessed an exponential growth in research activity into the advanced training of convolutional neural networks (CNNs), a field that has become known as deep learning. This has been triggered by a combination of the availability of significantly larger data sets, thanks in part to a corresponding growth in big data, and the arrival of new graphics-processing-unit (GPU)-based hardware that enables these large data sets to be processed in reasonable timescales. Suddenly, a wide variety of long-standing problems in machine learning, artificial intelligence, and computer vision have seen significant improvements, often sufficient to break through long-standing performance barriers. Across multiple fields, these achievements have inspired the development of improved tools and methodologies leading to even broader applicability of deep learning. The new generation of smart assistants, such as Alexa, Hello Google, and others, have their roots and learning algorithms tied to deep learning. In this article, we review the current state of deep learning, explain what it is, why it has managed to improve on the long-standing techniques of conventional neural networks, and, most importantly, how you can get started with adopting deep learning into your own research activities to solve both new and old problems and build better, smarter consumer devices and services.","2162-2248;21622248","","10.1109/MCE.2016.2640698","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879402","","Big Data;Convolution;Deep learning;Graphics processing unit;Machine learning;Neural networks;Training","computer vision;convolution;learning (artificial intelligence);neural nets","Alexa;CNN;GPU based hardware;Hello Google;artificial intelligence;big data;computer vision;consumer devices;consumer services;convolutional neural networks;deep learning;graphics processing unit based hardware;machine learning;smart assistants","","","","","","","April 2017","","IEEE","IEEE Journals & Magazines"
"An novel spectrum sensing scheme combined with machine learning","D. Wang; Z. Yang","EECS department, Wichita State University, USA","2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","20170216","2016","","","1293","1297","Cognitive radio (CR) network technology is widely used as a approach to the solve the scarce radio spectrum by allowing the unlicensed users to access the licensed spectrum. Since in the CR network, the licensed users are easily be affected by the introduce of the unlicensed user, we have to avoid to bring the interference to the licensed users when the unlicensed users try to transmit the data on the licensed radio spectrum. It is very difficult to solve this problem especially when the licensed users are mobile. The situation becomes even worse when the distribution of the licensed users is unknown. In this paper, in order to improve the throughput under the mobile network, we propose a novel algorithm which combines the random forest to decrease the interference of the unlicensed user to the licensed users, thus, the network throughput can be dramatically improved. The simulation results show that our proposed novel algorithm has good performance in improving the mobile network throughput.","","Electronic:978-1-5090-3710-0; POD:978-1-5090-3711-7; USB:978-1-5090-3709-4","10.1109/CISP-BMEI.2016.7852915","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852915","Cognitive radio;interference;primary users-component;random forest;throughput","Algorithm design and analysis;Data communication;Interference;Mobile communication;Mobile computing;Sensors;Throughput","cognitive radio;learning (artificial intelligence);mobile radio;radio spectrum management;radiofrequency interference","cognitive radio network technology;interference;licensed users;machine learning;mobile network;radio spectrum;spectrum sensing;unlicensed user","","","","","","","15-17 Oct. 2016","","IEEE","IEEE Conference Publications"
"Improving pseudo-code detection in ubiquitous scholarly data using ensemble machine learning","S. Tuarob","Faculty of Information and Communication Technology, Mahidol University, Thailand","2016 International Computer Science and Engineering Conference (ICSEC)","20170223","2016","","","1","6","A significant number of new algorithms constantly emerge ubiquitously as computer science and other computational related disciplines grow in advancement and complexity. A majority of these algorithms are developed by professional researchers who publish their algorithmic advancements in scholarly articles, especially in the form of pseudo-codes. The ability to automatically collect, manage, and index these pseudo-codes could prove to be useful for computer scientists and software developers seeking cutting-edge algorithmic solutions to their problems. In an effort towards automatic retrieval of these pseudo-codes, a machine learning based approach that detects and extracts these pseudo-codes in large scale scholarly documents has recently been proposed. In this paper, we extend the previous findings by investigating possible enhancement on the previously proposed classification methodology using ensemble learning techniques. The results illustrate that Random Forest is by far the most effective ensemble learning method which improves the classification performance by 13% over the best base classifier.","","Electronic:978-1-5090-4420-7; POD:978-1-5090-4421-4","10.1109/ICSEC.2016.7859944","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859944","","Algorithm design and analysis;Data mining;Decision trees;Feature extraction;Learning systems;Machine learning algorithms;Prediction algorithms","computer science;data handling;learning (artificial intelligence);pattern classification;ubiquitous computing","classification methodology;computer science;cutting-edge algorithmic solutions;ensemble machine learning;pseudo-code detection;ubiquitous scholarly data","","","","","","","14-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Similarity metric induced metrics with application in machine learning and bioinformatics","K. Zhang","Department of Computer Science, University of Western Ontario, London, ON N6A 5B7 Canada","2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20170223","2016","","","283","287","Similarity metric and distance metric are widely used in many research areas and applications. In this paper, for a given similarity metric, we will introduce a family of distance metrics of Minkowski type. We will then show general solutions to construct normalized similarity metric and normalized distance metric from a similarity metric and a distance metric. Applying the general solutions to a given non-negative similarity metric and its induced family of distance metrics, we derive general normalized similarity metrics and normalized distance metrics. Finally we briefly discuss some of the applications of our general similarity and distance metric formulations.","","CD:978-1-5090-3845-9; Electronic:978-1-5090-3846-6; POD:978-1-5090-3847-3","10.1109/ICCI-CC.2016.7862048","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862048","","Bioinformatics;Computer science;Context;Electronic mail;Entropy;Measurement;Mutual information","bioinformatics;learning (artificial intelligence);set theory","Minkowski type;bioinformatics;construct normalized similarity metric;data analysis;machine learning;normalized distance metric;set similarity;similarity metric induced metrics","","","","","","","22-23 Aug. 2016","","IEEE","IEEE Conference Publications"
"Prediction of post-operative implanted knee function using machine learning in clinical big data","S. Kobashi; B. Hossain; M. Nii; S. Kambara; T. Morooka; M. Okuno; S. Yoshiya","Advanced Medical Research Center, Graduate School of Engineering, University of Hyogo, Japan","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170223","2016","1","","195","200","Total knee arthroplasty (TKA) is one of the common knee surgeries. Because there are some types of TKA implant, it is hard to select appropriate type of TKA implant for individual patient. For the sake of pre-operative planning, this study presents a novel approach, which predicts post-operative implanted knee function of individuals. It is based on a clinical big data analysis. The big data is composed by a set of pre-operative knee mobility function and post-operative knee function. The method constructs a post-operative knee function prediction model by means of a machine learning approach. It extracts features using principal component analysis, and constructs a mapping function from pre-operative feature space to post-operative feature space. The method was validated by applying to prediction of post-operative anterior-posterior translation in 52 TKA operated knees. Leave-one-out cross validation test revealed the prediction performances with a mean correlation coefficients of 0.79 and a mean root-mean-squared-error of 3.44 mm.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7860900","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860900","Clinical big data;Machine learning;Predictive medicine;Principal component analysis;Total knee arthroplasty","Big data;Eigenvalues and eigenfunctions;Feature extraction;Kinematics;Knee;Machine learning algorithms;Principal component analysis","Big Data;data analysis;feature extraction;learning (artificial intelligence);mean square error methods;medical computing;principal component analysis;prosthetics;surgery","TKA;clinical Big Data analysis;feature extraction;knee surgeries;machine learning;mean correlation;mean root mean squared error;post operative implanted knee function prediction;preoperative planning;principal component analysis;total knee arthroplasty","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Using machine learning to predict hypertension from a clinical dataset","D. LaFreniere; F. Zulkernine; D. Barber; K. Martin","School of Computing, Queen's University, Kingston ON, Canada","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","7","Hypertension is an illness that often leads to severe and life threatening diseases such as heart failure, thickening of the heart muscle, coronary artery disease, and other severe conditions if left untreated. An artificial neural network is a powerful machine learning technique that allows prediction of the presence of the disease in susceptible populations while removing the potential for human error. In this paper, we identify the important risk factors based on patients' current health conditions, medical records, and demographics. These factors are then used to predict the presence of hypertension in an individual. These risk factors are also indicative of the probability of a person developing hypertension in the future and can, therefore, be used as an early warning system. We present a neural network model for predicting hypertension with about 82% accuracy. This is good performance given our chosen risk factors as inputs and the large integrated data used for the study. Our network model utilizes very large sample sizes (185,371 patients and 193,656 controls) from the Canadian Primary Care Sentinel Surveillance Network (CPCSSN) data set. Finally, we present a literature study to show the use of these risk factors in other works along with experimental results obtained from our model.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849886","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849886","Artificial neural network;backpropagation network;hypertension;medical decision support systems","Blood pressure;Diseases;Hypertension;Mathematical model;Medical diagnostic imaging;Neural networks;Predictive models","learning (artificial intelligence);medical computing;neural nets","CPCSSN data set;Canadian Primary Care Sentinel Surveillance Network;artificial neural network;clinical dataset;coronary artery disease;demographics;early warning system;health conditions;heart failure;heart muscle;human error;hypertension;illness;life threatening diseases;machine learning;medical records;neural network model;probability;risk factors;susceptible populations","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Risk factors of heart failure for patients classification with extreme learning machine","H. H. Zhang; P. C. Wang; Y. J. Wang; T. Ruan; H. F. Wang","East China University of Science and Technology, Shanghai, 200237, China","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170309","2016","2","","814","819","Heart failure (HF), the terminal stage of all kinds of cardiovascular disease, has a high level of morbidity and mortality. But the heavy burden of curing and managing HF can be largely reduced by early detection of it. Motivated by this problem, we study methods to determine its risk factors based on extreme learning machine (ELM). Several state-of-the-art data mining algorithms are employed to estimate the performance of various classification of the HF patients. Our data sets are extracted from reality hospital patients' data, which consist of patients' basic demographic, disease and assay information. The results show that ELM will have a better performance larger than 93% if the selected attributes have more strong correlation with the label.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7872992","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872992","Classifier;ELM;Feature Selection;Heart Failure","Classification algorithms;Cybernetics;Diseases;Heart;Hospitals;Hypertension;Inspection","cardiovascular system;data mining;diseases;electronic health records;learning (artificial intelligence);pattern classification","ELM;HF;HF patient classification;cardiovascular disease;data mining;early heart failure detection;extreme learning machine;hospital patient data;morbidity level;mortality level;patient assay information;patient basic demographic information;patient disease information;risk factors","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Capabilities of a Simulated Cerebellum","M. Hausknecht; W. K. Li; M. Mauk; P. Stone","Department of Computer Science, The University of Texas at Austin, Austin, TX, USA","IEEE Transactions on Neural Networks and Learning Systems","20170215","2017","28","3","510","522","This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional-integral-derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator's inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.","2162-237X;2162237X","","10.1109/TNNLS.2015.2512838","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7393590","Cerebellar pattern recognition;MNIST handwritten digit recognition;cerebellum;inverted pendulum balancing (cart–pole);proportional–integral–derivative (PID) control;robot balance","Brain modeling;Computational modeling;Data models;Eyelids;Mathematical model;Pattern recognition;Robots","biocontrol;biology computing;learning (artificial intelligence)","MNIST handwritten digit recognition;basal ganglia;biologically constrained bottom-up model;cerebellar simulation;credit assignment problem;error signals;eyelid conditioning;human brain;machine learning;mammalian cerebellum;pendulum balancing;proportional-integral-derivative control;reinforcement learning;robot balancing;sensory apparatus;static input pattern identification;supervised learning;temporal pattern recognition","","","","","","20160126","March 2017","","IEEE","IEEE Journals & Magazines"
"Design and implementation of user-oriented video streaming service based on machine learning","M. Oide; A. Takahashi; T. Abe; T. Suganuma","Graduate School of Information Sciences, Tohoku University, Sendai, Japan","2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)","20170223","2016","","","111","116","We propose a method to determine appropriate quality of service (QoS) dynamically required by users for video streaming services in this paper. In the proposed method, the QoS parameters for the video streaming are determined based on the machine learning algorithm, by using a regression analysis in particular, according to the user requirements, computational/network resources and service provisioning environments. In this paper, we describe the design and implementation of our method. Furthermore, we confirm the feasibility of our proposed method through an experiment of a prototype system.","","CD:978-1-5090-3845-9; Electronic:978-1-5090-3846-6; POD:978-1-5090-3847-3","10.1109/ICCI-CC.2016.7862023","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862023","","Prototypes;Quality assessment;Quality of service;Standards;Streaming media;Training data;Video recording","learning (artificial intelligence);multimedia communication;quality of service;regression analysis;telecommunication computing;video streaming","QoS parameters;machine learning algorithm;quality of service;regression analysis;service provisioning environments;user-oriented video streaming service","","","","","","","22-23 Aug. 2016","","IEEE","IEEE Conference Publications"
"Reusability of the Output of Map-Matching Algorithms Across Space and Time Through Machine Learning","M. Hashemi","School of Computing and Information, University of Pittsburgh, Pittsburgh, PA 15260 USA.","IEEE Transactions on Intelligent Transportation Systems","","2017","PP","99","1","10","A map-matching algorithm outputs a vector per GPS point, projecting the moving object on one of the segments of the transportation network. Although developing more sophisticated map-matching algorithms for vehicle and pedestrian navigation systems have been the focus of research in this field, reusability of the historical information already provided by map-matching algorithms has not been addressed yet. In other words, although researchers have been attempting to improve the accuracy of the aforementioned vector to correctly project GPS points on the transportation network, no research has exploited the spatial-temporal pattern in the arrangement of these projection vectors. This pattern, if properly detected, can be used as a rough surrogate for map-matching algorithms, in addition to other applications that require better positional accuracy for moving objects in smart cities. This paper detects and validates the spatial-temporal pattern in projection vectors produced by map-matching algorithms via machine learning. Projection vectors showed a strong spatial-temporal pattern in Chicago, IL, USA, which was captured best via a local nonlinear regressor, K-nearest neighbors, and helped double the positional accuracy of unseen GPS points. While a global nonlinear regressor, multilayer Perceptron was able to slightly improve the positional accuracy of GPS points, the linear least squares had an exacerbating effect on the positional accuracy.","1524-9050;15249050","","10.1109/TITS.2017.2669085","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7876714","GPS error.;Map-matching algorithm;machine learning;spatial-temporal pattern","Encoding;Global Positioning System;Machine learning algorithms;Receivers;Satellites;Training","","","","","","","","20170313","","","IEEE","IEEE Early Access Articles"
"Semisupervised Incremental Support Vector Machine Learning Based on Neighborhood Kernel Estimation","J. Wang; D. Yang; W. Jiang; J. Zhou","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing 100029, China.","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2017","PP","99","1","11","Semisupervised scheme has emerged as a popular strategy in the machine learning community due to the expensiveness of getting enough labeled data. In this paper, a semisupervised incremental support vector machine (SE-INC-SVM) algorithm based on neighborhood kernel estimation is proposed. First, kernel regression is constructed to estimate the unlabeled data from the labeled neighbors and its estimation accuracy is discussed from the analogy with tradition RBF neural network. The incremental scheme is derived to improve the learning efficiency and reduce the computing time. Simulations for manual data set and industrial benchmark-penicillin fermentation process demonstrate the effectiveness of the proposed SE-INC-SVM method.","2168-2216;21682216","","10.1109/TSMC.2017.2667703","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866831","Incremental training;neighborhood kernel estimation (KE);semisupervised scheme;support vector machine (SVM)","Data models;Estimation;Interpolation;Kernel;Semisupervised learning;Support vector machines;Training","","","","","","","","20170302","","","IEEE","IEEE Early Access Articles"
"Analysis of post-harvest losses: An Internet of Things and machine learning approach","H. Purandare; N. Ketkar; S. Pansare; P. Padhye; A. Ghotkar","Computer Engineering Department, PICT, Pune, Maharashtra, India","2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)","20170316","2016","","","222","226","Reduction of post-harvest losses is a critical component of food security. World population is increasing at an alarming rate and thus is the food requirement. Due to limited cultivable land, increasing the food production to meet the needs of people, solely, cannot be the solution. In this paper, we have proposed to build an end-to-end system for farmers and warehouse managers to reduce post-harvest losses. It will consist of a notification-suggestion system which will include data about the current status of farm, suggestions about correct harvesting time and diseases that might affect the crop in its cultivation stages. The system will also include a prediction system for warehouse managers which will suggest the correct dispatch sequence of the stocks and also the optimum temperature and humidity at which one or more crops can be transported so as incur minimum storage and transportation loss. Here, for the prediction-analysis and suggestions, various statistical and probabilistic techniques such as classification and regression are used.","","Electronic:978-1-5090-2080-5; POD:978-1-5090-2081-2","10.1109/ICACDOT.2016.7877583","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877583","Android;Embedded Systems;Internet of Things;Machine Learning;Sensor Application","Agriculture;Androids;Diseases;Humanoid robots;Humidity;Temperature distribution;Temperature sensors","Internet of Things;agricultural engineering;crops;goods dispatch data processing;learning (artificial intelligence);plant diseases;probability;statistical analysis;warehouse automation","crop diseases;cultivable land;cultivation stages;end-to-end system;food production;food requirement;food security;harvesting time;optimum humidity;optimum temperature;post-harvest loss reduction;prediction system;probabilistic techniques;statistical techniques;stocks dispatch sequence;storage loss;transportation loss;warehouse managers","","","","","","","9-10 Sept. 2016","","IEEE","IEEE Conference Publications"
"The role of machine learning in botnet detection","S. Miller; C. Busby-Earle","Department of Computing, The University of the West Indies Mona, Kingston Jamaica","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","20170216","2016","","","359","364","Over the past ten to fifteen years botnets have gained the attention of researchers worldwide. A great deal of effort has been given to developing systems that would efficiently and effectively detect the presence of a botnet. This unique problem saw researchers applying machine learning (ML) to solve this problem. In this paper we provide a brief overview the different machine learning (ML) methods and the part they play in botnet detection. The main aim of this paper is to clearly define the role different ML methods play in Botnet detection. A clear understanding of these roles are critical for developing effective and efficient real-time online detection approaches and more robust models.","","Electronic:978-1-908320-73-5; POD:978-1-5090-4852-6","10.1109/ICITST.2016.7856730","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856730","botnet detection;cyber-security;machine learning;supervised learning;unsupervised learning","Servers","invasive software;learning (artificial intelligence)","ML methods;machine learning;real-time online botnet detection","","","","","","","5-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine learning based identification of pathological heart sounds","T. Gokhale","Duke University, Durham, USA","2016 Computing in Cardiology Conference (CinC)","20170302","2016","","","553","556","Automated interpretation of heart sounds holds great promise in increasing the diagnostic accuracy and consistency of cardiac auscultation and allowing for use in remote, tele-health settings. However, existing algorithms for classification of hearts sounds have been constrained by limited idealized training sets and methodological issues with validation. As part of the 2016 PhysioNet Challenge competition, we present an algorithm for automated heart sound classification sthat uses Hilbert-envelope and wavelet features to attempt to capture the qualities of the heart sounds that physicians are trained to interpret. We perform a two-step classification of heart sounds into poor quality, normal or abnormal with sensitivity of 0.7958 and specificity of 0.7459.","","Electronic:978-1-5090-0895-7; POD:978-1-5090-0896-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868802","","Blood;Classification algorithms;Feature extraction;Heart;Hidden Markov models;Training;Valves","Hilbert transforms;bioacoustics;biomedical ultrasonics;cardiology;learning (artificial intelligence);medical signal processing;signal classification;wavelet transforms","Hilbert-envelope;automated heart sound classification;cardiac auscultation;machine learning;pathological heart sound identification;telehealth setting;wavelet feature","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"Eye refractive error classification using machine learning techniques","S. O. Fageeri; S. M. M. Ahmed; S. A. Almubarak; A. A. Mu'azu","Faculty of computer Science & Information Technology, Alzaiem Alazhari University, Sudan","2017 International Conference on Communication, Control, Computing and Electronics Engineering (ICCCCEE)","20170302","2017","","","1","6","Machine learning is a subdivision of Artificial Intelligence (AI) that is concerned with the design and development of intelligent algorithms that enables machines to learn from data without being programmed. Machine learning mainly focus on how to automatically recognize complex patterns among data and make intelligent decisions. In this paper, intelligent machine learning algorithms are used to classify the type of an eye disease based on ophthalmology data collected from patients of Mecca hospital in Sudan. Three machine-learning techniques are used to predict the severity of the eye that occurred during the investigation, which are Naïve Bayesian, SVM, and J48 decision tree. The obtained result showed that J48 classifier outperforms both Naïve Bayesian as well as SVM.","","Electronic:978-1-5090-1809-3; POD:978-1-5090-1810-9","10.1109/ICCCCEE.2017.7867660","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867660","Classification;Data Mining;Machine learning","Classification algorithms;Computational modeling;Data mining;Decision trees;Diseases;Prediction algorithms;Support vector machines","Bayes methods;decision trees;diseases;eye;learning (artificial intelligence);medical computing;pattern classification;support vector machines","J48 decision tree;Mecca hospital;SVM;Sudan;artificial intelligence;data complex pattern recognition;eye disease type classification;eye refractive error classification;eye severity prediction;intelligent decision making;intelligent machine learning algorithm;naive Bayesian;ophthalmology data","","","","","","","16-18 Jan. 2017","","IEEE","IEEE Conference Publications"
"An ensemble approach to detect review spam using hybrid machine learning technique","M. N. I. Ahsan; T. Nahian; A. A. Kafi; M. I. Hossain; F. M. Shah","Department of Computer Science & Engineering, Ahsanullah University of Science & Technology, Dhaka, Bangladesh","2016 19th International Conference on Computer and Information Technology (ICCIT)","20170223","2016","","","388","394","Online reviews are becoming one of the vital components of e-commerce in recent years as so many people consider having different opinions prior to buying online products or apprehending any online service. Nowadays, in the era of web 2.0, it is completely understandable that people rely on online reviews more than ever while taking a decision. However, guaranteeing the authenticity of these sensitive and valuable information is hardly visible. Due to fulfill some immoral benefits, many people post fake review or fabricated opinion to uphold or devalue a certain product or service which certainly hampers the ingenuousness of the real fact. To detect fake reviews, many methodologies were introduced by harvesting the obvious content features, rating consistency, empirical conditions, helpfulness voting etc. The most of them are supervised models which mostly rely on pseudo fake reviews and the scarcity of good quality large-scale labeled dataset is still a hindrance. In this paper, we introduce an ensemble learning approach which combines two different types of learning methods (active and supervised) by creating a hybrid dataset of both real-life and pseudo reviews. This model holds 3 different filtering phases that is based on KL and JS distance, TF-IDF features and n-gram features of the review content. It achieves phenomenal results while working on almost 3600 reviews from different domains. In the best case, the precision, recall and f-score are above 95% and the accuracy it achieved is slightly above 88%. In the process, about 2000 reviews were manually labeled. After evaluating and comparing the results with other successful methods, it is quite clear that this detecting method is efficient and very promising.","","Electronic:978-1-5090-4090-2; POD:978-1-5090-4091-9","10.1109/ICCITECHN.2016.7860229","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860229","Fake Review;Machine learning;Review spam detection;Spam Detection;Spam Review","Computers;Data models;Information technology;Internet;Support vector machines;Training;Writing","Internet;content-based retrieval;feature extraction;feature selection;information filtering;learning (artificial intelligence);unsolicited e-mail","JS distance;KL distance;TF-IDF features;Web 2.0;active learning methods;content features;content review;e-commerce;ensemble learning approach;filtering phases;hybrid machine learning technique;information authenticity;n-gram features;online products;online reviews;online service;review spam detection;supervised learning methods","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Learning self-awareness in committee machines","Y. Liu","School of Computer Science and Engineering, The University of Aizu, Aizu-Wakamatsu, Fukushima 965-8580, Japan","2016 International Conference on Machine Learning and Cybernetics (ICMLC)","20170309","2016","2","","888","893","Self-awareness is a kind of ability of recognizing oneself as an individual being different from the environment and other individuals. This paper proposes negative correlation learning with self-awareness in order for each artificial neural network (ANN) in a committee machine to be self-aware in learning so that it could decide by itself to learn more or less. On one hand, when the learning would force itself to be closer to the ensemble, an individual ANN would choose to learn less so that the learning on that direction would be disencouraged. On the other hand, when the learning would help itself to be more different to the ensemble, an individual ANN would let itself to learn more so that the learning on that direction would be encouraged. It is expected that such ANNs being aware of their own behavior and performance can manage trade-offs between goals at run-time. Such self-awareness enables a committee machine to better meet their requirements for predictions on the unknown data. Measurement results have been presented to how self-awareness could support the different behaviors and maintain the performance.","","CD:978-1-5090-0388-4; Electronic:978-1-5090-0390-7; POD:978-1-5090-0391-4","10.1109/ICMLC.2016.7873004","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873004","Negative correlation learning;Neural network ensembles;Private awareness","Boosting;Cancer;Correlation;Cybernetics;Diabetes;Error analysis;Training","data analysis;learning (artificial intelligence);neural nets","ANN;artificial neural network;committee machines;negative correlation learning;self-awareness;unknown data","","","","","","","10-13 July 2016","","IEEE","IEEE Conference Publications"
"Machine learning algorithm for retinal image analysis","Santhakumar R; M. Tandur; E. R. Rajkumar; Geetha K S; G. Haritz; K. T. Rajamani","School of Electronics Engineering, VIT University, Vellore, India","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","1236","1240","Diabetic retinopathy is the most general diabetes complication that affects eyes and results in blindness. It's due to impairment of the arteries a veins located in the fundus of eye (retina) that are composed of light sensitive tissues. The aim of this research work is to design an efficient and sensitive tool for Diabetic Retinopathy using the images acquired from portable fundus camera. The screening tool is based on advanced machine learning and computer vision algorithm which includes patch level prediction. In patch level prediction algorithm will localize the diseased region in the Diabetic Retinopathy image like Hard Exudates and Hemorrhage. The patch level classification uses Support Vector Machine (SVM) machine learning classifier model to predict the potential patch of Hard Exudates and Hemorrhage. In this algorithm, the image is broken into regular rectangular patch. The feature for each patch along with the different class label based on the ground truth is computed and passed to strong classifier SVM. The data sets are split into training dataset and testing dataset. The classifier model is built on training dataset and tested against the test dataset. The performance results of rectangular patch level prediction using SVM the average performance for Hard Exudates was Accuracy 96 %, Sensitivity 94%, Specificity 96%. The average performance for Hemorrhage was Accuracy 85 %, Sensitivity 77%, and Specificity 85%.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848208","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848208","Diabetic Retinopathy;Fundus Imaging;Hard Exudates;Hemorrhage;Machine Learning;Medical Imaging;Retina;Support Vector Machine","Diabetes;Feature extraction;Hemorrhaging;Retina;Retinopathy;Sensitivity;Support vector machines","diseases;eye;image classification;learning (artificial intelligence);medical image processing;support vector machines","SVM;computer vision algorithm;diabetic retinopathy;hard exudates;hemorrhage;light sensitive tissues;machine learning algorithm;machine learning classifier model;patch level classification;portable fundus camera;rectangular patch level prediction;retinal image analysis;support vector machine","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Stochastic performance tuning of complex simulation applications using unsupervised machine learning","O. Shadura; F. Carminati","CERN, Geneve, Switzerland","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","Machine learning for complex multi-objective problems (MOP) can substantially speedup the discovery of solutions belonging to Pareto landscapes and improve Pareto front accuracy. Studying convergence speedup of multi-objective search on well-known benchmarks is an important step in the development of algorithms to optimize complex problems such as High Energy Physics particle transport simulations. In this paper we will describe how we perform this optimization via a tuning based on genetic algorithms and machine learning for MOP. One of the approaches described is based on the introduction of a specific multivariate analysis operator that can be used in case of expensive fitness function evaluations, in order to speed-up the convergence of the “black-box” optimization problem.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7850200","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850200","","Convergence;Covariance matrices;Genetic algorithms;Optimization;Principal component analysis;Sociology","Pareto analysis;convergence;genetic algorithms;search problems;stochastic processes;unsupervised learning","MOP;Pareto front accuracy;Pareto landscapes;black-box optimization;complex problem optimization;complex simulation applications;convergence speedup;fitness function evaluations;genetic algorithms;high energy physics particle transport simulations;multi-objective problems;multiobjective search;multivariate analysis operator;solution discovery;stochastic performance tuning;unsupervised machine learning","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Machine Learning for SQL injection prevention on server-side scripting","K. Kamtuo; C. Soomlek","Department of Computer Science, Khon Kaen University, Thailand","2016 International Computer Science and Engineering Conference (ICSEC)","20170223","2016","","","1","6","SQL injection is the most common web application vulnerability. The vulnerability can be generated unintentionally by software developer during the development phase. To ensure that all secure coding practices are adopted to prevent the vulnerability. The framework of SQL injection prevention using compiler platform and machine learning is proposed. The machine learning part will be described primarily since it is the core of this framework to support SQL injection prediction by conducting 1,100 datasets of vulnerabilities to train machine learning model. The results indicated that decision tree is the best model in term of processing time, highest efficiency in prediction.","","Electronic:978-1-5090-4420-7; POD:978-1-5090-4421-4","10.1109/ICSEC.2016.7859950","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859950","Machine Learning;SQL Injection;Web application vulnerability","Analytical models;Computational modeling;Correlation;Databases;Encoding;Predictive models;Support vector machines","Internet;SQL;decision trees;learning (artificial intelligence);program compilers;security of data","SQL injection prediction;SQL injection prevention;Web application vulnerability;compiler platform;decision tree;machine learning;secure coding practices;software development phase","","","","","","","14-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Intelligent Forwarding Strategy Based on Online Machine Learning in Named Data Networking","L. Gong; J. Wang; X. Zhang; K. Lei","Shenzhen Key Lab. for Cloud Comput. Technol. & Applic. (SPCCTA, Peking Univ., Shenzhen, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","1288","1294","The content-oriented model of Named Data Networking (NDN) allows consumers to pay more attention to the targeting data itself instead of the location of where the data is stored. Different from IP, NDN has a unique feature that forwarding plane enables each router to select the next forwarding hop independently without relying on routing. Therefore, forwarding strategies play a significant role for adaptive and efficient data transmission in NDN. Existing forwarding strategies are not smart enough to cope with the complexity of network and diversity of application demands. This paper presents an intelligent forwarding strategy, which integrates online machine learning method into the optimization of interface probabilities during forwarding process. Originally, a probabilistic binary tree structure is proposed to abstract the forwarding process as a path selection process traversing from the root node to the leaf node, which provides theoretical support for machine learning and reduces the complexity of forwarding process. In addition, we improved our strategy to prevent the convergence into limited local optimal solution by adopting the idea of simulated an nealing. Experimental results show that the proposed strategy can reduce time complexity, as well as achieve higher throughput, better load balance and lower packet drop rates in comparison with other existing forwarding strategies. The drop rates are reduced by 60% and 34% respectively in different scenarios compared with BestRoute, a strategy widely used in NDN.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0206","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847089","NDN;forwarding strategy;machine learning;probabilistic binary tree","Binary trees;IP networks;Optimization;Probabilistic logic;Routing;Time complexity","computational complexity;computer networks;learning (artificial intelligence);probability;simulated annealing;telecommunication network routing;trees (mathematics)","BestRoute;IP;NDN;forwarding hop;forwarding plane;intelligent forwarding strategy;leaf node;load balance;named data networking;network complexity;online machine learning;packet drop rates;path selection process;probabilistic binary tree structure;root node;simulated annealing;time complexity","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Computer aided analysis of cognitive disorder in patients with Parkinsonism using machine learning method with multilevel ROI-based features","B. Peng; Z. Zhou; C. Geng; B. Tong; Z. Zhou; T. Zhang; Y. Dai","Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences, 215163, China","2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","20170216","2016","","","1792","1796","This paper proposes to use multilevel ROI-based features and machine learning method to improve the accuracy of qualitative recognition of mild cognitive disorder in parkinsonism. 77 Parkinson's patients and 32 normal controls with neuropsychological assessments and structural magnetic resonance images from the Parkinson's Progression Markers Initiative dataset are tested. Specifically, the BrainLab software is used to process images and measure volume of gray matter, thickness of the cortex, and surface area of the cortex at each region of interest (ROI). We utilize t-test, support vector machine (SVM), and minimum redundancy and maximum relevance (mRMR) methods conjunctively to select features and get the optimal features and the classifier. The experimental results reveal that our method with multilevel ROI-based features gives significant improvement of the classification performance compared with other methods using single-level ROI-based features (i.e., using only volume of gray matter, thickness of cortex, or surface area of cortex).","","Electronic:978-1-5090-3710-0; POD:978-1-5090-3711-7; USB:978-1-5090-3709-4","10.1109/CISP-BMEI.2016.7853008","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7853008","Cognitive disorder;Magnetic resonance imaging;Multilevel ROI-based features;Parkinson's disease;Support Vector Machine","Dementia;Feature extraction;Learning systems;Parkinson's disease;Support vector machines;Surface morphology","biomedical MRI;brain;computer aided analysis;diseases;feature selection;image recognition;learning (artificial intelligence);medical image processing;neurophysiology;pattern classification;support vector machines","BrainLab software;Parkinson's progression markers initiative dataset;Parkinsonism;SVM;classifier;computer aided analysis;cortex surface area;cortex thickness;feature selection;gray matter volume measurement;image processing;mRMR method;machine learning;minimum redundancy and maximum relevance;multilevel ROI-based features;neuropsychological assessments;patient cognitive disorder;qualitative recognition;region of interest;structural magnetic resonance images;support vector machine;t-test","","","","","","","15-17 Oct. 2016","","IEEE","IEEE Conference Publications"
"Automated weather event analysis with machine learning","N. Hasan; M. T. Uddin; N. K. Chowdhury","Department of Computer Science and Engineering, International Islamic University Chittagong, Bangladesh","2016 International Conference on Innovations in Science, Engineering and Technology (ICISET)","20170216","2016","","","1","5","Weather forecasting has numerous impacts in our daily life from cultivation to event planning. Previous weather forecasting models used the complicated blend of mathematical instruments which was insufficient in order to get higher classification rate. In contrast, simple analytical models are well-suited for weather forecasting tasks. In this work, we focus on the weather forecasting by means of classifying different weather events such as normal, rain, and fog by applying comprehensible C4.5 learning algorithm on weather and climate features. The C4.5 classifier classifies weather events by building the decision tree using information entropy from the set of training samples. We conducted experiments on LA weather history dataset; from evaluation results, it is revealed that C4.5 classifier classifies weather events with f-score of around 96.1%. This model also indicates that climate features such as rainfall, visibility, temperature, humidity, and wind speed are highly discriminative toward events classification.","","Electronic:978-1-5090-6122-8; POD:978-1-5090-6123-5","10.1109/ICISET.2016.7856509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856509","Forecasting;Machine Learning;Weather Events","Atmospheric modeling;History;Ocean temperature;Predictive models;Rain;Weather forecasting","atmospheric humidity;atmospheric temperature;climatology;decision trees;entropy;geophysics computing;learning (artificial intelligence);pattern classification;rain;weather forecasting;wind","C4.5 classifier;Los Angeles weather history dataset;USA;atmospheric humidity;atmospheric temperature;automated weather event analysis;climate feature;decision tree;fog;information entropy;machine learning;mathematical instrument;rain;weather forecasting model;wind speed","","","","","","","28-29 Oct. 2016","","IEEE","IEEE Conference Publications"
"Code review analysis of software system using machine learning techniques","H. Lal; G. Pahwa","Aricent, Gurgaon, India","2017 11th International Conference on Intelligent Systems and Control (ISCO)","20170216","2017","","","8","13","Code review is systematic examination of a software system's source code. It is intended to find mistakes overlooked in the initial development phase, improving the overall quality of software and reducing the risk of bugs among other benefits. Reviews are done in various forms such as pair programming, informal walk-through, and formal inspections. Code review has been found to accelerate and streamline the process of software development like very few other practices in software development can. In this paper we propose a machine learning approach for the code reviews in a software system. This would help in faster and a cleaner reviews of the checked in code. The proposed approach is evaluated for feasibility on an open source system eclipse. [1], [2], [3].","","Electronic:978-1-5090-2717-0; POD:978-1-5090-2718-7","10.1109/ISCO.2017.7855962","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855962","Code Review;Machine learning;Software development process","Computer bugs;Control systems;Data models;Predictive models;Software systems;Standards","formal specification;learning (artificial intelligence);program diagnostics;public domain software;software quality","code review analysis;formal inspections;informal walk-through;machine learning techniques;open source system eclipse;pair programming;software quality;software system;source code","","","","","","","5-6 Jan. 2017","","IEEE","IEEE Conference Publications"
"A review on respiratory sound analysis using machine learning","G. Altan; Y. Kutlu","Enformatik A.B.D, Mustafa Kemal &#x00DC;niversitesi, Hatay, T&#x00FC;rkiye","2016 20th National Biomedical Engineering Meeting (BIYOMUT)","20170213","2016","","","1","4","Auscultation of the respiratory sounds is an inexpensive and effective method for diagnosing cardio-pulmonary disorders using lung sounds from chest and back. Nowadays, high system performances in the management of robust processes that require great attention were increased using the computer-aided analysis methods and the developments of the diagnosis system. Analysis of the respiratory sounds with computer-aided systems allows objective and useful assessments. In this study, a brief description of the abnormal respiratory sounds was presented. The main aims of the study are performing a systematic review about methods and the machine learning algorithms that are used to classify the abnormal respiratory sounds for diagnosis of cardio-pulmonary disorders and evaluating the development of possible methods on respiratory sounds in the future.","","Electronic:978-1-5090-5829-7; POD:978-1-5090-5830-3","10.1109/BIYOMUT.2016.7849379","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849379","Respiratory sounds;crackles;machine learning;oscultation;wheeze","Biomedical engineering;Conferences;Diseases;Hidden Markov models;Lungs;Markov processes;Signal processing","bioacoustics;cardiology;learning (artificial intelligence);medical diagnostic computing;medical disorders;patient diagnosis;pneumodynamics","abnormal respiratory sounds;auscultation;cardiopulmonary disorders;chest;computer-aided analysis methods;diagnosis system;high-system performances;lung sounds;machine learning algorithms;respiratory sound analysis;robust process management","","","","","","","3-5 Nov. 2016","","IEEE","IEEE Conference Publications"
"Robust channel coding strategies for machine learning data","K. Mazooji; F. Sala; G. Van den Broeck; L. Dolecek","UCLA, Los Angeles, CA 90095, United States of America","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","609","616","Two important recent trends are the proliferation of learning algorithms along with the massive increase of data stored on unreliable storage mediums. These trends impact each other; noisy data can have an undesirable effect on the results provided by learning algorithms. Although traditional tools exist to improve the reliability of data storage devices, these tools operate at a different abstraction level and therefore ignore the data application, leading to an inefficient use of resources. In this paper we propose taking the operation of learning algorithms into account when deciding how to best protect data. Specifically, we examine several learning algorithms that operate on data that is stored on noisy mediums and protected by error-correcting codes with a limited budget of redundancy; we develop a principled way to allocate resources so that the harm on the output of the learning algorithm is minimized.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852288","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852288","Channel Coding;Machine Learning;Optimization;Statistics","Encoding;Machine learning algorithms;Maximum likelihood decoding;Prediction algorithms;Redundancy;Robustness","channel coding;data protection;learning (artificial intelligence);minimisation;resource allocation","channel coding strategy;data protection;error-correcting code;harm minimization;learning algorithm operation;machine learning data;resource allocation","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Improved Arabic characters recognition by combining multiple machine learning classifiers","M. Alabbas; R. S. Khudeyer; S. Jaf","Department of Computer Science, University of Basrah, Iraq","2016 International Conference on Asian Language Processing (IALP)","20170313","2016","","","262","265","In this paper, we investigate a range of strategies for combining multiple machine learning techniques for recognizing Arabic characters, where we are faced with imperfect and dimensionally variable input characters. Experimental results show that combined confidence-based backoff strategies can produce more accurate results than each technique produces by itself and even the ones exhibited by the majority voting combination.","","Electronic:978-1-5090-0922-0; POD:978-1-5090-0923-7; USB:978-1-5090-0921-3","10.1109/IALP.2016.7875982","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875982","PNN;SVM;kNN;optical character recognition (OCR);systems combination","Character recognition;Discrete cosine transforms;Feature extraction;Optical character recognition software;Support vector machines;Training;Writing","learning (artificial intelligence);natural language processing;optical character recognition;pattern classification","Arabic characters;Arabic characters recognition;confidence-based backoff strategies;dimensionally variable input characters;machine learning classifiers;majority voting combination","","","","","","","21-23 Nov. 2016","","IEEE","IEEE Conference Publications"
"Principal component selection of machine learning algorithms based on orthogonal transformation by using interactive evolutionary computation","Y. Pei","Computer Science Division, the University of Aizu, Aizu-wakamatsu, Fukushima, Japan 965-8580","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000308","000313","We propose a method to solve the selection problem of principal components in machine learning algorithms based on orthogonal transformation by using interactive evolutionary computation. One of the addressed subjects for machine learning algorithms based on orthogonal transformation is how to decide the number of principal components, and which of the principal components should be used to reconstruct the original data. In this work, we use the interactive differential evolution algorithm to study these subjects by using real humans' subjective evaluation in an optimization process. An image compression problem using principal component analysis is introduced to study the proposed method. From the evaluation, we do not only solve the selection problem of principal components for machine learning algorithms based on orthogonal transformation, but also can analyse the human aesthetical characteristics on visual perception and feature selection from the designed method and experimental evaluation. We also discuss and analyse potential research subjects and some open topics, which are invited to further investigate.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844258","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844258","image compression;interactive differential evolution;interactive evolutionary computation;orthogonal transformation;principal component analysis;principal component selection","Conferences;Evolutionary computation;IEC;Image coding;Machine learning algorithms;Optimization;Principal component analysis","data compression;feature selection;image coding;learning (artificial intelligence);principal component analysis","feature selection;human aesthetical characteristics;image compression problem;interactive differential evolution algorithm;interactive evolutionary computation;machine learning algorithms;optimization process;orthogonal transformation;principal component analysis;principal component selection;subjective evaluation;visual perception","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
