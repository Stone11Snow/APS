"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7395886,7395848,7395847,7396226,7396401,7398569,7399061,7396781,7396702,7396449,7396827,7394723,7391553,7391019,7392180,7389806,7390351,7384138,7387315,7389220,7387013,7384087,7386124,7387260,7383540,7384321,7380512,7383023,7382212,7379293,7379618,7379187,7376697,7374895,7377971,7377669,7375223,7376695,7376652,7374168,7374127,7372544,7372634,7370680,7372639,7373977,7371712,7371798,7372628,7370528,7370156,7367038,7367424,7367615,7370086,7035040,7363760,7363871,7363215,7364066,7363854,7363889,7366707,7366709,7366548,7363729,7359454,7361084,7361134,7110380,7361150,7361085,7347331,7358050,7359788,7359941,7352513,7357672,7357211,7352473,7359925,7352512,7353738,7359041,7355066,7357562,7357228,7359867,7354915,7352559,7354052,7359878,7350539,7350533,7349729,7351753,7352208,7350062,7346730,7344863",2017/05/04 23:28:28
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Use of machine learning in big data analytics for insider threat detection","M. Mayhew; M. Atighetchi; A. Adler; R. Greenstadt","United States Air Force Research Laboratory, Rome, NY, USA","MILCOM 2015 - 2015 IEEE Military Communications Conference","20151217","2015","","","915","922","In current enterprise environments, information is becoming more readily accessible across a wide range of interconnected systems. However, trustworthiness of documents and actors is not explicitly measured, leaving actors unaware of how latest security events may have impacted the trustworthiness of the information being used and the actors involved. This leads to situations where information producers give documents to consumers they should not trust and consumers use information from non-reputable documents or producers. The concepts and technologies developed as part of the Behavior-Based Access Control (BBAC) effort strive to overcome these limitations by means of performing accurate calculations of trustworthiness of actors, e.g., behavior and usage patterns, as well as documents, e.g., provenance and workflow data dependencies. BBAC analyses a wide range of observables for mal-behavior, including network connections, HTTP requests, English text exchanges through emails or chat messages, and edit sequences to documents. The current prototype service strategically combines big data batch processing to train classifiers and real-time stream processing to classifier observed behaviors at multiple layers. To scale up to enterprise regimes, BBAC combines clustering analysis with statistical classification in a way that maintains an adjustable number of classifiers.","","Electronic:978-1-5090-0073-9; POD:978-1-5090-0074-6","10.1109/MILCOM.2015.7357562","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357562","HTTP;TCP;big data;chat;documents;email;insider threat;machine learning;support vector machine;trust;usage patterns","Access control;Big data;Computer security;Electronic mail;Feature extraction;Monitoring","Big Data;authorisation;data analysis;document handling;learning (artificial intelligence);pattern classification;pattern clustering;trusted computing","BBAC;English text exchanges;HTTP requests;actor trustworthiness;behavior-based access control;big data analytics;big data batch processing;chat messages;classifier training;clustering analysis;document trustworthiness;emails;enterprise environments;information trustworthiness;insider threat detection;interconnected systems;machine learning;mal-behavior;network connections;real-time stream processing;security events;statistical classification","","3","","37","","","26-28 Oct. 2015","","IEEE","IEEE Conference Publications"
"Exploring machine learning techniques for identification of cues for robot navigation with a LIDAR scanner","A. Bieszczad","Channel Islands, California State University, One University Drive, Camarillo, 93012 U.S.A.","2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","20151210","2015","01","","645","652","In this paper, we report on our explorations of machine learning techniques based on backpropagation neural networks and support vector machines in building a cue identifier for mobile robot navigation using a LIDAR scanner. We use synthetic 2D laser data to identify a technique that is most promising for actual implementation in a robot, and then validate the model using realistic data. While we explore data preprocessing applicable to machine learning, we do not apply any specific extraction of features from the raw data; instead, our feature vectors are the raw data. Each LIDAR scan represents a sequence of values for measurements taken from progressive scans (with angles vary from 0° to 180°); i.e., a curve plotting distances as a functions of angles. Such curves are different for each cue, and so can be the basis for identification. We apply varied grades of noise to the ideal scanner measurement to test the capability of the generated models to accommodate for both laser inaccuracy and robot motion. Our results indicate that good models can be built with both back-propagation neural network applying Broyden-Fletcher-Goldfarb-Shannon (BFGS) optimization, and with Support Vector Machines (SVM) assuming that data shaping took place with a [-0.5, 0.5] normalization followed by a principal component analysis (PCA). Furthermore, we show that SVM can create models much faster and more resilient to noise, so that is what we will be using in our further research and can recommend for similar applications.","","Electronic:978-9-8975-8149-6; POD:978-1-4673-6944-2","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350539","Classification;Clustering;Cue Identification;Machine Learning;Mobile Robots;Navigation;Neural Networks;Support Vector Machines","Mobile robots;Navigation;Principal component analysis;Standards;Support vector machines;Training","backpropagation;control engineering computing;learning (artificial intelligence);mobile robots;navigation;neural nets;optical radar;optical scanners;principal component analysis;support vector machines","BFGS optimization;Broyden-Fletcher-Goldfarb-Shannon optimization;LIDAR scanner;PCA;SVM;backpropagation neural networks;cues identification;curve plotting distances;feature vectors;ideal scanner measurement;machine learning techniques;mobile robot navigation;principal component analysis;raw data;support vector machines;synthetic 2D laser data","","","","16","","","21-23 July 2015","","IEEE","IEEE Conference Publications"
"Memetic Self-Configuring Genetic Programming for Solving Machine Learning Problems","M. Semenkina; E. Semenkin","Inst. of Comput. Sci. & Telecommun., Siberian State Aerosp. Univ., Krasnoyarsk, Russia","2015 IIAI 4th International Congress on Advanced Applied Informatics","20160107","2015","","","599","604","A hybridization of self-configuring genetic programming algorithms (SelfCGPs) with a local search in the space of trees is fulfilled to improve their performance for symbolic regression problem solving and artificial neural network automated design. The local search is implemented with two neighborhood systems (1-level and 2-level neighborhoods), three strategies of a tree scanning (""full"", ""incomplete"" and ""truncated"") and two ways of a movement between adjacent trees (transition by the first improvement and the steepest descent). The Lamarckian local search is applied on each generation to ten percent of best individuals. The performance of all developed memetic algorithms is estimated on a representative set of test problems of the functions approximation as well as on real-world machine learning problems. It is shown that developed memetic algorithms require comparable amount of computational efforts but outperform the original SelfCGPs both for the symbolic regression and neural network design. The best variant of the local search always uses the steepest descent but different tree scanning strategies, namely, full scanning for the solving of symbolic regression problems and incomplete scanning for the neural network automated design. Additional advantage of the approach proposed is a possibility of the automated features selection.","","Electronic:978-1-4799-9958-3; POD:978-1-4799-9959-0","10.1109/IIAI-AAI.2015.290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373977","genetic programming;hybridization;local search;machine learning;self-adaptation","Algorithm design and analysis;Artificial neural networks;Memetics;Neurons;Reliability;Search problems","function approximation;genetic algorithms;learning (artificial intelligence);neural nets;regression analysis","Lamarckian local search;SelfCGP;artificial neural network automated design;function approximation;memetic algorithm;memetic self-configuring genetic programming algorithm;real-world machine learning problem;symbolic regression problem;symbolic regression problem solving;tree scanning strategy","","","","18","","","12-16 July 2015","","IEEE","IEEE Conference Publications"
"Machine learning techniques for improved data prefetching","D. Guttman; M. T. Kandemir; M. Arunachalam; R. Khanna","Computer Science and Engineering, Penn State University, USA","5th International Conference on Energy Aware Computing Systems & Applications","20151210","2015","","","1","4","With the advent of teraflop-scale computing on both a single coprocessor and many-core designs, there is tremendous need for techniques to fully utilize the compute power by keeping cores fed with data. Data prefetching has been used as a popular method to hide memory latencies by fetching data proactively before the processor needs the data. Fetching data ahead of time from the memory subsystem into faster caches reduces observable latencies or wait times on the processor end and this improves overall program execution times. We study two types of prefetching techniques that are available on a 61-core Intel Xeon Phi co-processor, namely software (compiler-guided) prefetching and hardware prefetching on a variety of workloads. Using machine learning techniques, we synthesize workload phases and the sequence of phase patterns using raw performance data from hardware counters such as memory bandwidth, miss ratios, prefetches issued, etc. Furthermore, we use performance data from workloads with different impacts and behaviors under various prefetcher settings. Our contribution can help in future prefetching design in the following ways: (1) to identify phases within workloads that have different characteristics and behaviors and help dynamically modify prefetch types and intensities to suit the phase; (2) to manage auto setting of prefetcher knobs without great effort from the user; (3) to influence software and hardware prefetching interaction designs in future processors; and (4) to use valuable insights and performance data in many areas such as power provisioning for the nodes in a large cluster to maximize both energy and performance efficiencies.","","Electronic:978-1-4799-1771-6; POD:978-1-4799-1772-3","10.1109/ICEAC.2015.7352208","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352208","","Coprocessors;Hardware;Measurement;Prefetching;Tuning","cache storage;learning (artificial intelligence);multiprocessing systems;software performance evaluation","61-core Intel Xeon Phi co-processor;cache memory latencies;data prefetching;machine learning technique;many-core design;program execution time;single coprocessor;teraflop-scale computing","","","","33","","","24-26 March 2015","","IEEE","IEEE Conference Publications"
"Machine learning-based energy management in a hybrid electric vehicle to minimize total operating cost","X. Lin; P. Bogdan; N. Chang; M. Pedram","University of Southern California, Los Angeles, 90089, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20160107","2015","","","627","634","This paper investigates the energy management problem in hybrid electric vehicles (HEVs) focusing on the minimization of the operating cost of an HEV, including both fuel and battery replacement cost. More precisely, the paper presents a nested learning framework in which both the optimal actions (which include the gear ratio selection and the use of internal combustion engine versus the electric motor to drive the vehicle) and limits on the range of the state-of-charge of the battery are learned on the fly. The inner-loop learning process is the key to minimization of the fuel usage whereas the outer-loop learning process is critical to minimization of the amortized battery replacement cost. Experimental results demonstrate a maximum of 48% operating cost reduction by the proposed HEV energy management policy.","","Electronic:978-1-4673-8388-2; POD:978-1-4673-8109-3; USB:978-1-4673-8389-9","10.1109/ICCAD.2015.7372628","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372628","","Batteries;Energy management;Fuels;Hybrid electric vehicles;Ice;Propulsion","cost reduction;energy management systems;fuel economy;hybrid electric vehicles;learning (artificial intelligence)","HEV energy management policy;battery replacement cost;hybrid electric vehicle;inner-loop learning process;machine learning-based energy management;nested learning framework;outerloop learning process;total operating cost minimizatino","","1","","30","","","2-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Spreadsheet interfaces for usable machine learning","A. Sarkar","Computer Laboratory, University of Cambridge, 15 JJ Thomson Avenue, United Kingdom","2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20151217","2015","","","283","284","In the 21st century, it is common for people of many professions to have interesting datasets to which machine learning models may be usefully applied. However, they are often unable to do so due to the lack of usable tools for statistical non-experts. We present a line of research into using the spreadsheet - already familiar to end-users as a paradigm for data manipulation - as a usable interface which lowers the statistical and computing knowledge barriers to building and using these models.","","Electronic:978-1-4673-7457-6; POD:978-1-4673-7458-3","10.1109/VLHCC.2015.7357228","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357228","","Computational modeling;Google;Visualization","data analysis;learning (artificial intelligence);spreadsheet programs;user interfaces","data manipulation;machine learning;spreadsheet interface","","1","","14","","","18-22 Oct. 2015","","IEEE","IEEE Conference Publications"
"Generic and specific impression estimation of clothing fabric images based on machine learning","Yen-Wei Chen; D. Chen; Xian-hua Han; X. Huang","College of Computer Science and Information Technology, Central South University of Forestry and Technology, Changsha, Hunan 410004, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","20160114","2015","","","1753","1757","Consumers' psychological feeling or impression is an important factor for product design. The impression estimation becomes an important issue. In this paper, we propose generic and specific impression estimation methods based on machine learning for cloth fabric images. We use a semantic differential (SD) method to measure the user's impression such as bright, warm while they viewing a cloth fabric image. We also extract both global and local features of cloth fabric images such as color and texture using computer vision techniques. Then we use support vector regression to model the mapping functions between the generic impression (or specific impression) and image features. The learned mapping functions are used to estimate the generic or specific impression of cloth fabric images.","","CD-ROM:978-1-4673-7681-5; Electronic:978-1-4673-7682-2; POD:978-1-4673-7683-9","10.1109/FSKD.2015.7382212","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382212","Clothing Fabric Image;Generic and Specific Impression;Global and Local Features;Machine Learning;Semantic Differential method","Clothing;Correlation;Estimation;Fabrics;Feature extraction;Image color analysis;Semantics","clothing;computer vision;consumer behaviour;estimation theory;fabrics;image colour analysis;image texture;learning (artificial intelligence);product design;psychology;regression analysis;support vector machines","clothing fabric images;computer vision;consumers psychological feeling;generic impression estimation;image color;image texture;machine learning;product design;specific impression estimation;support vector regression","","","","7","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Identification of Type 2 Diabetes Risk Factors Using Phenotypes Consisting of Anthropometry and Triglycerides based on Machine Learning","B. J. Lee; J. Y. Kim","Medical Research Division, Korea Institute of Oriental Medicine, Daejeon, Korea","IEEE Journal of Biomedical and Health Informatics","20151231","2016","20","1","39","46","The hypertriglyceridemic waist (HW) phenotype is strongly associated with type 2 diabetes; however, to date, no study has assessed the predictive power of phenotypes based on individual anthropometric measurements and triglyceride (TG) levels. The aims of the present study were to assess the association between the HW phenotype and type 2 diabetes in Korean adults and to evaluate the predictive power of various phenotypes consisting of combinations of individual anthropometric measurements and TG levels. Between November 2006 and August 2013, 11 937 subjects participated in this retrospective cross-sectional study. We measured fasting plasma glucose and TG levels and performed anthropometric measurements. We employed binary logistic regression (LR) to examine statistically significant differences between normal subjects and those with type 2 diabetes using HW and individual anthropometric measurements. For more reliable prediction results, two machine learning algorithms, naive Bayes (NB) and LR, were used to evaluate the predictive power of various phenotypes. All prediction experiments were performed using a tenfold cross validation method. Among all of the variables, the presence of HW was most strongly associated with type 2 diabetes (p <; 0.001, adjusted odds ratio (OR) = 2.07 [95% CI, 1.72-2.49] in men; p <; 0.001, adjusted OR = 2.09 [1.79-2.45] in women). When comparing waist circumference (WC) and TG levels as components of the HW phenotype, the association between WC and type 2 diabetes was greater than the association between TG and type 2 diabetes. The phenotypes tended to have higher predictive power in women than in men. Among the phenotypes, the best predictors of type 2 diabetes were waist-to-hip ratio + TG in men (AUC by NB = 0.653, AUC by LR = 0.661) and rib-to-hip ratio + TG in women (AUC by NB = 0.73, AUC by LR = 0.735). Although the presence of HW demonstrated the stro- gest association with type 2 diabetes, the predictive power of the combined measurements of the actual WC and TG values may not be the best manner of predicting type 2 diabetes. Our findings may provide clinical information concerning the development of clinical decision support systems for the initial screening of type 2 diabetes.","2168-2194;21682194","","10.1109/JBHI.2015.2396520","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035040","Anthropometric measurements;Data mining;Hypertriglyceridemic waist phenotype;Machine learning;Predictor;Triglycerides;Type 2 diabetes;data mining;hypertriglyceridemic waist (HW) phenotype;machine learning;predictor;triglycerides (TG);type 2 diabetes","Atmospheric measurements;Biomedical measurement;Diabetes;Indexes;Niobium;Optical wavelength conversion;Power measurement","Bayes methods;anthropometry;biomedical measurement;blood;data mining;decision support systems;diseases;learning (artificial intelligence);medical diagnostic computing;proteins;sensitivity analysis;sugar","Korean adults;TG levels;anthropometry;binary logistic regression;clinical decision support systems;clinical information;fasting plasma glucose;hypertriglyceridemic waist phenotype;individual anthropometric measurements;machine learning algorithms;naive Bayes;predictive power;retrospective cross-sectional study;rib-to-hip ratio+TG;tenfold cross-validation method;triglycerides;type 2 diabetes risk factor identification;waist-to-hip ratio+TG","0","2","","56","","20150206","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"LTE Connectivity and Vehicular Traffic Prediction Based on Machine Learning Approaches","C. Ide; F. Hadiji; L. Habel; A. Molina; T. Zaksek; M. Schreckenberg; K. Kersting; C. Wietfeld","Commun. Networks Inst., Tech. Univ. Dortmund Univ., Dortmund, Germany","2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)","20160128","2015","","","1","5","The prediction of both, vehicular traffic and communication connectivity are important research topics. In this paper, we propose the usage of innovative machine learning approaches for these objectives. For this purpose, Poisson Dependency Networks (PDNs) are introduced to enhance the prediction quality of vehicular traffic flows. The machine learning model is fitted based on empirical vehicular traffic data. The results show that PDNs enable a significantly better short-term prediction in comparison to a prediction based on the physics of traffic. To combine vehicular traffic with cellular communication networks, a correlation between connectivity indicators and vehicular traffic flow is shown based on measurement results. This relationship is leveraged by means of Poisson regression trees in both directions, and hence, enabling the prediction of both types of network utilization.","","Electronic:978-1-4799-8091-8; POD:978-1-4799-8092-5","10.1109/VTCFall.2015.7391019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391019","","Communication systems;Correlation;Data models;Detectors;Physics;Predictive models;Roads","Long Term Evolution;cellular radio;learning (artificial intelligence);regression analysis;road traffic;road vehicles;stochastic processes;traffic engineering computing;trees (mathematics)","LTE connectivity;PDN;Poisson dependency networks;Poisson regression trees;cellular communication networks;communication connectivity;connectivity indicators;machine learning model;network utilization;prediction quality enhancement;vehicular traffic flows;vehicular traffic prediction","","1","","16","","","6-9 Sept. 2015","","IEEE","IEEE Conference Publications"
"Unsupervised adaptation of ASR systems: An application of dynamic programming in machine learning","A. A. Babu; A. A. Rao; R. Yellasiri","CSE Department, JNIAS - JNTUA, Ananthapuramu, Andhra Pradesh, India","2015 SAI Intelligent Systems Conference (IntelliSys)","20151221","2015","","","245","253","Dynamic Programming (DP) is used for solving various complex problems. In this paper, it is proposed to use DP for measuring the shortest phonetic distance between two words called Dynamic Phone Warping (DPW) and use DPW engine to classify whether a given pronunciation is a new accent or a new word. Humans learn new accents and words from ""Every day speech"" whereas Humanoids lack this capability. In this paper, an adaptation framework is proposed using DPW algorithm to enable the Automatic Speech Recognition (ASR) systems to learn from the unlabeled data. The algorithms are implemented using Java language. Data sets are extracted from CMU Pronunciation Dictionary CMUDICT, TIMIT speech corpus and Hindu newspaper. The new algorithms have application to unsupervised learning and adaptation of ASR systems. It makes the ASR systems inexpensive, fast and improves performance of the existing systems.","","Electronic:978-1-4673-7606-8; POD:978-1-4673-7607-5; USB:978-1-4673-7605-1","10.1109/IntelliSys.2015.7361150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361150","ASR Systems;dynamic phone warping;everyday speech;machine learning;multi-layer code book;phonetic distance","Acoustics;Adaptation models;Classification algorithms;Dynamic programming;Heuristic algorithms;Hidden Markov models;Speech","Java;dynamic programming;speech recognition;unsupervised learning","CMU pronunciation dictionary;CMUDICT;DP;DPW engine;Hindu newspaper;Java language;TIMIT speech corpus;automatic speech recognition systems;dynamic phone warping;dynamic programming;machine learning;performance improvement;pronunciation;shortest-phonetic distance measurement;unlabeled data;unsupervised ASR system adaptation;unsupervised learning","","","","25","","","10-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Unified Algorithmic Framework for Block-Structured Optimization Involving Big Data: With applications in machine learning and signal processing","M. Hong; M. Razaviyayn; Z. Q. Luo; J. S. Pang","Digital Technology Center, University of Minnesota, Minneapolis, Minnesota 55406 United States","IEEE Signal Processing Magazine","20151225","2016","33","1","57","77","This article presents a powerful algorithmic framework for big data optimization, called the block successive upper-bound minimization (BSUM). The BSUM includes as special cases many well-known methods for analyzing massive data sets, such as the block coordinate descent (BCD) method, the convex-concave procedure (CCCP) method, the block coordinate proximal gradient (BCPG) method, the nonnegative matrix factorization (NMF) method, the expectation maximization (EM) method, etc. In this article, various features and properties of the BSUM are discussed from the viewpoint of design flexibility, computational efficiency, parallel/distributed implementation, and the required communication overhead. Illustrative examples from networking, signal processing, and machine learning are presented to demonstrate the practical performance of the BSUM framework.","1053-5888;10535888","","10.1109/MSP.2015.2481563","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366709","","Algorithm design and analysis;Approximation methods;Big data;Machine learning algorithms;Optimization;Signal processing algorithms;Upper bound","Big Data;concave programming;expectation-maximisation algorithm;gradient methods;learning (artificial intelligence);matrix algebra;minimisation","BCD method;BCPG method;BSUM;CCCP method;EM method;NMF method;big data optimization;block coordinate descent method;block coordinate proximal gradient;block structured optimization;block successive upper-bound minimization;computational efficiency;convex concave procedure;design flexibility;expectation maximization;machine learning;nonnegative matrix factorization;parallel-distributed implementation;signal processing;unified algorithmic framework","","9","","107","","","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Supervised Machine Learning and Bag of Words applied to Polen Grains Classification","C. Nascimento Martins Rodrigues; A. Barbosa Goncalves; H. Pistori; G. Goncalves da Silva","Univ. Catolica Dom Bosco - UCDB, Campo Grande, Brazil","IEEE Latin America Transactions","20160120","2015","13","10","3498","3504","This work presents some new results regarding the automation of pollen grains classification using computer vision. A technique based on the Bag of Visual Words and Supervised Machine Learning algorithms is proposed and evaluated. A dataset of pollen grain images taken from 9 different pollen types was created and used in the experiments.","1548-0992;15480992","","10.1109/TLA.2015.7387260","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387260","bag-of-words;pattern recognition;polen","Automation;Computer vision;Detectors;Malignant tumors;Support vector machines;Visualization","biology computing;botany;computer vision;image classification;learning (artificial intelligence)","bag of visual words techniques;computer vision;pollen grain images;pollen grains classification;pollen types;supervised machine learning algorithms","","","","","","","Oct. 2015","","IEEE","IEEE Journals & Magazines"
"Machine Learning Techniques in Storm","Z. Han; M. Xu","Inst. of Data & Knowledge Eng., Henan Univ., Kaifeng, China","2015 Seventh International Symposium on Parallel Architectures, Algorithms and Programming (PAAP)","20160121","2015","","","139","142","Storm is used to process real-time and bulk data and with the exponential growth of the amount of real-time data, storm has a wide range of usefulness. In order to be better used by companies and projects, Apache adds the technology of machine learning into storm which is called trident-ml. Trident-ml can provides more and more accurate services for us. The paper introduce briefly storm and presents the detail information about trident-ml.","2168-3042;21683042","Electronic:978-1-4673-9117-7; POD:978-1-4673-9118-4","10.1109/PAAP.2015.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387315","Machine learning;Storm;Trident;Trident-ml","Clustering algorithms;Fasteners;Libraries;Machine learning algorithms;Real-time systems;Storms;Topology","learning (artificial intelligence);public domain software","Storm;Trident-ml;machine learning technique","","","","12","","","12-14 Dec. 2015","","IEEE","IEEE Conference Publications"
"Weighting technique on multi-timeline for machine learning-based anomaly detection system","K. Limthong; K. Fukuda; Y. Ji; S. Yamada","Computer Engineering Department, Bangkok University, Prathumthani 12120 Thailand","2015 International Conference on Computing, Communication and Security (ICCCS)","20160107","2015","","","1","6","Anomaly detection is one of the crucial issues of network security. Many techniques have been developed for certain application domains, and recent studies show that machine learning technique contains several advantages to detect anomalies in network traffic. One of the issues applying this technique to real network is to understand how the learning algorithm contains more bias on new traffic than old traffic. In this paper, we investigate the dependency of the time period for learning on the performance of anomaly detection in Internet traffic. For this, we introduce a weighting technique that controls influence of recent and past traffic data in an anomaly detection system. Experimental results show that the weighting technique improves detection performance between 2.7-112% for several learning algorithms, such as multivariate normal distribution, knearest neighbor, and one-class support vector machine.","","Electronic:978-1-4673-9354-6; POD:978-1-4673-9355-3","10.1109/CCCS.2015.7374168","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374168","anomaly detection;machine learning;multiple timeline;weighting technique","Delays;Routing;Routing protocols;Throughput;Vehicular ad hoc networks","learning (artificial intelligence);security of data;support vector machines","Internet traffic;k-nearest neighbor;machine learning-based anomaly detection system;multivariate normal distribution;network security;network traffic;support vector machine;weighting technique","","","","10","","","4-5 Dec. 2015","","IEEE","IEEE Conference Publications"
"Beatmap generator for Osu Game using machine learning approach","D. B. Perkasa; N. U. Maulidevi","Computer Science Department, Institut Teknologi Bandung, Bandung, Indonesia","2015 International Conference on Electrical Engineering and Informatics (ICEEI)","20151217","2015","","","77","81","Rhythm game as one of the most-played game genres has its own attractiveness. Each song in the game gives its player new excitement to try another song or another difficulty level. However, behind every song being played is a lot of work. A beatmap should be created in order for a song to be played in the game. This paper presents an alternate way to create a beatmap that is considered playable for Osu Game utilizing beat and melody detection using machine learning approach and SVM as its learning method. The steps consists of notes detection and notes placement. Notes detection basically consists of features extraction from an audio file using DSP Java Library and learning process using Weka and LibSVM. However, detect the presence of notes only does not solve anything. The notes should be placed in the game using PRAAT and Note Placement Algorithm. From this process, a beatmap can be created from a song in about 3 minutes and the accuracy of the note detection is 86%.","","Electronic:978-1-4673-7319-7; POD:978-1-4673-7320-3","10.1109/ICEEI.2015.7352473","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352473","beat detection;beatmap;beatmap generator;machine learning;melody detection;osu","Feature extraction;Games;Generators;Instruments;Libraries;Rhythm;Support vector machines","Java;audio signal processing;computer games;feature extraction;learning (artificial intelligence);music;software libraries;support vector machines","DSP Java Library;LibSVM;Osu game;PRAAT;SVM;Weka;audio file;beatmap generator;feature extraction;machine learning approach;melody detection;note detection;note placement;rhythm game","","","","6","","","10-11 Aug. 2015","","IEEE","IEEE Conference Publications"
"Prediction of Long-Lead Heavy Precipitation Events Aided by Machine Learning","Y. Di","Dept. of Comput. Sci., Univ. of Massachusetts Boston, Boston, MA, USA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","20160204","2015","","","1496","1497","Long-lead prediction of heavy precipitation events has a significant impact since it can provide an early warning of disasters, like a flood. However, the performance of existed prediction models has been constrained by the high dimensional space and non-linear relationship among variables. In this study, we study the prediction problem from the prospective of machine learning. In our machine-learning framework for forecasting heavy precipitation events, we use global hydro-meteorological variables with spatial and temporal influences as features, and the target weather events that last several days have been formulated as weather clusters. Our study has three phases: 1) identify weather clusters in different sizes, 2) handle the imbalance problem within the data, 3) select the most-relevant features through the large feature space. We plan to evaluate our methods with several real world data sets for predicting the heavy precipitation events.","","Electronic:978-1-4673-8493-3; POD:978-1-4673-8494-0","10.1109/ICDMW.2015.218","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395847","Heavy Precipitation Event;Imbalance;Large Feature Space;Long Lead","Computer science;Conferences;Floods;Oceans;Predictive models;Wind","atmospheric precipitation;emergency management;geophysics;learning (artificial intelligence);meteorology","disasters warning;global hydro-meteorological variables;heavy precipitation events;high dimensional space;long-lead prediction;machine learning;nonlinear relationship;weather clusters","","","","7","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Heterogeneous Feature Space Based Task Selection Machine for Unsupervised Transfer Learning","S. Xue; J. Lu; G. Zhang; L. Xiong","Centre for Quantum Comput. & Intell. Syst., Univ. of Technol. Sydney, Sydney, NSW, Australia","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","20160114","2015","","","46","51","Transfer learning techniques try to transfer knowledge from previous tasks to a new target task with either fewer training data or less training than traditional machine learning techniques. Since transfer learning cares more about relatedness between tasks and their domains, it is useful for handling massive data, which are not labeled, to overcome distribution and feature space gaps, respectively. In this paper, we propose a new task selection algorithm in an unsupervised transfer learning domain, called as Task Selection Machine (TSM). It goes with a key technical problem, i.e., feature mapping for heterogeneous feature spaces. An extended feature method is applied to feature mapping algorithm. Also, TSM training algorithm, which is main contribution for this paper, relies on feature mapping. Meanwhile, the proposed TSM finally meets the unsupervised transfer learning requirements and solves the unsupervised multi-task transfer learning issues conversely.","","Electronic:978-1-4673-9323-2; POD:978-1-4673-9324-9","10.1109/ISKE.2015.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383023","feature mapping;heterogeneous feature space;multi-task learning;transfer learning;unsupervised learnig","Algorithm design and analysis;Feature extraction;Intelligent systems;Knowledge engineering;Speech recognition;Support vector machines;Training","feature selection;unsupervised learning","TSM training algorithm;extended feature method;feature mapping algorithm;feature selection methods;heterogeneous feature space;task selection machine;unsupervised multitask transfer learning","","","","16","","","24-27 Nov. 2015","","IEEE","IEEE Conference Publications"
"Path Planning Efficiency Maximization for Ball-Picking Robot Using Machine Learning Algorithm","Y. Liu; S. Li; Z. Xia","Inst. of Optoelectron., Shenzhen Univ., Shenzhen, China","2015 International Conference on Intelligent Transportation, Big Data and Smart City","20160121","2015","","","551","555","This paper aims to find a ball-picking optimal path and drives the robots to collect all the tennis in the shortest time, according to the optimal path. Thus, the work to be completed for us includes: establish agent model for robot working environment in tennis yard, analyze the advantage and shortcoming of ACO and provide an improved ACO. Our scheme improves the pheromone updating strategy. The global and local updates are integrated to strength the pheromone strength of optimal ant. Then we add the crossover and mutation operation of GA to speed the convergence of algorithm. By the simulation results analysis we find that the improved ACO has stronger optimizing ability and stability, which further improves the performance of ball-picking path.","","Electronic:978-1-5090-0464-5; POD:978-1-5090-0465-2","10.1109/ICITBS.2015.141","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384087","ACO;GA;optimization;pheromone;robot","Algorithm design and analysis;Convergence;Optimization;Path planning;Robot kinematics;Sports equipment","ant colony optimisation;genetic algorithms;grippers;learning (artificial intelligence);mobile robots;optimal control;optimisation;path planning","ACO;GA;agent model;ball-picking optimal path;ball-picking robot;crossover operation;machine learning algorithm;mutation operation;optimal ant pheromone strength;path planning efficiency maximization;pheromone updating strategy;robot working environment;tennis yard","","","","5","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"PICO extraction by combining the robustness of machine-learning methods with the rule-based methods","S. Chabou; M. Iglewski","Computer Science and Engineering Department, Universit&#233; du Qu&#233;bec en Outaouais, Gatineau, Canada, J8Y 3G5","2015 World Congress on Information Technology and Computer Applications (WCITCA)","20160104","2015","","","1","4","Machine-learning methods (MLMs) are robust methods in the extraction of the information; they have been also used in the extraction of PICO elements in order to answer clinical questions; MLMs are only used at coarse-grained level in PICO extraction, because of lack of training corpora for PICO at the fine-grained level. Coarse-grained level cannot explore the semantics within the sentence for use as a means of relevance between different answers. We propose a hybrid approach combining the robustness of MLMs and the fine grained level of RBMs to enhance PICO extraction process and facilitate the validity and the pertinence of the answers to clinic questions formulated with the PICO framework.","","Electronic:978-1-4673-6636-6; POD:978-1-4673-6637-3","10.1109/WCITCA.2015.7367038","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367038","CRFs;PICO extraction;machine-learning methods;rule-based methods","Robustness;Semantics;Text analysis;Training;Unified modeling language;Yttrium","knowledge based systems;learning (artificial intelligence);medical information systems","MLM;PICO elements;PICO extraction;RBM;clinic questions;coarse-grained level;fine-grained level;information extraction;machine-learning methods;robust methods;rule-based methods","","","","14","","","11-13 June 2015","","IEEE","IEEE Conference Publications"
"Stepping up theoretical investigations of ultrashort and intense laser pulses with overdense plasmas. Combining particle-in-cell simulations with machine learning and big data","A. Mihailescu","Lasers Department, National Institute for Laser, Plasma and Radiation Physics, Magurele, Romania","2015 Conference Grid, Cloud & High Performance Computing in Science (ROLCG)","20160104","2015","","","1","4","Over the past decade, advances in the laser technology brought about an increase in the maximum achievable laser intensity of six orders. At the same time, the pulse duration was considerably shortened. The interaction of such ultrashort and intense laser pulses with solid targets and dense plasmas is a rapidly developing area of physics. Hence, a growing interest in characterizing as accurately as possible the phenomena of absorption and reflection that occur during this interaction. Particle-in-Cell (PIC) simulations have traditionally been known to be one of the most important numerical tools employed in plasma physics and in laser-plasma interaction investigations. However, PIC codes are subject to non-physical behaviours such as statistical noise, non-physical instabilities, non-conservation, and numerical heating. Secondly, they require considerable computational resources. This paper proposes a novel approach by combining PIC simulations with machine learning in order to derive optimal laser-plasma interaction scenarios for particular given laboratory experiments. Over 2TB of interaction data consisting of PIC output and also of available literature data have been processed using Hadoop and Apache Mahout, respectively. The combination is a reliable tool for estimations of electron temperatures, plasma densities, parametric instabilities, offering valuable insights on potential interaction phenomena.","","CD-ROM:978-6-0673-7039-3; Electronic:978-6-0673-7040-9; POD:978-1-4799-8917-1","10.1109/ROLCG.2015.7367424","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367424","Particle-in-Cell;big data;laser-plasma interaction computational investigations;machine learning","Big data;Cloud computing;Computational modeling;Data models;Harmonic analysis;Physics;Plasmas","Big Data;learning (artificial intelligence);numerical analysis;physics computing;plasma density;plasma instability;plasma light propagation;plasma simulation;plasma temperature","PIC simulations;electron temperatures;intense laser pulses;laser intensity;laser technology;laser-plasma interaction;machine learning;numerical heating;parametric instabilities;particle-in-cell simulations;plasma densities;plasma physics;ultrashort laser pulses","","","","15","","","28-30 Oct. 2015","","IEEE","IEEE Conference Publications"
"Vehicle models identification based on the double updating support vector machine online learning algorithm","Xiang Liu; Qing Ye; Sunfu Liu","College of Electrical and Information Engineering, Changsha University of Science and Technology, China","2015 11th International Conference on Natural Computation (ICNC)","20160111","2015","","","87","93","For the traditional online support vector machine classification algorithm based on the kernel function, the weight of the misclassified sample in the learning process of classification remains unchanged, which will inevitably affect the classification accuracy. This paper presents a Double Updating Online Support Vector Machine Learning Algorithm that can update the weight real-timely. According to the change of the training support vector set when the new sample is added, the algorithm can update the weights of misclassified sample and update the existing sample weights at the same time, making the algorithm achieve better classification performance in large-scale data situation. The online support vector machine double update algorithm is applied to the vehicle recognition, the added newly vehicle models classification and recognition can be done beautifully from the experiment, and the experiment proved the validity and feasibility of the algorithm robustness.","","CD-ROM:978-1-4673-7678-5; Electronic:978-1-4673-7679-2; POD:978-1-4673-7680-8","10.1109/ICNC.2015.7377971","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377971","Double Update;Online learning;Support Vector machines;Vehicle models","Classification algorithms;Learning systems;Machine learning algorithms;Support vector machine classification;Training;Vehicles","image classification;learning (artificial intelligence);object recognition;set theory;support vector machines;traffic engineering computing","classification accuracy;double updating support vector machine online learning algorithm;kernel function;misclassified sample weight;training support vector set;vehicle models classification;vehicle models identification;vehicle models recognition","","","","20","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Computational intelligence, fuzzy systems, and machine learning: Academic vs industrial learning","R. Neruda; J. C. Figueroa-García","Institute of Computer Science, Academy of Sciences of the Czech Republic, Prague - Czech Republic","2015 Workshop on Engineering Applications - International Congress on Engineering (WEA)","20160104","2015","","","1","6","Computational intelligence, fuzzy systems, and machine learning have become very popular in different academies around the globe, including mathematics, statistical, computer science, philosophy departments, etc. While a large body of academic research has been done in the last 50 years, industrial and real world applications are incipient when compared to the amount of people involved in academic research. Thus, there are many questions and complaints to be answered in order to apply all we (scientists) have done. To do so, we provide a short-comprehensive diagnosis of the current situation and provide some short term strategies to improve the link between industry applications of academic research.","","Electronic:978-1-5090-0228-3; POD:978-1-5090-0229-0","10.1109/WEA.2015.7370156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370156","AI;Fuzzy systems;Industrial applications;Machine Learning","Artificial intelligence;Companies;Computer science;Education;Fuzzy sets;Fuzzy systems;Uncertainty","fuzzy systems;learning (artificial intelligence)","academic learning;computational intelligence;computer science;fuzzy systems;industrial learning;machine learning;mathematics departments;philosophy departments;statistical departments","","","","29","","","28-30 Oct. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning and Decision Support in Critical Care","A. E. W. Johnson; M. M. Ghassemi; S. Nemati; K. E. Niehaus; D. A. Clifton; G. D. Clifford","Inst. for Med. Eng. & Sci., Massachusetts Inst. of Technol., Boston, MA, USA","Proceedings of the IEEE","20160125","2016","104","2","444","466","Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding “secondary use of medical records” and “Big Data” analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of “precision medicine.” This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.","0018-9219;00189219","","10.1109/JPROC.2015.2501978","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390351","Critical care;feature extraction;machine learning;signal processing","Complexity theory;Data privacy;Databases;Hospitals;Interoperability;Monitoring","Big Data;health care;learning (artificial intelligence)","Big Data analytics;caregiver teams;clinical data management systems;clinical monitoring;critical care;decision support;event prediction;machine learning;online patient tracking;personalized prediction;precision medicine;risk assessment;standard clinical databases;static acuity scoring","","3","","208","","","Feb. 2016","","IEEE","IEEE Journals & Magazines"
"Throughput and Delay Estimator for 2.4GHz WiFi APs: A Machine Learning-Based Approach","S. Kajita; H. Yamaguchi; T. Higashino; H. Urayama; M. Yamada; M. Takai","Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan","2015 8th IFIP Wireless and Mobile Networking Conference (WMNC)","20160204","2015","","","223","226","This paper reports our recent result in designing a function for autonomous APs to estimate throughput and delay of its clients in 2.4GHz WiFi channels to support those APs' dynamic channel selection. Our function takes as inputs the traffic volume and strength of signals emitted from nearby interference APs as well as the target AP's traffic volume. By this function, the target AP can estimate throughput and delay of its clients without actually moving to each channel, it is just required to monitor IEEE802.11 MAC frames sent or received by the interference APs. The function is composed of an SVM-based classifier to estimate capacity saturation and a regression function to estimate both throughput and delay in case of saturation in the target channel. The training dataset for the machine learning is created by a highly-precise network simulator. We have conducted over 10,000 simulations to train the model, and evaluated using additional 2,000 simulation results. The result shows that the estimated throughput error is less than 10%.","","Electronic:978-1-5090-0351-8; POD:978-1-5090-0091-3","10.1109/WMNC.2015.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396702","Channel Selection Algorithm;Support Vector Machine","Channel estimation;Delays;Estimation;IEEE 802.11 Standard;Interference;Monitoring;Throughput","access protocols;delay estimation;learning (artificial intelligence);multi-access systems;radiofrequency interference;regression analysis;support vector machines;telecommunication traffic;wireless LAN","IEEE 802.11 MAC frames;SVM-based classifier;WiFi AP;WiFi channels;access point;autonomous AP;delay estimator;dynamic channel selection;frequency 2.4 GHz;interference AP;machine learning;network simulator;regression function;support vector machines;throughput error;traffic volume","","","","9","","","5-7 Oct. 2015","","IEEE","IEEE Conference Publications"
"Machine learning approaches on map reduce for Big Data analytics","J. V. N. Lakshmi; A. Sheshasaayee","Research Department of Computer Science, SCSVMV University, Kanchipuram, Tamil Nadu - India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","20160114","2015","","","480","484","To analyze enormous datasets, collection of algorithms, associated systems and perform necessary processing on massive data structures there is obligation for a novel trend, which is framed by Big Data. Architecture of Big Data varies across compound machines and clusters with unique purpose sub systems. The data produced from several sources requires analysis and organization with meager amounts of time. To potentially speed up the processing, a unified way of machine learning is applied on MapReduce frame work. A broadly applicable programming model MapReduce is applied on different learning algorithms belonging to machine learning family for all business decisions. By using ML algorithms with Hadoop for better storage distribution will improve the time and processing speed. This paper presents parallel implementation of various machine learning algorithms implemented on top of MapReduce model for time and processing efficiency.","","Electronic:978-1-4673-7910-6; POD:978-1-4673-7911-3; USB:978-1-4673-7909-0","10.1109/ICGCIoT.2015.7380512","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380512","Big Data;Hadoop;Machine Learning;Machine Learning Algorithms;MapReduce","Algorithm design and analysis;Big data;Clustering algorithms;Computational modeling;Computer architecture;Data models;Machine learning algorithms","Big Data;data analysis;learning (artificial intelligence);parallel programming;storage management","Big Data analytics;ML algorithms;MapReduce framework;MapReduce model;business decisions;compound machines;machine learning algorithms;machine learning approach;machine learning family;parallel implementation;processing efficiency;programming model;storage distribution","","","","20","","","8-10 Oct. 2015","","IEEE","IEEE Conference Publications"
"Decision tree-based machine learning algorithm for in-node vehicle classification","K. Ying; A. Ameri; A. Trivedi; D. Ravindra; D. Patel; M. Mozumdar","Department of Electrical Engineering, California State University Long Beach","2015 IEEE Green Energy and Systems Conference (IGESC)","20151221","2015","","","71","76","In this paper, we propose an in-node microprocessor-based vehicle classification approach to analyze and determine the types of vehicles passing over a 3-axis magnetometer sensor. Our approach for vehicle classification utilizes J48 classification algorithm implemented in Weka (a machine learning software suite). J48 is a Quinlan's C4.5 algorithm, an extension of decision tree machine learning based on ID3 algorithm. The decision tree model is generated from a set of features extracted from vehicles passing over the 3-axis sensor. The features are attributes provided with correct classifications to the J48 training algorithm to generate a decision tree model with varying degrees of classification rates based on crossvalidation. Ideally, using fewer attributes to generate the model allows for the highest computational efficiency due to fewer features needed to be calculated while minimalizing the tree with fewer branches. The generated tree model can then be easily implemented using nested if-loops in any language on a multitude of microprocessors. In addition, setting an adaptive baseline to negate the effects of the background magnetic field allows reuse of the same tree model in multiple environments. The result of our experiment shows that the vehicle classification system is effective and efficient with the accuracy at nearly 100%.","","Electronic:978-1-4673-7263-3; POD:978-1-4673-7264-0","10.1109/IGESC.2015.7359454","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359454","anisotropic magnetoresistive (AMR) sensors;machine learning algorithm;vehicle classification","Classification algorithms;Computational modeling;Decision trees;Feature extraction;Machine learning algorithms;Magnetic fields;Vehicles","decision trees;image classification;learning (artificial intelligence);magnetometers;microprocessor chips;road vehicles;traffic engineering computing","3-axis sensor;ID3 algorithm;J48 classification algorithm;J48 training algorithm;Quinlan's C4.5 algorithm;Weka;background magnetic field;decision tree machine learning;in-node microprocessor-based vehicle classification approach;machine learning software suite;magnetometer sensor;nested if-loops","","","","13","","","9-9 Nov. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning based criminal short listing using Modus Operandi features","M. Munasinghe; H. Perera; S. Udeshini; R. Weerasinghe","University of Colombo School of Computing, 07, Sri Lanka","2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)","20160111","2015","","","69","76","One of the most challenging problems faced by crime analysts is identifying sets of crimes committed by the same individual or group. Amount of criminal records piling up daily has made it cumbersome to manually process connections between crimes. These Crime series' possess certain attributes that are characteristic of the criminal(s) involved in them, which are useful in defining their modus operandi (MO). After a careful study in the grave crime category of House breaking and Theft in Sri Lanka, we have identified certain MO attributes which we have used to collect from past crime scene data from police records. Then we have explored whether it is possible to group suspects who have similar MO patterns through a machine learning approach and give a short list for a new crime from the existing data. The evaluation of the research presented an accuracy above 75% which proved that Machine Learning is capable of short listing criminals based on their Modus Operandi features.","","CD-ROM:978-1-4673-9439-0; Electronic:978-1-4673-9441-3; POD:978-1-4673-9442-0","10.1109/ICTER.2015.7377669","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7377669","Criminal Profiling;Feature Extraction;Hierarchical Clustering;Machine Learning;Modus Operandi","Encoding;Feature extraction;Unsupervised learning;Weapons","criminal law;learning (artificial intelligence);police data processing","Sri Lanka;crime analysis;criminal record amount;grave crime category;machine learning;modus operandi feature","","","","16","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Data integration in machine learning","Y. Li; A. Ngom","Information and Communications Technologies, National Research Council of Canada, Ottawa, Ontario, Canada","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1665","1671","Modern data generated in many fields are in a strong need of integrative machine learning models in order to better make use of heterogeneous information in decision making and knowledge discovery. How data from multiple sources are incorporated in a learning system is key step for a successful analysis. In this paper, we provide a comprehensive review on data integration techniques from a machine learning perspective.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359925","Bayesian network;data integration;decision tree;deep learning;feature extraction;multiple kernel learning;random forest","Bioinformatics;Genomics;Lead;Loading;Yttrium","bioinformatics;learning (artificial intelligence)","data integration technique;decision making;heterogeneous information;integrative machine learning model;knowledge discovery","","2","","53","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Automatic security classification by machine learning for cross-domain information exchange","H. Hammer; K. W. Kongsgård; A. Bai; A. Yazidi; N. A. Nordbotten; P. E. Engelstad","Oslo and Akershus University College of Applied Sciences (HiOA), Norway","MILCOM 2015 - 2015 IEEE Military Communications Conference","20151217","2015","","","1590","1595","Cross-domain information exchange is necessary to obtain information superiority in the military domain, and should be based on assigning appropriate security labels to the information objects. Most of the data found in a defense network is unlabeled, and usually new unlabeled information is produced every day. Humans find that doing the security labeling of such information is labor-intensive and time consuming. At the same time there is an information explosion observed where more and more unlabeled information is generated year by year. This calls for tools that can do advanced content inspection, and automatically determine the security label of an information object correspondingly. This paper presents a machine learning approach to this problem. To the best of our knowledge, machine learning has hardly been analyzed for this problem, and the analysis on topical classification presented here provides new knowledge and a basis for further work within this area. Presented results are promising and demonstrates that machine learning can become a useful tool to assist humans in determining the appropriate security label of an information object.","","Electronic:978-1-5090-0073-9; POD:978-1-5090-0074-6","10.1109/MILCOM.2015.7357672","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357672","Security;classification;cross-domain information exchange;labeling;machine learning","Computer security;Digital signatures;Electronic mail;Information exchange;Labeling;Metadata","learning (artificial intelligence);security of data","automatic security classification;cross-domain information exchange;machine learning;security labeling","","1","","23","","","26-28 Oct. 2015","","IEEE","IEEE Conference Publications"
"A multi-stage protein secondary structure prediction system using machine learning and information theory","M. Zamani; S. C. Kremer","School of Computer Science, University of Guelph, Canada","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1304","1309","In this paper, we evaluated the performance of a multi-stage protein secondary structure (PSS) prediction model. The proposed classifier uses statistical information and protein profiles. The statistical information is derived from protein sequences and structures by using a k-means clustering technique and Information theory. In the first stage, a feed-forward artificial neural network maps a sequence fragment to a region in the Ramachandran plot (2D-plot). A score vector is constructed with the mapped region using clustering and statistical information. The score vector represents the tendency of pairing an identified region in the 2D-plot and secondary structures for a residue. The score vectors which are used in the second stage have fewer dimensions compared to input vectors that are commonly derived from protein sequences or profile information. In the second stage, a two-tier classifier is employed based on an artificial neural network and a genetic programming (GP) method. The GP method uses IF rules for a three-state classification. The two-tier classifier's performance is compared to those of two-tier artificial neural networks (ANNs) and support vector machines (SVMs). The prediction method is examined with a common protein dataset, RS126. The performance of the proposed classification model is measured based on Q<sub>3</sub> and segment overlap (SOV) scores. The proposed PSS prediction model improves over 3% the Q<sub>3</sub> score and 2% the SOV score in comparison to those of two-tier ANN and SVMs architectures.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359867","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359867","amino acids;genetic programming;information theory;machine learning;protein secondary structure","Artificial neural networks;Information theory;Proteins;Support vector machines","biology computing;genetic algorithms;information theory;learning (artificial intelligence);molecular biophysics;neural nets;proteins;statistical analysis;support vector machines","2D-plot;PSS prediction model;Q<sub>3</sub> score;RS126;Ramachandran plot;SOV score;SVM;classification model;feed-forward artificial neural network maps;genetic programming method;k-means clustering technique;machine learning;multistage protein secondary structure prediction model;protein sequences;score vector;segment overlap scores;statistical information theory;support vector machines;three-state classification;two-tier artificial neural networks","","","","44","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning in Genomic Medicine: A Review of Computational Problems and Data Sets","M. K. K. Leung; A. Delong; B. Alipanahi; B. J. Frey","Dept. of Electr. & Comput. Eng., Univ. of Toronto, Toronto, ON, Canada","Proceedings of the IEEE","20151218","2016","104","1","176","197","In this paper, we provide an introduction to machine learning tasks that address important problems in genomic medicine. One of the goals of genomic medicine is to determine how variations in the DNA of individuals can affect the risk of different diseases, and to find causal explanations so that targeted therapies can be designed. Here we focus on how machine learning can help to model the relationship between DNA and the quantities of key molecules in the cell, with the premise that these quantities, which we refer to as cell variables, may be associated with disease risks. Modern biology allows high-throughput measurement of many such cell variables, including gene expression, splicing, and proteins binding to nucleic acids, which can all be treated as training targets for predictive models. With the growing availability of large-scale data sets and advanced computational techniques such as deep learning, researchers can help to usher in a new era of effective genomic medicine.","0018-9219;00189219","","10.1109/JPROC.2015.2494198","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347331","Computational biology;deep learning;genetic variants;genome analysis;genome biology;genomic medicine;machine learning;precision medicine","Big data;Bioinformatics;Diseases;Genomics;Machine learning;Medical treatment","DNA;cellular biophysics;diseases;genetics;genomics;learning (artificial intelligence);medical computing;medicine","DNA variations;cell variables;deep learning;disease risks;gene expression;gene splicing;genomic medicine;high-throughput measurement;large-scale data sets;machine learning;nucleic acids;proteins","","4","","231","","20151204","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Feasibility Study of a Machine Learning Approach to Predict Dementia Progression","C. L. Chi; W. Oh; S. Borson","Inst. for Health Inf., Univ. of Minnesota, Minneapolis, MN, USA","2015 International Conference on Healthcare Informatics","20151210","2015","","","450","450","We conducted a feasibility study of machine-learning to predict progression of cognitive impairment to Alzheimer's disease (AD) among individuals enrolled in the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our approach uses diverse participant information including genetic, imaging, biomarker, and neuropsychological data to predict transition to dementia in three clinical scenarios: short-term prediction (half or one year) based on a single assessment (simulating a ""new patient"" visit), short-term prediction based on information from two time points (simulating a ""follow up"" visit), and long-term (multiple years) prediction (simulating ongoing follow-up with repeated opportunities for assessment).","","Electronic:978-1-4673-9548-9; POD:978-1-4673-9549-6","10.1109/ICHI.2015.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349729","data mining;dementia progression;machine learning","Biomedical imaging;Dementia;Genetics;Informatics","cognition;diseases;learning (artificial intelligence);medical computing","ADNI;Alzheimer disease neuroimaging initiative;cognitive impairment;dementia progression;machine learning;short-term prediction","","","","","","","21-23 Oct. 2015","","IEEE","IEEE Conference Publications"
"Review of the Use of AI Techniques in Serious Games: Decision making and Machine Learning","M. Frutos-Pascual; B. Garcia Zapirain","Maite Frutos-Pascual Deustotech Life [eVIDA] University of Deusto Avda Universidades 24, Bilbao, Spain. (email: maitefrutos@deusto.es)","IEEE Transactions on Computational Intelligence and AI in Games","","2016","PP","99","1","1","The video-games market has become an established and ever-growing global industry. The health of the video and computer games industry, together with the variety of genres and technologies available, mean that videogame concepts and programmes are being applied in numerous different disciplines. One of these is the field known as serious games. The main goal of this article is to collect all the relevant articles published during the last decade and create a trend analysis about the use of certain artificial intelligence algorithms related to decision making and learning in the field of serious games. A categorization framework was designed and outlined to classify the 129 papers that met the inclusion criteria. The authors made use of this categorization framework for drawing some conclusions regarding the actual use of intelligent serious games. The authors consider that over recent years enough knowledge has been gathered to create new intelligent serious games to consider not only the final aim but also the technologies and techniques used to provide players with a nearly real experience. However, researchers may need to improve their testing methodology for developed serious games, so as to ensure they meet their final purposes.","1943-068X;1943068X","","10.1109/TCIAIG.2015.2512592","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366548","","Algorithm design and analysis;Artificial intelligence;Decision making;Decision trees;Games;Industries;Market research","","","","","","","","20151225","","","IEEE","IEEE Early Access Articles"
"Proactive Scalability and Management of Resources in Hybrid Clouds via Machine Learning","D. R. Avresky; P. D. Sanzo; A. Pellegrini; B. Ciciani; L. Forte","IRIANC, Munich, Germany","2015 IEEE 14th International Symposium on Network Computing and Applications","20160107","2015","","","114","119","In this paper, we present a novel framework for supporting the management and optimization of application subject to software anomalies and deployed on large scale cloud architectures, composed of different geographically distributed cloud regions. The framework uses machine learning models for predicting failures caused by accumulation of anomalies. It introduces a novel workload balancing approach and a proactive system scale up/scale down technique. We developed a prototype of the framework and present some experiments for validating the applicability of the proposed approaches.","","Electronic:978-1-5090-1849-9; POD:978-1-5090-1850-5","10.1109/NCA.2015.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371712","cloud;hybrid cloud;overlay networks;rejuvenation;software aging;workload balancing","Cloud computing;Computational modeling;Computer architecture;Computer crashes;Predictive models;Proposals","cloud computing;learning (artificial intelligence);resource allocation;security of data;virtual machines","geographically distributed cloud regions;hybrid clouds;large scale cloud architectures;machine learning;proactive scalability;proactive system scale up-scale down technique;resource management;software anomalies;workload balancing approach","","2","","25","","","28-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"Machine learning for stress detection from ECG signals in automobile drivers","N. Keshan; P. V. Parimi; I. Bichindaritz","Advanced Wireless Systems Research Center, State University of New York at Oswego, Oswego, NY 13126, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2661","2669","Physiological sensor analytics is becoming an important tool to monitor health as the availability of sensor-enabled portable, wearable, and implantable devices becomes ubiquitous in the growing Internet of Things (IoT). Physiological multi-sensor studies have been conducted previously to detect stress. In this study, we focus on ECG monitoring that can now be performed with minimally invasive wearable patches and sensors, to develop an efficient and robust mechanism for accurate stress identification. A unique aspect of our research is personalized individual stress analysis including three stress levels: low, medium and high. Using machine learning algorithms from the ECG signals alone, we could achieve 88.24% accuracy in detecting the three classes of stress. We also find that high stress can be successfully detected for a person in comparison to his or her rest period with 100% accuracy.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364066","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364066","classification;driver monitoring;driving;ecg signals;machine learning;physiological sensors;precision medicine;stress medicine;time series","Biomedical monitoring;Electrocardiography;Feature extraction;Monitoring;Stress;Training;Vehicles","Internet of Things;electrocardiography;learning (artificial intelligence);medical signal processing;sensor fusion","ECG monitoring;ECG signals;Internet of things;IoT;automobile drivers;machine learning;machine learning algorithms;minimally invasive wearable patches;personalized individual stress analysis;physiological multisensor studies;physiological sensor analytics;stress detection;stress identification","","","","16","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Robust blind watermarking technique for color images using Online Sequential Extreme Learning Machine","A. Rajpal; A. Mishra; R. Bala","Department of Computer Science, Deendayal Upadhyay College, University of Delhi, New Delhi, India","2015 International Conference on Computing, Communication and Security (ICCCS)","20160107","2015","","","1","7","In this paper, a robust blind color image watermarking technique using Online Sequential Extreme Learning Machine (OS-ELM) is proposed. Blue channel is utilized and transformed using DWT. Low frequency LL4 subband is used for watermark embedding. A variant of mini-batch machine learning algorithm i.e. OS-ELM is initially tuned with a fixed number of training data used in its initial phase and size of block of data learned by it in each step. The training data to OSELM is constructed by combining the quantized and desired LL4 sub-band coefficients of the DWT domain. A random key decides the starting watermark embedding position of the coefficients. Two binary images are used as watermark. The robustness towards common image processing attacks is enhanced using this process. Experimental results show that the extracted watermarks from watermarked and attacked images are similar to the original watermarks. Computed time spans for embedding and extraction are of the order of milliseconds, which is suitable for developing real time watermarking applications.","","Electronic:978-1-4673-9354-6; POD:978-1-4673-9355-3","10.1109/CCCS.2015.7374127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374127","BER;Blind Watermarking;Extreme Learning Machine;MSSIM;Normalized Correlation (NC);Online Sequential;PSNR","Color;Discrete wavelet transforms;Image processing;Robustness;Training data;Visualization;Watermarking","discrete wavelet transforms;image coding;image watermarking;learning (artificial intelligence)","DWT domain;OS-ELM;blue channel utilization;discrete wavelet transform;image processing attacks;low frequency LL4 subband coefficients;mini batch machine learning algorithm;online sequential extreme learning machine;random key;robust blind color image watermarking technique","","1","","23","","","4-5 Dec. 2015","","IEEE","IEEE Conference Publications"
"Reduced overhead error compensation for energy efficient machine learning kernels","S. Zhang; N. R. Shanbhag","Dept. of Electr. & Comput. Eng., Univ. of Illinois at Urbana-Champaign, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20160107","2015","","","15","21","Low overhead error-resiliency techniques such as RAZOR [1] and algorithmic noise-tolerance (ANT) [2] have proven effective in reducing energy consumption. ANT has been shown to be particularly effective for signal processing and machine learning kernels. In ANT, an explicit estimator block compensates for large magnitude errors in a main block. The estimator represents the overhead in ANT and can be as large as 30%. This paper presents a low overhead ANT technique referred to as ALG-ANT. In ALG-ANT, the estimator is embedded inside the main block via algorithmic reformulation and thus completely eliminates the overhead associated with ANT. However, ALG-ANT is algorithm-specific. This paper demonstrates the ALG-ANT concept in the context of a finite impulse response (FIR) filter kernel and a dot product kernel, both of which are commonly employed in signal processing and machine learning applications. The proposed ALG-ANT FIR filter and dot product kernels are applied to the feature extractor (FE) and SVM classification engine (CE) of an EEG seizure classification system. Simulation results in a commercial 45nm CMOS process show that ALG-ANT can compensate for error rates of up to 0.41 (errors in FE only), and up to 0.19 (errors in FE and CE) and maintain the true positive rate ptp > 0.9 and false positive rate pfp ≤ 0.01. This represents a greater than 3-orders-of-magnitude improvement in error tolerance over the conventional architecture. This error tolerance is employed to reduce energy via the use of voltage overscaling (VOS). ALG-ANT is able to achieve 44.3% energy savings when errors are in FE only, and up to 37.1% savings when errors are in both FE and CE.","","Electronic:978-1-4673-8388-2; POD:978-1-4673-8109-3; USB:978-1-4673-8389-9","10.1109/ICCAD.2015.7372544","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372544","","Electroencephalography;Error analysis;Finite impulse response filters;Iron;Kernel;Signal processing algorithms;Support vector machines","FIR filters;electroencephalography;energy conservation;energy consumption;error compensation;feature extraction;learning (artificial intelligence);power aware computing;signal classification;support vector machines","ALG-ANT FIR filter;CMOS process;EEG seizure classification;SVM classification engine;algorithmic noise-tolerance;dot product kernel;energy consumption;energy efficient machine learning kernels;energy savings;explicit estimator block;feature extractor;finite impulse response filter kernel;low overhead error-resiliency techniques;machine learning applications;reduced overhead error compensation;signal processing;voltage overscaling","","","","20","","","2-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"A comprehensive evaluation of air pollution prediction improvement by a machine learning method","X. Xi; Z. Wei; R. Xiaoguang; W. Yijie; B. Xinxin; Y. Wenjun; D. Jin","IBM CRL, Beijing, China","2015 IEEE International Conference on Service Operations And Logistics, And Informatics (SOLI)","20160104","2015","","","176","181","Urban air pollution prediction is one of the most important tasks in the treatment of urban air pollution. Due to the disadvantage that source list updated not in time for WRF-Chem which is a numeric model, the prediction result may be not good enough. In this paper, we take full advantages of forecast on pollution, weather, chemical component from WRF-Chem model as input features, design a comprehensive evaluation framework to improve the prediction performance. Experiments are implemented with different features groups and classification algorithms in machine learning method for 74 cities in China, to find the best model for each city. From experiments, for different city, the best result can be obtained by different group of feature selection and model selection. Experimental results indicate that the more feature we used, the more possibility to enhance the accuracy. For method aspect, the result from combined model is better than the unique model.","","Electronic:978-1-4673-8480-3; POD:978-1-4673-8481-0","10.1109/SOLI.2015.7367615","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367615","air pollutation prediction;air quality index prediction;combined method;machine learning","Air pollution;Atmospheric modeling;Cities and towns;Numerical models;Predictive models;Weather forecasting","air pollution;learning (artificial intelligence)","China;WRF-Chem model;air pollution prediction improvement comprehensive evaluation;chemical component;comprehensive evaluation framework;machine learning method;urban air pollution prediction","","","","19","","","15-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Engine performance optimization using machine learning techniques","P. Dutta; S. Sharma; P. A. Rathnam","School of Electronics Engineering, VIT University, Vellore, India","2015 SAI Intelligent Systems Conference (IntelliSys)","20151221","2015","","","120","126","The purpose of this paper is to integrate the concept of Supervised Learning Algorithms in Engine tuning. These days Machine learning has become a very valuable tool for prediction. A given subset of this domain involves using supervised algorithms to intake data, analyze the data and `learn' from it. The more the data that is processed by it (training stage), the better it learns (Fitting Parameters on Training Set) and the better it will be able to predict (Prediction Stage). By feeding data to the system we are teaching the system about how the input parameters (plenum volume, exhaust and intake runner length, Engine rpm) in the data are inter-related with one another and how the values of a set of variables can change by changing the value of any one variable. The efficiencies of various regression models were used and neural networks were also implemented.","","Electronic:978-1-4673-7606-8; POD:978-1-4673-7607-5; USB:978-1-4673-7605-1","10.1109/IntelliSys.2015.7361134","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361134","Back propagation Algorithm;Neural Networks;Plenum Volume;Regression Model;Runner Length","Engines;Intelligent systems;Machine learning algorithms;Prediction algorithms;Supervised learning;Torque;Valves","engines;learning (artificial intelligence);mechanical engineering computing;neural nets;regression analysis","engine performance optimization;engine tuning;input parameters;machine learning techniques;neural networks;regression models;supervised algorithms;supervised learning algorithms","","","","9","","","10-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"ADMM based scalable machine learning on Spark","S. Dhar; C. Yi; N. Ramakrishnan; M. Shah","Research and Technology Center, Robert Bosch LLC, Palo Alto, CA 94304, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1174","1182","Most machine learning algorithms involve solving a convex optimization problem. Traditional in-memory convex optimization solvers do not scale well with the increase in data. This paper identifies a generic convex problem for most machine learning algorithms and solves it using the Alternating Direction Method of Multipliers (ADMM). Finally such an ADMM problem transforms to an iterative system of linear equations, which can be easily solved at scale in a distributed fashion. We implement this framework in Apache Spark and compare it with the widely used Machine Learning LIBrary (MLLIB) in Apache Spark 1.3.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363871","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363871","ADMM;Distributed Optimization;ML-LIB;Spark","Big data;Convex functions;Distributed databases;Loss measurement;Machine learning algorithms;Optimization;Sparks","convex programming;iterative methods;learning (artificial intelligence)","ADMM problem;Apache Spark 1.3;MLLIB;Machine Learning Library;alternating direction method of multipliers;convex problem;iterative linear equations system;machine learning algorithms","","1","","37","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Systematic analysis of machine learning algorithms on EEG data for brain state intelligence","A. Chan; C. E. Early; S. Subedi; Yuezhe Li; H. Lin","Department of Statistics, University of California, Los Angeles, 90024, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","793","799","Electroencephalography (EEG) is a highly promising medium for brain-computer interfaces (BCI) with potentially extraordinary applications such as the direct control of prosthetics and exoskeletons. However, effective analysis and modeling of EEG data has been limited by its poor spatial resolution. EEG's low amplitude, brief and sporadic nature, compounded by extra-cranial noise, contribute to the difficulty of this problem. This systematic analysis provides strong evidence to guide future research in machine learning applied to real-time analysis of brain states using EEGs. The main goal of this research is to understand how the construction of data sets used in training models affects the accuracy of prominent machine learning algorithms, specifically: Random Forest, Boosting, Naïve Bayesian Classifier, k-Nearest Neighbors (KNN) and Support Vector Machine (SVM). Herein, we present a systematic method (N = 153) to test the accuracy of prominent machine learning algorithms when varying three main components of the training data set: the permutations of subjects, the number of unique data sets used to generate the training data set, and the number of samples in each training data set. Our results strongly indicate that Random Forest consistently yields superior results when analyzing EEG data compared to other prominent machine learning algorithms. Furthermore, a pilot investigation was conducted on a mean-normalized feature for EEG data analysis. The pilot analysis (N = 28) confirmed Random Forest's analytical superiority in EEG data, shows signs of improved accuracy, and identifies a distinctive correlation between beta and delta waves and their respective active or idle brain states.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359788","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359788","Bayes;Boosting;Data Mining;Electroencephalography;KNN;Machine Learning;Random Forest;SVM;Systematic Analysis","Brain models;Computational modeling;Electroencephalography;Electronic mail;Machine learning algorithms;Systematics","brain-computer interfaces;decision trees;electroencephalography;learning (artificial intelligence);medical signal processing;support vector machines","EEG data analysis;EEG data modeling;Naive Bayesian classifier;beta wave;boosting;brain state intelligence;brain-computer interface;delta wave;electroencephalography;exoskeleton;k-Nearest neighbor;machine learning algorithm systematic analysis;prosthetics;random forest;support vector machine","","","","12","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Android anomaly detection system using machine learning classification","H. Kurniawan; Y. Rosmansyah; B. Dabarsyah","School of Informatics and Electrical, Engineering, Institut Teknologi, Bandung, Jl. Ganeca no. 10, bandung 40132, Indonesia","2015 International Conference on Electrical Engineering and Informatics (ICEEI)","20151217","2015","","","288","293","Android is one of the most popular open-source smartphone operating system and its access control permission mechanisms cannot detect any malware behavior. In this paper, new software behavior-based anomaly detection system is proposed to detect anomaly caused by malware. It works by analyzing anomalies on power consumption, battery temperature and network traffic data using machine learning classification algorithm. The result shows that this method can detect anomaly with 85.6% accuracy.","","Electronic:978-1-4673-7319-7; POD:978-1-4673-7320-3","10.1109/ICEEI.2015.7352512","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352512","android;anomaly;battery;internet;machine learning;malware;power;temperature","Androids;Batteries;Humanoid robots;Malware;Support vector machines;Temperature measurement;Testing","Android (operating system);authorisation;learning (artificial intelligence);pattern classification;power aware computing;public domain software;smart phones;telecommunication traffic","Android anomaly detection system;access control permission mechanisms;battery temperature;machine learning classification algorithm;malware behavior detection;network traffic data;open-source smartphone operating system;power consumption;software behavior-based anomaly detection system","","","","34","","","10-11 Aug. 2015","","IEEE","IEEE Conference Publications"
"A Review of Relational Machine Learning for Knowledge Graphs","M. Nickel; K. Murphy; V. Tresp; E. Gabrilovich","Lab. for Comput. & Stat. Learning (LCSL), Massachusetts Inst. of Technol., Cambridge, MA, USA","Proceedings of the IEEE","20151218","2016","104","1","11","33","Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.","0018-9219;00189219","","10.1109/JPROC.2015.2483592","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358050","Graph-based models;knowledge extraction;knowledge graphs;latent feature models;statistical relational learning","Big data;Biological system modeling;Computer graphs;Data mining;Knowledge based systems;Machine learning;Predictive models;Resource description framework;Statistical analysis","data mining;graph theory;learning (artificial intelligence);statistical analysis;text analysis","Google knowledge vault project;automatic knowledge graph construction;computational cost;graph edges;graph-structured data;latent feature models;multiway neural networks;observable pattern mining;relational data;relational machine learning;statistical analysis;statistical relational models;tensor factorization;text-based information extraction methods","","10","","147","","20151217","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Bayesian Machine Learning: EEG/MEG signal processing measurements","W. Wu; S. Nagarajan; Z. Chen","Psychiatry and Behavioral Sciences, Stanford University, Stanford, California 94305 United States","IEEE Signal Processing Magazine","20151225","2016","33","1","14","36","Electroencephalography (EEG) and magnetoencephalography (MEG) are the most common noninvasive brain-imaging techniques for monitoring electrical brain activity and inferring brain function. The central goal of EEG/MEG analysis is to extract informative brain spatiotemporal?spectral patterns or to infer functional connectivity between different brain areas, which is directly useful for neuroscience or clinical investigations. Due to its potentially complex nature [such as nonstationarity, high dimensionality, subject variability, and low signal-to-noise ratio (SNR)], EEG/MEG signal processing poses some great challenges for researchers. These challenges can be addressed in a principled manner via Bayesian machine learning (BML). BML is an emerging field that integrates Bayesian statistics, variational methods, and machine-learning techniques to solve various problems from regression, prediction, outlier detection, feature extraction, and classification. BML has recently gained increasing attention and widespread successes in signal processing and big-data analytics, such as in source reconstruction, compressed sensing, and information fusion. To review recent advances and to foster new research ideas, we provide a tutorial on several important emerging BML research topics in EEG/MEG signal processing and present representative examples in EEG/MEG applications.","1053-5888;10535888","","10.1109/MSP.2015.2481559","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7366707","","Bayes methods;Brain modeling;Electroencephalography;Magnetoencephalography;Signal processing;Signal processing algorithms","Bayes methods;compressed sensing;electroencephalography;feature extraction;learning (artificial intelligence);magnetoencephalography;medical signal processing","Bayesian machine learning;Bayesian statistics;EEG-MEG signal processing measurement;big-data analytics;brain function;brain spatiotemporal spectral pattern;brain-imaging technique;compressed sensing;electrical brain activity monitoring;electroencephalography;feature extraction;information fusion;machine-learning technique;magnetoencephalography;neuroscience;regression analysis;signal classification;source reconstruction","","4","","50","","","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Traffic forecasting in complex urban networks: Leveraging big data and machine learning","F. Schimbinschi; X. V. Nguyen; J. Bailey; C. Leckie; H. Vu; R. Kotagiri","Department of Computing and Information Systems, The University of Melbourne","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1019","1024","Accurate network-wide real time traffic forecasting is essential for next generation smart cities. In this context, we study a novel and complex traffic data set and explore the potential to apply big data and machine learning analysis. We evaluate several hypotheses and find that the availability of big data is able to facilitate more accurate predictions. Furthermore, we find that spatial aspects have more influence than temporal ones and that careful choice of thresholding parameters is crucial for high performance classification.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363854","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363854","big data;time series prediction;traffic forecasting","Big data;Forecasting;Neural networks;Prediction algorithms;Roads;Spatiotemporal phenomena;Traffic control","Big Data;learning (artificial intelligence);pattern classification;smart cities;traffic engineering computing","big data;complex urban networks;high performance classification;machine learning;network-wide real time traffic forecasting;next generation smart cities","","","","21","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"On machine learning technique selection for classification","R. Kurniawan; M. Z. A. Nazri; M. Irsyad; R. Yendra; A. Aklima","UIN Sultan Syarif Kasim, Riau, Indonesia","2015 International Conference on Electrical Engineering and Informatics (ICEEI)","20151217","2015","","","540","545","Extracting meaningful pattern from data can be challenging. Irrelevant, redundant, noisy and unreliable data, misinterpretation of results and incompatibility of a technique to extract unknown patterns from data may lead analyst to develop an erroneous classifier. This research is encouraged by `No Free Lunch' theorem that can be simplified as no classification technique that works best for every problem. This study tries to make a comparison amongst three main approaches in data mining, i.e. Decision Tree (DT), Artificial Neural Network (ANN), and Rough Set Theory (RST). A comparative analysis of the above techniques has been conducted by using open source's software ROSETTA and WEKA on five different datasets. The sample sizes are categorized in relation to the number of attributes and number of instances available in the dataset. Assessments on the classification model are based on accuracy, amount and length of the generated rules, error rate and standard deviation. Based on nine experiments, results show that Artificial Neural Network provides better accuracy than Decision Tree and Rough Set approach while Rough Set creates more rules and Decision Tree generate rules faster than the compared techniques. The results show the trade off of using different approaches for other researchers in finding the best model for a particular problem.","","Electronic:978-1-4673-7319-7; POD:978-1-4673-7320-3","10.1109/ICEEI.2015.7352559","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352559","artificial neural network;decision tree;rough set theory","Algorithm design and analysis;Artificial neural networks;Classification algorithms;Data mining;Decision trees;Set theory","data mining;decision trees;learning (artificial intelligence);neural nets;rough set theory","ANN;ROSETTA;RST;WEKA;artificial neural network;data mining;decision tree;machine learning technique;no free lunch theorem;open source;rough set theory;standard deviation","","","","29","","","10-11 Aug. 2015","","IEEE","IEEE Conference Publications"
"Dynamic machine learning based matching of nonvolatile processor microarchitecture to harvested energy profile","K. Ma; X. Li; Y. Liu; J. Sampson; Y. Xie; V. Narayanan","Dept. of Computer Science and Engineering, The Pennsylvania State University, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20160107","2015","","","670","675","Energy harvesting systems without an energy storage device have to efficiently harness the fluctuating and weak power sources to ensure the maximum computational progress. While a simpler processor enables a higher turn-on potential with a weak source, a more powerful processor can utilize more energy that is harvested. Earlier work shows that different complexity levels of nonvolatile microarchitectures provide best fit for different power sources, and even different trails within same power source. In this work, we propose a dynamic nonvolatile microarchitecture by integrating all non-pipelined (NP), N-stage-pipeline (NSP), and Out of Order (OoO) cores together. Neural network machine learning algorithms are also integrated to dynamically adjust the microarchitecture to achieve the maximum forward progress. This integrated solution can achieve forward progress equal to 2.4× of the baseline NP architecture (1.82× of an OoO core).","","Electronic:978-1-4673-8388-2; POD:978-1-4673-8109-3; USB:978-1-4673-8389-9","10.1109/ICCAD.2015.7372634","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372634","Dynamic Matching;Energy Harvesting;Machine Learning;Neural Networks;Nonvolatile Processor","Computer architecture;Energy storage;Microarchitecture;Nonvolatile memory;Registers;Switches","learning (artificial intelligence);neural nets;random-access storage","N-stage-pipeline;NSP;OoO cores;baseline NP architecture;dynamic machine learning based matching;dynamic nonvolatile microarchitecture;energy harvesting systems;energy storage device;harvested energy profile;neural network machine learning algorithms;nonvolatile microarchitectures;nonvolatile processor microarchitecture;simpler processor;weak power sources","","4","","13","","","2-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Smartphone-Based Tele-Rehabilitation System for Frozen Shoulder Using a Machine Learning Approach","K. Ongvisatepaiboon; J. H. Chan; V. Vanijja","Data & Knowledge Eng. Lab., King Mongkut's Univ. of Technol. Thonburi, Bangkok, Thailand","2015 IEEE Symposium Series on Computational Intelligence","20160111","2015","","","811","815","Frozen shoulder is a very painful condition that affects patients' daily life. Patients with frozen shoulder have to go to a hospital or medical center to get appropriate rehabilitation. Transportation to the hospital raises healthcare costs and the process can be time-consuming. We have developed a tele rehabilitation system which allows patients to perform an at-home exercise. According to our existing system, it is only available for high-end smartphones with multiple sensors that include accelerometer, gyroscope, and magnetic field sensors. In this work, we propose a novel approach using machine learning to estimate the arm angle of rotation using only the accelerometer sensor. Results show that reasonable accuracy can be obtained so that it may be used with lower-end Android smartphone devices that only have an accelerometer available. A web-based interface enables the medical practitioner such as a physiotherapist to monitor and administer an appropriate rehabilitation program for more effective recovery.","","Electronic:978-1-4799-7560-0; POD:978-1-4799-7561-7","10.1109/SSCI.2015.120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376695","","Accelerometers;Data models;Gyroscopes;Magnetic sensors;Shoulder;Smart phones","learning (artificial intelligence);medical computing;patient rehabilitation;smart phones;telemedicine","Web-based interface;accelerometer sensor;at-home exercise;frozen shoulder;gyroscope;machine learning approach;magnetic field sensors;medical practitioner;physiotherapist;smart phone-based telerehabilitation system","","","","10","","","7-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Minutiae Based Automatic Fingerprint Recognition: Machine Learning Approaches","A. Ali; R. Khan; I. Ullah; A. D. Khan; A. Munir","Dept. of Electr. Eng. Eng., Sarhad Univ. of Sci. & IT, Peshawar, Pakistan","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","20151228","2015","","","1148","1153","Personnel identification has become a mandatory requirement in a large number of applications extending from security to commercial nature in recent years. Identification mechanism using Biometric-based solutions has shown to overcome several drawbacks of traditional security measures. Among different biometric traits, fingerprint is one of the most universal, permanent and easy to acquire trait for personal identification. In this article, we investigate and evaluate the performance of the state-of-the-art machine learning algorithms employed in Minutiae based automatic fingerprint recognition. Fingerprint images from Public Domain Database (DB1) of FVC 2002 are used to carry out the experiments. Fingerprint image is initially preprocessed to enhance, binarize and skeletonize. Ridge ending and ridge bifurcation Minutiae features are then extracted and used for training and testing the Random Forest, Multilayer Perceptron, Radial Basis Functions and Naïve Bayesian machine learning Algorithms. A total of 80 instances and 150 attributes have been used in the experiments. The results show that Random Forest and Radial Basis Functions give better results for varying quality images compared to the other machine learning Algorithms and show the efficacy of these algorithms.","","CD:978-1-5090-0153-8; Electronic:978-1-5090-0154-5","10.1109/CIT/IUCC/DASC/PICOM.2015.171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363215","Fingerprint Recognition;Minutiae features;Multilayer Perceptron;Naïve Bayesian;Radial Basis Functions;Random Forests","Bayes methods;Databases;Feature extraction;Fingerprint recognition;Fingers;Image matching;Machine learning algorithms","Bayes methods;biometrics (access control);feature extraction;fingerprint identification;image enhancement;learning (artificial intelligence);multilayer perceptrons;radial basis function networks;security of data","DB1;FVC 2002;Naïve Bayesian machine learning Algorithms;biometric-based solution;fingerprint image enhancement;minutiae based automatic fingerprint recognition;multilayer perceptron function;personnel Identification mechanism;public domain database;radial basis function;random forest function;ridge bifurcation minutiae feature extraction;ridge ending","","","","18","","","26-28 Oct. 2015","","IEEE","IEEE Conference Publications"
"Machine Learning Approach to Identify Users Across Their Digital Devices","T. R. Anand; O. Renov","","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","20160204","2015","","","1676","1680","This paper discusses methods to identify individual users across their digital devices as part of the ICDM 2015 competition hosted on Kaggle. The competition's data set and prize pool were provided by http://www.drawbrid.ge/ in sponsorship with the ICDM 2015 conference. The methods described in this paper focuses on feature engineering and generic machine learning algorithms like Extreme Gradient Boosting (xgboost), Follow the Reguralized Leader Proximal etc. Machine learning algorithms discussed in this paper can help improve the marketer's ability to identify individual users as they switch between devices and show relevant content/recommendation to users wherever they go.","","Electronic:978-1-4673-8493-3; POD:978-1-4673-8494-0","10.1109/ICDMW.2015.243","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395886","","Boosting;Companies;Conferences;IP networks;Measurement;Object recognition;Prediction algorithms","learning (artificial intelligence)","ICDM 2015 competition;content/recommendation;digital devices;extreme gradient boosting;feature engineering;generic machine learning algorithms;machine learning approach;reguralized leader proximal;users identification;xgboost","","","","7","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Falling Detection System Based on Machine Learning","M. Nadi; N. El-Bendary; A. E. Hassanien; T. h. Kim","Fac. of Comput. & Inf., Cairo Univ., Cairo, Egypt","2015 4th International Conference on Advanced Information Technology and Sensor Application (AITS)","20160204","2015","","","71","75","As falling is the most important issue that faces elderly people all over the world, this paper proposes a detection system for falling based on Machine Learning (ML). In the proposed system, a dataset of videos containing falling actions has been utilized via dividing each video into many shots that are consequently being converted into gray-level images. Then, for detecting the moving objects in videos, the foreground is firstly detected, then noise and shadow are deleted to detect the moving object. Finally, a number of features, including aspect ratio and falling angle, are extracted and a number of classifiers are being applied in order to detect the occurrence of falling. Experimental results, using 10-fold cross validation, shown that the proposed falling detection approach based on Linear Discriminant Analysis (LDA) classification algorithm has outperformed both support vector machines (SVMs) and Knearest neighbor (KNN) classification algorithms via achieving falling detection with accuracy of 96.59 %.","","Electronic:978-1-4673-7573-3; POD:978-1-4673-7574-0","10.1109/AITS.2015.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396449","K-nearest neighbor (KNN);aspect ratio;fall angle;falling detection;feature extraction;foreground subtraction;linear discriminant analysis (LDA);support vector machines (SVMs)","Electronic mail;Feature extraction;Image color analysis;Mathematical model;Shape;Support vector machines;Videos","assisted living;learning (artificial intelligence);object detection;pattern classification;support vector machines;video signal processing","10-fold cross validation;KNN;LDA;ML;SVM;aspect ratio;elderly people;falling actions;falling angle;falling detection system;gray-level images;k-nearest neighbor classification algorithms;linear discriminant analysis classification algorithm;machine learning;support vector machines;video dataset","","","","12","","","21-23 Aug. 2015","","IEEE","IEEE Conference Publications"
"Machine learning guided exploration for sampling-based motion planning algorithms","O. Arslan; P. Tsiotras","Institute for Robotics and Intelligent Machines at the Georgia Institute of Technology, Atlanta, 30332-0150, USA","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20151217","2015","","","2646","2652","We propose a machine learning (ML)-inspired approach to estimate the relevant region of a motion planning problem during the exploration phase of sampling-based path-planners. The algorithm guides the exploration so that it draws more samples from the relevant region as the number of iterations increases. The approach works in two steps: first, it predicts if a given sample is collision-free (classification phase) without calling the collision-checker, and it then estimates if it is a promising sample, i.e., if it has the potential to improve the current best solution (regression phase), without solving the local steering problem. The proposed exploration strategy is integrated to the RRT<sup>#</sup> algorithm. Numerical simulations demonstrate the efficiency of the proposed approach.","","Electronic:978-1-4799-9994-1; POD:978-1-4799-9995-8; USB:978-1-4799-9993-4","10.1109/IROS.2015.7353738","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353738","","Approximation algorithms;Machine learning algorithms;Planning;Prediction algorithms;Search problems;Training;Yttrium","learning (artificial intelligence);path planning;regression analysis;sampling methods","RRT# algorithm;classification phase;machine learning guided exploration;motion planning algorithm;regression phase;sampling-based path-planner","","","","22","","","Sept. 28 2015-Oct. 2 2015","","IEEE","IEEE Conference Publications"
"Automatic Recognition of Books Based on Machine Learning","B. Zhu; L. Yang; X. Wu; T. Guo","Sch. of Inf. Eng., Commun. Univ. of China, Beijing, China","2015 3rd International Symposium on Computational and Business Intelligence (ISCBI)","20160118","2015","","","74","78","The content-based image recognition is a research focus in the field of computer vision. Machine learning especially deep learning has a great potential in the field of image recognition. This paper adopts the support vector machine algorithm and deep learning method convolutional neural network to recognize books in the digital image library and compares their performance. Experiments show that both methods used in this paper realize a fast and efficient image classification and help improve the intelligence of books retrieval.","","Electronic:978-1-4673-8501-5; POD:978-1-4673-8502-2","10.1109/ISCBI.2015.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383540","deep learning;image recognition;recognition of books;support vector machine","Feature extraction;Image recognition;Image resolution;Libraries;Machine learning;Support vector machines;Training","content-based retrieval;digital libraries;electronic publishing;image classification;learning (artificial intelligence);neural nets;support vector machines","automatic recognition;books;computer vision;content-based image recognition;convolutional neural network;deep learning method;digital image library;image classification;machine learning;support vector machine algorithm","","1","","9","","","7-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Selecting Machine Learning Algorithms Using Regression Models","T. Doan; J. Kalita","Dept. of Comput. Sci., Univ. of Colorado, Colorado Springs, CO, USA","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","20160204","2015","","","1498","1505","In performing data mining, a common task is to search for the most appropriate algorithm(s) to retrieve important information from data. With an increasing number of available data mining techniques, it may be impractical to experiment with many techniques on a specific dataset of interest to find the best algorithm(s). In this paper, we demonstrate the suitability of tree-based multi-variable linear regression in predicting algorithm performance. We take into account prior machine learning experience to construct meta-knowledge for supervised learning. The idea is to use summary knowledge about datasets along with past performance of algorithms on these datasets to build this meta-knowledge. We augment pure statistical summaries with descriptive features and a misclassification cost, and discover that transformed datasets obtained by reducing a high dimensional feature space to a smaller dimension still retain significant characteristic knowledge necessary to predict algorithm performance. Our approach works well for both numerical and nominal data obtained from real world environments.","","Electronic:978-1-4673-8493-3; POD:978-1-4673-8494-0","10.1109/ICDMW.2015.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395848","Meta-learning;combined metric;dimensionality reduction;regression","Data mining;Error analysis;Machine learning algorithms;Measurement;Prediction algorithms;Regression tree analysis;Training","data mining;information retrieval;learning (artificial intelligence);pattern classification;regression analysis;trees (mathematics)","algorithm performance prediction;data mining;descriptive features;information retrieval;machine learning algorithms;meta-knowledge;misclassification cost;regression models;statistical summaries;supervised learning;tree-based multivariable linear regression","","","","40","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Prediction of radioprotectors targeting p53 for suppression of acute effect of cancer radiotherapy using machine learning","A. Matsumoto; T. Ito; Y. Nishi; T. Teraoka; S. Aoki; H. Ohwada","Department of Industrial Administration, Faculty of Science and Technology, Tokyo University of Science, Japan","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1725","1727","Radiation therapy and some chemotherapeutic agents mainly target the DNA of growing cancer cells, whereas these therapies have adverse side effects, including p53-induced apoptosis of normal tissues and cells. It is considered that p53 would be a target for therapeutic and mitigative radioprotection to escape from the apoptotic fate. So far, only three radioprotective p53 inhibitors have been reported, namely, pifithrin-α (PFTα), pifithrin-μ (PFTμ), and sodium orthovanadate (vanadate), which protect mice from acute lethality due to hematopoietic syndrome, indicating that pharmacologically temporary suppression of p53 effectively minimize the radiation damage. In this study, we examined the inhibitory activity of some zinc(II) chelators against radiation-induced apoptosis of MOLT-4 cells, based on the assumption that the binding of these compounds to zinc(II) in p53 proteins or removal of zinc(II) from the protein would temporally inhibit the function of p53. However, we have had some problems. The development of drug has been slow, due to the time required and the high cost of screening candidate compounds. It is possible to efficiently search for drugs by using machine learning. So we predict compounds that radioprotectors using Random Forest to study compound futures and using other machine learning methods for comparison with Random Forest. Procedure of learning is as follows: First, compounds were divided into several groups based on the toxicity and protection capability. Next, it was performed classification using machine learning. These results may contribute to discover of new radioprotectors.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359941","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359941","Machine Learning;Radioprotector;Random Forest;Sepport Vector Machinem","Cancer;Drugs;Labeling;Support vector machines","cancer;cellular effects of radiation;learning (artificial intelligence);proteins;radiation protection;radiation therapy;zinc","DNA;MOLT-4 cell;Zn<sup>2+</sup>;cancer cell;cancer radiotherapy;chemotherapeutic agent;hematopoietic syndrome;inhibitory activity;machine learning method;mitigative radioprotection;p53 function;p53 protein;p53 suppression;p53-induced apoptosis;pifithrin-alpha;pifithrin-mu;radiation damage;radiation-induced apoptosis;radioprotective p53 inhibitor;radioprotector prediction;random forest;sodium orthovanadate;therapeutic radioprotection;zinc ion chelator;zinc ion removal","","","","4","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Nonintrusive Load Monitoring Using Wavelet Design and Machine Learning","J. M. Gillis; S. M. Alshareef; W. G. Morsi","Department of Electrical, Computer, and Software Engineering, Faculty of Engineering and Applied Science, University of Ontario Institute of Technology (UOIT), Oshawa, ON, Canada","IEEE Transactions on Smart Grid","20151221","2016","7","1","320","328","This paper presents a new concept based on wavelet design and machine learning applied to nonintrusive load monitoring. The wavelet coefficients of length-6 filter are determined using procrustes analysis and are used to construct new wavelets to match the load signals to be detected, unlike previous work which used previously designed wavelet functions that are special cases of Daubechies filters to suit other nonpower system applications such as communications and image processing. The results of applying the new concept to a test system consisting of four loads have shown that the newly designed wavelet can improve the prediction accuracy compared with that obtained using Daubechies filter of order three while keeping the prominent features of the pattern in the detail levels.","1949-3053;19493053","","10.1109/TSG.2015.2428706","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7110380","Load signature;machine learning;nonintrusive load monitoring (NILM);wavelet design","Feature extraction;Indexes;Switches;Transient analysis;Wavelet analysis;Wavelet transforms","learning (artificial intelligence);power engineering computing;power system measurement;wavelet transforms","Daubechies filters;length-6 filter;machine learning;nonintrusive load monitoring;procrustes analysis;wavelet coefficients;wavelet design","","5","","29","","20150519","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"A study on several machine learning methods for estimating cabin occupant equivalent temperature","D. Hintea; J. Brusey; E. Gaura","Coventry University, Priory Lane, CV1 5FB, U.K.","2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","20151210","2015","01","","629","634","Occupant comfort oriented Heating, Ventilation and Air Conditioning (HVAC) control rises to the challenge of delivering comfort and reducing the energy budget. Equivalent temperature represents a more accurate predictor for thermal comfort than air temperature in the car cabin environment, as it integrates radiant heat and airflow. Several machine learning methods were investigated with the purpose of estimating cabin occupant equivalent temperature from sensors throughout the cabin, namely Multiple Linear Regression, MultiLayer Perceptron, Multivariate Adaptive Regression Splines, Radial Basis Function Network, REPTree, K-Nearest Neighbour and Random Forest. Experimental equivalent temperature and cabin data at 25 points was gathered in a variety of environmental conditions. A total of 30 experimental hours were used for training and evaluating the estimators' performance. Most machine learning tehniques provided a Root Mean Square Error (RMSE) between 1.51 °C and 1.85 °C, while the Radial Basis Function Network performed the worst, with an average RMSE of 3.37 °C. The Multiple Linear Regression had an average RMSE of 1.60 °C over the eight body part equivalent temperatures and also had the fastest processing time, enabling a straightforward real-time implementation in a car's engine control unit.","","Electronic:978-9-8975-8149-6; POD:978-1-4673-6944-2","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350533","Equivalent Temperature;HVAC Control;Machine Learning;Parameter Estimation","Learning systems;Magnetic heads;Radial basis function networks;Temperature measurement;Temperature sensors;Vehicles","HVAC;heat radiation;internal combustion engines;learning (artificial intelligence);mean square error methods;multilayer perceptrons;radial basis function networks;regression analysis;splines (mathematics);temperature control;trees (mathematics)","HVAC control;REPTree;airflow;average RMSE;cabin occupant equivalent temperature estimation;car cabin environment;car engine control unit;energy budget reduction;environmental conditions;heating-ventilation-and-air conditioning control;k-nearest neighbour;machine learning methods;multilayer perceptron;multiple linear regression;multivariate adaptive regression splines;occupant comfort;processing time;radial basis function network;radiant heat;random forest;root mean square error;thermal comfort","","","","16","","","21-23 July 2015","","IEEE","IEEE Conference Publications"
"A machine learning technique in a multi-agent framework for online outliers detection in Wireless Sensor Networks","H. Martins; F. Januário; L. Palma; A. Cardoso; P. Gil","Departamento de Engenharia Electrot&#x00E9;cnica, Faculdade de Ci&#x00EA;ncias e Tecnologia, Universidade Nova de Lisboa, Portugal","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","20160128","2015","","","000688","000693","Wireless Sensor Networks enable flexibility, low operational and maintenance costs, as well as scalability in a variety of scenarios. However, in the context of industrial monitoring scenarios the use of Wireless Sensor Networks can compromise the system's performance due to several factors, being one of them the presence of outliers in raw data. In order to improve the overall system's resilience, this paper proposes a distributed hierarchical multi-agent architecture where each agent is responsible for a specific task. This paper deals with online detection and accommodation of outliers in non-stationary time-series by appealing to a machine learning technique. The methodology is based on a Least Squares Support Vector Machine along with a sliding window-based learning algorithm. A modification to this method is considered to improve its performance in transient raw data collected from transmitters over a Wireless Sensor Networks (WSNs). An empirical study based on laboratory test-bed show the feasibility and relevance of incorporating the proposed methodology in the context of monitoring systems over Wireless Sensor Networks.","","Electronic:978-1-4799-1762-4; POD:978-1-4799-1763-1","10.1109/IECON.2015.7392180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392180","","Context;Kernel;Memory;Monitoring;Support vector machines;Symmetric matrices;Wireless sensor networks","data acquisition;learning (artificial intelligence);least squares approximations;multi-agent systems;support vector machines;time series;transmitters;wireless sensor networks","WSN;distributed hierarchical multiagent architecture;industrial monitoring;least square support vector machine;machine learning technique;multiagent framework;nonstationary time-series;online outlier detection;sliding window-based learning algorithm;transmitter;wireless sensor network","","1","","13","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"An integrated pattern matching and machine learning approach for question classification","V. Singh; S. K. Dwivedi","Department of Computer Science, B. B. Ambedkar University, Lucknow-226025, India","2015 1st International Conference on Next Generation Computing Technologies (NGCT)","20160111","2015","","","762","767","In question answering system, the process of classifying a question to appropriate class and identification of the focus word play key role in determining accurate answer. In this paper, we propose an integrated pattern matching and machine learning approach for higher education domain that focuses on factoid question answering. We have developed a question taxonomy for higher education domain and defined 9 coarse classes and 63 fine classes. We adopted pattern matching for the primary stage of classification and focus word identification and used machine learning approach i.e., Support Vector Machine (SVM) for the secondary classification approach only to those questions whose pattern are not present in question pattern corpus. Our experimental result shows that the accuracy of question classification using integrated approach outperforms the accuracy shown by individual approaches. SVM enhances the classification accuracy while focus word identification is achieved by virtue of pattern matching. The integrated approach shows the accuracy of 92.5% and 87.8% for coarse and fine class respectively and achieved focus word identification up to 83.4%.","","DVD:978-1-4673-6807-0; Electronic:978-1-4673-6809-4; POD:978-1-4673-6810-0","10.1109/NGCT.2015.7375223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375223","SVM;corase grained;fine grained;pattern matching;question classification;taxonomy","Education;Kernel;Magnetic heads;Pattern matching;Shape;Support vector machines;Taxonomy","computer aided instruction;further education;learning (artificial intelligence);pattern classification;pattern matching;question answering (information retrieval);support vector machines","factoid question answering system;focus word identification;higher education domain;integrated pattern matching;machine learning approach;question classification accuracy;question pattern corpus;question taxonomy;secondary classification approach;support vector machine","","","","16","","","4-5 Sept. 2015","","IEEE","IEEE Conference Publications"
"An ensemble of machine learning and anti-learning methods for predicting tumour patient survival rates","C. Roadknight; D. Suryanarayanan; U. Aickelin; J. Scholefield; L. Durrant","Horizon Digital Economy Research, School of Computer Science, University of Nottingham","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","20151207","2015","","","1","8","This paper primarily addresses a dataset relating to cellular, chemical and physical conditions of patients gathered at the time they are operated upon to remove colorectal tumours. This data provides a unique insight into the biochemical and immunological status of patients at the point of tumour removal along with information about tumour classification and post-operative survival. The relationship between severity of tumour, based on TNM staging, and survival is still unclear for patients with TNM stage 2 and 3 tumours. We ask whether it is possible to predict survival rate more accurately using a selection of machine learning techniques applied to subsets of data to gain a deeper understanding of the relationships between a patient's biochemical markers and survival. We use a range of feature selection and single classification techniques to predict the 5 year survival rate of TNM stage 2 and 3 patients which initially produces less than ideal results. The performance of each model individually is then compared with subsets of the data where agreement is reached for multiple models. This novel method of selective ensembling demonstrates that significant improvements in model accuracy on an unseen test set can be achieved for patients where agreement between models is achieved. Finally we point at a possible method to identify whether a patients prognosis can be accurately predicted or not.","","Electronic:978-1-4673-8273-1; POD:978-1-4673-8274-8","10.1109/DSAA.2015.7344863","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344863","Bioinformatics;Ensemble;Machine Learning","Biological system modeling;Cancer;Data models;Immune system;Prediction algorithms;Support vector machines;Tumors","learning (artificial intelligence);medical diagnostic computing;pattern classification;tumours","TNM staging;antilearning methods;biochemical markers;biochemical status;cellular condition;chemical condition;colorectal tumours;feature selection;immunological status;machine learning;patients prognosis;physical condition;post-operative survival;single classification techniques;tumour classification;tumour patient survival rate prediction;tumour removal","","","","28","","","19-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Using Analytical Models to Bootstrap Machine Learning Performance Predictors","D. Didona; P. Romano","INESC-ID, Univ. de Lisboa, Lisbon, Portugal","2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)","20160118","2015","","","405","413","Performance modeling is a crucial technique to enable the vision of elastic computing in cloud environments. Conventional approaches to performance modeling rely on two antithetic methodologies: white box modeling, which exploits knowledge on system's internals and capture its dynamics using analytical approaches, and black box techniques, which infer relations among the input and output variables of a system based on the evidences gathered during an initial training phase. In this paper we investigate a technique, which we name Bootstrapping, which aims at reconciling these two methodologies and at compensating the cons of the one with the pros of the other. We analyze the design space of this gray box modeling technique, and identify a number of algorithmic and parametric trade-offs which we evaluate via two realistic case studies, a Key-Value Store and a Total Order Broadcast service.","","Electronic:978-0-7695-5785-4; POD:978-1-4673-8669-2","10.1109/ICPADS.2015.58","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384321","","Analytical models;Cloud computing;Computational modeling;Knowledge based systems;Prediction algorithms;Predictive models;Training","cloud computing;computer bootstrapping;learning (artificial intelligence);software performance evaluation","analytical models;black box techniques;bootstrap machine learning performance predictors;cloud environments;elastic computing;gray box modeling technique;key-value store;performance modeling;total order broadcast service;white box modeling","","","","28","","","14-17 Dec. 2015","","IEEE","IEEE Conference Publications"
"A machine learning approach to find association between imaging features and XRF signatures of rocks in underground mines","A. Rahman; M. S. Shahriar; G. Timms; C. Lindley; A. B. Davie; D. Biggins; A. Hellicar; C. Sennersten; G. Smith; M. Coombe","Autonomous Systems Program, CSIRO, Sandy Bay, Tasmania, Australia","2015 IEEE SENSORS","20160107","2015","","","1","4","This study investigated the applicability of machine learning algorithms to detect the presence of elements in underground mines from rock surface images, which is proposed as a heuristic classification method inspired by the ability of human geologists to make judgments about the location of ore veins by eye. A regression algorithm was investigated to find associations between image features and X-Ray Fluorescence (XRF) signatures indicating elemental content of the surface and near-surface region of the rocks. A set of image processing algorithms was used to extract color distribution, edge orientation statistics, and texture of the rock surfaces. XRF signatures were obtained from the same samples, providing a semi-quantitative measure of element concentration. The process was performed on a set of 20 rock samples. The regression algorithm was then trained to find a mapping between image features and the semi-quantitative element concentrations (corresponding with XRF peaks). Experimental results demonstrate the potential effectiveness of the proposed approach in the context of a specific ore body.","","Electronic:978-1-4799-8203-5; POD:978-1-4799-8204-2","10.1109/ICSENS.2015.7370680","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370680","XRF signatures;image processing;machine learning;mining;regression","Histograms;Image color analysis;Image edge detection;Imaging;Machine learning algorithms;Rocks;Surface texture","X-ray fluorescence analysis;edge detection;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);mining;regression analysis;rocks;surface texture","X-ray fluorescence signatures;color distribution;edge orientation statistics;heuristic classification method;image features;image processing algorithms;machine learning algorithms;regression algorithm;rock surface images;rock surface texture;underground mines","","1","","7","","","1-4 Nov. 2015","","IEEE","IEEE Conference Publications"
"Big Data and Machine Learning for Applied Weather Forecasts: Forecasting Solar Power for Utility Operations","S. E. Haupt; B. Kosovic","Weather Syst. & Assessment Program, Res. Applic. Lab., Boulder, CO, USA","2015 IEEE Symposium Series on Computational Intelligence","20160111","2015","","","496","501","To blend growing amounts of renewable energy into utility grids requires accurate estimate of the power from those resources for both day ahead planning and real-time operations. This requires predicting the wind and solar resource on those timescales. Accurate prediction of these meteorological variables is a big data problem that requires a multitude of disparate data, multiple models that are each applicable to a specific time frame, and application of computational intelligence techniques to successfully blend all of the model and observational information in real-time and deliver it to the decision-makers at utilities and grid operators. Considering that the capacity of renewable energy continues to grow an additional challenge includes selecting and archiving data for continuous retraining of machine learning algorithms.","","Electronic:978-1-4799-7560-0; POD:978-1-4799-7561-7","10.1109/SSCI.2015.79","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376652","","Atmospheric modeling;Data models;Forecasting;Predictive models;Wind forecasting","Big Data;geophysics computing;learning (artificial intelligence);power grids;solar power;weather forecasting;wind power","Big Data;applied weather forecasts;computational intelligence techniques;day ahead planning;machine learning algorithms;meteorological variable prediction;renewable energy;solar power forecasting;solar resource prediction;utility grids;utility operations;wind resource prediction","","3","","12","","","7-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Data deidentification in medical transcriptions using regular expressions and machine learning","J. Seeger; A. Culotta; J. Keller; P. van Kessel; M. Jugovich","NORC at the University of Chicago, 1 North State Street, 14th Floor, Chicago, IL 60602","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1322","1323","A system is developed to redact personally identifiable information (PII) through a combination of entity recognition, regular expressions, and machine learning with very high precision from millions of medical transcriptions. This system is trained and tested with manually redacted medical transcriptions using an internally developed coding system, providing double blind classification capabilities.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363889","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363889","","Big data;Encoding;Floors;Manuals;Medical diagnostic imaging;Medical services;Pipelines","data handling;learning (artificial intelligence);medical computing","PII;blind classification;coding system;data deidentification;entity recognition;machine learning;medical transcriptions;personally identifiable information;regular expressions","","","","5","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Performance analysis of machine learning for arbitrary downsizing of pre-encoded HEVC video","L. P. Van; J. De Praeter; G. Van Wallendael; J. De Cock; R. Van de Walle","Department of Electronics and Information Systems - Multimedia Lab, Ghent University - iMinds, Ghent, Belgium","IEEE Transactions on Consumer Electronics","20160125","2015","61","4","507","515","Nowadays, broadcasters deliver ultra-high resolution video to their consumers. This live video is sent to a set-top box for display on a television. However, if one or more users in the home want to view the same video on their personal mobile devices with a lower display resolution and limited processing power, decoding the original ultra-high resolution video would result in stuttering and quickly drain the battery life on these devices. To enable a satisfactory consumer experience, the resolution of the video stream should be adapted to the target mobile device at the set-top box. The aim of this paper is to investigate the performance of different machine learning strategies to arbitrary downsize video pre-encoded with the high efficiency video coding standard (HEVC). These machine learning techniques exploit correlation between input and output coding information to predict the splitting behavior of HEVC coding units. Several machine learning algorithms are optimized. Additionally, both online and offline training strategies are tested. Of the tested algorithms, online-trained random forests achieve the best compression-efficiency with a bit rate increase of 5.4% and an average complexity reduction of 70%<sup>1</sup>.","0098-3063;00983063","","10.1109/TCE.2015.7389806","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389806","Video adaptation;arbitrary downsizing;highefficiency video coding;machine learning","Complexity theory;Machine learning algorithms;Predictive models;Streaming media;Training;Transcoding","data compression;learning (artificial intelligence);video coding","compression-efficiency;high efficiency video coding standard;machine learning strategy;mobile device;pre-encoded HEVC video arbitrary downsizing;random forest;set-top box;television display;video stream resolution","","1","","33","","","November 2015","","IEEE","IEEE Journals & Magazines"
"Temperature Distribution Prediction in Data Centers for Decreasing Power Consumption by Machine Learning","Y. Tarutani; K. Hashimoto; G. Hasegawa; Y. Nakamura; T. Tamura; K. Matsuda; M. Matsuoka","Cybermedia Center, Osaka Univ., Toyonaka, Japan","2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)","20160204","2015","","","635","642","To decrease the power consumption of data centers, coordinated control of air conditioners and task assignment on servers is crucial. It takes tens of minutes for changes of operational parameters of air conditioners including outlet air temperature and volume to be actually reflected in the temperature distribution in the whole data center. Proactive control of the air conditioners is therefore required according to the predicted temperature distribution, which is highly dependent on the task assignment on the servers. In this paper, we apply a machine learning technique for predicting the temperature distribution in a data center. The temperature predictor employs regression models for describing the temperature distribution as it is predicted to be several minutes in the future, with the model parameters trained using operational data monitored at the target data center. We evaluated the performance of the temperature predictor for an experimental data center, in terms of the accuracy of the regression models and the calculation times for training and prediction. The temperature distribution was predicted with an accuracy of 0.095°C. The calculation times for training and prediction were around 1,000 seconds and 10 seconds, respectively. Furthermore, the power consumption of air conditioners was decreased by roughly 30% through proactive control based on the predicting temperature distribution.","","Electronic:978-1-4673-9560-1; POD:978-1-4673-9561-8","10.1109/CloudCom.2015.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396226","Data center;Energy management;Machine learning;Temperature pridiction","Data models;Power demand;Predictive models;Servers;Temperature distribution;Temperature sensors;Training","computer centres;learning (artificial intelligence);power consumption;regression analysis;temperature distribution","air conditioner;coordinated control;data center;machine learning;power consumption;proactive control;regression model;task assignment;temperature distribution","","2","","16","","","Nov. 30 2015-Dec. 3 2015","","IEEE","IEEE Conference Publications"
"An Empirical Overview of the No Free Lunch Theorem and Its Effect on Real-World Machine Learning Classification","D. Gómez; A. Rojas","Telematics Engineering Department, Polytechnical University of Catalonia, Barcelona 08034, Spain david.gomez.guillen@entel.upc.edu","Neural Computation","20151223","2016","28","1","216","228","<para>A sizable amount of research has been done to improve the mechanisms for knowledge extraction such as machine learning classification or regression. Quite unintuitively, the no free lunch (NFL) theorem states that all optimization problem strategies perform equally well when averaged over all possible problems. This fact seems to clash with the effort put forth toward better algorithms. This letter explores empirically the effect of the NFL theorem on some popular machine learning classification techniques over real-world data sets.</para>","0899-7667;08997667","","10.1162/NECO_a_00793","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363729","","","","","","","","","","","Jan. 2016","","MIT Press","MIT Press Journals"
"Utilizing machine learning in Sentiment Analysis: SentiRobo approach","V. A. Rohani; S. Shayaa","Department of Software Engineering, Faculty of Computer Science and Information Technology, University of Malaya, Kuala Lumpur, Malaysia","2015 International Symposium on Technology Management and Emerging Technologies (ISTMET)","20151217","2015","","","263","267","Following the rapid evolution of Web 2.0, Sentiment Analysis has become one of the major techniques for mining the social media content. It aims to analyze opinions, sentiments, attitudes, and emotions towards entities such as topics, products, organizations, individuals, communities, and services. This paper presents SentiRobo, a supervised machine learning approach for the process of Sentiment Analysis. An enhanced version of Naive Bayes algorithm is introduced to predict the sentiment polarity of social media large data sets. Empirical evaluation over different twitter datasets with more than 300,000 records reveals the merit of this approach in processing of social media datasets.","","Electronic:978-1-4799-1723-5; POD:978-1-4799-1724-2; USB:978-1-4799-1722-8","10.1109/ISTMET.2015.7359041","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359041","Machine Learning;Naïve Bayes Text Classification;SentiRobo;Sentiment Analysis;Social Media","Algorithm design and analysis;Classification algorithms;Data mining;Media;Sentiment analysis;Support vector machines;Training","Internet;data mining;learning (artificial intelligence);sentiment analysis;social networking (online)","Naive Bayes algorithm;SentiRobo approach;Web 2.0 sentiment analysis;social media content mining;supervised machine learning approach","","1","","26","","","25-27 Aug. 2015","","IEEE","IEEE Conference Publications"
"Comparative study of machine learning techniques for pre-processing of network intrusion data","F. Rahat; S. N. Ahsan","Karachi Institute of Power Engineering, Paradise Point, Karachi, Pakistan","2015 International Conference on Open Source Systems & Technologies (ICOSST)","20160204","2015","","","46","51","Machine learning is widely used for network intrusion detection but the data it uses faces problems of large feature set and class imbalance which is inherent in network traffic data. This paper focuses on the performance evaluation of different strategies used for mitigating both the problems. Data used for classification was KDDCUP'99 which is a benchmark data set for intrusion detection and suffers greatly from class imbalance problem. Noise was also added to data to evaluate the performance of classifiers for noisy data. Different combinations of strategies form different scenarios. Four possible scenarios are tested by using different combinations of sampling, feature set reduction and classification. Classifiers are used for evaluating the performance of each scenario. Feature set was reduced to nine features from forty one features. Stratified remove folds and Resampling were applied to remove class imbalance problem. Results have shown that Nearest Neighbor, J48 classifier are best suited for real time detection with pre-processing whereas Gain Ratio is suitable for feature selection.","","CD-ROM:978-1-4799-7811-3; Electronic:978-1-4799-7812-0; POD:978-1-4799-7813-7","10.1109/ICOSST.2015.7396401","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396401","Sampling;class imbalance;classifiers;feature set;intrusion detection","Benchmark testing;Classification algorithms;Data mining;Feature extraction;Intrusion detection;Machine learning algorithms;Real-time systems","feature selection;learning (artificial intelligence);pattern classification;sampling methods;security of data","J48 classifier;KDDCUP'99 dataset;class imbalance problem;feature selection;feature set reduction;gain ratio;machine learning techniques;nearest neighbor classifier;network intrusion data preprocessing;network intrusion detection;resampling;stratified remove folds","","","","20","","","17-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Malware detection on Android smartphones using API class and machine learning","Westyarian; Y. Rosmansyah; B. Dabarsyah","Electr. Eng. Dept., Inst. Teknol. Bandung, Bandung, Indonesia","2015 International Conference on Electrical Engineering and Informatics (ICEEI)","20151217","2015","","","294","297","This paper proposes a (new) method to detect malware in Android smartphones using API (application programming interface) classes. We use machine learning to classify whether an application is benign or malware. Furthermore, we compare classification precision rate from machine learning. This research uses 51 APIs package classes from 16 APIs classes and employs cross validation and percentage split test to classify benign and malware using Random Forest, J48, and Support Vector Machine algorithms. We use 412 total application samples (205 benign, 207 malware). We obtain that the classification precision average is 91.9%.","","Electronic:978-1-4673-7319-7; POD:978-1-4673-7320-3","10.1109/ICEEI.2015.7352513","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352513","APIs class;Android;Machine Learning;Malware detection","Androids;Humanoid robots;Machine learning algorithms;Malware;Smart phones;Software;Support vector machines","application program interfaces;decision trees;invasive software;learning (artificial intelligence);smart phones;support vector machines","API class;Android smartphones;J48 algorithm;Random Forest algorithm;application programming interface;machine learning;malware detection;percentage split test;support vector machine algorithm","","","","26","","","10-11 Aug. 2015","","IEEE","IEEE Conference Publications"
"Detection technique for hardware Trojans using machine learning in frequency domain","T. Iwase; Y. Nozaki; M. Yoshikawa; T. Kumaki","Dept. of Information Engineering, Meijo University, 1-501 Shiogamaguchi Tenpaku Nagoya Aichi Japan","2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)","20160204","2015","","","185","186","Recently, the threat of hardware Trojan has been highlighted. A hardware Trojan is a hardware virus. When predetermined conditions are satisfied, that malicious virus performs subversive activities, such as a system shutdown and the leaking of important information, without the circuit users even being aware of that activity. Therefore, it is important to detect the consumer electronic devices with hardware Trojans from a viewpoint of security. This study proposes a new detection technique for hardware Trojan. The proposed method introduces machine learning for the detection. Experiments using actual devices prove the validity of the proposed method.","","Electronic:978-1-4799-8751-1; POD:978-1-4799-8752-8","10.1109/GCCE.2015.7398569","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398569","Detection technique;Hardware Trojan;Machine learning;Security module","Conferences;Consumer electronics;Hardware;Large scale integration;Power demand;Support vector machines;Trojan horses","invasive software;learning (artificial intelligence)","consumer electronic devices;detection technique;frequency domain;hardware Trojans;machine learning;malicious virus","","","","4","","","27-30 Oct. 2015","","IEEE","IEEE Conference Publications"
"A comperative study on novel machine learning algorithms for estimation of energy performance of residential buildings","Y. Sonmez; U. Guvenc; H. T. Kahraman; C. Yilmaz","Gazi University, Technical Sciences Vocational College, Ankara Turkey","2015 3rd International Istanbul Smart Grid Congress and Fair (ICSG)","20151217","2015","","","1","7","This study aims to improve the energy performance of residential buildings. heating load (HL) and cooling load (CL) are considered as a measure of heating ventilation and air conditioning (HVAC) system in this process. In order to achive an effective estimation, hybrid machine learning algorithms including, artificial bee colony-based k-nearest neighbor (abc-knn), genetic algorithm-based knn (ga-knn), adaptive artificial neural network with genetic algorithm (ga-ann) and adaptive ann with artificial bee colony (abc-ann) are used. Results are compared classical knn and ann methods. Thence, relations between input and target parameters are defined and performance of well-known classical knn and ann is improved substantialy.","","Electronic:978-1-4673-6624-3; POD:978-1-4673-6625-0","10.1109/SGCF.2015.7354915","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354915","artificial bee colony algorithm;artificial neural network;cooling load;energy performance of residential buildings;genetic algorithm;heating load;k-nearest neighbor","Artificial intelligence;Artificial neural networks;Buildings;Chlorine;Estimation;Measurement;Standards","HVAC;buildings (structures);genetic algorithms;learning (artificial intelligence)","HVAC;adaptive ann with artificial bee colony;adaptive artificial neural network with genetic algorithm;artificial bee colony-based k-nearest neighbor;cooling load;energy performance estimation;genetic algorithm-based knn;heating load;heating ventilation and air conditioning system;hybrid machine learning algorithms;residential buildings","","1","","15","","","29-30 April 2015","","IEEE","IEEE Conference Publications"
"Heat event detection in dairy cows with collar sensors: An unsupervised machine learning approach","M. S. Shahriar; D. Smith; A. Rahman; D. Henry; G. Bishop-Hurley; R. Rawnsley; M. Freeman; J. Hills","Computational Intelligence, Autonomous Systems, CSIRO Digital Productivity Flagship, Hobart, 7005, Australia","2015 IEEE SENSORS","20160107","2015","","","1","4","The detection of heat (estrus) events in pasture-based dairy cows fitted with on-animal sensors was investigated using an unsupervised learning. Accelerometer data from the cow collar sensors were used in this approach where the aim was to identify increased activity level (restlessness, increased walking for mating) and to find association with recorded heat events. High dimensional time series data from accelerometers were first segmented in windows followed by feature extractions. The extracted features are standard deviation, amplitude, energy and Fast Fourier Transform (FFT). K-means clustering algorithm was then applied across the windows for grouping. The groups were labeled in terms of activity intensities: high, medium and low. An activity index level (AIxL) was derived from the activity intensity labels. We compared the AIxL with recorded heat events and observed significant associations between the increased activities through high AIxL values and the observed heat events.","","Electronic:978-1-4799-8203-5; POD:978-1-4799-8204-2","10.1109/ICSENS.2015.7370528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370528","","Accelerometers;Cows;Feature extraction;Heating;Indexes;Sensors;Time series analysis","accelerometers;agricultural engineering;fast Fourier transforms;feature extraction;learning (artificial intelligence);pattern clustering;veterinary medicine","K-means clustering algorithm;accelerometer data;activity index level;collar sensor;dairy cows;estrus events;fast Fourier transform;feature extraction;heat event detection;signal amplitude;signal energy;standard deviation;unsupervised machine learning","","2","","25","","","1-4 Nov. 2015","","IEEE","IEEE Conference Publications"
"Hybrid penetration depth computation using local projection and machine learning","Y. Kim; D. Manocha; Y. J. Kim","Department of Computer Science and Engineering at Ewha Womans University in Seoul, Korea","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","20151217","2015","","","4804","4809","We present a new hybrid approach to computing penetration depth (PD) for general polygonal models. Our approach exploits both local and global approaches to PD computation and can compute error-bounded PD approximations for both deep and shallow penetrations. We use a two-step formulation: the first step corresponds to a global approximation approach that samples the configuration space with bounded error using support vector machines; the second step corresponds to a local optimization that performs a projection operation refining the penetration depth. We have implemented this hybrid algorithm on a standard PC platform and tested its performance with various benchmarks. The experimental results show that our algorithm offers significant benefits over previously developed local-only and global-only methods used to compute the PD.","","Electronic:978-1-4799-9994-1; POD:978-1-4799-9995-8; USB:978-1-4799-9993-4","10.1109/IROS.2015.7354052","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354052","","Approximation algorithms;Approximation methods;Benchmark testing;Computational modeling;Handheld computers;Robots;Support vector machines","approximation theory;control engineering computing;learning (artificial intelligence);robots;solid modelling;spatial variables measurement;support vector machines","PC platform;configuration space;deep penetrations;error-bounded PD approximations;global approximation approach;global-only methods;hybrid penetration depth computation;local projection;local-only methods;machine learning;polygonal models;shallow penetrations;support vector machines","","1","","19","","","Sept. 28 2015-Oct. 2 2015","","IEEE","IEEE Conference Publications"
"Study on Case-Based Reasoning-Inspired Approaches to Machine-Learning","T. Zhe; C. Jian; X. Huicheng","Dalian Naval Acad., Dalian, China","2015 International Conference on Intelligent Transportation, Big Data and Smart City","20160121","2015","","","760","763","This commentary briefly reviews work on the application of case-based reasoning (CBR) to the design and construction of machine-learning approaches and computer-based teaching systems. The CBR cognitive model is at the core of constructivist learning approaches such as Goal-Based Scenarios and Learning by Design. Case libraries can play roles as intelligent resources while learning and frameworks for articulating one understands. More recently, CBR techniques have been applied to design and construction of simulation-based learning systems and serious games. The main ideas of CBR are explained and pointers to relevant references are provided, both for finished work and on-going research.","","Electronic:978-1-5090-0464-5; POD:978-1-5090-0465-2","10.1109/ICITBS.2015.192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384138","Case-based Reasoning;Design;Machine-learning","Big data;Smart cities;Transportation","case-based reasoning;cognition;computer aided instruction;learning (artificial intelligence)","CBR cognitive model;CBR technique;case-based reasoning-inspired approach;computer-based teaching system;constructivist learning approach;machine-learning approach;serious game;simulation-based learning system","","","","24","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"On Multi-tier Sentiment Analysis Using Supervised Machine Learning","M. Moh; A. Gajjala; S. C. R. Gangireddy; T. S. Moh","Dept. of Comput. Sci., San Jose State Univ., San Jose, CA, USA","2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)","20160204","2015","1","","341","344","Document management and Information Retrieval tasks have rapidly increased due to the availability of digital documents anytime, any place. The need for automatic extraction of document information has become prominent in information organization and knowledge discovery. Text Classification is one such solution, where in the natural language text is assigned to one or more predefined categories based on the content. This work focuses on sentiment analysis, also known as opinion mining. It is a way of automatically extracting and analyzing the emotions and opinions, and not facts, of messages and posts. A multi-tier classification architecture is proposed, which consists of major modules such as data cleaning and pre-processing, feature selection, and classifier training that includes a multi-tier prediction model. The architecture and its components are carefully described. Four classifiers (Naïve Bayes, SVM, Random Forest, and SGD) are used in the experiments, which evaluate the performance of the proposed multi-tier architecture by analyzing the sentiments and opinions of 150,000 movie reviews. Results have shown that the multi-tier model is able to significantly improve prediction accuracy over the single-tier model by more than 10%, the improvement is significant when customized dictionary is used. We believe that the proposed multi-tier classification architecture, with the various feature selection techniques described and used, are significant, and are readily applicable to many other areas of sentiment analysis.","","Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6","10.1109/WI-IAT.2015.154","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396827","hierarchial model;machine learning;multi-level analysis;opinion mining;sentiment analysis;text classification","Data mining;Data models;Dictionaries;Motion pictures;Predictive models;Sentiment analysis;Training","behavioural sciences computing;data mining;feature selection;learning (artificial intelligence);pattern classification;support vector machines","SGD classifier;SVM classifier;classifier training module;data cleaning module;data preprocessing module;document information extraction;document management;emotion analysis;feature selection module;information organization;information retrieval;knowledge discovery;multitier classification architecture;multitier prediction model;multitier sentiment analysis;naive Bayes classifier;natural language text;opinion analysis;opinion mining;random forest classifier;supervised machine learning;support vector machines;text classification","","3","","6","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Forecasting Time Series Water Levels on Mekong River Using Machine Learning Models","T. T. Nguyen; Q. N. Huu; M. J. Li","Fac. of Comput. Sci. & Eng., Thuyloi Univ., Hanoi, Vietnam","2015 Seventh International Conference on Knowledge and Systems Engineering (KSE)","20160107","2015","","","292","297","Forecasting water levels on Mekong river is an important problem needed to be studied for flood warning. In this paper, we investigate the application to forecasting of daily water levels at Thakhek station on Mekong river using machine learning models such as LASSO, Random Forests and Support Vector Regression (SVR). Experimental results showed that SVR was able to achieve feasible results, the mean absolute error of SVR is 0.486(m) while the acceptable error of a flood forecast model required by the Mekong River Commission is between 0.5(m) and 0.75(m).","","Electronic:978-1-4673-8013-3; POD:978-1-4673-8014-0","10.1109/KSE.2015.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371798","Data mining;LASSO;Random Forests;Support Vector Regression;Time series forecasting","Data models;Forecasting;Predictive models;Radio frequency;Rivers;Support vector machines;Yttrium","alarm systems;floods;forecasting theory;learning (artificial intelligence);public administration;regression analysis;rivers;support vector machines;time series","LASSO;Mekong River Commission;Thakhek station;flood warning;machine learning models;random forests;support vector regression;time series;water level forecasting","","","","15","","","8-10 Oct. 2015","","IEEE","IEEE Conference Publications"
"A Machine Learning-Based Approach to Digital Triage","A. T. S. Ho; S. Li","","Handbook of Digital Forensics of Multimedia Data and Devices","20160203","2015","","","","","Dealing with machine learning-based digital triage, this chapter presents a framework for selective pre-examination and statistical classification of digital data sources that may be deployed both on the crime scene and at Digital Forensic Laboratory (DFLs). This framework provides investigators with quick actionable intelligence on the crime scene when time is a critical factor. It also protects a suspect's privacy where excessive search and seizure of data is not allowed by the legal system. The framework presents two main advantages with respect to most examination techniques in use today as it requires limited manual intervention and produces measurable and reproducible error rates. The chapter presents an overview about the history and state-of-the-art research on digital triage. The chapter presents a case study on mobile phones classification in court cases of child pornography exchange. It concludes with a discussion on the challenges and future directions for the digital forensics community.","","97811187057","10.1002/9781118705773.ch3","http://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=7394723.pdf&bkn=7394656&pdfType=chapter","","","","","","","","","2015","","","","Wiley-IEEE Press","Wiley-IEEE Press eBook Chapters"
"Cross-validation and cross-study validation of chronic lymphocytic leukemia with exome sequences and machine learning","N. Patel; B. Jhadav; A. Aljouie; U. Roshan","Department of Genetics and Genomics Sciences, Icahn School of Medicine at Mount Sinai Hospital, Hess Center for Science and Medicine, New York City, 10029, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","1367","1374","The era of genomics brings the potential of better DNA based risk prediction and treatment. While genome-wide association studies are extensively studied for risk prediction, the potential of using whole exome data for this purpose is unclear. We explore this problem for chronic lymphocytic leukemia that is one of the largest whole exome dataset of 186 case and 169 controls available from the NIH dbGaP database. We perform a standard next generation sequence procedure to obtain SNP variants on 153 cases and 144 controls after exclusion of samples with missing data. To evaluate their predictive power we first conduct a 50% training and 50% test cross-validation study on the full dataset with the support vector machine as the classifier. There we obtain a mean accuracy of 82% with top 20 ranked SNPs obtained by the Pearson correlation coefficient. We then perform a cross-study validation on case and controls from a lymphoma external study and just controls from head and neck cancer and breast cancer studies (all obtained from NIH dbGaP). On the external dataset we obtain an accuracy of 70% with top ranked SNPs obtained from the original dataset. We also find our top Pearson ranked SNPs to lie on previously implicated genes for this disease. Our study shows that even with a small sample size we can obtain moderate to high accuracy with exome sequences and is thus encouraging for future work.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359878","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359878","","Bioinformatics;Biological information theory;Correlation;DNA;Genomics","cancer;genomics;learning (artificial intelligence);medical administrative data processing;pattern classification;support vector machines","DNA based risk prediction;NIH dbGaP database;Pearson correlation coefficient;SNP variants;breast cancer;chronic lymphocytic leukemia;classifier;exome sequences;genome-wide association studies;genomics;head cancer;machine learning;neck cancer;single nucleotide polymorphisms;support vector machine","","","","","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"Accelerating Support Vector Machine Learning with GPU-Based MapReduce","T. Sun; H. Wang; Y. Shen; J. Wu","Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China","2015 IEEE International Conference on Systems, Man, and Cybernetics","20160114","2015","","","876","881","With the exploding growth of data, the computational complexity required by learning Support Vector Machine (SVM) lays a heavy burden on real-world applications. To address this issue, parallel computational techniques can be employed such as the Graphics Processing Units (GPUs) and MapReduce model. As it is well known, GPUs are microprocessors on a multi-core architecture which reveal high performance in mass data parallel computing, and MapReduce allows computational tasks to be divided into a plurality of parts, distributed to various computing nodes and combined on a single node. In this paper, we propose a GPU-based MapReduce framework to accelerate SVM learning by jointly utilizing the parallel computing power of GPU and MapReduce. Extensive experimental results have verified the effectiveness and efficiency of the proposed approach.","","Electronic:978-1-4799-8697-2; POD:978-1-4799-8698-9","10.1109/SMC.2015.161","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379293","GPU;Hadoop;MapReduce;Support vector machine","Acceleration;Graphics processing units;Kernel;Optimization;Parallel processing;Support vector machines;Training","computational complexity;data handling;graphics processing units;learning (artificial intelligence);parallel processing;support vector machines","GPU-based MapReduce;computational complexity;graphics processing units;mass data parallel computing;microprocessors;multicore architecture;parallel computational techniques;support vector machine learning","","","","16","","","9-12 Oct. 2015","","IEEE","IEEE Conference Publications"
"Optimizing 3D NoC design for energy efficiency: A machine learning approach","S. Das; J. R. Doppa; D. H. Kim; P. P. Pande; K. Chakrabarty","School of EECS, Washington State University, Pullman, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20160107","2015","","","705","712","Three-dimensional (3D) Network-on-Chip (NoC) is an emerging technology that has the potential to achieve high performance with low power consumption for multicore chips. However, to fully realize their potential, we need to consider novel 3D NoC architectures. In this paper, inspired by the inherent advantages of small-world (SW) 2D NoCs, we explore the design space of SW network-based 3D NoC architectures. We leverage machine learning to intelligently explore the design space to optimize the placement of both planar and vertical communication links for energy efficiency. We demonstrate that the optimized 3D SW NoC designs perform significantly better than their 3D MESH counterparts. On an average, the 3D SW NoC shows 35% energy-delay-product (EDP) improvement over 3D MESH for the nine PARSEC and SPLASH2 benchmarks considered in this work. The highest performance improvement of 43% was achieved for RADIX. Interestingly, even after reducing the number of vertical links by 50%, the optimized 3D SW NoC performs 25% better than the fully connected 3D MESH, which is a strong indication of the effectiveness of our optimization methodology.","","Electronic:978-1-4673-8388-2; POD:978-1-4673-8109-3; USB:978-1-4673-8389-9","10.1109/ICCAD.2015.7372639","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372639","3D NoC;Discrete Optimization;Machine Learning;Small-World","Algorithm design and analysis;Energy consumption;Machine learning algorithms;Optimization;Search problems;Space exploration;Three-dimensional displays","circuit CAD;circuit optimisation;energy conservation;learning (artificial intelligence);network-on-chip;power aware computing","3D NoC architectures;EDP improvement;PARSEC benchmarks;RADIX;SPLASH2 benchmarks;SW network-based 3D NoC architecture design space;energy efficiency;energy- delay-product improvement;machine learning approach;multicore chips;optimization methodology;optimized 3D SW NoC designs;performance improvement;power consumption;small-world 2D NoC;three-dimensional network-on-chip","","1","","30","","","2-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Study on implementation of machine learning methods combination for improving attacks detection accuracy on Intrusion Detection System (IDS)","B. W. Masduki; K. Ramli; F. A. Saputra; D. Sugiarto","Department of Electrical Engineering, Universitas Indonesia, Jakarta, Indonesia","2015 International Conference on Quality in Research (QiR)","20160111","2015","","","56","64","Many computer-based devices are now connected to the internet technology. These devices are widely used to manage critical infrastructure such energy, aviation, mining, banking and transportation. The strategic value of the data and the information transmitted over the Internet infrastructure has a very high economic value. With the increasing value of the data and the information, the higher the threats and attacks on such data and information. Statistical data shows a significant increase in threats to cyber security. The Government is aware of the threats to cyber security and respond to cyber security system that can perform early detection of threats and attacks the internet. The success of a nation's cyber security system depends on the extent to which it is able to produce independently their cyber defense system. Independence is manifested in the form of the ability to process, analyze and create an action to prevent threats or attacks originating from within and outside the country. One of the systems can be developed independently is Intrusion Detection System (IDS) which is very useful for early detection of cyber threats and attacks. The advantages of an IDS is determined by its ability to detect cyber attacks with little false. This study learn how to implement a combination of various methods of machine-learning to the IDS to improve the accuracy in detecting attacks. This study is expected to produce a prototype IDS. This prototype IDS, will be equipped with a combination of machine-learning methods to improve the accuracy in detecting various attacks. The addition of machine-learning feature is expected to identify the specific characteristics of the attacks occurred in the Indonesian Internet network. Novel methods used and techniques in implementation and the national strategic value are becoming the unique value and advantages of this research.","","CD-ROM:978-1-4799-6549-6; Electronic:978-1-4799-6551-9; POD:978-1-4799-6552-6","10.1109/QiR.2015.7374895","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374895","attack;intrusion detection system;machine-learning;support vector machine;threat","Artificial neural networks;Databases;Engines;Internet;Ports (Computers);Support vector machines;Training data","Internet;computer network security;data communication;learning (artificial intelligence)","IDS;Indonesian Internet network;Internet technology infrastructure;attack detection accuracy improvement;computer-based device;critical infrastructure management;cyber defense system;cyber security system;data transmission;information transmission;intrusion detection system;machine learning method","","","","11","","","10-13 Aug. 2015","","IEEE","IEEE Conference Publications"
"Practical machine learning solution for increasing profit in a car repair service","M. C. Mihăescu; C. P. Sapunaru; P. S. Popescu","Department of Computers and Information Technology, University of Craiova, Craiova, Romania","2015 SAI Intelligent Systems Conference (IntelliSys)","20151221","2015","","","55","60","This paper presents a practical usage of a machine learning algorithm for increasing sales in a car repair service. The application is implemented for iPad and stores records of the performed reparations by previous clients. The proposed solution computes the costs of reparation, generates a number of promotional packages and offers a sales simulation for proposed packages with a certain discount and sale rise. The classical Apriori algorithm is used for data processing. The obtained most frequent packages are the ones more likely to be bought by future clients, taking into account the history of services and products provided by the car service during a certain period of time.","","Electronic:978-1-4673-7606-8; POD:978-1-4673-7607-5; USB:978-1-4673-7605-1","10.1109/IntelliSys.2015.7361084","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361084","car repair service;frequent item sets;practical usage","Business;Databases;Intelligent systems;Machine learning algorithms;Maintenance engineering;Mirrors;Paints","automobiles;learning (artificial intelligence);maintenance engineering;mechanical engineering computing;notebook computers;sales management","apriori algorithm;car repair service;iPad;machine learning algorithm;practical machine learning solution;profit increase;promotional packages;sales simulation","","","","17","","","10-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"Detection of De-Authentication DoS Attacks in Wi-Fi Networks: A Machine Learning Approach","M. Agarwal; S. Biswas; S. Nandi","Dept. of Comput. Sci. & Eng., Indian Inst. of Technol., Guwahati, Guwahati, India","2015 IEEE International Conference on Systems, Man, and Cybernetics","20160114","2015","","","246","251","Media Access Layer (MAC) vulnerabilities are the primary reason for the existence of the significant number of Denial of Service (DoS) attacks in 802.11 Wi-Fi networks. In this paper we focus on the de-authentication DoS (Deauth-DoS) attack in Wi-Fi networks. In Deauth-DoS attack an attacker sends a large number of spoofed de-authentication frames to the client (s) resulting in their disconnection. Existing solutions to mitigate Deauth-DoS attack rely on encryption, protocol modifications, 802.11 standard up gradation, software and hardware upgrades which are costly. In this paper we propose a Machine Learning (ML) based Intrusion Detection System (IDS) to detect the Deauth-DoS attack in Wi-Fi network which does not suffer from these drawbacks. To the best of our knowledge ML based techniques have never been used for detection of Deauth-DoS attack. We have used a variety of ML based classifiers for detection of Deauth-DoS attack enabling an administrator to choose among a host of classification algorithms. Experiments performed on in-house test bed shows that the proposed ML based IDS detects Deauth-DoS attack with precision (accuracy) and recall (detection rate) exceeding 96% mark.","","Electronic:978-1-4799-8697-2; POD:978-1-4799-8698-9","10.1109/SMC.2015.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379187","Deauthentication DoS;Intrusion Detection System;Wi-Fi Security","Authentication;Computer crime;Encryption;IEEE 802.11 Standard;Protocols;Software","computer network security;cryptographic protocols;learning (artificial intelligence);pattern classification;wireless LAN","Deauth-DoS attack;IDS;IEEE 802.11 Wi-Fi network;MAC vulnerability;ML based classifier;deauthentication DoS attack detection;denial of service;encryption;intrusion detection system;machine learning approach;media access layer vulnerability;protocol modification","","1","","13","","","9-12 Oct. 2015","","IEEE","IEEE Conference Publications"
"Reinvention of the cardiovascular diseases prevention and prediction due to ubiquitous convergence of mobile apps and machine learning","S. Nikolaiev; Y. Timoshenko","Institute for Applied System Analysis, NTUU &#8220;KPI&#8221;, Kiev, Ukraine","2015 Information Technologies in Innovation Business Conference (ITIB)","20151217","2015","","","23","26","The paradigm change from delayed interventional to Predictive, Preventive and Personalized Medicine is a leading global challenge in the 21st century. Ubiquitous convergence of mobile applications, new intelligent sensors and machine learning methods make possible creation of new generation personalized automatic healthcare monitoring and pathologies detection systems. These systems will help to make platforms for more effective treatments tailored to the person, that is considered as the “medicine of the future”. Implementation of these latest trends in medicine will make it possible to detect health deterioration remotely and will avoid millions of hospitalizations costing billions of dollars in the world every year. In the article the problem of cardiovascular diseases and the state of healthcare industry is described and general architecture of automatic system for heart pathologies detection is proposed.","","Electronic:978-1-5090-0235-1; POD:978-1-5090-0236-8","10.1109/ITIB.2015.7355066","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7355066","ECG;Predictive;Preventive and Personalized Medicine;cardiovascular diseases;exponential medicine;machine learning;personal automatic systems","Cardiovascular diseases;Electrocardiography;Heart;Monitoring;Pathology","cardiovascular system;electrocardiography;health care;learning (artificial intelligence);medical signal processing;mobile computing","automatic heart pathology detection system;cardiovascular diseases;cardiovascular diseases prediction reinvention;cardiovascular diseases prevention reinvention;health deterioration detection;healthcare industry;intelligent sensors;machine learning methods;mobile applications;pathology detection systems;personalized automatic healthcare monitoring generation;ubiquitous convergence","","","","10","","","7-9 Oct. 2015","","IEEE","IEEE Conference Publications"
"A benchmark study regarding Extreme Learning Machine, modified versions of Na??ve Bayes Classifier and Fast Support Vector Classifier","M. Enache; R. Dogaru","Doctoral School in Electronics, Telecommunication and Information Technology, University "Politehnica", Bucharest, Romania","2015 E-Health and Bioengineering Conference (EHB)","20160128","2015","","","1","4","This paper aims to highlight the performances and advantages of three improved and fast AI algorithms that are mainly used in classification problems suitable for various fields. The discussions regarding the benchmark results appeal to the Modified version of Radial Basis Function (RBF-M) mentioned in the paper as Fast Support Vector Classifier (FSVC) or Fast Support Vector Machine, Extreme Learning Machine (ELM) with its randomness model and a reduced complexity version for Naïve Bayes (NB) algorithm. The performance studies conducted shows a good capacity of these networks to be used in medical embedded systems.","","Electronic:978-1-4673-7545-0; POD:978-1-4673-7546-7","10.1109/EHB.2015.7391553","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391553","artificial intelligence;machine learning;neural networks;radial basis function;universal approximation","Biological neural networks;Classification algorithms;Kernel;Neurons;Niobium;Support vector machines;Training","Bayes methods;learning (artificial intelligence);medical computing;random processes;support vector machines","extreme learning machine;fast support vector classifier;fast support vector machine;medical embedded systems;naive Bayes classifier;radial basis function-M;randomness model","","","","16","","","19-21 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Novel Ontology and Machine Learning Inspired Hybrid Cardiovascular Decision Support Framework","A. Hussain; K. Farooq; B. Luo; W. Slack","Dept. of Comput. Sci. & Math., Univ. of Stirling, Stirling, UK","2015 IEEE Symposium Series on Computational Intelligence","20160111","2015","","","824","832","Healthcare information management systems (HIMS) have a substantial amount of limitations such as rigidity and nonconformity to complex clinical processes like Electronic Healthcare records and effective utilisation of clinical practice guidelines to help provide effective clinical decision support. The conventional healthcare systems suffer from a general lack of intelligence, they are successful in offering basic patient management capabilities, but they do not offer consistent and holistic decision support capabilities for clinicians working under tight deadlines in a fast paced environment. The conventional healthcare information management systems are designed using branching logic based rigid architectures, which are hard to maintain and upgrade without considerable labour intensive effort. The proposed ontology and machine learning driven hybrid clinical decision support framework comprises of two key components (1) ontology driven clinical risk assessment and recommendation system and (2) machine learning driven prognostic system. The key aim of our research is to utilise information collected through the knowledge based ontology driven clinical risk assessment and recommendation system and non-knowledge based/evidence based machine learning driven prognostic system to deliver a holistic clinical decision support framework in the cardiovascular domain. The ontology driven clinical risk assessment and recommendation system could be used as a triage system for cardiovascular patients as a preventative solution, this could help clinicians prioritise patient referrals after reviewing a snapshot of patient's medical history (collected through an ontology driven intelligent context aware information collection using standardised clinical questionnaires) containing patient demographics information, cardiac risk scores, cardiac chest pain score, medication and recommended lab tests details. The machine learning driven prognostic system is developed using a chest p- in clinical case study identified by the consultant cardiologist, Professor Stephen Leslie from Raigmore Hospital in Inverness. The key aim of this clinical case study UK is to provide a clinical decision support mechanism for Raigmore Hospital's Rapid Access Chest Pain Clinic (RACPC) patients by combining evidence, extrapolated through legacy patient data (based on machine learning driven techniques) to facilitate evidence based cardiovascular preventative care. The machine learning driven prognostic system provides cardiac chest pain prognosis through a cardiac chest pain specific prognostic model which is validated through consultant cardiologist from Raigmore Hospital. The cardiac chest pain prognostic model could help clinicians diagnose cardiac chest pain patients efficiently and could also help clinicians reduce load on overly prescribed angiography treatment in a cost effective manner. Additional two clinical case studies in the heart disease and breast cancer domains are considered for the development and clinical validation of the machine learning driven prognostic system. The proposed novel ontology and machine learning driven hybrid clinical decision support framework will also be validated in other application areas.","","Electronic:978-1-4799-7560-0; POD:978-1-4799-7561-7","10.1109/SSCI.2015.122","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376697","","Diseases;Hospitals;Ontologies;Pain;Risk management","cardiovascular system;electronic health records;health care;learning (artificial intelligence);ontologies (artificial intelligence);risk management","HIMS;breast cancer;cardiac chest pain prognosis;clinical recommendation system;clinical risk assessment;electronic healthcare record;healthcare information management systems;heart disease;hybrid cardiovascular decision support framework;machine learning;ontology","","","","20","","","7-10 Dec. 2015","","IEEE","IEEE Conference Publications"
"Integrative Machine Learning augmentation","R. Khan","Department of Electrical Engineering, Sarhad University of Science and IT, Peshawar, Pakistan","2015 International Conference on Emerging Technologies (ICET)","20160121","2015","","","1","3","In this article, an integrative approach for augmenting the segmentation capabilities of the off-line trained Machine Learning (ML) classifier is presented. The proposed approach augments the ML performance in the graph cut setup. The integration of the prediction capabilities of the classifiers and neighborhood relationship of the pixels result in increase of segmentation performance. The experimental setup includes an evaluation of the Bayesian Network, Multilayer Perceptron, Random Forest and the Histogram approach of Jones and Rehg [1]. The evaluation results based on the color based detection dataset reveal that the proposed integrative approach improves the detection performance compared to using the off-line classifiers alone.","","CD-ROM:978-1-5090-0435-5; Electronic:978-1-5090-0436-2; POD:978-1-5090-0437-9","10.1109/ICET.2015.7389220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389220","Classifier;Detection;Graph Cuts;Machine Learning;Segmentation","Bayes methods;Classification algorithms;Histograms;Image color analysis;Image segmentation;Multilayer perceptrons;Skin","Bayes methods;graph theory;image classification;image colour analysis;image segmentation;learning (artificial intelligence);multilayer perceptrons;random processes","Bayesian network;Jones-Rehg histogram approach;color based detection dataset;graph cut setup;integrative machine learning augmentation;multilayer perceptron;off-line trained machine learning classifier;pixel neighborhood relationship;random forest;segmentation capabilities","","","","21","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Fast CU partition decision using machine learning for screen content compression","F. Duanmu; Z. Ma; Y. Wang","New York University, Brooklyn, NY 11201, USA","2015 IEEE International Conference on Image Processing (ICIP)","20151210","2015","","","4972","4976","Screen Content Coding (SCC) extension is currently being developed by Joint Collaborative Team on Video Coding (JCT-VC), as the final extension for the latest High-Efficiency Video Coding (HEVC) standard. It employs some new coding tools and algorithms (including palette coding mode, intra block copy mode, adaptive color transform, adaptive motion compensation precision, etc.), and outperforms HEVC by over 40% bitrate reduction on typical screen contents. However, enormous computational complexity is introduced on encoder primarily due to heavy optimization processing, especially rate distortion optimization (RDO) for Coding Unit (CU) partition decision and mode selection. This paper proposes a novel machine learning based approach for fast CU partition decision using features that describe CU statistics and sub-CU homogeneity. The proposed scheme is implemented as a ""preprocessing"" module on top of the Screen Content Coding reference software (SCM-3.0). Compared with SCM-3.0, experimental results show that our scheme can achieve 36.8% complexity reduction on average with only 3.0% BD-rate increase over 11 JCT-VC testing sequences when encoded using ""All Intra"" (AI) configuration.","","Electronic:978-1-4799-8339-1; POD:978-1-4799-8340-7","10.1109/ICIP.2015.7351753","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351753","HEVC;Machine Learning;Neural Network;Partition Decision;Screen Content Coding","Channel coding;Color;Histograms;Image color analysis;Neural networks;Training","computational complexity;data compression;learning (artificial intelligence);optimisation;rate distortion theory;statistics;video coding","CU partition decision;CU statistics;HEVC standard;JCT-VC;Joint Collaborative Team on Video Coding;RDO;SCC extension;SCM-3.0;all intra configuration;coding unit partition decision;computational complexity;high-efficiency video coding standard;machine learning;preprocessing module;rate distortion optimization;screen content coding;screen content coding reference software;screen content compression;subCU homogeneity","","2","","12","","","27-30 Sept. 2015","","IEEE","IEEE Conference Publications"
"Feasibility of using machine learning to access control in Squid proxy server","K. Ihalagedara; R. Kithuldeniya; S. Weerasekara; S. Deegalla","Department of Computer Engineering, Faculty of Engineering, University of Peradeniya, 20400 Sri Lanka","2015 IEEE 10th International Conference on Industrial and Information Systems (ICIIS)","20160204","2015","","","491","494","Fast Internet connectivity and billions of web sites have made World Wide Web an attractive place for people to use the Internet in their day-to-day life. Educational institutes provide the Internet access to students mainly for educational purposes. However, most of the time, students are allowed to access any content on the web. Therefore, the full bandwidth is consumed due to access to non-educational content such as streaming non-educational videos and downloading large image files, etc. Prevention of Internet usage on non-education content is practically difficult due to various reasons. Usually, this is implemented in the proxy server through maintaining a blacklist of URLs. Most of the time, this is a static list of URLs. With the fast growing content on the World Wide Web maintaining a static blacklist is impractical. In this paper, we propose a methodology to generate dynamic blacklist of URLs using machine learning techniques. We experimentally investigate several machine learning algorithms to predict whether the URL in concern is educational or noneducational. The results of the initial experiments show that linear support vector machines can be used to predict the content with 98.9% accuracy.","","Electronic:978-1-4799-1876-8; POD:978-1-4799-1877-5; USB:978-1-5090-1740-9","10.1109/ICIINFS.2015.7399061","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7399061","","Manuals;Static VAr compensators;XML","Internet;Web sites;authorisation;cache storage;file servers;learning (artificial intelligence);support vector machines","Internet connectivity;Internet usage;Squid proxy server;URL;Web sites;World Wide Web;access control;dynamic blacklist;linear support vector machine;machine learning;static blacklist","","","","15","","","18-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Integrating Static and Dynamic Malware Analysis Using Machine Learning","R. J. Mangialardo; J. C. Duarte","Inst. Mil. de Eng., Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Latin America Transactions","20151209","2015","13","9","3080","3087","Malware Analysis and Classification Systems use static and dynamic techniques, in conjunction with machine learning algorithms, to automate the task of identification and classification of malicious codes. Both techniques have weaknesses that allow the use of analysis evasion techniques, hampering the identification of malwares. In this work, we propose the unification of static and dynamic analysis, as a method of collecting data from malware that decreases the chance of success for such evasion techniques. From the data collected in the analysis phase, we use the C5.0 and Random Forest machine learning algorithms, implemented inside the FAMA framework, to perform the identification and classification of malwares into two classes and multiple categories. In our experiments, we showed that the accuracy of the unified analysis achieved an accuracy of 95.75% for the binary classification problem and an accuracy value of 93.02% for the multiple categorization problem. In all experiments, the unified analysis produced better results than those obtained by static and dynamic analyzes isolated.","1548-0992;15480992","","10.1109/TLA.2015.7350062","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350062","Dynamic Analysis;Information Security;Machine Learning;Malware;Static Analysis;Unified Analysis","Heuristic algorithms;Information security;Linux;Machine learning algorithms;Malware;Software;Support vector machines","data acquisition;learning (artificial intelligence);pattern classification;program diagnostics;security of data","C5.0 machine learning algorithm;FAMA framework;analysis phase;binary classification problem;data collection;dynamic malware analysis;evasion technique;malicious code classification;malicious code identification;random forest machine learning algorithm;static malware analysis","","","","","","","Sept. 2015","","IEEE","IEEE Journals & Magazines"
"An optimal formulation of feature weight allocation for CBR using machine learning techniques","M. Tamoor; H. Gul; H. Qaiser; A. Ali","Computer Science Depatrment, Forman Christian College, Lahore, Pakistan","2015 SAI Intelligent Systems Conference (IntelliSys)","20151221","2015","","","61","67","Case based reasoning (CBR) is frequently used for data classification problems, it can be considered as similarity based reasoning but equal importance are assigned to every attribute in the dataset. By identifying the features which are more important in the process of classification we can have better accuracy of CBR system. This paper proposes use of ranked attribute selection on the basis of their relevance. This attribute ranking is done by assigning different weights to different features. The results obtained after conducting experiments also indicated great improvement in the overall accuracy while classifying similar cases in CBR systems. The results are compared by using three famous ranking methods on three different datasets. The obtained results show that the proposed method is effective in terms of ranking the relevant features as compare to irrelevant features.","","Electronic:978-1-4673-7606-8; POD:978-1-4673-7607-5; USB:978-1-4673-7605-1","10.1109/IntelliSys.2015.7361085","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361085","CBR;classification;feature wrights;machine learning;weighted distance function","Cognition;Diabetes;Euclidean distance;Gain measurement;Intelligent systems;Problem-solving;Weight measurement","case-based reasoning;feature selection;learning (artificial intelligence);pattern classification","CBR system;case based reasoning;data classification problems;feature identification;machine learning techniques;optimal feature weight allocation formulation;ranked attribute selection;weighted distance function","","","","19","","","10-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"Interactive visual machine learning in spreadsheets","A. Sarkar; M. Jamnik; A. F. Blackwell; M. Spott","Computer Laboratory, University of Cambridge, 15 JJ Thomson Avenue, United Kingdom","2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","20151217","2015","","","159","163","BrainCel is an interactive visual system for performing general-purpose machine learning in spreadsheets, building on end-user programming and interactive machine learning. BrainCel features multiple coordinated views of the model being built, explaining its current confidence in predictions as well as its coverage of the input domain, thus helping the user to evolve the model and select training examples. Through a study investigating users' learning barriers while building models using BrainCel, we found that our approach successfully complements the Teach and Try system [1] to facilitate more complex modelling activities.","","Electronic:978-1-4673-7457-6; POD:978-1-4673-7458-3","10.1109/VLHCC.2015.7357211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357211","","Artificial neural networks;Brain modeling;Computational modeling;Numerical models","computer aided instruction;graphical user interfaces;interactive systems;learning (artificial intelligence);spreadsheet programs","BrainCel;Teach-and-Try system;complex modelling activities;end-user programming;general-purpose machine learning;input domain;interactive visual machine learning;spreadsheets;training example selection;user learning barriers","","1","","22","","","18-22 Oct. 2015","","IEEE","IEEE Conference Publications"
"Controlled islanding of power networks using machine learning algorithm","D. N. Trakas; E. M. Voumvoulakis; N. D. Hatziargyriou","Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece","MedPower 2014","20160121","2014","","","1","6","Wide-area blackouts have occurred the last decades due to severe disturbances and cascading failures. Controlled islanding of a power system is a measure for preventing blackout and limiting the effects of disturbances. Intention islanding aims to divide the network into electromechanically stable islands, which should satisfy a number of constraints such as generator coherency, generation-demand balance, system limits, and network dynamic constraints. The controlled islanding addresses the problem of finding which lines should be disconnected to create the islands. In this paper a modified algorithm of SelfOrganizing Map (SOM) in combination with a Particle Swarm Optimization (PSO) algorithm is proposed in order to determine which lines should be disconnected. To evaluate the algorithm performance the method is applied to test networks.","","Paper:978-1-78561-146-9","10.1049/cp.2014.1683","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7386124","Controlled islanding;graph theory;particle swarm optimization;power network;selforganiznig map","","particle swarm optimisation;power system control;power system faults;power system reliability","controlled islanding;electromechanically stable islands;generation-demand balance;generator coherency;machine learning algorithm;network dynamic constraints;particle swarm optimization algorithm;power networks;selforganizing map;system limits;wide-area blackouts","","","","","","","2-5 Nov. 2014","","IET","IET Conference Publications"
"OpenMP-Based Multi-core Parallel Cooperative PSO with ICS Using Machine Learning for Global Optimization Problem","Z. H. Liu; X. H. Li; W. Tan; Z. Zhang","Sch. of Inf. & Electr. Eng., Hunan Univ. of Sci. & Technol., Xiangtan, China","2015 IEEE International Conference on Systems, Man, and Cybernetics","20160114","2015","","","2786","2791","Novel parallel cooperative multiple particles swarm optimization algorithm with immune clonal selection (ICS) using machine learning based on multi-core architecture is presented for global optimization problem in this paper, the proposed method named O-PCPSO-ICS. The O-PCPSO-ICS consists of one memory and bottom multiple swarms. In O-PCPSO-ICS, the global best individuals are saved into the antibody memory and promoted by using the improved ICS operator. An opposition-based learning operator is employed to accelerate the convergence speed of Pbests. Furthermore, excellent search information is spread among different subpopulations by a migration scheme. Finally, the proposed method is running on multi-core architecture using open multiprocessing (OpenMP). The numerical simulations validated the O-PCPSO-ICS has a better performance in global search, solution accuracy, and convergence speed. Meanwhile, the computational efficiency of the proposed method is greatly enhanced by parallelization.","","Electronic:978-1-4799-8697-2; POD:978-1-4799-8698-9","10.1109/SMC.2015.486","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379618","and parallel;artificial immune system (AIS);immune network;multi-core;opposition-based learning (OBL);particle swarm optimization (PSO)","Acceleration;Cloning;Convergence;Optimization;Sociology;Standards;Statistics","application program interfaces;convergence;learning (artificial intelligence);mathematics computing;multiprocessing systems;parallel processing;particle swarm optimisation;search problems","O-PCPSO-ICS;OpenMP-based multicore parallel cooperative PSO;Pbests;antibody memory;convergence speed;global optimization problem;global search;immune clonal selection;improved ICS operator;machine learning;migration scheme;multicore architecture;open multiprocessing;opposition-based learning operator;parallel cooperative multiple particles swarm optimization algorithm","","","","21","","","9-12 Oct. 2015","","IEEE","IEEE Conference Publications"
"A settings tracking and providing scheme for differential protection based on machine learning","Y. Feng; B. Duan; C. Tan; Z. Yao","College of Information Engineering, Xiangtan University, Xiangtan, Hunan411105, China","2015 IEEE Innovative Smart Grid Technologies - Asia (ISGT ASIA)","20160121","2015","","","1","5","With the extensive use of differential protection in micro-grid, the demand for online obtaining the settings becomes more urgent than the process in the past. This paper proposes a new differential protection scheme for micro-grid where a settings tracking and providing scheme is implemented to acquire the latest enabled protection settings by tracking multiple setting group control block (SGCB) class services. A machine learning technique is implemented to assist classifier in identifying the most relevant electrical features which are required for the fault detection and to establish the best efficient differential protection strategy to micro-grid. A practical case study has successfully verified the adaptability and practicability of the micro-grids protection scheme where statistical classifier will make a decision based on protection settings and differential features.","","CD:978-1-5090-1237-4; Electronic:978-1-5090-1238-1","10.1109/ISGT-Asia.2015.7387013","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387013","Differential protection;IEC Standards;Machine Learning;Micro-grid;Protection setting","Computational modeling;Feature extraction;IEC Standards;Mathematical model;Predictive models;Relays;Switches","distributed power generation;learning (artificial intelligence);power generation protection","SGCB class services;differential features;differential protection scheme;electrical features;fault detection;machine learning technique;microgrid;protection settings;setting group control block class services;statistical classifier","","","","21","","","3-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Machine learning at the limit","J. Canny; H. Zhao; B. Jaros; Y. Chen; J. Mao","UC Berkeley, Berkeley, CA 94720, USA","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","233","242","Many systems have been developed for machine learning at scale. Performance has steadily improved, but there has been relatively little work on explicitly defining or approaching the limits of performance. In this paper we describe the application of roofline design, an approach borrowed from computer architecture, to large-scale machine learning. In roofline design, one exposes ALU, memory, and network limits, and the constraints they imply for algorithms. Using roofline design, we have developed a system called BIDMach which has demonstrated the highest performance to date for many ML problems. On one GPU-accelerated node, it generally outperforms other single-machine toolkits and cluster toolkits running on 100s of nodes. This performance level is enabled by a relatively small number of rooflined matrix primitives. Such performance implies a dramatic reduction in the energy used to perform these calculations. Beyond matrix kernels, roofline design can be applied to the end-to-end design of machine learning algorithms which minimize memory usage to optimize speed. This approach offers a further 2x to 3x gain in performance. Roofline design can also be applied to network primitives. We describe recent work on a sparse allreduce primitive called Kylix. We have shown that Kylix approaches the practical network throughput limit for allreduce, a basic primitive for distributed machine learning. Using Kylix, we describe an efficient transformation from model-parallel to data-parallel calculations. This transformation uses a secondary storage roofline, with similar parameters to the network. Finally, we describe several deployments of these techniques on real-world problems in two large internet companies. Once again, single node rooflined design demonstrated substantial gains over alternatives on either single nodes or clusters.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363760","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363760","Big Data;Distributed Systems;Scalable Machine Learning","Algorithm design and analysis;Benchmark testing;Graphics processing units;Machine learning algorithms;Scalability;Sparse matrices;Throughput","graphics processing units;learning (artificial intelligence);matrix algebra;parallel processing","ALU;BIDMach;GPU-accelerated node;Kylix;ML problems;data-parallel calculation;distributed machine learning;end-to-end design;matrix kernels;memory usage minimization;model-parallel calculation;rooflined matrix primitives;secondary storage roofline;single node rooflined design;sparse allreduce primitive","","","","13","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Behavior analysis of malware using machine learning","A. Dhammi; M. Singh","CSED, Thapar University, Patiala, India-147004","2015 Eighth International Conference on Contemporary Computing (IC3)","20151207","2015","","","481","486","In today's scenario, cyber security is one of the major concerns in network security and malware pose a serious threat to cyber security. The foremost step to guard the cyber system is to have an in-depth knowledge of the existing malware, various types of malware, methods of detecting and bypassing the adverse effects of malware. In this work, machine learning approach to the fore-going static and dynamic analysis techniques is investigated and reported to discuss the most recent trends in cyber security. The study captures a wide variety of samples from various online sources. The peculiar details about the malware such as file details, signatures, and hosts involved, affected files, registry keys, mutexes, section details, imports, strings and results from different antivirus have been deeply analyzed to conclude origin and functionality of malware. This approach contributes to vital cyber situation awareness by combining different malware discovery techniques, for example, static examination, to alter the session of malware triage for cyber defense and decreases the count of false alarms. Current trends in warfare have been determined.","","CD-ROM:978-1-4673-7946-5; Electronic:978-1-4673-7948-9; POD:978-1-4673-7949-6","10.1109/IC3.2015.7346730","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7346730","Classification;Clustering;Dynamic Analysis;Machine Learning;Static Analysis","Classification algorithms;HTML;Internet;Machine learning algorithms;Malware;Monitoring","invasive software;learning (artificial intelligence);program diagnostics;system monitoring","affected files;cyber defense;cyber security;cyber situation awareness;dynamic analysis techniques;file details;imports;machine learning approach;malware adverse effect bypass;malware adverse effect detection;malware behavior analysis;malware discovery techniques;malware functionality;malware origin;malware triage;mutexes;network security;registry keys;section details;signatures;static analysis techniques;static examination;strings","","","","21","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Comparing Tweet Classifications by Authors' Hashtags, Machine Learning, and Human Annotators","C. Nishioka; A. Scherp; K. Dellschaft","ZBW - Leibniz Inf. Centre for Econ., Germany","2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)","20160204","2015","1","","67","74","Over the last years, many papers have been published about how to use machine learning for classifying postings on microblogging platforms like Twitter, e.g., in order to assist users to reach tweets that interest them. Typically, the automatic classification results are then evaluated against a gold standard classification which consists of either (i) the hashtags of the tweets' authors, or (ii) manual annotations of independent human annotators. In this paper, we show that there are fundamental differences between these two kinds of gold standard classifications, i.e., human annotators are more likely to classify tweets like other human annotators than like the tweets' authors. Furthermore, we discuss how these differences may influence the evaluation of automatic classifications, like they may be achieved by Latent Dirichlet Allocation (LDA). We argue that researchers who conduct machine learning experiments for tweet classification should pay particular attention to the kind of gold standard they use. One may even argue that hashtags are not appropriate as a gold standard for tweet classification.","","Electronic:978-1-4673-9618-9; POD:978-1-4673-9619-6","10.1109/WI-IAT.2015.69","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396781","comparative study;human experimentation;short text classification;social media","Electronic mail;Gold;Probability distribution;Resource management;Standards;Tagging;Twitter","learning (artificial intelligence);pattern classification;social networking (online)","Twitter;author hashtags;automatic classification evaluation;gold standard classifications;human annotator;latent Dirichlet allocation;machine learning;microblogging platforms;postings classification;tweet classification","","","","24","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"Interweaving deep learning and semantic techniques for emotion analysis in human-machine interaction","D. Kollias; G. Marandianos; A. Raouzaiou; A. G. Stafylopatis","School of Electrical and Computer Engineering National Technical University of Athens 9, Iroon Politechniou street, Athens, Greece","2015 10th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)","20160104","2015","","","1","6","This paper presents a new data classification approach which is based on the one hand on deep learning neural networks for effectively extracting well defined categorical information from data and on the other hand on an adaptable support vector machine, which appropriately represents existing related knowledge about user and context specific data. The proposed approach is implemented and successfully tested experimentally for emotion analysis in human machine interaction.","","Electronic:978-1-4673-8395-0; POD:978-1-4673-8396-7","10.1109/SMAP.2015.7370086","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370086","convolutional networks;deep learning;emotion analysis;human computer interaction;kernel based semantic classification","Computer architecture;Convolution;Emotion recognition;Kernel;Machine learning;Semantics;Support vector machines","human computer interaction;learning (artificial intelligence);neural nets;pattern classification;support vector machines","adaptable support vector machine;categorical information;data classification approach;deep learning neural network;emotion analysis;human machine interaction;human-machine interaction;interweaving deep learning;semantic technique","","","","33","","","5-6 Nov. 2015","","IEEE","IEEE Conference Publications"
