"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7555963,7556822,7557524,7555923,7555961,7554459,7550517,7550762,7550908,7550763,7550294,7548905,7551240,7548007,7533152,7544751,7545963,7546330,7544264,7543604,7545826,7546208,7544877,7456223,7546909,7542876,7542841,7542340,7542860,7538632,7539804,7536995,7541478,7538652,7538344,7527183,7537619,7530676,7533763,7531734,7535556,7535925,7535456,7478442,7528443,7530259,7529388,7533399,7530344,7529319,7529504,7527766,7530131,7533509,7525689,7526944,7526907,7526668,7524318,7523678,7520948,7522360,7519307,7518973,7519380,7519263,7518371,7515417,7516018,7515688,7516249,7514421,7515728,7516385,7518045,7516031,7518050,7516130,7515747,7514599,7514617,7063894,7091914,7513217,7509354,7510026,7509892,7508050,7506773,7507421,7507416,7508614,7504333,7505190,7506248,7504615,7504694,7502737,7502858,7502901",2017/05/04 23:25:46
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Part-of-speech tagging based on dictionary and statistical machine learning","Z. Ye; Z. Jia; J. Huang; H. Yin","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","2016 35th Chinese Control Conference (CCC)","20160829","2016","","","6993","6998","Part-of-speech tagging is the basis of Natural Language Processing, and is widely used in information retrieval, text processing and machine translation fields. The traditional statistical machine learning methods of POS tagging rely on the high quality training data, but obtaining the training data is very time-consuming. The methods of POS tagging based on dictionaries ignore the context information, which lead to lower performance. This paper proposed a POS tagging approach which combines methods based on dictionaries and traditional statistical machine learning. The experimental results show that the approach not only can solve the problem that the training data are insufficient in statistical methods, but also can improve the performance of the methods based on dictionaries. The People's Daily corpus in January 1998 is used as testing data, and the accurate rate of POS tagging achieves 95.80%. For the ambiguity word POS tagging, the accuracy achieves 88%.","","Electronic:978-9-8815-6391-0; POD:978-1-5090-0910-7","10.1109/ChiCC.2016.7554459","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7554459","ambiguity word;big data;maximum entropy;part-of-speech tagging;word segmentation dictionary","Decision support systems","learning (artificial intelligence);natural language processing;text analysis","ambiguity word POS tagging;context information;dictionary learning;natural language processing;part-of-speech tagging;peoples daily corpus;performance improvement;statistical machine learning","","","","","","","27-29 July 2016","","IEEE","IEEE Conference Publications"
"Probability estimation for Predicted-Occupancy Grids in vehicle safety applications based on machine learning","P. Nadarajan; M. Botsch","Technische Hochschule Ingolstadt, Esplanade 10, Germany","2016 IEEE Intelligent Vehicles Symposium (IV)","20160808","2016","","","1285","1292","This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.","","Electronic:978-1-5090-1821-5; POD:978-1-5090-1822-2","10.1109/IVS.2016.7535556","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535556","","Acceleration;Computational modeling;Predictive models;Uncertainty;Vehicle dynamics;Vehicle safety;Vehicles","estimation theory;grid computing;learning (artificial intelligence);probability;road safety;road vehicles;traffic engineering computing","POG;complex traffic scenario;criticality estimation;grid-based probabilistic representation;machine learning;predicted-occupancy grids;probability estimation;random forest algorithm;traffic participants;trajectory planning;vehicle safety","","","","","","","19-22 June 2016","","IEEE","IEEE Conference Publications"
"A machine learning methods: Outlier detection in WSN","H. Ayadi; A. Zouinkhi; B. Boussaid; M. N. Abdelkrim","National Engineering, School of Gabes, University of Gabes, MACS Research Unit","2015 16th International Conference on Sciences and Techniques of Automatic Control and Computer Engineering (STA)","20160707","2015","","","722","727","Wireless sensor networks are gaining more and more attention these days. They gave us the chance of collecting data from noisy environment. So it becomes possible to obtain precise and continuous monitoring of different phenomenons. However wireless Sensor Network (WSN) is affected by many anomalies that occur due to software or hardware problems. So various protocols are developed in order to detect and localize faults then distinguish the faulty node from the right one. In this paper we are concentrated on a specific type of faults in WSN which is the outlier. We are focus on the classification of data (outlier and normal) using three different methods of machine learning then we compare between them. These methods are validated using real data obtained from motes deployed in an actual living lab.","","Electronic:978-1-4673-9234-1; POD:978-1-4673-9235-8","10.1109/STA.2015.7505190","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505190","Data;Fault Detection;Machine Learning;Outlier;Wireless Sensor Network","Base stations;Fault detection;Learning systems;Mathematical model;Monitoring;Training;Wireless sensor networks","fault diagnosis;learning (artificial intelligence);pattern classification;protocols;telecommunication computing;wireless sensor networks","WSN;data classification;hardware problems;living lab;machine learning methods;outlier detection;software problems;wireless sensor networks","","","","","","","21-23 Dec. 2015","","IEEE","IEEE Conference Publications"
"Data Classification Using Feature Selection and kNN Machine Learning Approach","S. Begum; D. Chakraborty; R. Sarkar","Comput. Sci. & Eng.., Gov. Coll. of Eng. & Textile Technol., Murshidabad, India","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","20160818","2015","","","811","814","The k Nearest Neighbour (kNN) method is one of the most popular algorithm in clustering and data classification. The kNN algorithm founds to be performed very efficient in the experiments on different dataset. In this paper, we focus on the classification problem. The algorithm is experienced over Leukemia dataset. Initially three feature selection algorithm Consistency Based Feature Selection (CBFS), Fuzzy Preference Based Rough Set (FPRS) and Kernelized Fuzzy Rough Set (KFRS) is applied on the dataset and then kNN is applied as a classifier onto the dataset. The results of our experiment demonstrates that CBFS algorithm generally perform better than other two KFRS and FPRS algorithm respectively.","","Electronic:978-1-5090-0076-0; POD:978-1-5090-0077-7","10.1109/CICN.2015.165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546208","Cancer Classification;Consistency based feature selection (CBFS);Feature Selection;Kernelized fuzzy rough","Algorithm design and analysis;Cancer;Classification algorithms;Kernel;Prediction algorithms;Rough sets;Training","cancer;feature extraction;fuzzy set theory;learning (artificial intelligence);medical diagnostic computing;pattern classification;rough set theory","consistency based feature selection;data classification;fuzzy preference based rough set;k nearest neighbour;kNN machine learning;kernelized fuzzy rough set;leukemia dataset","","","","","","","12-14 Dec. 2015","","IEEE","IEEE Conference Publications"
"An alternative technique for populating Thai tourism ontology from texts based on machine learning","A. Imsombut; C. Sirikayon","Faculty of Information Technology, Dhurakij Pundit University, Bangkok, Thailand","2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)","20160825","2016","","","1","4","This paper proposes an alternative technique to perform ontology population by using natural language processing and machine learning techniques. This study conceptually considers the population task as classifying terms into ontological subcategories. The proposed technique adopts the recognition method named Conditional Random Fields (CRFs) to identify boundary of instances and define types of subconcepts to generate relationships between instance-of and related concept. Also, the lexico-syntactic pattern is used to identify the relationships between instances. The experiments are conducted on Thai language documents in the tourism domain. The experimental results showed that the instances extraction step provided 77.62% and 70.87% of precision and recall measures, respectively, and relationships extraction step yielded 82.67% and 72.61% of recall measures.","","Electronic:978-1-5090-0806-3; POD:978-1-5090-0807-0","10.1109/ICIS.2016.7550762","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550762","Conditional Random Fields (CRFs);Machine Learning;Ontology Population;Tourism Ontology","Cultural differences;Dictionaries;Feature extraction;Natural language processing;Ontologies;Sociology;Statistics","Internet;document handling;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence);random processes;travel industry","CRFs;Thai language Web documents;Thai tourism ontology population;conditional random fields;instances extraction;lexico-syntactic pattern;machine learning;natural language processing;relationships extraction","","","","","","","26-29 June 2016","","IEEE","IEEE Conference Publications"
"Reducing data storage requirements for machine learning algorithms using principle component analysis","S. Kinkiri; W. J. C. Melis","Faculty of Engineering and Science, University of Greenwich, Chatham Maritime ME4 4TB, UK","2016 International Conference on Applied System Innovation (ICASI)","20160811","2016","","","1","4","While current computers have shown to be particular useful for arithmetic and logic implementations, their accuracy and efficiency for applications such as e.g. face, object and speech recognition, are not that impressive, especially when compared to what the human brain can do. Machine learning algorithms have been useful, especially for these type of applications, as they operate in a similar way to the human brain, by learning the data provided and storing it for future recognition. Until now, there has been a strong focus on developing the process of data storage and retrieval, merely neglecting the value of the provided information and the amount of data required to store. Hence, currently all information provided is stored, because it is difficult for the machine to decide which information needs to be stored. Consequently, large amounts of data are stored, which then affects the processing of the data. Thus, this paper investigates the opportunity to reduce data storage through the use of differentiation and combine it with an existing similarity detection algorithm. The differentiation isWhile current computers have shown to be particular useful for arithmetic and logic implementations, their accuracy and efficiency for applications such as e.g. face, object and speech recognition, are not that impressive, especially when compared to what the human brain can do. Machine learning algorithms have been useful, especially for these type of applications, as they operate in a similar way to the human brain, by learning the data provided and storing it for future recognition. Until now, there has been a strong focus on developing the process of data storage and retrieval, merely neglecting the value of the provided information and the amount of data required to store. Hence, currently all information provided is stored, because it is difficult for the machine to decide which information needs to be stored. Consequently, large amounts of data are stored, which then- affects the processing of the data. Thus, this paper investigates the opportunity to reduce data storage through the use of differentiation and combine it with an existing similarity detection algorithm. The differentiation is achieved through the use of, Principal Component Analysis (PCA), which not only reduces the data storage requirements by about 80%, but also improves the overall detection accuracy around 50 to nearly 80%. achieved through the use of, Principal Component Analysis (PCA), which not only reduces the data storage requirements by about 80%, but also improves the overall detection accuracy around 50 to nearly 80%.","","Electronic:978-1-4673-9888-6; POD:978-1-4673-9889-3","10.1109/ICASI.2016.7539804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539804","Data Storage Efficiency;Machine Learning;Principle Component Analysis (PCA)","Databases;Decision support systems;Euclidean distance;Image reconstruction;Learning systems;Principal component analysis;Training","data reduction;information retrieval;information storage;learning (artificial intelligence);principal component analysis","PCA;data learning;data processing;data recognition;data retrieval;data storage requirement reduction;differentiation;machine learning algorithms;principal component analysis;similarity detection algorithm","","","","","","","26-30 May 2016","","IEEE","IEEE Conference Publications"
"Cycle-slip-tolerant decision-boundary creation with machine learning","H. Kawase; Y. Mori; H. Hasegawa; K. i. Sato","Department of Electrical Engineering and Computer Science, Nagoya University Furo-cho, Chikusa-ku, Nagoya, 464-8603 Japan","2016 IEEE 6th International Conference on Photonics (ICP)","20160714","2016","","","1","3","Optimum symbol-decision boundaries created using machine learning approaches such as support-vector machine (SVM) can enhance system performance in the presence of nonlinear signal distortion. However, they fail if the received training signals for boundary creation include cycle slips induced by laser phase noise. In this paper, we propose a novel decision-boundary generation algorithm that is tolerant to cycle slips. Our proposed scheme groups rotationally symmetric constellation symbols and detects cycle slips by monitoring the phase differences between symbols in the same group. Through numerical analysis, we confirm that our proposed decision-boundary creation technique is immune to cycle slips.","","Electronic:978-1-4673-8153-6; POD:978-1-4673-8154-3","10.1109/ICP.2016.7510026","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7510026","cycle slip;laser phase noise;support vector machine (SVM)","Bit error rate;Kernel;Laser noise;Phase noise;Support vector machines;Training;Training data","laser noise;learning (artificial intelligence);nonlinear distortion;optical computing;optical receivers;phase noise;support vector machines","SVM;cycle slip-tolerant decision-boundary creation;laser phase noise;machine learning;nonlinear signal distortion;optimum symbol-decision boundaries;phase differences;rotationally symmetric constellation symbols;support vector machine","","","","","","","14-16 March 2016","","IEEE","IEEE Conference Publications"
"Cloud-based machine learning for the detection of anonymous web proxies","S. Miller; K. Curran; T. Lunney","Faculty of Computing and Engineering, Ulster University, Northern Ireland","2016 27th Irish Signals and Systems Conference (ISSC)","20160804","2016","","","1","6","The emergence and growth of cloud computing has made a serious impact on the IT industry in recent years with large companies starting to offer powerful, reliable and cost-efficient platforms for businesses to build and reshape their business models. Showing no sign of slowing down, cloud computing capabilities now include machine learning, with facilities for both designing and deploying models. With this capability of machine learning using cloud computing comes the increasing need to be able to classify whether an incoming connection is from a legitimate originating IP address or if it is being sent through an intermediary like a web proxy. Taking inspiration from Intrusion Detection Systems that make use of machine learning capabilities to improve anomaly detection accuracy, this paper proposes that cloud based machine learning can be used in order to detect and classify web proxy usage by capturing packet data and feeding it into a cloud based machine learning web service.","","Electronic:978-1-5090-3409-3; POD:978-1-5090-3410-9","10.1109/ISSC.2016.7528443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528443","Anonymous proxy;Cloud Computing;Machine Learning;SSL/TLS encryption;Traffic analysis;network traffic","Cloud computing;Companies;Intrusion detection;Servers","DP industry;Web services;cloud computing;learning (artificial intelligence);security of data","IP address;IT industry;Web proxy usage;Web service;anomaly detection accuracy;anonymous Web proxies;business models;cloud computing;cloud-based machine learning;cost-efficient platforms;intrusion detection systems;packet data","","1","","","","","21-22 June 2016","","IEEE","IEEE Conference Publications"
"A Preliminary Survey on Domain-Specific Languages for Machine Learning in Big Data","I. Portugal; P. Alencar; D. Cowan","David R. Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","2016 IEEE International Conference on Software Science, Technology and Engineering (SWSTE)","20160721","2016","","","108","110","The proliferation of data often called Big Data has created problems with traditional approaches to data capture, storage, analysis and visualization, thus opening up new areas of research. Machine Learning algorithms are one area that has been used in Big Data for analysis. However, because of the challenges Big Data imposes, these algorithms need to be adapted and optimized to specific applications. One important decision made by software engineers is the choice of the language that is used in the implementation of these algorithms. This literature survey identifies and describes domain-specific languages and frameworks used for Machine Learning in Big Data with the intention of assisting software engineers in making more informed choices and providing beginners with an overview of the main languages used in this domain. This is the first survey that aims at better understanding how domain-specific languages for Machine Learning are used as a tool for research in Big Data.","","Electronic:978-1-5090-1018-9; POD:978-1-5090-1019-6","10.1109/SWSTE.2016.23","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515417","BD;Big Data;DSL;ML;Machine Learning;domain-specific languages;literature survey","Big data;Computational modeling;DSL;Domain specific languages;Machine learning algorithms;Programming;Software","Big Data;learning (artificial intelligence);specification languages","Big Data;data analysis;data capture;data proliferation;data storage;data visualization;domain-specific languages;machine learning","","","","","","","23-24 June 2016","","IEEE","IEEE Conference Publications"
"A quick view on current techniques and machine learning algorithms for big data analytics","J. L. Berral-García","Barcelona Supercomputing Center, Jordi Girona 29-31, 08034, Spain","2016 18th International Conference on Transparent Optical Networks (ICTON)","20160825","2016","","","1","4","Big-data is an excellent source of knowledge and information from our systems and clients, but dealing with such amount of data requires automation, and this brings us to data mining and machine learning techniques. In the ICT sector, as in many other sectors of research and industry, platforms and tools are being served and developed in order to help professionals to treat their data and learn from it automatically; most of those platforms coming from big companies like Google or Microsoft, or from incubators at the Apache Foundation. This brief review explains the basics of machine learning with some ICT examples, and enumerates some (but not all) of the most used tools for analyzing and modelling big-data.","","Electronic:978-1-5090-1467-5; POD:978-1-5090-1468-2; USB:978-1-5090-1466-8","10.1109/ICTON.2016.7550517","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550517","analytics;big data;frameworks;knowledge discovery;machine learning","Classification algorithms;Clustering algorithms;Data mining;Data models;Libraries;Machine learning algorithms;Prediction algorithms","Big Data;data analysis;data mining;learning (artificial intelligence)","Apache Foundation;Big data analytics;Big data modelling;Google;ICT;ICT sector;Microsoft;data mining;knowledge source;machine learning","","","","","","","10-14 July 2016","","IEEE","IEEE Conference Publications"
"Designing adaptive learning support through machine learning techniques","R. O. Oboko; E. M. Maina; P. W. Waiganjo; E. I. Omwenga; R. D. Wario","University of Nairobi, P. O. Box 30197, 00100, Nairobi, Kenya","2016 IST-Africa Week Conference","20160808","2016","","","1","9","The use of web 2.0 technologies in web based learning systems has made learning more learner-centered. In a learner centered environment, there is need to provide appropriate support to learners based on individual learner characteristics in order to maximize learning. This requires a Web-based learning system to have an adaptive interface to suit individual learner characteristics in order to accommodate diversity of learner needs and abilities and to maintain an appropriate context for interaction and for achieving personalized learning. The purpose of this paper is to discuss how machine learning techniques can provide adaptive learning support in a Web-based learning system. In this research, two machine learning algorithms namely: Heterogeneous Value Difference Metric (HVDM) and Naive Bayes Classifier (NBC) were used. HVDM was used to determine those learners who were similar to the current learner while NBC was used to estimate the likelihood that the learner would need to use additional materials for the current concept. To demonstrate the concept we used a course in object oriented programming (OOP).","","Electronic:978-1-9058-2455-7; POD:978-1-5090-1955-7","10.1109/ISTAFRICA.2016.7530676","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530676","Adaptive interface;Heterogeneous Value Difference Metric;Machine learning;Naive Bayes Classifier;Web-based learning system","Adaptation models;Adaptive systems;Learning systems;Machine learning algorithms;Navigation;Object oriented modeling;User interfaces","Bayes methods;Internet;computer aided instruction;learning (artificial intelligence);object-oriented programming","HVDM;NBC;OOP;Web 2.0 technologies;Web based learning systems;adaptive learning;heterogeneous value difference metric;machine learning;naive Bayes classifier;object oriented programming;personalized learning","","","","","","","11-13 May 2016","","IEEE","IEEE Conference Publications"
"When quantitative trading meets machine learning: A pilot survey","Yelin Li; Junjie Wu; Hui Bu","School of Economics and Management, Beihang University, Beijing, China","2016 13th International Conference on Service Systems and Service Management (ICSSSM)","20160811","2016","","","1","6","Quantitative trading strategies are designed to look for relationships between data about an underlying security and its future price and then to generate alpha on a trading desk. Recent years have witnessed the increasing attention from both academic and corporate sectors on enhancing quantitative trading by machine learning techniques due to their excellent predictive powers, with a few successful stories from the markets further boosting optimism for this method of analysis. In this paper, we aim to conduct a comprehensive survey on the pilot study of applying machine learning for quantitative trading. We will review some earlier studies of using NNs and SVMs for stock price prediction. We will also touch some recent studies on designing online learning algorithms based on characteristics of financial time series, e.g., mean reversion of stock price. Another application of machine learning in quantitative trading is called meta-learning algorithm which considers how to assign weights to strategies. We will finally summarize the above research by pointing out promising machine learning techniques for different categories of trading strategies. We will also discuss slightly the potentials of machine learning techniques in helping generate strategies that do not only base on financial market data, like behavioral strategy, event-driven and untraditional index strategy.","","Electronic:978-1-5090-2842-9; POD:978-1-5090-2843-6","10.1109/ICSSSM.2016.7538632","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538632","machine learning;portfolio selection;prediction;quantitative trading","Analytical models;Kernel;Lead;Mathematical model;Neural networks;Predictive models;Support vector machines","economic forecasting;investment;learning (artificial intelligence);neural nets;pricing;stock markets;support vector machines;time series","SVM;academic sector;behavioral strategy;corporate sectors;event-driven strategy;financial market data;financial time series;future price;index strategy;investment analysis;machine learning;mean reversion;meta-learning algorithm;neural network;online learning algorithms;quantitative trading;security;stock price prediction;support vector machine;trading desk","","","","","","","24-26 June 2016","","IEEE","IEEE Conference Publications"
"Machine Learning","P. Louridas; C. Ebert","Athens University of Economics and Business","IEEE Software","20160824","2016","33","5","110","115","In machine learning, a computer first learns to perform a task by studying a training set of examples. The computer then performs the same task with data it hasn't encountered before. This article presents a brief overview of machine-learning technologies, with a concrete case study from code analysis.","0740-7459;07407459","","10.1109/MS.2016.114","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548905","ANNs;H2O;Hadoop;Julia;Matlab;Python;R;SAS;Spark;TensorFlow;artificial neural networks;classification;clustering;deep learning;dimensionality reduction;machine learning;regression;software development;software engineering;software technology;supervised learning;unsupervised learning","Artificial neural networks;Classification;Clustering;Complexity theory;Machine learning;Neural networks;Software engineering;Supervised learning","learning (artificial intelligence)","code analysis;machine learning","","","","","","","Sept.-Oct. 2016","","IEEE","IEEE Journals & Magazines"
"Revealing encrypted WebRTC traffic via machine learning tools","M. Di Mauro; M. Longo","University of Salerno, Via Giovanni Paolo II 132, 84084, Fisciano (SA), Italy","2015 12th International Joint Conference on e-Business and Telecommunications (ICETE)","20160721","2015","04","","259","266","The detection of encrypted real-time traffic, both streaming and conversational, is an increasingly important issue for agencies in charge of lawful interception. Aside from well established technologies used in real-time communication (e.g. Skype, Facetime, Lync etc.) a new one is recently spreading: Web Real-Time Communication (WebRTC), which, with the support of a robust encryption method such as DTLS, offers capabilities for encrypted voice and video without the need of installing a specific application but using a common browser, like Chrome, Firefox or Opera. Encrypted WebRTC traffic cannot be recognized through methods of semantic recognition since it does not exhibit a discernible sequence of information pieces and hence statistical recognition methods are called for. In this paper we propose and evaluate a decision theory based system allowing to recognize encrypted WebRTC traffic by means of an open-source machine learning environment: Weka. Besides, a reasoned comparison among some of the most credited algorithms (J48, Simple Cart, Naïve Bayes, Random Forests) in the field of decision systems has been carried out, indicating the prevalence of Random Forests.","","Electronic:978-9-8975-8140-3; POD:978-1-4673-8532-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518045","DTLS;Decision Trees;Encrypted Real-Time Traffic;Machine Learning;WebRTC;Weka","Browsers;Classification algorithms;Cryptography;Protocols;Training;WebRTC","","","","","","","","","20-22 July 2015","","IEEE","IEEE Conference Publications"
"Mystic: Predictive Scheduling for GPU Based Cloud Servers Using Machine Learning","Y. Ukidave; X. Li; D. Kaeli","Dept. of Electr. & Comput. Eng., Northeastern Univ., Boston, MA, USA","2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","20160721","2016","","","353","362","GPUs have become the primary choice of accelerators for high-end data centers and cloud servers, which can host thousands of disparate applications. With the growing demands for GPUs on clusters, there arises a need for efficient co-execution of applications on the same accelerator device. However, the resource contention among co-executing applications causes interference which leads to degradation in execution performance, impacts QoS requirements of applications and lowers overall system throughput. While previous work has proposed techniques for detecting interference, the existing solutions are either developed for CPU clusters, or use static profiling approaches which can be computationally intensive and do not scale well. We present Mystic, an interference-aware scheduler for efficient co-execution of applications on GPU-based clusters and cloud servers. The most important feature of Mystic is the use of learning-based analytical models for detecting interference between applications. We leverage a collaborative filtering framework to characterize an incoming application with respect to the interference it may cause when co-executing with other applications while sharing GPU resources. Mystic identifies the similarities between new applications and the executing applications, and guides the scheduler to minimize the interference and improve system throughput. We train the learning model with 42 CUDA applications, and consider another separate set of 55 diverse, real-world GPU applications for evaluation. Mystic is evaluated on a live GPU cluster with 32 NVIDIA GPUs. Our framework achieves performance guarantees for 90.3% of the evaluated applications. When compared with state-of-the art interference-oblivious schedulers, Mystic improves the system throughput by 27.5% on average, and achieves a 16.3% improvement on average in GPU utilization.","1530-2075;15302075","Electronic:978-1-5090-2140-6; POD:978-1-5090-2141-3","10.1109/IPDPS.2016.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516031","GPU;cloud;machine learning;scheduling;server","Collaboration;Graphics processing units;Interference;Processor scheduling;Scheduling;Servers;Throughput","cloud computing;graphics processing units;learning (artificial intelligence);network servers;scheduling","CPU clusters;CUDA applications;GPU based cloud servers;GPU resources;GPU utilization;GPU-based clusters;Mystic;NVIDIA GPU applications;QoS requirements;collaborative filtering framework;high-end data centers;interference-aware schedulers;learning-based analytical models for;machine learning;predictive scheduling;resource contention;static profiling","","","","","","","23-27 May 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques for age at death estimation from long bone lengths","V. S. Ionescu; M. Teletin; E. M. Voiculescu","Faculty of Mathematics and Computer Science, Babe&#337;-Bolyai University, 1, M. Kog&#259;lniceanu Street, 400084, Cluj-Napoca, Romania","2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)","20160709","2016","","","457","462","Estimating age at death of cadavers is an important ability in various subfields of forensic science and bioarchaeology. It can allow investigators to pinpoint someone's identity, more accurately locate an event of interest in time and clarify other societal or legal issues concerning a given skeletal collection. There are two main categories of methods for estimating age at death: biochemical methods - which use various biological or chemical processes to obtain an estimation -, and mathematical methods - which employ the use of data mining tools such as regression in order to estimate age from various numerical features. In this paper, we propose two machine learning approaches for the age estimation problem and prove that they outperform existing mathematical approaches on a number of case studies derived from publicly available data used for this task. Moreover, our methods are more robust and easier to reuse on new data.","","Electronic:978-1-5090-2380-6; POD:978-1-5090-2381-3; USB:978-1-5090-2379-0","10.1109/SACI.2016.7507421","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507421","Age estimation;Artificial neural networks;Bioarchaeology;Machine learning;Regression;Support vector machines","Biological neural networks;Bones;Estimation;Neurons;Support vector machines;Training","data mining;learning (artificial intelligence)","age estimation problem;bioarchaeology;biochemical methods;biological processes;cadavers;chemical processes;data mining tools;death estimation;forensic science;long bone lengths;machine learning techniques;mathematical methods;skeletal collection","","","","","","","12-14 May 2016","","IEEE","IEEE Conference Publications"
"A practice guide of software aging prediction in a web server based on machine learning","Y. Yan; P. Guo","School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China","China Communications","20160714","2016","13","6","225","235","In the past two decades, software aging has been studied by both academic and industry communities. Many scholars focused on analytical methods or time series to model software aging process. While machine learning has been shown as a very promising technique in application to forecast software state: normal or aging. In this paper, we proposed a method which can give practice guide to forecast software aging using machine learning algorithm. Firstly, we collected data from a running commercial web server and preprocessed these data. Secondly, feature selection algorithm was applied to find a subset of model parameters set. Thirdly, time series model was used to predict values of selected parameters in advance. Fourthly, some machine learning algorithms were used to model software aging process and to predict software aging. Fifthly, we used sensitivity analysis to analyze how heavily outcomes changed following input variables change. In the last, we applied our method to an IIS web server. Through analysis of the experiment results, we find that our proposed method can predict software aging in the early stage of system development life cycle.","1673-5447;16735447","","10.1109/CC.2016.7513217","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513217","machine learning;software aging;software rejuvenation;web server","Aging;Machine learning algorithms;Prediction algorithms;Predictive models;Software systems;Web servers","Internet;file servers;learning (artificial intelligence);sensitivity analysis;software engineering","IIS server;academic communities;analytical methods;commercial Web server;industry communities;machine learning algorithm;sensitivity analysis;software aging prediction;system development life cycle","","","","","","","June 2016","","IEEE","IEEE Journals & Magazines"
"A Hindi Question Answering System using Machine Learning approach","G. Nanda; M. Dua; K. Singla","Department of Computer Science and Engineering, Banasthali Vidyapith, Jaipur, Rajasthan, India","2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)","20160718","2016","","","311","314","A Question Answering (QA) System is fairly an Information Retrieval(IR) system in which a query is stated to the system and it relocates the correct or closest results to the specific question asked in natural language. It is one of the consequences of Natural Language Interface to Database (NLIDB). The paper discusses the implementation of a Hindi Language QA system developed using Machine Learning approach. The implemented QA system is divided into three phases: Accessing natural language (NL) Query; where the input query is read, preprocessed and get tokenized; next is feature extraction (FE) phase; where specific features vectors are identified from the results of previous phase and finally the Classification phase; where the Naïve Baye's classifier has been used, along with the knowledge base already stored in the system. This paper reflects that the concepts of similarity and classification provide better results than the use of `equals' concept by defining the overall accuracy of finding the relevant answers of the specific questions asked by the user.","","Electronic:978-1-5090-0082-1; POD:978-1-5090-0083-8","10.1109/ICCTICT.2016.7514599","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514599","Classification;Information Retrieval;Machine Learning;Natural Language;Question Answering System","Computer science;Databases;Feature extraction;Keyboards;Knowledge based systems;Knowledge discovery;Natural languages","Bayes methods;learning (artificial intelligence);natural language processing;question answering (information retrieval)","Hindi question answering system;IR system;NLIDB;Naïve Baye's classifier;QA System;feature extraction;information retrieval;machine learning approach;natural language interface to database;natural language query","","","","","","","11-13 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques applied to system characterization and equalization","D. Zibar; J. Thrane; J. Wass; R. Jones; M. Piels; C. Schaeffer","DTU Fotonik, Technical University of Denmark, DTU, Kongens Lyngby, 2800, Denmark, DK","2016 Optical Fiber Communications Conference and Exhibition (OFC)","20160811","2016","","","1","3","Linear signal processing algorithms are effective in combating linear fibre channel impairments. We demonstrate the ability of machine learning algorithms to combat nonlinear fibre channel impairments and perform parameter extraction from directly detected signals.","","Electronic:978-1-9435-8007-1; POD:978-1-5090-0735-6","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7537619","","Estimation;Modulation;Nonlinear optics;Optical noise;Optical polarization;Phase noise;Signal to noise ratio","equalisers;learning (artificial intelligence);optical fibre communication;signal processing","linear signal processing algorithms;machine learning algorithms;nonlinear fibre channel;parameter extraction;system characterization;system equalization","","","","","","","20-24 March 2016","","IEEE","IEEE Conference Publications"
"Purely Structural Protein Scoring Functions Using Support Vector Machine and Ensemble Learning","S. Mirzaei; T. Sidi; C. Keasar; S. Crivelli","Shokoufeh Mirzaei is with the California State Polytechnic University, Pomona; Industrial and Manufacturing Engineering Department, 3801 W Temple Ave, Pomona , CA 91768.(Email: smirzaei@cpp.edu)","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2016","PP","99","1","1","The function of a protein is determined by its structure, which creates a need for efficient methods of protein structure determination to advance scientific and medical research. Because current experimental structure determination methods carry a high price tag, computational predictions are highly desirable. Given a protein sequence, computational methods produce numerous 3D structures known as decoys. However, selection of the best quality decoys is challenging as the end users can handle only a few ones. Therefore, scoring functions are central to decoy selection. They combine measurable features into a single number indicator of decoy quality. Unfortunately, current scoring functions do not consistently select the best decoys. Machine learning techniques offer great potential to improve decoy scoring. This paper presents two machine-learning based scoring functions to predict the quality of proteins structures, i.e., the similarity between the predicted structure and the experimental one without knowing the latter. We use different metrics to compare these scoring functions against three state-of-the-art scores. This is a first attempt at comparing different scoring functions using the same non-redundant dataset for training and testing and the same features. The results show that adding informative features may be more significant than the method used.","1545-5963;15455963","","10.1109/TCBB.2016.2602269","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551240","Algorithms;Protein structure prediction;decoy quality assessment;ensemble learning;machine learning;performance;protein features;scoring functions","Amino acids;Electronic mail;Feature extraction;Machine learning algorithms;Proteins;Support vector machines;Training","","","","","","","","20160824","","","IEEE","IEEE Early Access Articles"
"Machine learning based detection of vandalism in Wikipedia across languages","A. Susuri; M. Hamiti; A. Dika","Faculty of Contemporary Sciences and Technologies, South East European University, Tetovo, Macedonia","2016 5th Mediterranean Conference on Embedded Computing (MECO)","20160801","2016","","","446","451","Applying machine learning algorithms for detecting vandalism in two languages are described in this paper. Vandalism is a major issue in Wikipedia as it accounts for about 1% of edits during 2015. The majority of vandalism is from human editors, whose vandalism can be traced through access and edit logs. In this paper, we propose using a list of classifiers in one language, and then evaluate them across languages in two datasets: the hourly count of views of each Wikipedia article, and the used edit history of articles. For this purpose, Simple English and Albanian Wikipedia datasets will be used. The results obtained show that the characteristic features of vandalism can be learned from view and edit patterns, and models built in one language can be applied successfully to other languages.","","CD-ROM:978-1-5090-2220-5; Electronic:978-1-5090-2222-9; POD:978-1-5090-2223-6","10.1109/MECO.2016.7525689","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7525689","Wikipedia;machine learning;vandalism","Electronic publishing;Encyclopedias;History;Internet;Machine learning algorithms;Metadata","Web sites;learning (artificial intelligence);natural language processing;text analysis","Albanian Wikipedia datasets;Wikipedia article;access logs;edit logs;human editors;machine learning based vandalism detection;simple English Wikipedia datasets","","","","","","","12-16 June 2016","","IEEE","IEEE Conference Publications"
"Machine learning techniques and the existence of variant processes in humans declarative memory","A. Frid; H. Hazan; E. Koilis; L. M. Manevitz; M. Merhav; G. Star","Edmond J. Safra Brain Research Center, University of Haifa, Israel","2015 7th International Joint Conference on Computational Intelligence (IJCCI)","20160804","2015","3","","114","121","This work uses supervised machine learning methods over fMRI brain scans to establish the existence of two different encoding procedures for human declarative memory. Declarative knowledge refers to the memory for facts and events and initially depends on the hippocampus. Recent studies which used patients with hippocampal lesions and neuroimaging data, suggested the existence of an alternative process to form declarative memories. This process is triggered by learning mechanism called “Fast Mapping (FM)”, as opposed to the ‘standard’ “Explicit Encoding (EE)” learning procedure. The present work gives a clear biomarker on the existence of two distinct encoding procedures as we can accurately predict which of the processes is being used directly from voxel activity in fMRI scans. The scans are taken during retrieval of information wherein the tasks are identical regardless of which procedure was used for acquisition and by that reflect conclusive prediction. This is an identification of a more subtle cognitive task than direct perceptual cognitive tasks as it requires some encoding and processing in the brain.","","Electronic:978-9-8975-8165-6; POD:978-1-5090-1968-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533399","Classification;Declarative Memory;Feature Selection;Information Biomarkers;Machine Learning;Radial Basis Function Kernel;Support Vector Machines;functional Magnetic Resonance Imaging (fMRI)","Biomarkers;Encoding;Frequency modulation;Hippocampus;Learning systems;Neuroimaging;Standards","","","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"Accuracy analysis of machine learning-based performance modeling for microprocessors","Y. Tanaka; K. Oka; T. Ono; K. Inoue","Graduate School of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan","2016 Fourth International Japan-Egypt Conference on Electronics, Communications and Computers (JEC-ECC)","20160725","2016","","","83","86","This paper analyzes accuracy of performance models generated by machine learning-based empirical modeling methodology. Although the accuracy strongly depends on the quality of learning procedure, it is not clear what kind of learning algorithms and training data set (or feature) should be used. This paper inclusively explores the learning space of processor performance modeling as a case study. We focus on static architectural parameters as training data set such as cache size and clock frequency. Experimental results show that a tree-based non-linear regression modeling is superior to a stepwise linear regression modeling. Another observation is that clock frequency is the most important feature to improve prediction accuracy.","","Electronic:978-1-4673-8937-2; POD:978-1-4673-8938-9; USB:978-1-4673-8936-5","10.1109/JEC-ECC.2016.7518973","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518973","","Analytical models;Clocks;Computational modeling;Data models;Machine learning algorithms;Regression tree analysis;Training data","learning (artificial intelligence);microprocessor chips;regression analysis;trees (mathematics)","empirical modeling methodology;learning algorithms;machine learning based performance modeling;microprocessors;processor performance modeling;stepwise linear regression modeling;training data set;tree based nonlinear regression modeling","","","","","","","May 31 2016-June 2 2016","","IEEE","IEEE Conference Publications"
"Performance Analysis of Ensemble Supervised Machine Learning Algorithms for Missing Value Imputation","S. Kumar; M. K. Pandey; A. Nath; K. Subbiah","Dept. of Comput. Sci., Banaras Hindu Univ., Varanasi, India","2016 2nd International Conference on Computational Intelligence and Networks (CINE)","20160901","2016","","","160","165","In this era of cloud computing, web services based solutions are gaining popularity. The applications running on distributed environment seek new parameters for them to perform efficiently to satisfy end user's requirements. Finding these parameters for increasing efficiency has become a talk of researchers now days. Non functional performance of a web service is described through User dependent QoS properties. These QoS parameters are generally described in WS-Policy in Service Level Agreement (SLA). Usually in web service QoS datasets, web service QoS values are missing, which makes missing value imputations an important job while working with cloud web services. In the current work we compared the prediction accuracy of two groups of supervised machine learning ensembles based Meta learners: bagging and additive regression (boosting) with a fusion of the seven base learners in both. Random forest is found to be better performing in both Meta learners: bagging and boosting than other learning algorithms.","2375-5822;23755822","Electronic:978-1-5090-0451-5; POD:978-1-5090-0452-2","10.1109/CINE.2016.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556822","Bagging;Cloud computing;QoS dataset;Web services;boosting;missing data imputation;random forest","Additives;Bagging;Boosting;Cloud computing;Quality of service;Throughput","Web services;cloud computing;learning (artificial intelligence);quality of service;regression analysis","QoS values;Web services;additive regression;bagging;boosting;cloud computing;ensemble supervised machine learning algorithms;meta learners;missing value imputation;random forest","","","","","","","11-11 Jan. 2016","","IEEE","IEEE Conference Publications"
"Genetic Algorithm as Machine Learning for profiles recognition","Y. Carbonne; C. Jacob","University of Technology of Troyes, 12 rue Marie Curie, France","2015 7th International Joint Conference on Computational Intelligence (IJCCI)","20160804","2015","1","","157","166","Persons are often asked to provide information about themselves. These data are very heterogeneous and result in as many “profiles” as contexts. Sorting a large amount of profiles from different contexts and assigning them back to a specific individual is quite a difficult problem. Semantic processing and machine learning are key tools to achieve this goal. This paper describes a framework to address this issue by means of concepts and algorithms selected from different Artificial Intelligence fields. Indeed, a Vector Space Model is customized to first transpose semantic information into a mathematical model. Then, this model goes through a Genetic Algorithm (GA) which is used as a supervised learning algorithm for training a computer to determine how much two profiles are similar. Amongst the GAs, this study introduces a new reproduction method (Best Together), and compare it to some usual ones (Wheel, Binary Tournament). This paper also evaluates the accuracy of the GAs predictions for profiles clustering with the computation of a similarity score, as well as its ability to classify two profiles are similar or non-similar. We believe that the overall methodology can be used for any kind of sources using profiles and, more generally, for similar data recognition.","","Electronic:978-9-8975-8165-6; POD:978-1-5090-1968-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529319","Clustering;Genetic Algorithm;Machine Learning;Natural Language Processing;Profiles Recognition","Biological cells;Computers;Genetic algorithms;Semantics;Sociology;Statistics;Training","","","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"Robust signal recognition algorithm based on machine learning in heterogeneous networks","X. Liu; R. Li; C. Zhao; P. Wang","Beijing University of Posts and Telecommunications, Beijing 100876, China","Journal of Systems Engineering and Electronics","20160721","2016","27","2","333","342","There are various heterogeneous networks for terminals to deliver a better quality of service. Signal system recognition and classification contribute a lot to the process. However, in low signal to noise ratio (SNR) circumstances or under time-varying multipath channels, the majority of the existing algorithms for signal recognition are already facing limitations. In this series, we present a robust signal recognition method based upon the original and latest updated version of the extreme learning machine (ELM) to help users to switch between networks. The ELM utilizes signal characteristics to distinguish systems. The superiority of this algorithm lies in the random choices of hidden nodes and in the fact that it determines the output weights analytically, which result in lower complexity. Theoretically, the algorithm tends to offer a good generalization performance at an extremely fast speed of learning. Moreover, we implement the GSM/WCDMA/LTE models in the Matlab environment by using the Simulink tools. The simulations reveal that the signals can be recognized successfully to achieve a 95% accuracy in a low SNR (0 dB) environment in the time-varying multipath Rayleigh fading channel.","","","10.1109/JSEE.2016.00034","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514421","Rayleigh fading channel;automatic signal classification;extreme learning machine (ELM);features-extracted;heterogeneous networks","Algorithm design and analysis;Classification algorithms;Fading channels;Machine learning algorithms;OFDM;Robustness;Signal to noise ratio","","","","","","","","","April 20 2016","","BIAI","BIAI Journals & Magazines"
"Improving software quality using machine learning","K. Chandra; G. Kapoor; R. Kohli; A. Gupta","CSE Department, Dr. A.P.J. AKTU, Uttar Pradesh, India","2016 International Conference on Innovation and Challenges in Cyber Security (ICICCS-INBUSH)","20160815","2016","","","115","118","Software is an entity that keeps on progressing and endures continuous changes, in order to boost its functionality and maintain its effectiveness. During the development of software, even with advanced planning, well documentation and proper process control, are problems that are countered. These defects influence the quality of software in one way or the other which may result into failure. Therefore, in today's neck to neck competition, it is our requirement to control and minimize these defects in software engineering. Software prediction models are typically used to map the patterns of classes of software that are prone to change. This paper highlights the significant analysis in the area's subject to learn and stimulate the association between the metric specifying the object orientation & the concept of change proneness. This would often lead us to rigorous testing so as to find all kinds of possibilities in the data set. We have two views to be addressed: (1) Parameters quantification that affects the quality, functionality and productivity of the software. (2) Machine learning technologies are used for predicting software Here, the focus of the research paper is to equate and compare all of learning methods corresponding to performance parameter with its statistical method & methodology which would often results enhanced. Data points are the basis for prediction of models.","","Electronic:978-1-5090-2084-3; POD:978-1-5090-2085-0","10.1109/ICICCS.2016.7542340","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542340","Change proneness;Empirical validation;Receiver operating characteristics (ROC);Software Defects and Prediction;Software Metric;Software Quality","Androids;Humanoid robots;Predictive models;Software;Software measurement;Testing","learning (artificial intelligence);software quality;statistical analysis;system documentation","defect minimization;learning methods;machine learning technologies;software development;software engineering;software prediction models;software quality improvement;statistical method","","","","","","","3-5 Feb. 2016","","IEEE","IEEE Conference Publications"
"Machine learning approach to automatic bucket loading","S. Dadhich; U. Bodin; F. Sandin; U. Andersson","Lule&#x00E5; University of Technology, Sweden","2016 24th Mediterranean Conference on Control and Automation (MED)","20160808","2016","","","1260","1265","The automation of bucket loading for repetitive tasks of earth-moving operations is desired in several applications at mining sites, quarries and construction sites where larger amounts of gravel and fragmented rock are to be moved. In load and carry cycles the average bucket weight is the dominating performance parameter, while fuel efficiency and loading time also come into play with short loading cycles. This paper presents the analysis of data recorded during loading of different types of gravel piles with a Volvo L110G wheel loader. Regression models of lift and tilt actions are fitted to the behavior of an expert driver for a gravel pile. We present linear regression models for lift and tilt action that explain most of the variance in the recorded data and outline a learning approach for solving the automatic bucket loading problem. A general solution should provide good performance in terms of average bucket weight, cycle time of loading and fuel efficiency for different types of material and pile geometries. We propose that a reinforcement learning approach can be used to further refine models fitted to the behavior of expert drivers, and we briefly discuss the scooping problem in terms of a Markov decision process and possible value functions and policy iteration schemes.","","Electronic:978-1-4673-8345-5; POD:978-1-4673-8347-9","10.1109/MED.2016.7535925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535925","","Fuels;Load modeling;Loading;Pistons;Rocks;Trajectory;Wheels","Markov processes;foundations;learning (artificial intelligence);loading equipment;regression analysis;rocks;wheels","Markov decision process;Volvo L110G wheel loader;automatic bucket loading;average bucket weight;carry cycles;construction sites;data analysis;fragmented rock;fuel efficiency;gravel piles;lift action;linear regression models;loading cycle time;loading time;machine learning approach;material geometries;mining sites;performance parameter;pile geometries;policy iteration schemes;quarries;reinforcement learning approach;repetitive earth-moving operation tasks;tilt action;value functions","","","","","","","21-24 June 2016","","IEEE","IEEE Conference Publications"
"Improving Classification Accuracy of a Machine Learning Approach for FPGA Timing Closure","Q. Yanghua; N. Kapre; H. Ng; K. Teo","Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore","2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)","20160818","2016","","","80","83","We can use Cloud Computing and Machine Learning to help deliver timing closure of FPGA designs using InTime [2], [3]. This approach requires no modification to the input RTL and relies exclusively on manipulating the CAD tool parameters that drive the optimization heuristics. By running multiple combinations of the parameters in parallel, we learn from results and identify which parameters caused an improvement in the final results. By systematically building a classification model and training it with the results of the parallel CAD runs, we can build an accurate estimation flow for helping identify which parameters are more likely to improve the timing. In this paper, we consider strategies for improving the predictive accuracy of our classifier models to help guide the CAD run towards timing convergence. With ensemble learning we are able to increase average AUC score from 0.74 to 0.79, which could also translate into 2.7× savings in machine learning effort.","","Electronic:978-1-5090-2356-1; POD:978-1-5090-2357-8","10.1109/FCCM.2016.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544751","","Benchmark testing;Design automation;Field programmable gate arrays;Machine learning algorithms;Prediction algorithms;Solid modeling;Timing","cloud computing;field programmable gate arrays;learning (artificial intelligence);logic CAD;optimisation","CAD tool parameter;FPGA design;FPGA timing closure;InTime;classification model;cloud computing;ensemble learning;machine learning;optimization heuristics;parallel CAD","","","","","","","1-3 May 2016","","IEEE","IEEE Conference Publications"
"Developing machine learning-based models to estimate time to failure for PHM","C. Yang; T. Ito; Y. Yang; J. Liu","National Research Council Canada Ottawa, Ontario Canada","2016 IEEE International Conference on Prognostics and Health Management (ICPHM)","20160815","2016","","","1","6","The core of PHM (Prognostic and Health Monitoring) technology is prognostics which is able to estimate time to failure (TTF) for the monitored components or systems using the built-in predictive models. However the development of predictive models for TTF estimation remains a challenge. To address this issue, we proposed to develop machine learning-based models for TTF estimation by using the techniques from machine learning and data mining. In the past decade, we have been working on the development of machine learning-based models for estimating TTF and applied the developed technology to various real-world applications such as train wheel prognostics, and aircraft engine prognostics. In this paper, we report two kinds of machine learning-based models for estimating TTF, including multistage classification, on-demand regression. The multistage classification improves the TTF estimation over one stage classification by dividing the time window into more small narrow time windows. A case study, APU prognostics, demonstrates the usefulness of the developed methods. The results from the case study show that the machine learning-based modeling method is an effective and feasible way to develop predictive models to estimate TTF for PHM.","","Electronic:978-1-5090-0382-2; POD:978-1-5090-0383-9","10.1109/ICPHM.2016.7542876","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542876","PHM;classification;on-demand regression;prognostics;regression;time to failure estimation","Atmospheric modeling;Computational modeling;Data models;Estimation;Mathematical model;Predictive models;Prognostics and health management","condition monitoring;data mining;engines;failure (mechanical);failure analysis;learning (artificial intelligence);mechanical engineering computing;pattern classification;regression analysis","APU prognostics;PHM;TTF estimation;auxiliary power unit engines;data mining;machine learning-based models;multistage classification;on-demand regression;predictive models;prognostic and health monitoring;time to failure estimation","","","","","","","20-22 June 2016","","IEEE","IEEE Conference Publications"
"Clinical text analysis using machine learning methods","K. P. Chodey; G. Hu","Department of Computer Science, Central Michigan University, Mount Pleasant, Michigan, 48859, USA","2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)","20160825","2016","","","1","6","SemEval (Semantic Evaluation) is an annual workshop where attendees participate in a series of evaluations (competitions) of computational semantic analysis for natural language processing (NLP). The series evaluations include 10-20 tasks each year. In this paper we present our entry to the SemEval-2014 Task 7 on the Analysis of Clinical Text evaluation. The main aim of this task is to analyze large amounts of clinical data and to find the mentions of clinical disorders. This task consists of two sub tasks: a) named entity recognition i.e, identifying disorder concepts that belong to Unified Medical Language System (UMLS) semantic group; and b) normalization, i.e, mapping mentions of disorders to UMLS Concept Unique Identifier (CUI). In this paper, we present a supervised machine learning system for prediction of disorder named entities based on the conditional random field model. The data set provided by the Task 7 organizer was used to evaluate our model.","","Electronic:978-1-5090-0806-3; POD:978-1-5090-0807-0","10.1109/ICIS.2016.7550908","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550908","clinical text analysis;conditional random fields;natural language processing;semantic evaluation","Feature extraction;Natural language processing;Semantics;Testing;Text analysis;Training data;Unified modeling language","Unified Modeling Language;learning (artificial intelligence);medical information systems;natural language processing;text analysis","CUI;NLP;SemEval;UMLS concept unique identifier;UMLS semantic group;clinical disorder;clinical text analysis;conditional random field;disorder named entities;named entity recognition;natural language processing;normalization;semantic evaluation;supervised machine learning;unified medical language system","","","","","","","26-29 June 2016","","IEEE","IEEE Conference Publications"
"Randomized Prediction Games for Adversarial Machine Learning","S. Rota Bulò; B. Biggio; I. Pillai; M. Pelillo; F. Roli","ICT-Tev, Fondazione Bruno Kessler, Trento 38123, Italy.","IEEE Transactions on Neural Networks and Learning Systems","","2016","PP","99","1","13","In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time, e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hiding information about the classifier to the attacker. Recent work has proposed game-theoretical formulations to learn secure classifiers, by simulating different evasion attacks and modifying the classification function accordingly. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization. In this paper, we overcome this limitation by proposing a randomized prediction game, namely, a noncooperative game-theoretic formulation in which the classifier and the attacker make randomized strategy selections according to some probability distribution defined over the respective strategy set. We show that our approach allows one to improve the tradeoff between attack detection and false alarms with respect to the state-of-the-art secure classifiers, even against attacks that are different from those hypothesized during design, on application examples including handwritten digit recognition, spam, and malware detection.","2162-237X;2162237X","","10.1109/TNNLS.2016.2593488","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533509","Adversarial learning;computer security;evasion attacks;game theory;pattern classification;randomization.","Algorithm design and analysis;Cost function;Games;Generators;Malware;Security;Training","","","","","","","","20160804","","","IEEE","IEEE Early Access Articles"
"Machine learning in tracking associations with stereo vision and lidar observations for an autonomous vehicle","M. Allodi; A. Broggi; D. Giaquinto; M. Patander; A. Prioletti","VisLab srl, c/o Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; degli Studi di Parma, Italy","2016 IEEE Intelligent Vehicles Symposium (IV)","20160808","2016","","","648","653","Obstacles detection is used nowdays for a number of road safety applications, increasing the drivers awareness in potential dangerous situations. A reliable and robust obstacles detection continues to be largely investigated and still remains an open challenge, especially for difficult scenarios and in general cases, with loosened constraints and multiple simultaneous use-cases. This work presents an obstacles detection, tracking and fusion algorithm which allows to reconstruct the environment surrounding the vehicle. While the techniques used for the detection are well-known in literature, the improvements introduced by this paper regard the data association and tracking approach of heterogeneous sensors observations. An innovative multi-dimensional structure based on association costs originating from a classifier provides an optimal solution to the association problem with respect to the total association cost. An Unscented Kalman Filter (UKF) managing a variable number of observations, arbitrarily composable, allows to correctly address the combined tracking and fusion challenge. The results, obtained on a public benchmark, show improvements with respect to state of the art systems.","","Electronic:978-1-5090-1821-5; POD:978-1-5090-1822-2","10.1109/IVS.2016.7535456","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535456","","Cameras;Laser radar;Radar tracking;Reliability;Sensor fusion;Three-dimensional displays","Kalman filters;collision avoidance;learning (artificial intelligence);mobile robots;nonlinear filters;object tracking;optical radar;road safety;road vehicles;sensor fusion;stereo image processing","UKF;association costs;autonomous vehicle;dangerous situations;data association;fusion algorithm;heterogeneous sensors observations;lidar observations;machine learning;multidimensional structure;obstacles detection;road safety applications;stereo vision;tracking associations;unscented Kalman filter","","","","","","","19-22 June 2016","","IEEE","IEEE Conference Publications"
"Anomaly detection based on profile signature in network using machine learning technique","K. Atefi; S. Yahya; A. Rezaei; S. H. B. M. Hashim","Faculty of Computer and Mathematical sciences, Malaysia Institute of Transport (MITRANS) Universiti Teknologi MARA (UiTM) 40450 Shah Alam, Selangor, Malaysia","2016 IEEE Region 10 Symposium (TENSYMP)","20160725","2016","","","71","76","Within the last couple of years, a lot of IDSs have already been developed, both as research prototypes and as commercial systems. The aim of Intrusion Detection would be to positively identify all true attacks and adversely identify all non-attacks. Through the study of the procedure and signature of intrusion behaviors, IDS can produce a real-time reaction to intrusion occasions and invasion processes. The objective of this study is to work with appropriate algorithms for intrusion detection and hybrid them to find acceptable results and investigate on new hybrid techniques of intrusion detecting system with high accuracy. Moreover, it also aims to minimize a false positive and false negative alarms in networks.","","Electronic:978-1-5090-0931-2; POD:978-1-5090-0932-9","10.1109/TENCONSpring.2016.7519380","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519380","Anomaly detection;Genetic Algorithms;Intrusion Detection System;Machine Learning;Support Vector Machine;hybrid model;profile signature","Algorithm design and analysis;Computer architecture;Genetic algorithms;Grippers;Intrusion detection;Support vector machines","learning (artificial intelligence);security of data","IDS;anomaly detection;intrusion detecting system;intrusion invasion processes;intrusion occasions;machine learning;profile signature","","","","","","","9-11 May 2016","","IEEE","IEEE Conference Publications"
"Provenance Constraints and Attributes Definition in OWL Ontology to Support Machine Learning","M. Pandey; R. Pandey","Amity Inst. of Inf. Technol., Amity Univ., Lucknow, India","2015 International Conference on Computational Intelligence and Communication Networks (CICN)","20160818","2015","","","1408","1414","The Semantic Web considered an intelligent web is an effective way to retrieve data. Though the semantic data is returned by the semantic web it may lack trust and cannot be suitable for consumption by man or machine without the evaluation of provenance. The integration of Provenance in the Trust layer of Semantic web not only allows for intelligent knowledge retrieval but also leads to retrieving trustworthy data. This can be efficiently achieved if we bring to use the various layers of the Provenance Stack thereby creating valid provenance instances. This paper explores the need for information trustworthiness in OWL Ontology and has deliberated the rules to incorporate trustworthiness so as to develop valid provenance instances by means of the constraints provided in the PROV-CONSTRAINTS layer. The resultant Ontology can thus be used by machines for learning & making semantic inferences.","","Electronic:978-1-5090-0076-0; POD:978-1-5090-0077-7","10.1109/CICN.2015.334","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546330","Ontology;PROV-Constraints;Provenance Stack;Semantic Web;Trust","Data models;OWL;Ontologies;Semantics;Service-oriented architecture;Vocabulary","information retrieval;knowledge representation languages;learning (artificial intelligence);ontologies (artificial intelligence);semantic Web;trusted computing","OWL ontology;PROV-CONSTRAINTS layer;information trustworthiness;intelligent knowledge retrieval;machine learning;provenance constraints;provenance stack;semantic Web trust layer;semantic data;semantic inferences;trustworthy data retrieval","","","","","","","12-14 Dec. 2015","","IEEE","IEEE Conference Publications"
"Fault Modeling of Extreme Scale Applications Using Machine Learning","A. Vishnu; H. v. Dam; N. R. Tallent; D. J. Kerbyson; A. Hoisie","Pacific Northwest Nat. Lab., Richland, WA, USA","2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","20160721","2016","","","222","231","Faults are commonplace in large scale systems. These systems experience a variety of faults such as transient, permanent and intermittent. Multi-bit faults are typically not corrected by the hardware resulting in an error. This paper attempts to answer an important question: Given a multi-bit fault in main memory, will it result in an application error - and hence a recovery algorithm should be invoked - or can it be safely ignored? We propose an application fault modeling methodology to answer this question. Given a fault signature (a set of attributes comprising of system and application state), we use machine learning to create a model which predicts whether a multi-bit permanent/transient main memory fault will likely result in error. We present the design elements such as the fault injection methodology for covering important data structures, the application and system attributes which should be used for learning the model, the supervised learning algorithms (and potentially ensembles), and important metrics. We use three applications - NWChem, LULESH and SVM - as examples for demonstrating the effectiveness of the proposed fault modeling methodology.","1530-2075;15302075","Electronic:978-1-5090-2140-6; POD:978-1-5090-2141-3","10.1109/IPDPS.2016.111","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516018","Applications;Exascale;Fault;Machine Learning;Modeling","Algorithm design and analysis;Data structures;Hardware;Machine learning algorithms;Space vehicles;Supervised learning;Support vector machines","fault diagnosis;fault tolerant computing;learning (artificial intelligence);system recovery","LULESH;NWChem;SVM;application attributes;application fault modeling methodology;data structures;extreme scale applications;fault injection methodology;fault signature;intermittent faults;large scale systems;machine learning;main memory fault;multibit faults;permanent faults;recovery algorithm;supervised learning algorithms;system attributes;transient faults","","","","","","","23-27 May 2016","","IEEE","IEEE Conference Publications"
"Machine learning methods for surgery cancellation","Li Luo; Hangjiang Liu; Xiaolong Hou; Yingkang Shi","Business School of Sichuan University, Chengdu, China","2016 13th International Conference on Service Systems and Service Management (ICSSSM)","20160811","2016","","","1","4","Surgery cancellation has a greatly negative impact on both hospital and patient. A commonly used method for improving this problem is analyzing cancellation reasons through statistical analysis, while the improvement is inefficient. Based on surgery data from the hospital information system (HIS) of West China Hospital (WCH), we proposed to use machine learning methods include Random forests (RF), Bagging, Boosting and Bayesian additive regression trees (BART) to predict surgery cancellation. The experimental result shows that the Boosting with Over-sampling performed well in prediction accuracy (Sensitive: 0.5234, Specific: 0.8105, Accuracy: 0.7651 and AUC: 0.6670). And our research also points out the latent causes for surgery cancellation. It's useful for hospital managers.","","Electronic:978-1-5090-2842-9; POD:978-1-5090-2843-6","10.1109/ICSSSM.2016.7538652","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538652","AUC;machine learning;predict;surgery cancellation","Bayes methods;History;Radio frequency;Surgery","Bayes methods;learning (artificial intelligence);medical computing;medical information systems;regression analysis;surgery;trees (mathematics)","BART;Bayesian additive regression tree;HIS;RF;WCH;West China Hospital;hospital information system;machine learning;random forest;surgery cancellation","","","","","","","24-26 June 2016","","IEEE","IEEE Conference Publications"
"A comprehensive study on machine learning concepts for text mining","K. Surya; R. Nithin; S. Prasanna; R. Venkatesan","(CSE) SRC-Sastra University, Kumbakonam","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)","20160804","2016","","","1","5","The aim of machine learning is to solve a given problem using past experience or example data. Many machine learning applications are using now-a-days already. More aspiring problems can be handled as more data become accessible. Here. in this context we learn in detail about text mining as a multi-dimensional field which involves the closely linked areas or sections like 1. Retrieving information, 2. Machine learning concepts shortly termed as ML, 3. Statistics, 4. And finally Computational linguistics and specifically to be mentioned, data mining. With the use of sample data or previously gained experience, machine learning is included into computers to enhance or improve a performance decisive factor. In this context we have detailed a model up to some level of constraints, and learning is the processing of a main content to enhance the parameter of the form using the training or sample data or previously gained experience. This may be designed to gain knowledge from the given data, or use the effect for changes in the future, or both. These learning techniques also helps us to make solutions to various bugs which includes vision, speech recognition, and robotics. We take the example of the main analysis of preprocessing of tasks and procedures, then classification, then clustering, information extraction and finally visualization.","","DVD:978-1-5090-1276-3; Electronic:978-1-5090-1277-0; POD:978-1-5090-1278-7","10.1109/ICCPCT.2016.7530259","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530259","","Computers;Context;Hidden Markov models;Knowledge discovery;Pragmatics;Text mining","computational linguistics;data mining;information retrieval;learning (artificial intelligence);statistical analysis;text analysis","computational linguistics;data mining;information retrieval;machine learning;multidimensional field;statistics;text mining","","","","","","","18-19 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning barycenter approach to identifying LPV state-space models","R. A. Romano; P. L. dos Santos; F. Pait; T. P. Perdicoúlis; J. A. Ramos","Escola de Engenharia Mau&#225;, Instituto Mau&#225; de Tecnologia, S&#227;o Caetano do Sul, SP, Brazil","2016 American Control Conference (ACC)","20160801","2016","","","6351","6356","In this paper an identification method for state-space LPV models is presented. The method is based on a particular parameterization that can be written in linear regression form and enables model estimation to be handled using Least-Squares Support Vector Machine (LS-SVM). The regression form has a set of design variables that act as filter poles to the underlying basis functions. In order to preserve the meaning of the Kernel functions (crucial in the LS-SVM context), these are filtered by a 2D-system with the predictor dynamics. A data-driven, direct optimization based approach for tuning this filter is proposed. The method is assessed using a simulated example and the results obtained are twofold. First, in spite of the difficult nonlinearities involved, the nonparametric algorithm was able to learn the underlying dependencies on the scheduling signal. Second, a significant improvement in the performance of the proposed method is registered, if compared with the one achieved by placing the predictor poles at the origin of the complex plane, which is equivalent to considering an estimator based on an LPV auto-regressive structure.","","CD-ROM:978-1-4673-8680-7; Electronic:978-1-4673-8682-1; POD:978-1-4673-8683-8","10.1109/ACC.2016.7526668","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526668","","Context;Electronic mail;Kernel;Mathematical model;Prediction algorithms;Predictive models;Support vector machines","identification;learning (artificial intelligence);least squares approximations;linear parameter varying systems;multidimensional systems;optimisation;pole assignment;regression analysis;state-space methods;support vector machines","2D-system;LPV auto-regressive structure;LPV state-space model identification;LS-SVM;basis functions;data-driven based approach;design variables;direct optimization based approach;filter poles;filter tuning;kernel functions;least-squares support vector machine;linear regression form;machine learning barycenter approach;model estimation;nonparametric algorithm;predictor dynamics;predictor pole placement","","","","","","","6-8 July 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Methods for Attack Detection in the Smart Grid","M. Ozay; I. Esnaola; F. T. Yarman Vural; S. R. Kulkarni; H. V. Poor","School of Computer Science, University of Birmingham, Birmingham, U.K.","IEEE Transactions on Neural Networks and Learning Systems","20160715","2016","27","8","1773","1786","Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semisupervised) are employed with decision- and feature-level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than attack detection algorithms that employ state vector estimation methods in the proposed attack detection framework.","2162-237X;2162237X","","10.1109/TNNLS.2015.2404803","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063894","Attack detection;classification;phase transition;smart grid security;sparse optimization","Kernel;Learning systems;Machine learning algorithms;Prediction algorithms;Smart grids;Statistical learning;Vectors","IEEE standards;learning (artificial intelligence);power engineering computing;power system security;smart power grids","IEEE test systems;attack detection algorithms;attack detection framework;attack detection problems;attack vectors;learning algorithms;machine learning algorithms;machine learning methods;smart grid;state vector estimation methods;statistical learning methods","","5","","49","","20150319","Aug. 2016","","IEEE","IEEE Journals & Magazines"
"A Deep Learning Scheme for Motor Imagery Classification based on Restricted Boltzmann Machines","N. Lu; T. Li; X. Ren; H. Miao","N. Lu is with the State Key Laboratory for Manufacturing Systems Engineering, Systems Engineering Institute, Xi&#x2019;an Jiaotong University, Xi&#x2019;an Shaanxi, China.","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2016","PP","99","1","1","Motor imagery classification is an important topic in brain computer interface (BCI) research that enables the recognition of a subject’s intension to, e.g., implement prosthesis control. The brain dynamics of motor imagery are usually measured by electroencephalography (EEG) as nonstationary time series of low signal-to-noise ratio. Although a variety of methods have been previously developed to learn EEG signal features, the deep learning idea has rarely been explored to generate new representation of EEG features and achieve further performance improvement for motor imagery classification. In this study, a novel deep learning scheme based on restricted Boltzmann machine (RBM) is proposed. Specifically, frequency domain representations of EEG signals obtained via fast Fourier transform (FFT) and wavelet package decomposition (WPD) are obtained to train three RBMs. These RBMs are then stacked up with an extra output layer to form a four-layer neural network, which is named the frequential deep belief network (FDBN). The output layer employs the softmax regression to accomplish the classification task. Also, the conjugate gradient method and backpropagation are used to fine tune the FDBN. Extensive and systematic experiments have been performed on public benchmark datasets, and the results show that the performance improvement of FDBN over other selected state-of-the-art methods is statistically significant. Also, several findings that may be of significant interest to the BCI community are presented in this article.","1534-4320;15344320","","10.1109/TNSRE.2016.2601240","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546909","Motor imagery;brain computer interface;deep learning,;restricted Boltzman machine","Benchmark testing;Biological neural networks;Electroencephalography;Feature extraction;Frequency-domain analysis;Machine learning;Training","","","","","","","","20160817","","","IEEE","IEEE Early Access Articles"
"Machine learning classifiers for android malware analysis","C. C. U. Lopez; A. N. Cadavid","Grupo de Investigacion i2t, Universidad Icesi, Cali, Colombia","2016 IEEE Colombian Conference on Communications and Computing (COLCOM)","20160721","2016","","","1","6","Android is an operating system which currently has over one billion active users for all their mobile devices, with a market impact that is influencing an increase in the amount of information that can be obtained from different users, facts that have motivated the development of malware by cybercriminals. To solve the problems caused by malware, Android implements a different architecture and security controls, such as unique user ID (UID) for each application, system permissions, and its distribution platform Google Play. It has been shown that there are ways to violate that protection, and how the complexity for create a new solutions are increased while cybercriminals improve their skills to develop malware. The developer and researchers community has been developing alternatives aimed at improving the level of safety, some solutions have been proposed: analysis techniques, frameworks, sandboxes, and systems security. Most solutions have adopted a cloud computing model with different tools and analysis techniques, one of the most promising ways is the implementation of artificial intelligence solutions for malware analysis. This work proposes a new module that implements a static analysis framework with six algorithms of machine learning for detect malware for Android.","","Electronic:978-1-5090-1084-4; POD:978-1-5090-1085-1","10.1109/ColComCon.2016.7516385","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516385","Google;Machine Learning;Malware;Security;Smart Phone","Androids;Humanoid robots;Kernel;Malware;Smart phones;Support vector machines","cloud computing;invasive software;learning (artificial intelligence);mobile computing;pattern classification;smart phones","Android malware analysis;artificial intelligence;cloud computing model;distribution platform Google Play;machine learning classifier;mobile device;operating system;sandboxe;security control;smart phone;static analysis framework;system security","","","","","","","27-29 April 2016","","IEEE","IEEE Conference Publications"
"Dynamic Adaptation of Policies Using Machine Learning","A. Pelaez; A. Quiroz; M. Parashar","Rutgers Discovery Inf. Inst., Rutgers Univ., New Brunswick, NJ, USA","2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)","20160721","2016","","","501","510","Managing large systems in order to guarantee certain behavior is a difficult problem due to their dynamic behavior and complex interactions. Policies have been shown to provide a very expressive and easy way to define such desired behaviors, mainly because they separate the definition of desired behavior from the enforcement mechanism, allowing either one to be changed fairly easily. Unfortunately, it is often difficult to define policies in terms of attributes that can be measured and/or directly controlled, or to set adaptable (i.e. non-static) parameters in order to account for rapidly changing system behavior. Dynamic policies are meant to solve these problems by allowing system administrators to define higher level parameters, which are more closely related to the business goals, while providing an automated mechanism to adapt them at a lower level, where attributes can be measured and/or controlled. Here, we present a way to define such policies, and a machine learning model that is able to dynamically apply lower level static policies by learning a hidden relationship between the high level business attribute space, and the low level monitoring space. We show that this relationship exists, and that we can learn it producing an error of at most 8.78% at least 96% of the time.","","Electronic:978-1-5090-2453-7; POD:978-1-5090-2454-4","10.1109/CCGrid.2016.64","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515728","adaptation;dynamic;large-scale systems;machine learning;management;policy;reliability","Business;Context;Context modeling;Heuristic algorithms;Mathematical model;Monitoring;Time measurement","formal specification;learning (artificial intelligence);system monitoring","dynamic policies adaptation;high level business attribute space;low level monitoring space;machine learning;static policies","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Early diagnosis of Alzheimer's disease using machine learning techniques: A review paper","A. Khan; M. Usman","Dept. of Computing, Shaheed Zulfikar Ali Bhutto Institute of Science and Technology (SZABIST), Islamabad, Pakistan","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","20160801","2015","01","","380","387","Alzheimer's, an irreparable brain disease, impairs thinking and memory while the aggregate mind size shrinks which at last prompts demise. Early diagnosis of AD is essential for the progress of more prevailing treatments. Machine learning (ML), a branch of artificial intelligence, employs a variety of probabilistic and optimization techniques that permits PCs to gain from vast and complex datasets. As a result, researchers focus on using machine learning frequently for diagnosis of early stages of AD. This paper presents a review, analysis and critical evaluation of the recent work done for the early detection of AD using ML techniques. Several methods achieved promising prediction accuracies, however they were evaluated on different pathologically unproven data sets from different imaging modalities making it difficult to make a fair comparison among them. Moreover, many other factors such as pre-processing, the number of important attributes for feature selection, class imbalance distinctively affect the assessment of the prediction accuracy. To overcome these limitations, a model is proposed which comprise of initial pre-processing step followed by imperative attributes selection and classification is achieved using association rule mining. Furthermore, this proposed model based approach gives the right direction for research in early diagnosis of AD and has the potential to distinguish AD from healthy controls.","","Electronic:978-9-8975-8164-9; POD:978-1-5090-1967-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526944","Alzheimer's Disease;Class Imbalance;Computer Aided Diagnosis;Early Diagnosis;Machine Learning;Pathologically Proven Data","Alzheimer's disease;Data mining;Positron emission tomography;Single photon emission computed tomography;Standards;Support vector machines","artificial intelligence;brain;data mining;diseases;medical diagnostic computing;optimisation;probability","AD diagnosis;Alzheimer disease early diagnosis;artificial intelligence;association rule mining;attribute classification;attribute selection;brain disease;class imbalance;feature selection;machine learning;optimization technique;probabilistic technique","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"Diagnosing wind turbine faults using machine learning techniques applied to operational data","K. Leahy; R. L. Hu; I. C. Konstantakopoulos; C. J. Spanos; A. M. Agogino","Department of Mechanical Engineering University of California, Berkeley","2016 IEEE International Conference on Prognostics and Health Management (ICPHM)","20160815","2016","","","1","8","Unscheduled or reactive maintenance on wind turbines due to component failures incurs significant downtime and, in turn, loss of revenue. To this end, it is important to be able to perform maintenance before it's needed. By continuously monitoring turbine health, it is possible to detect incipient faults and schedule maintenance as needed, negating the need for unnecessary periodic checks. To date, a strong effort has been applied to developing Condition monitoring systems (CMSs) which rely on retrofitting expensive vibration or oil analysis sensors to the turbine. Instead, by performing complex analysis of existing data from the turbine's Supervisory Control and Data Acquisition (SCADA) system, valuable insights into turbine performance can be obtained at a much lower cost. In this paper, data is obtained from the SCADA system of a turbine in the South-East of Ireland. Fault and alarm data is filtered and analysed in conjunction with the power curve to identify periods of nominal and fault operation. Classification techniques are then applied to recognise fault and fault-free operation by taking into account other SCADA data such as temperature, pitch and rotor data. This is then extended to allow prediction and diagnosis in advance of specific faults. Results are provided which show success in predicting some types of faults.","","Electronic:978-1-5090-0382-2; POD:978-1-5090-0383-9","10.1109/ICPHM.2016.7542860","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542860","FDD;Fault Detection;SCADA Data;SVM;Wind Turbine","Blades;Generators;Maintenance engineering;Monitoring;Sensors;Wind speed;Wind turbines","SCADA systems;condition monitoring;fault diagnosis;learning (artificial intelligence);wind turbines","CMS;SCADA system;alarm data;condition monitoring systems;fault data;fault diagnosis;fault prediction;faulty operation period;incipient fault detection;machine learning techniques;maintenance scheduling;nominal operation period;operational data;pitch data;power curve;rotor data;south-east Ireland;temperature data;turbine health monitoring;turbine performance;turbine supervisory control and data acquisition system;wind turbine fault diagnosis","","","","","","","20-22 June 2016","","IEEE","IEEE Conference Publications"
"A comparison between heuristic and machine learning techniques in fall detection using Kinect v2","A. Amini; K. Banitsas; J. Cosmas","Department of Electronics and Computer Engineering, College of Engineering Design and Physical Sciences, Brunel University London, London, UK","2016 IEEE International Symposium on Medical Measurements and Applications (MeMeA)","20160808","2016","","","1","6","In this paper, two algorithms were tested on 11 healthy adults: one based on heuristic and another one on video tagging machine learning methods for automatic fall detection; both utilizing Microsoft Kinect v2. For our heuristic approach, we used skeletal data to detect falls based on a set of instructions and signal filtering methods. For the machine learning approach, we implemented a dataset utilizing the Adaptive Boosting Trigger (AdaBoostTrigger) algorithm via video tagging to enable fall detection. For each approach, each subject on average has performed six true positive and six false positive fall incidents in two different conditions: one with objects partially blocking the sensor's view and one with partial obstructed field of view. The accuracy of each approach has been compared against one another in different conditions. The result showed an average of 95.42 % accuracy in the heuristic approach and 88.33 % in machine learning technique. We conclude that heuristic approach performs more accurately for fall detection when there is a limited number of training dataset available. Nevertheless, as the gesture detection's complexity increases, the need for a machine learning technique is inevitable.","","Electronic:978-1-4673-9172-6; POD:978-1-4673-9173-3","10.1109/MeMeA.2016.7533763","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533763","AAL;Kinect;fall detection;heuristic;machine learning","Floors;Machine learning algorithms;Magnetic heads;Sensors;Software;Three-dimensional displays;Training","biomechanics;biomedical optical imaging;filtering theory;infrared imaging;learning (artificial intelligence);medical signal processing;video signal processing","AdaBoostTrigger algorithm;Microsoft Kinect v2;adaptive boosting trigger algorithm;automatic fall detection;dataset;false positive fall incidents;gesture detection complexity;heuristic learning techniques;infrared imaging;machine learning techniques;partial obstructed field-of-view;sensor view;signal filtering methods;skeletal data;training dataset;video tagging machine learning methods","","","","","","","15-18 May 2016","","IEEE","IEEE Conference Publications"
"A Combined Rule-Based and Machine Learning Audio-Visual Emotion Recognition Approach","K. Seng; L. M. Ang; C. Ooi","Kah Phooi Seng is with the School of Computing and Mathematics, Sturt University, NSW 2678, Australia.(Email: jasmine.seng@gmail.com)","IEEE Transactions on Affective Computing","","2016","PP","99","1","1","This paper proposes an audio-visual emotion recognition system that uses a mixture of rule-based and machine learning techniques to improve the recognition efficacy in the audio and video paths. The visual path is designed using the Bidirectional Principal Component Analysis (BDPCA) and Least-Square Linear Discriminant Analysis (LSLDA) for dimensionality reduction and class discrimination. The extracted visual features are passed into a newly designed Optimized Kernel-Laplacian Radial Basis Function (OKL-RBF) neural classifier. The audio path is designed using a combination of input prosodic features (pitch, log-energy, zero crossing rates and Teager energy operator) and spectral features (Mel-scale frequency cepstral coefficients). The extracted audio features are passed into an audio feature level fusion module that uses a set of rules to determine the most likely emotion contained in the audio signal. An audio visual fusion module fuses outputs from both paths. The performances of the proposed audio path, visual path, and the final system are evaluated on standard databases. Experiment results and comparisons reveal the good performance of the proposed system.","1949-3045;19493045","","10.1109/TAFFC.2016.2588488","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506248","Emotion recognition;audio-visual processing;machine learning;multimodal system;rule-based","","","","","","","","","20160707","","","IEEE","IEEE Early Access Articles"
"Questioning the reliability of Monte Carlo simulation for machine learning test validation","G. Leger; M. J. Barragan","Instituto de Microlectr&#243;nica de Sevilla, IMSE-CNM (CSIC-Universidad de Sevilla), Av. Am&#233;rico Vespucio s/n, 41092, Spain","2016 21th IEEE European Test Symposium (ETS)","20160725","2016","","","1","6","Machine learning indirect test - also known as Alternate Test- has shown its potential to reduce test cost while maintaining an interpretation of the results compatible with the standard specification-based test. Since its introduction, many papers have been proposed to refine the idea and address its shortcomings. In particular, the issue of early validation of the test at the design stage has been considered and some methodologies have been proposed to assess test quality. These methodologies rely essentially on Monte Carlo simulations. In this paper, we propose a set of thought experiments to show that small inaccuracies and variations in the Monte Carlo models included in current technology process design kits may have a significant impact in the validation of machine learning indirect test, in particular in the estimation of test quality metrics. Despite of this, machine learning indirect test has actually succeeded in actual industrial cases. Some hints are thus given to the conditions that the test has to fulfill to guarantee good results.","","Electronic:978-1-4673-9659-2; POD:978-1-4673-9660-8","10.1109/ETS.2016.7519307","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519307","","Circuit faults;Computational modeling;Integrated circuit modeling;Measurement;Monte Carlo methods;Principal component analysis;Reliability","Monte Carlo methods;circuit testing;learning (artificial intelligence);system-on-chip","AMS-RF circuit testing;Monte Carlo simulation;SoCs;alternate test;machine learning indirect test;machine learning test validation;standard specification-based test;technology process design kits;test cost reduction;test quality assessment;test quality metrics estimation","","","","","","","23-27 May 2016","","IEEE","IEEE Conference Publications"
"Machine Learning Approach for Cloud NoSQL Databases Performance Modeling","V. A. E. Farias; F. R. C. Sousa; J. G. R. Maia; J. P. P. Gomes; J. C. Machado","Comput. Sci. Dept., Fed. Univ. of Ceara, Fortaleza, Brazil","2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)","20160721","2016","","","617","620","Cloud computing is a successful, emerging paradigm that supports on-demand services with pay-as-you-go model. With the exponential growth of data, NoSQL databases have been used to manage data in the cloud. In these newly emerging settings, mechanisms to guarantee Quality of Service heavily relies on performance predictability, i.e., the ability to estimate the impact of concurrent query execution on the performance of individual queries in a continuously evolving workload. This paper presents a performance modeling approach for NoSQL databases in terms of performance metrics which is capable of capturing the non-linear effects caused by concurrency and distribution aspects. Experimental results confirm that our performance modeling can accurately predict mean response time measurements under a wide range of workload configurations.","","Electronic:978-1-5090-2453-7; POD:978-1-5090-2454-4","10.1109/CCGrid.2016.83","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515747","Cloud Computing;NoSQL;Performance Modeling","Cloud computing;Computational modeling;Concurrent computing;Database systems;Measurement;Predictive models","cloud computing;concurrency control;database management systems;learning (artificial intelligence)","cloud NoSQL databases performance modeling;cloud computing;concurrency aspects;concurrent query execution;data exponential growth;data management;distribution aspects;machine learning;on-demand services;pay-as-you-go model;performance metrics;performance predictability;quality of service","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Integrating performance of web search engine with Machine Learning approach","P. A. Jadhav; P. N. Chatur; K. P. Wagh","Department of Computer Science and engineering, Maharashtra, India - 444604","2016 2nd International Conference on Advances in Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB)","20160811","2016","","","519","524","Todays diversified user query over web search engine for information retrieval; semantic information for relevant web document on web has been plethora of web search research. A lot many web search engine developed based on semantic meaning like ontolook, swoogle etc., for finding relevant information, which helps to find user based semantic meaning related documents. The concept of semantic similarity or semantic information widely focused in many important fields such as Machine Learning, Artificial Intelligence, Cognitive Science, Natural Language Processing and Web Information Retrieval etc., Traditional web search engines and semantic web search engines relates user keyword with terms, entities, texts, documents which have semantic correlation with user query. Both search engines does not use images within web pages to find more relevant information. Now in this paper we have formulated a web document integrated ranking method based on text semantic information and image based object matching information. This integrated approach presented in this paper does not depend upon semantic information of user query but also consider image appearing within web pages to find more relevant information. Approach proposed in this paper includes finding semantic information using ontology based meaning of user query and feature based object matching over image to find image matching score. In proposed approach combined use of ontology based semantic information and image based object matching score will improve web document ranking.","","Electronic:978-1-4673-9745-2; POD:978-1-4673-9746-9","10.1109/AEEICB.2016.7538344","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7538344","Machine Learning;Object Matching;Ranking;Semantic Similarity;Semantic Web;Web Document","Feature extraction;Ontologies;Search engines;Semantics;Web pages;Web search","document handling;learning (artificial intelligence);ontologies (artificial intelligence);query processing;search engines","Web document integrated ranking method;Web document ranking;Web information retrieval;Web search engine;Web search research;artificial intelligence;cognitive science;diversified user query;feature based object matching;image matching score;information retrieval;machine learning approach;natural language processing;ontolook;swoogle;user based semantic meaning related documents","","","","","","","27-28 Feb. 2016","","IEEE","IEEE Conference Publications"
"Using Machine Learning approaches to detect opponent formation","E. Asali; M. Valipour; N. Zare; A. Afshar; M. Katebzadeh; G. Dastghaibyfard","Department of Computer Science and Engineering & IT - Shiraz University, Iran","2016 Artificial Intelligence and Robotics (IRANOPEN)","20160804","2016","","","140","144","Making a correct decision is a difficult task in a Soccer Simulation 2D environment due to the fact that there is a lack of information for each agent. Therefore, coach agent can take role as a mediator for agents to analyze data and inform players about crucial events by sending command messages. This paper proposes a new method to detect the formation of opponents which is not still possible for agents to extract. In the experimental results of this paper, we show that team formation is successfully learned by various well-known classification algorithms.","","Electronic:978-1-5090-2169-7; POD:978-1-5090-2170-3","10.1109/RIOS.2016.7529504","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529504","Classification;Formation Detection;Multi-Agent Systems;RoboCup;Soccer Simulation","Kernel","decision making;learning systems;mobile robots;multi-agent systems;multi-robot systems","RoboCup;Soccer Simulation 2D environment;classification algorithms;coach agent;decision making;machine learning;opponent formation detection;team formation","","","","","","","9-9 April 2016","","IEEE","IEEE Conference Publications"
"Prediction of Hidden Dangers in Mine Production Using Timeliness Managing Extreme Learning Machine for Cloud Services","X. Luo; X. Yang; X. Chang; C. Zhang","Sch. of Comput. & Commun. Eng., Univ. of Sci. & Technol. Beijing, Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","20160721","2015","","","1030","1036","Recently, there has been an ever-increasing interest in the study of data-driven analytics to predict hidden dangers in the cloud service-based coal mine production, with the purpose of the prevention of possible accidents. In this paper, to achieve the above prediction, a machine learning algorithm based on the single-hidden layer feed forward network (SLFN) using timeliness managing extreme learning machine (TMELM) is utilized. Compared with those traditional learning algorithms, extreme learning machine (ELM) has its unique feature of a higher generalization capability at a much faster learning speed. In addition, the timeliness managing ELM has been proposed by incorporating timeliness management scheme into ELM approach. Under the timeliness managing ELM scheme used to predict the hidden dangers, the newly incremental data could be prior to the historical data while maximizing the contribution of the newly increasing training data, since it may be more feasible that the incremental data can contribute reasonable weights to represent the current production situation in accordance with the practical analysis for accidents in coal mine production. The experimental results on the coal mines of Beijing show that by using timeliness managing ELM, the prediction accuracy of hidden danger can be improved with better stability compared with other similar machine learning methods.","","Electronic:978-1-4673-7211-4; POD:978-1-4673-7212-1","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518371","cloud service;extreme learning machine;incremental learning;prediction;timeliness managing extreme learning machine","Accidents;Cloud computing;Coal mining;Neurons;Production;Training;Training data","accident prevention;cloud computing;coal;feedforward neural nets;learning (artificial intelligence);mining;production engineering computing","SLFN;TMELM;accident prevention;cloud service;coal mine production;hidden danger prediction;single-hidden layer feedforward network;timeliness managing extreme learning machine","","","","","","","10-14 Aug. 2015","","IEEE","IEEE Conference Publications"
"Grammatical Inference and Machine Learning Approaches to Post-Hoc LangSec","S. S. Curley; R. E. Harang","U.S. Army Reserach Lab., Adelphi, MD, USA","2016 IEEE Security and Privacy Workshops (SPW)","20160804","2016","","","171","178","Formal Language Theory for Security (LangSec) applies the tools of theoretical computer science to the problem of protocol design and analysis. In practice, most results have focused on protocol design, showing that by restricting the complexity of protocols it is possible to design parsers with desirable and formally verifiable properties, such as correctness and equivalence. When we consider existing protocols, however, many of these were not subjected to formal analysis during their design, and many are not implemented in a manner consistent with their formal documentation. Determining a grammar for such protocols is the first step in analyzing them, which places this problem in the domain of grammatical inference, for which a deep theoretical literature exists. In particular, although it has been shown that the higher level categories of the Chomsky hierarchy cannot be generically learned, it is also known that certain subcategories of that hierarchy can be effectively learned. In this paper, we summarize some theoretical results for inferring well-known Chomsky grammars, with special attention to context-free grammars (CFGs) and their generated languages (CFLs). We then demonstrate that, despite negative learnability results in the theoretical regime, we can use long short-term memory (LSTM) networks, a type of recurrent neural network (RNN) architecture, to learn a grammar for URIs that appear in Apache HTTP access logs for a particular server with high accuracy. We discuss these results in the context of grammatical inference, and suggest avenues for further research into learnability of a subgroup of the context-free grammars.","","Electronic:978-1-5090-3690-5; POD:978-1-5090-3691-2","10.1109/SPW.2016.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7527766","LangSec","Conferences;Context;Gold;Grammar;Privacy;Protocols;Security","context-free grammars;inference mechanisms;learning (artificial intelligence);neural net architecture;recurrent neural nets","Apache HTTP access logs;CFG;CFL;Chomsky grammars;Chomsky hierarchy;Formal Language Theory for Security;LSTM networks;Post-Hoc LangSec;RNN architecture;URI;computer science;context-free grammars;formal analysis;formal documentation;generated languages;grammatical inference;long short-term memory networks;machine learning;negative learnability;protocol design;recurrent neural network architecture","","","","","","","22-26 May 2016","","IEEE","IEEE Conference Publications"
"Perform-ML: Performance optimized machine learning by platform and content aware customization","A. Mirhoseini; B. Darvish Rouhani; E. M. Songhori; F. Koushanfar","Rice University","2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)","20160818","2016","","","1","6","We propose Perform-ML, the first Machine Learning (ML) framework for analysis of massive and dense data which customizes the algorithm to the underlying platform for the purpose of achieving optimized resource efficiency. Perform-ML creates a performance model quantifying the computational cost of iterative analysis algorithms on a pertinent platform in terms of FLOPs, communication, and memory, which characterize runtime, storage, and energy. The core of Perform-ML is a novel parametric data projection algorithm, called Elastic Dictionary (ExD), that enables versatile and sparse representations of the data which can help in minimizing performance cost. We show that Perform-ML can achieve the optimal performance objective, according to our cost model, by platform aware tuning of the ExD parameters. An accompanying API ensures automated applicability of Perform-ML to various algorithms, datasets, and platforms. Proof-of-concept evaluations of massive and dense data on different platforms demonstrate more than an order of magnitude improvements in performance compared to the state of the art, within guaranteed user-defined error bounds.","","Electronic:978-1-4673-8729-3; POD:978-1-4673-8730-9","10.1145/2897937.2898060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544264","","","application program interfaces;learning (artificial intelligence);optimisation;storage management","API;ExD parameters;FLOP;Perform-ML;content aware customization;dense data;elastic dictionary;iterative analysis algorithms;parametric data projection algorithm;performance optimized machine learning;platform aware tuning;user-defined error bounds","","","","","","","5-9 June 2016","","IEEE","IEEE Conference Publications"
"Corporate risk estimation by combining machine learning technique and risk measure","Y. Hsu; M. F. Hsu; S. J. Lin","Administrator, Department of Surgery, Kaohsiung Municipal Ta-Tung Hospital, Taiwan, R.O.C.","2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)","20160825","2016","","","1","4","A precise measure of corporates' operating performance play critical role in it achieving sustainable development during turbulent financial markets, because operating performance is a suitable reflection of corporate management, which has been widely recognized as the main cause of financial troubles. However, most proportion of previous works take return on assets or return on investment as a proxy for operating performance assessment, but both assessing measures merely consider one input and one output features, making them not appropriate for describing the whole facets of a corporate's operating situation. These limitations suggest that there is a need for additional analytical model for effective and useful prediction of corporate operating performance ranking. Thus, this study proposes a reliable and sophisticated prediction architecture that incorporates risk metrics, dimensionality reduction technique, data envelopment analysis, and artificial intelligence technique for corporate operating performance forecasting. The experimental results show that the proposed architecture can reduce unnecessary information, satisfactorily forecast the corporate operating performance ranking, and yield directions for properly allocating limited financial resource on reliable objects. The introduced architecture is a promising alternative for predicting corporate operating performance ranking, it can assist in both internal and external decision makers.","","Electronic:978-1-5090-0806-3; POD:978-1-5090-0807-0","10.1109/ICIS.2016.7550763","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550763","artificial intelligence;decision making;performance measure;risk management","Data envelopment analysis;Forecasting;Neural networks;Predictive models;Reliability;Sensitivity;Support vector machines","business data processing;data envelopment analysis;learning (artificial intelligence);risk management","artificial intelligence;corporate management;corporate operating performance assessment;corporate operating performance forecasting;corporate operating performance ranking;corporate risk estimation;data envelopment analysis;dimensionality reduction;financial markets;financial resource allocation;machine learning technique;prediction architecture;return on assets;return on investment;risk measure;sustainable development","","","","","","","26-29 June 2016","","IEEE","IEEE Conference Publications"
"Nonlinearity Mitigation Using a Machine Learning Detector Based on <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula>-Nearest Neighbors","D. Wang; M. Zhang; M. Fu; Z. Cai; Z. Li; H. Han; Y. Cui; B. Luo","Beijing University of Posts and Telecommunications, Beijing, China","IEEE Photonics Technology Letters","20160817","2016","28","19","2102","2105","A powerful machine learning detector based on the k-nearest neighbors (KNN) algorithm is proposed to overcome system impairments. The zero-dispersion link (ZDL), dispersion managed link (DML), and dispersion unmanaged link (DUL) are considered. Meanwhile, an improved algorithm, the distance-weight KNN, is introduced, which outperforms the conventional maximum likelihood-post compensation approach. The numerical results show that KNN is feasible for overcoming various impairments, especially for non-Gaussian symmetric noise, such as laser phase noise and nonlinear phase noise in the ZDL or DML.","1041-1135;10411135","","10.1109/LPT.2016.2555857","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456223","$k$ -nearest neighbors (KNN);Machine learning;digital signal process (DSP);nonlinear phase noise (NLPN)","Digital signal processing;Machine learning algorithms;Optical fiber amplifiers;Optical fiber dispersion;Phase noise;Testing;Training data","laser noise;learning (artificial intelligence);nonlinear optics;optical fibre communication;optical fibre dispersion;optical links;phase noise;telecommunication computing","dispersion managed link;dispersion unmanaged link;distance-weight KNN;k-nearest neighbors algorithm;laser phase noise;machine learning detector;nonGaussian symmetric noise;nonlinear phase noise;nonlinearity mitigation;zero-dispersion link","","","","","","20160421","Oct.1, 1 2016","","IEEE","IEEE Journals & Magazines"
"Domain-specific Relation Extraction: using distant supervision Machine Learning","A. Aljamel; T. Osman; G. Acampora","School of Science and Technology, Nottingham Trent University, NG11 8NS, U.K.","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","20160801","2015","01","","92","103","The increasing accessibility and availability of online data provides a valuable knowledge source for information analysis and decision-making processes. In this paper we argue that extracting information from this data is better guided by domain knowledge of the targeted use-case and investigate the integration of a knowledge-driven approach with Machine Learning techniques in order to improve the quality of the Relation Extraction process. Targeting the financial domain, we use Semantic Web Technologies to build the domain Knowledgebase, which is in turn exploited to collect distant supervision training data from semantic linked datasets such as DBPedia and Freebase. We conducted a serious of experiments that utilise the number of Machine Learning algorithms to report on the favourable implementations/configuration for successful Information Extraction for our targeted domain.","","Electronic:978-9-8975-8164-9; POD:978-1-5090-1967-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526907","Information Extraction;Knowledge-Base;Natural Language Processing;Relation Extraction;Semantic Web;Supervised Machine Learning","Classification algorithms;Data mining;Feature extraction;Information retrieval;Natural language processing;Semantic Web;Training","data handling;decision making;financial data processing;knowledge based systems;learning (artificial intelligence);semantic Web","DBPedia;Freebase;decision-making processes;distant supervision machine learning;domain knowledge base;domain-specific relation extraction;financial domain;information analysis;information extraction;knowledge-driven approach;online data;semantic Web Technologies;semantic linked datasets","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"Resource Frequency Prediction in Healthcare: Machine Learning Approach","D. Vieira; J. Hollmén","X-akseli Oy, Espoo, Finland","2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)","20160818","2016","","","88","93","Determining the minimal amount of resources needed to ensure minimal number of bottlenecks in the patient flow not only promotes patient satisfaction but also provides financial benefits to hospitals. The increase of data gathering by healthcare facilities in the last years have brought new opportunities to apply machine learning techniques to tackle this problem. This work makes use of data gathered from the Oulu University Hospital in Finland between 2011 and 2014 to study the effectiveness of machine learning techniques to predict resources usage. This work investigates the problem of resource frequency prediction and compares the performance of Nearest Neighbours and Random Forest. The application of data clustering as a preprocessing step is also explored as a way to improve the prediction accuracy of resources whose behavior change over time. The results indicate that 1) highly frequented resources can be predicted with higher accuracy than the lowly frequented resources, 2) the Random Forest have similar performance to the Nearest Neighbours although Random Forest performs better, 3) clustering improves the performance of the Nearest Neighbours but not of Random Forest, and 4) if averages are used to determine the resource frequency then cluster averages yields higher accuracy than all data averages.","","Electronic:978-1-4673-9036-1; POD:978-1-4673-9037-8","10.1109/CBMS.2016.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545963","healthcare modelling;hierarchical clustering;regression;supervised learning;time series prediction","Forecasting;Hospitals;Measurement;Optimization;Time series analysis;Vegetation","health care;learning (artificial intelligence);medical administrative data processing;pattern clustering;resource allocation","Finland;Oulu University Hospital;data clustering;healthcare facilities;hospital financial benefits;machine learning;nearest neighbours;patient flow bottleneck;patient satisfaction;random forest;resource frequency prediction;resources usage prediction","","","","","","","20-24 June 2016","","IEEE","IEEE Conference Publications"
"A Framework for QoS-aware Traffic Classification Using Semi-supervised Machine Learning in SDNs","P. Wang; S. C. Lin; M. Luo","Dept. of Electr. Eng. & Comput. Sci., Wichita State Univ., Wichita, KS, USA","2016 IEEE International Conference on Services Computing (SCC)","20160901","2016","","","760","765","In this paper, a QoS-aware traffic classification framework for software defined networks is proposed. Instead of identifying specific applications in most of the previous work of traffic classification, our approach classifies the network traffic into different classes according to the QoS requirements, which provide the crucial information to enable the fine-grained and QoS-aware traffic engineering. The proposed framework is fully located in the network controller so that the real-time, adaptive, and accurate traffic classification can be realized by exploiting the superior computation capacity, the global visibility, and the inherent programmability of the network controller. More specifically, the proposed framework jointly exploits deep packet inspection (DPI) and semi-supervised machine learning so that accurate traffic classification can be realized, while requiring minimal communications between the network controller and the SDN switches. Based on the real Internet data set, the simulation results show the proposed classification framework can provide good performance in terms of classification accuracy and communication costs.","","Electronic:978-1-5090-2628-9; POD:978-1-5090-2629-6","10.1109/SCC.2016.133","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557524","QoS;SDN;Semi-supervised;Traffic classification","Classification algorithms;Control systems;Engines;Quality of service;Real-time systems;Support vector machines;Training","learning (artificial intelligence);quality of service;software defined networking","DPI;QoS aware traffic classification framework;QoS aware traffic engineering;SDN;deep packet inspection;network controller;network traffic;semisupervised machine learning;software defined networks","","","","","","","June 27 2016-July 2 2016","","IEEE","IEEE Conference Publications"
"Exploiting Sample Diversity in Distributed Machine Learning Systems","Z. Liu; X. Shi; H. Jin","Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China","2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)","20160721","2016","","","187","192","With the increase of machine learning scalability, there is a growing need for distributed systems which can execute machine learning algorithms on large clusters. Currently, most distributed machine learning systems are developed based on iterative optimization algorithm and parameter server framework. However, most systems compute on all samples in every iteration and this method consumes too much computing resources since the amount of samples is always too large. In this paper, we study on the sample diversity and find that most samples ontribute little to model updating during most iterations. Based on these findings, we propose a new iterative optimization algorithm to reduce the computation load by reusing the iterative computing results. The experiment demonstrates that, compared to the current methods, the algorithm proposed in this paper can reduce about 23% of the whole computation load without increasing of communications.","","Electronic:978-1-5090-2453-7; POD:978-1-5090-2454-4","10.1109/CCGrid.2016.75","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515688","","","distributed processing;iterative methods;learning (artificial intelligence);optimisation","computation load reduction;distributed machine learning systems;iterative optimization algorithm;machine learning scalability;model updating;parameter server framework;sample diversity","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Battling against DDoS in SIP: Is Machine Learning-based detection an effective weapon?","Z. Tsiatsikas; A. Fakis; D. Papamartzivanos; D. Geneiatakis; G. Kambourakis; C. Kolias","Dept. of Inform. and Comm. Systems Engineering, University of the Aegean, Karlovassi, Greece","2015 12th International Joint Conference on e-Business and Telecommunications (ICETE)","20160721","2015","04","","301","308","This paper focuses on network anomaly-detection and especially the effectiveness of Machine Learning (ML) techniques in detecting Denial of Service (DoS) in SIP-based VoIP ecosystems. It is true that until now several works in the literature have been devoted to this topic, but only a small fraction of them have done so in an elaborate way. Even more, none of them takes into account high and low-rate Distributed DoS (DDoS) when assessing the efficacy of such techniques in SIP intrusion detection. To provide a more complete estimation of this potential, we conduct extensive experimentations involving 5 different classifiers and a plethora of realistically simulated attack scenarios representing a variety of (D)DoS incidents. Moreover, for DDoS ones, we compare our results with those produced by two other anomaly-based detection methods, namely Entropy and Hellinger Distance. Our results show that ML-powered detection scores a promising false alarm rate in the general case, and seems to outperform similar methods when it comes to DDoS.","","Electronic:978-9-8975-8140-3; POD:978-1-4673-8532-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518050","Anomaly-detection;DDoS;Intrusion Detection Systems;Machine Learning;Session Initiation Protocol","Computer architecture;Computer crime;Ecosystems;Intrusion detection;Protocols;Servers","","","","","","","","","20-22 July 2015","","IEEE","IEEE Conference Publications"
"Evaluation of three machine learning algorithms as classifiers of premature ventricular contractions on ECG beats","M. M. Casas; R. L. Avitia; M. A. Reyna; A. Cárdenas","Bioengineering Graduate Program, Autonomous University of Baja California., Mexicali BC, Mex.","2016 Global Medical Engineering Physics Exchanges/Pan American Health Care Exchanges (GMEPE/PAHCE)","20160707","2016","","","1","6","According to the World Health Organization, cardiovascular diseases (CVD) are the main cause of death worldwide. An estimated 17.5 million people died from CVD in 2012, representing 31% of all global deaths. The electrocardiogram (ECG) is a central tool for the pre-diagnosis of heart diseases. Many advances on ECG arrhythmia classification have been developed in the last century; however, there is still research to identify malignant waveforms on ECG beats. The premature ventricular complexes (PVC) are known to be associated with malignant ventricular arrhythmias and in sudden cardiac death (SCD) cases. Detecting this kind of arrhythmia has been crucial in clinical applications. In this work, we extracted 80 different features from 108,653 ECG classified beats of the MIT-BIH database in order to classify the Normal, PVC and other kind of ECG beats. We evaluated three classifier algorithms based on Machine Learning with different parameters and we got a total of 14 models. We used the F1 score and we compared predictive values as a measured of classifier evaluations. Results show that we could have a F1 scores near to the unit, showing the models are higher than 93% of performance.","","DVD:978-1-5090-2484-1; Electronic:978-1-5090-2486-5; POD:978-1-5090-2487-2","10.1109/GMEPE-PAHCE.2016.7504615","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504615","Arrhythmia classifier;Electrocardiogram (ECG);Logistic Regression (LG);Neural Network (NN);Premature Ventricular Contraction (PVC);Support Vector Machines (SVM)","Artificial neural networks;Classification algorithms;Databases;Electrocardiography;Feature extraction;Heart rate variability;Neurons","cardiovascular system;diseases;electrocardiography;feature extraction;learning (artificial intelligence);medical signal processing;signal classification","ECG arrhythmia classification;ECG beats;F1 score;MIT-BIH database;cardiovascular diseases;electrocardiogram;feature extraction;heart disease prediagnosis;machine learning algorithms;malignant ventricular arrhythmias;malignant waveforms;premature ventricular contraction classifiers;sudden cardiac death","","","","","","","4-9 April 2016","","IEEE","IEEE Conference Publications"
"Applying Computational Aesthetics to a Video Game Application Using Machine Learning","A. N. Erdem; U. Halici","Middle East Technical University","IEEE Computer Graphics and Applications","20160805","2016","36","4","23","33","The authors have developed a novel approach to evaluating the aesthetic quality of the camera direction in video game scenes rendered in real time while the game is being played. Their goal was to improve the visual aesthetic quality of computer-generated images using a computational aesthetics approach via a regression machine learning model. Considering the challenges and limitations involved, the proposed approach yielded promising prediction performance. The results show that near-real-time aesthetic analysis and visual improvement is possible using a virtual camera director.","0272-1716;02721716","","10.1109/MCG.2016.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478442","aesthetic analysis;computational aesthetics;computer graphics;machine learning;quality assessment;regression machine learning;video games","Cameras;Correlation coefficient;Feature extraction;Games;Image color analysis;Predictive models;Training","cameras;computer games;image processing;learning (artificial intelligence);regression analysis","camera direction;computational aesthetics;computer-generated images;regression machine learning;video game;visual aesthetic quality","","","","","","20160525","July-Aug. 2016","","IEEE","IEEE Journals & Magazines"
"Secure remote user authentication for multi-server environment using machine learning technique","A. Choubey; K. Chatterjee","Dept-Computer Science and Engineering, National Institute of Technology, Patna 800005, Bihar, India","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)","20160804","2016","","","1","5","The earlier remote password authentication schemes required a service providing server to authenticate a legitimate user for remote login. However, the traditional schemes are not useful in multi-server architecture because of multiple user ids and passwords. In this paper, we present a remote password authentication scheme for multi-server architecture that can be robust and improved network security. This password authentication system is a trained classification system based on SVM. In this scheme, the users only remember own identity and password and own choices for the various server login which he filled during registration where the user can choose his password at will. Furthermore, this system does not require having any overhead of password or verification table and is very dynamic in nature and can also withstand the replay attack and masquerading.","","DVD:978-1-5090-1276-3; Electronic:978-1-5090-1277-0; POD:978-1-5090-1278-7","10.1109/ICCPCT.2016.7530131","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530131","SVM;authentication;keystroke;multi-server;password","Authentication;Computer architecture;Computers;Servers;Support vector machines;Training","file servers;message authentication;pattern classification;software architecture;support vector machines","SVM;classification system;machine learning;multiserver architecture;network security;remote user password authentication;support vector machine","","","","","","","18-19 March 2016","","IEEE","IEEE Conference Publications"
"Wearable body and wireless inertial sensors for machine learning classification of gait for people with Friedreich's ataxia","R. LeMoyne; F. Heerinckx; T. Aranca; R. De Jager; T. Zesiewicz; H. J. Saal","Department of Biological Sciences and Center for Bioengineering Innovation, Northern Arizona University, Flagstaff, AZ 86011-5640 USA","2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN)","20160721","2016","","","147","151","The integration of wearable and wireless inertial body sensors with machine learning offers the capacity to diagnose neurological disorders involving gait. Clinical rating scales may be unable to offer precise measurement of gait dysfunction in Friedreich's ataxia compared to wearable body and inertial sensors. Using wireless inertial sensors mounted about the ankle joint of a person with Friedreich's ataxia, the accelerometer and gyroscope signal recordings can be wirelessly transmitted to a cloud computing resource for postprocessing, such as the development of a machine learning feature set. Machine learning can be applied to distinguish between the gait features of a person with Friedreich's ataxia and a person with healthy gait characteristics as a comparator through the application of a multilayer perceptron neural network. A considerable degree of classification accuracy for distinguishing between the gait feature set for the person with Friedreich's ataxia and healthy subject was achieved. The synthesis of wearable and wireless inertial body sensors with machine learning may offer the potential to enhance clinical diagnostic acuity and conceivably prognostic foresight.","","Electronic:978-1-5090-3087-3; POD:978-1-5090-3088-0","10.1109/BSN.2016.7516249","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516249","","Gyroscopes;Multilayer perceptrons;Sensor phenomena and characterization;Wearable sensors;Wireless communication;Wireless sensor networks","body sensor networks;cloud computing;learning (artificial intelligence)","clinical diagnostic acuity;cloud computing resource;gait dysfunction;gait features;gyroscope signal recordings;machine learning classification;multilayer perceptron neural network;neurological disorders;prognostic foresight;wearable body sensor;wireless inertial body sensors","","","","","","","14-17 June 2016","","IEEE","IEEE Conference Publications"
"Machine learning and unlearning to autonomously switch between the functions of a myoelectric arm","A. L. Edwards; J. S. Hebert; P. M. Pilarski","Faculty of Rehabilitation Medicine, University of Alberta, Edmonton, Canada","2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)","20160728","2016","","","514","521","Powered prosthetic arms with numerous controllable degrees of freedom (DOFs) can be challenging to operate. A common control method for powered prosthetic arms, and other human-machine interfaces, involves switching through a static list of DOFs. However, switching between controllable functions often entails significant time and cognitive effort on the part of the user when performing tasks. One way to decrease the number of switching interactions required of a user is to shift greater autonomy to the prosthetic device, thereby sharing the burden of control between the human and the machine. Our previous work with adaptive switching showed that it is possible to reduce the number of user-initiated switches in a given task by continually optimizing and changing the order in which DOFs are presented to the user during switching. In this paper, we combine adaptive switching with a new machine learning control method, termed autonomous switching, to further decrease the number of manual switching interactions required of a user. Autonomous switching uses predictions, learned in real time through the use of general value functions, to switch automatically between DOFs for the user. We collected results from a subject performing a simple manipulation task with a myoelectric robot arm. As a first contribution of this paper, we describe our autonomous switching approach and demonstrate that it is able to both learn and subsequently unlearn to switch autonomously during ongoing use, a key requirement for maintaining human-centered shared control. As a second contribution, we show that autonomous switching decreases the time spent switching and number of user-initiated switches compared to conventional control. As a final contribution, we show that the addition of feedback to the user can significantly improve the performance of autonomous switching. This work promises to help improve other domains involving human-machine interaction - in particular, assistive or rehabilita- ive devices that require switching between different modes of operation such as exoskeletons and powered orthotics.","","Electronic:978-1-5090-3287-7; POD:978-1-5090-3288-4","10.1109/BIOROB.2016.7523678","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523678","","Electromyography;Manipulators;Prosthetics;Switches;Wrist","control engineering computing;human-robot interaction;learning (artificial intelligence);medical robotics;prosthetics","adaptive switching;autonomous switching;human-centered shared control;human-machine interface;machine learning;manipulation task;myoelectric robot arm;numerous controllable degree-of-freedom;powered prosthetic arms;prosthetic device","","","","","","","26-29 June 2016","","IEEE","IEEE Conference Publications"
"Machine learning based job status prediction in scientific clusters","W. Yoo; A. Sim; K. Wu","Lawrence Berkeley National Laboratory, Berkeley, CA, USA","2016 SAI Computing Conference (SAI)","20160901","2016","","","44","53","Large high-performance computing systems are built with increasing number of components with more CPU cores, more memory, and more storage space. At the same time, scientific applications have been growing in complexity. Together, they are leading to more frequent unsuccessful job statuses on HPC systems. From measured job statuses, 23.4% of CPU time was spent to the unsuccessful jobs. We set out to study whether these unsuccessful job statuses could be anticipated from known job characteristics. To explore this possibility, we have developed a job status prediction method for the execution of jobs on scientific clusters. The Random Forests algorithm was applied to extract and characterize the patterns of unsuccessful job statuses. Experimental results show that our method can predict the unsuccessful job statuses from the monitored ongoing job executions in 99.8% the cases with 83.6% recall and 94.8% precision. This prediction accuracy can be sufficiently high that it can be used to mitigation procedures of predicted failures.","","Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2","10.1109/SAI.2016.7555961","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555961","Job Log Analysis;Job Status Prediction;Reliability","Complexity theory;Decision trees;Hardware;Prediction algorithms;Prediction methods;Reliability;Software","learning (artificial intelligence);parallel processing;pattern classification;pattern clustering","HPC system;high-performance computing system;job status prediction;machine learning;random forest algorithm;scientific cluster","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"A hybrid approach to reducing the false positive rate in unsupervised machine learning intrusion detection","A. D. Landress","College of Engineering and Computing, Nova Southeastern University, Davie, Florida, USA","SoutheastCon 2016","20160709","2016","","","1","6","Network intrusion detection aims to uncover unauthorized access to computer networks. Anomaly intrusion detection uses unsupervised learning to detect attacks based on profiles of normal user behaviors. If the system is being used differently, it triggers an alarm. Current methods of intrusion detection are unable to produce alerts without a high number of false positives. The proposed research will utilize a set of artificial intelligence machine learning methods to decrease the number of false positives in anomalous intrusion detection data. This method combines data clustering using the simple K-means algorithm, feature selection that employs the J48 Decision Tree algorithm, and self organizing maps to effectively reduce false positives using the KDD CUP 99 data set.","","Electronic:978-1-5090-2246-5; POD:978-1-5090-2247-2","10.1109/SECON.2016.7506773","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506773","J48 Decision Tree;clustering;feature selection;network intrusion detection;self organizing maps;simple K-means","Algorithm design and analysis;Clustering algorithms;Decision trees;Feature extraction;Intrusion detection;Self-organizing feature maps;Training","authorisation;computer network security;decision trees;feature selection;pattern clustering;self-organising feature maps;unsupervised learning","J48 decision tree algorithm;K-means algorithm;KDD CUP 99 data set;anomalous intrusion detection data;artificial intelligence machine learning method;attack detection;computer network unauthorized access;data clustering;false positive rate reduction;feature selection;network intrusion detection;normal user behavior profile;self organizing map;unsupervised machine learning intrusion detection","","","","","","","March 30 2016-April 3 2016","","IEEE","IEEE Conference Publications"
"A machine learning algorithm for interference removal from a signal","S. Varma","Department of Computer Science and Engineering, Indian Institute of Technology (BHU) Varanasi, India","2015 National Conference on Recent Advances in Electronics & Computer Engineering (RAECE)","20160714","2015","","","211","215","Wireless Communication systems involve multiple transmission of signals from one place to another through a transmission medium. These signals may interfere with each other and cause loss of information, thus reducing the efficiency of these systems. This interference needs to be taken care of for developing reliable wireless communication systems. In this paper, we have dealt with this problem by using a two-staged machine learning algorithm over a real time signal for removing multiple interference with a minimal information loss. It is based on the working of the notch filter which is used to remove a single narrow band interference from a signal. The algorithm was simulated over a Binary Phase Shift Keying (BPSK) modulated signal with a number of continuous wave (CW) interference. Based on the results obtained, signal power loss and notch characteristics were analysed for optimal performance of the algorithm.","","Electronic:978-1-5090-2146-8; POD:978-1-5090-2177-2","10.1109/RAECE.2015.7509892","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509892","","Interference","interference suppression;learning (artificial intelligence);notch filters;phase shift keying;radiocommunication;radiofrequency interference;telecommunication computing;telecommunication network reliability","BPSK modulated signal;CW interference;binary phase shift keying modulated signal;continuous wave interference;minimal information loss;multiple interference removal;notch filter;signal power loss;signals multiple transmission;single narrow band interference removal;two-staged machine learning algorithm;wireless communication system reliability","","","","","","","13-15 Feb. 2015","","IEEE","IEEE Conference Publications"
"Machine learning based handover management for improved QoE in LTE","Z. Ali; N. Baldo; J. Mangues-Bafalluy; L. Giupponi","Centre Tecnologic de Telecomunicacions de Catalunya (CTTC), Castelldefels(Barcelona), Spain","NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium","20160704","2016","","","794","798","This paper presents a machine learning based handover management scheme for LTE to improve the Quality of Experience (QoE) of the user in the presence of obstacles. We show that, in this scenario, a state-of-the-art handover algorithm is unable to select the appropriate target cell for handover, since it always selects the target cell with the strongest signal without taking into account the perceived QoE of the user after the handover. In contrast, our scheme learns from past experience how the QoE of the user is affected when the handover was done to a certain eNB. Our performance evaluation shows that the proposed scheme substantially improves the number of completed downloads and the average download time compared to state-of-the-art. Furthermore, its performance is close to an optimal approach in the coverage region affected by an obstacle.","","Electronic:978-1-5090-0223-8; POD:978-1-5090-0224-5","10.1109/NOMS.2016.7502901","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502901","Handover Management;Machine Learning;QoE","Artificial neural networks;Computer architecture;Handover;Microprocessors;Training","Long Term Evolution;learning (artificial intelligence);mobility management (mobile radio);quality of experience;telecommunication computing","LTE;QoE;average download time;eNB;machine learning based handover management;quality of experience improvement;state-of-the-art handover algorithm","","","","","","","25-29 April 2016","","IEEE","IEEE Conference Publications"
"Online sequential extreme learning machines for fault detection","Y. Hu; O. Fink; T. Palmé","Institute for Data Analysis and Process Design (IDP), Zurich University of Applied Sciences, Rosenstrasse 3, CH-8401 Winterthur, Switzerland","2016 IEEE International Conference on Prognostics and Health Management (ICPHM)","20160815","2016","","","1","8","In this paper, we propose the application of a new fault detection approach with a sequential updating function under new operating conditions or natural evolving degradation processes. The proposed approach is based on Online Sequential Extreme Learning Machines (OS-ELM). OS-ELM have the advantages of a strong learning ability, very fast training, and online learning. The concept of applying OS-ELM to fault detection is demonstrated on an artificial case study. We expect that OS-ELM can contribute to improve the fault detection and also the associated initiation of maintenance activities for engineering components working in an evolving environment such as electric components, bearings, gears, alternators, shafts and pumps, in which the monitored signals are not only significantly affected by working load and surrounding environment but may also experience some modifications due to a slow aging and degradation process.","","Electronic:978-1-5090-0382-2; POD:978-1-5090-0383-9","10.1109/ICPHM.2016.7542841","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542841","Evolving Environment;Fault Detection;Online Sequential Extreme Learning Machines (OS-ELM)","Data models;Degradation;Fault detection;Machine learning algorithms;Mathematical model;Signal reconstruction;Training","condition monitoring;fault diagnosis;learning (artificial intelligence);mechanical engineering computing;signal reconstruction","OS-ELM;aging process;alternators;bearings;degradation process;electric components;fault detection approach;gears;maintenance activities;natural evolving degradation process;online sequential extreme learning machines;operating conditions;pumps;sequential updating function;shafts;signal monitoring;surrounding environment;working load","","","","","","","20-22 June 2016","","IEEE","IEEE Conference Publications"
"Consideration of manufacturing data to apply machine learning methods for predictive manufacturing","Ji-Hyeong Han; Su-Young Chi","Electronics and Telecommunications Research Institute (ETRI), Daejeon, 34129, Korea","2016 Eighth International Conference on Ubiquitous and Future Networks (ICUFN)","20160811","2016","","","109","113","According to the recent development of internet of things and big data, the serious tries of implementing smart factory have been increased. To realize the smart factory, firstly predictive manufacturing system should be implemented. As a first step of predictive manufacturing, this paper focuses on solving the simple but time consuming and high cost task in the predictive manner. The target problem of this paper is predicting CNC tool wear compensation offset using machine learning methods based on the data. To apply machine learning methods, we should understand the characteristics of the data and find the most suitable method according to the data characteristics. Thus, this paper discusses the characteristics of manufacturing data and compares various cases of applying machine learning methods.","","Electronic:978-1-4673-9991-3; POD:978-1-4673-9992-0; USB:978-1-4673-9990-6","10.1109/ICUFN.2016.7536995","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7536995","CNC tool wear compensation offset;Manufacturing data;machine learning;predictive manufacturing;smart factory","Big data;Companies;Computer numerical control;Learning systems;Manufacturing systems;Production facilities","Big Data;Internet of Things;computerised numerical control;factory automation;manufacturing systems;wear","Big Data;CNC tool wear compensation offset prediction;Internet of Things;machine learning methods;manufacturing data;predictive manufacturing system;smart factory","","","","","","","5-8 July 2016","","IEEE","IEEE Conference Publications"
"On-line machine learning accelerator on digital RRAM-crossbar","L. Ni; H. Huang; H. Yu","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore 639798","2016 IEEE International Symposium on Circuits and Systems (ISCAS)","20160811","2016","","","113","116","On-line machine learning has become the need for future data analytics. This work will show an ℓ<sub>2</sub> norm based hardware solver for on-line machine learning that can significantly reduce training time when compared to the traditional gradient-based solution using backward propagation. We will show that the intensive matrix-vector multiplication in ℓ<sub>2</sub> norm solution can be mapped onto a distributed in-memory accelerator using the recent resistive switching random access memory (RRAM) device. A digitized matrix-vector multiplication accelerator will be developed based on the distributed RRAM-crossbar. Such a distributed RRAM-crossbar architecture can utilize the reformulated ℓ<sub>2</sub> norm solver with a scalable and energy-efficient solution for real-time training and testing in image recognition. Experiment results have shown that significant speedup can be achieved for matrix-vector multiplication in the ℓ<sub>2</sub> norm solver such hat the overall training and testing time can be reduced respectively. In addition, large energy saving can be also achieved when compared to the traditional CMOS-based out-of-memory computing architecture.","","Electronic:978-1-4799-5341-7; POD:978-1-4799-5342-4; USB:978-1-4799-5340-0","10.1109/ISCAS.2016.7527183","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7527183","","Computer architecture;Data analysis;Encoding;Feature extraction;Resistance;Testing;Training","learning (artificial intelligence);matrix multiplication;neural chips;resistive RAM;vectors","ℓ<sub>2</sub> norm based hardware solver;data analytics;digital RRAM-crossbar;digitized matrix-vector multiplication accelerator;distributed RRAM-crossbar architecture;distributed in-memory accelerator;energy saving;online machine learning accelerator;resistive switching random access memory device;training time reduction","","","","","","","22-25 May 2016","","IEEE","IEEE Conference Publications"
"Application of machine learning to classify surface marker of screw","C. Y. Chang; H. C. Shie","National Yunlin University of Science & Technology, Taiwan","2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW)","20160728","2016","","","1","2","In recent years, many character recognition methods had proposed for recognizing handwritten or computer characters. However, there is few paper discusses marker recognition on screw. General speaking, screw images suffered from the influences of splotch and reflection. How to classify the marker of the screw is still a challenging problem. Therefore, we applied the machine learning to recognize the markers on surface of the screw. Experimental results shown that the proposed method achieves reasonable classification accuracy.","","Electronic:978-1-5090-2073-7; POD:978-1-5090-2074-4","10.1109/ICCE-TW.2016.7520948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7520948","","Character recognition;Computer vision;Electronic mail;Fasteners;Inspection;Reflection;Support vector machines","fasteners;image classification;learning (artificial intelligence)","computer character recognition;handwritten character recognition;machine learning;screw images;screw marker recognition;screw surface marker classification","","","","","","","27-29 May 2016","","IEEE","IEEE Conference Publications"
"Comparison of Selected Machine Learning Algorithms for Sub-Pixel Imperviousness Change Assessment","W. Drzewiecki","Dept. of Geoinf., Photogrammetry & Remote Sensing of Environ., AGH Univ., Cracow, Poland","2016 Baltic Geodetic Congress (BGC Geomatics)","20160823","2016","","","67","72","The paper presents the comparison of nine machine learning algorithms for sub-pixel impervious surface area change assessment. Predictive models were tuned and trained using the caret package in R environment. Their performance was analyzed based on both cross-validation results and results obtained for validation dataset. A paired t-test was used to determine if the differences between model accuracies are statistically significant. In case of imperviousness mapping for individual time points the regression trees based models outperformed other ones both for cross-validation on calibration dataset and for validation dataset. The Cubist algorithm seems to be the best performed one. The best assessment method for ISA change cannot be unambiguously pointed out. Random Forest gave the lowest RMS errors, random kNN was the best one according to MAE measure and support vector machines with radial basis kernel gave the highest mean value of the R2.","","Electronic:978-1-5090-2421-6; POD:978-1-5090-2422-3","10.1109/BGC.Geomatics.2016.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548007","computational and artificial intelligence;environmental monitoring;geoscience and remote sensing;land surface;prediction methods","Earth;Land surface;Machine learning algorithms;Predictive models;Remote sensing;Satellites;Vegetation mapping","calibration;geophysics computing;learning (artificial intelligence);mean square error methods;radial basis function networks;regression analysis;support vector machines;terrain mapping","Cubist algorithm;ISA change;MAE measure;R environment;RMS errors;calibration dataset;caret package;cross-validation results;imperviousness mapping;individual time points;paired t-test;predictive models;radial basis kernel;random forest;random kNN;regression trees;selected machine learning algorithms;subpixel impervious surface area change assessment;support vector machines;validation dataset","","","","","","","2-4 June 2016","","IEEE","IEEE Conference Publications"
"Can malware analysts be assisted in their work using techniques from machine learning?","I. Novkovic; S. Groš","Faculty of Electrical and Computing Engineering, University of Zagreb, Unska bb, 10000 Zagreb, Croatia","2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","20160728","2016","","","1408","1413","When a malware analyst analyzes some code to determine if it's malicious or not and what it is doing, he has to overcome protections built in by malware writer that tries to make it as hard as possible to get to the main functionality of the malware code. In practice that means that when malware analyst, while stepping through the code in some debugger like OllyDbg, hits call instruction or something similar has to decide if he is going to follow call or skip over it. Obviously, if the call is unimportant the best would be to skip it, but at that point analyst doesn't know if it is important or not. The problem is that creative malware writer can use anti debug techniques in such a way that they are hard to recognize and analyze, they can even come up with a new ways to make malware analysis harder. So, the question is is it possible to write a plugin for a debugger that, based on the current call instruction and data behind it, can suggest malware analyst what to do? In this paper we present a system that we are designing and developing that would allow experiments to be performed to find out the answer to the aforementioned question.","","CD-ROM:978-953-233-088-5; Electronic:978-953-233-086-1; POD:978-1-5090-2543-5","10.1109/MIPRO.2016.7522360","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7522360","anti-debug technique;debugging;machine learning;malware;malware analyst","Computer architecture;Data mining;Feature extraction;Malware;Prototypes;Registers","invasive software;learning (artificial intelligence);program debugging","debug techniques;debugger;machine learning;malware analysts;malware code functionality;malware writer","","","","","","","May 30 2016-June 3 2016","","IEEE","IEEE Conference Publications"
"Coarse-grained reconfigurable hardware accelerator of machine learning classifiers","V. Vranjković; R. Struharik","Faculty of Technical Sciences, University of Novi Sad, Trg Dositeja Obradovi&#263;a 6, Novi Sad, 21000, Serbia","2016 International Conference on Systems, Signals and Image Processing (IWSSIP)","20160704","2016","","","1","5","In this paper a universal, coarse-grained reconfigurable architecture for hardware acceleration of decision trees (DTs), artificial neural networks (ANNs), and support vector machines (SVMs) is proposed. Using proposed architecture, two versions of DTs (Functional DT and Axis-Parallel DT), two versions of SVMs (with polynomial and radial kernels) and two versions of ANNs (Multi Layer Perceptron and Radial Basis), have been implemented in FPGA. Experimental results, based on 18 benchmark datasets from standard UCI Machine Learning Repository Database, indicate that FPGA implementation provides significant improvement (1-3 orders of magnitude) in the average instance classification time, in comparison with software implementations, based on WEKA and R project.","","Electronic:978-1-4673-9555-7; POD:978-1-4673-9556-4; USB:978-1-4673-9554-0","10.1109/IWSSIP.2016.7502737","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502737","Artificial Neural Networks;Data Mining;Decision Trees;FPGA;Hardware Acceleration;Machine Learning;R project;Reconfigurable Hardware;Support Vector Machines;WEKA","Artificial neural networks;Computer architecture;Data processing;Hardware;Random access memory;Software;Support vector machines","decision trees;learning (artificial intelligence);neural nets;reconfigurable architectures;support vector machines","ANN;FPGA;R project;SVM;UCI machine learning repository database;WEKA;artificial neural networks;axis-parallel DT;coarse-grained reconfigurable architecture;coarse-grained reconfigurable hardware accelerator;decision trees;functional DT;hardware acceleration;machine learning classifiers;multilayer perceptron;polynomial;radial basis;radial kernels;support vector machines","","","","22","","","23-25 May 2016","","IEEE","IEEE Conference Publications"
"A Novel Approach for Logo Recognition System Using Machine Learning Algorithm SVM","S. Soma; B. V. Dhandra","Dept. of Comput. Sci., P.D.A. Coll. of Eng., Gulbarga, India","2016 IEEE 6th International Conference on Advanced Computing (IACC)","20160818","2016","","","440","445","In different applications like Complex document image processing, Advertisement and Intelligent transportation logo recognition is an important issue. Logo Recognition is an essential sub process although there are many approaches to study logos in these fields. In this paper a robust method for recognition of a logo is proposed, which involves K-nearest neighbors distance classifier and Support Vector Machine classifier to evaluate the similarity between images under test and trained images. For test images eight set of logo image with a rotation angle of 0°, 45°, 90°, 135°, 180°, 225°, 270°, and 315° are considered. A Dual Tree Complex Wavelet Transform features were used for determining features. Final result is obtained by measuring the similarity obtained from the feature vectors of the trained image and image under test. Total of 31 classes of logo images of different organizations are considered for experimental results. An accuracy of 87.49% is obtained using KNN classifier and 92.33% from SVM classifier.","","Electronic:978-1-4673-8286-1; POD:978-1-4673-8287-8","10.1109/IACC.2016.88","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544877","Document Pre-Processing;Dual-Tree Complex Wavelet Transform (DTCWT);K-Nearest Neighbor (KNN) algorithm;Logo Recognition;Support Vector Meachine(SVM)","Feature extraction;Image recognition;Support vector machines;Testing;Training;Wavelet transforms","image classification;learning (artificial intelligence);object recognition;support vector machines;trees (mathematics);wavelet transforms","K-nearest neighbor distance classifier;KNN classifier;SVM;dual tree complex wavelet transform features;feature vectors;logo recognition system;machine learning;support vector machine classifier","","","","","","","27-28 Feb. 2016","","IEEE","IEEE Conference Publications"
"A machine learning based prognostic prediction of cervical myelopathy using diffusion tensor imaging","R. Jin; K. D. Luk; J. Cheung; Y. Hu","Department of Orthopaedics & Traumatology, University of Hong Kong, Hong Kong, China","2016 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)","20160728","2016","","","1","4","Diffusion Tensor imaging (DTI), composing of various metrics, including fractional anisotropy (FA), axial diffusivity (AD), mean diffusivity (MD) and radial diffusivity (RD) has been considered as a useful clinical tool to reveal microstructure of spinal cord. Previous studies have intensively applied DTI in investigating the pathology of cervical spondylotic myelopathy (CSM), as well the symptomatic level diagnosis of CSM. However, it still remains unclear whether the DTI metric could be used in the prognosis of CSM, which is of great significance for selection of the best treatment strategy. Thus, the present study attempted to establish a prognosis model of CSM based on DTI metrics using machine learning methods. Particularly, three conventional machine learning algorithms, Naive Bayesian, Least Squares Support Vector Machine (LS-SVM), and Multi-label K-nearest Neighbour (ML-KNN) were tested on DTI data from 35 CSM patients accepting surgery treatments with post-operative outcomes followed. The results showed that prognosis of CSM with DTI metrics using LS-SVM algorithms could achieve higher prediction performance, with accuracy of 88.62%, and the learning curve of LS-SVM showed that the performance would be significantly improved if the sample size is greater than 202, indicating the potential application of the prognosis prediction of CSM from DTI metrics using machine learning algorithms.","","Electronic:978-1-4673-9759-9; POD:978-1-4673-9760-5","10.1109/CIVEMSA.2016.7524318","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524318","Cervical Spondylotic Myelopathy (CSM);Diffusion Tensor Imaging (DTI);Machine Learning;Prognosis","Diffusion tensor imaging;Feature extraction;Measurement;Prognostics and health management;Spinal cord;Surgery","biomedical MRI;learning (artificial intelligence);medical disorders;neurophysiology;support vector machines","DTI metric;Least Squares Support Vector Machine;Multi-label K-nearest Neighbour;Naive Bayesian;axial diffusivity;cervical myelopathy;cervical spondylotic myelopathy;diffusion tensor imaging;fractional anisotropy;machine learning;mean diffusivity;microstructure;prognostic prediction;radial diffusivity;spinal cord","","","","","","","27-28 June 2016","","IEEE","IEEE Conference Publications"
"A novel screen content fast transcoding framework based on statistical study and machine learning","F. Duanmu; Z. Ma; W. Wang; M. Xu; Y. Wang","New York University, Tandon School of Engineering, Brooklyn, NY 11201, USA","2016 IEEE International Conference on Image Processing (ICIP)","20160819","2016","","","4205","4209","In this paper, a novel screen content transcoding framework is presented to efficiently bridge the state-of-art High Efficiency Video Coding (HEVC) standard and its incoming screen content coding (SCC) extension currently pending finalization. The proposed scheme is implemented as an Intra-coding “pre-processing” module on top of official SCC test model software (SCM). Both Coding Unit (CU) statistical features (such as CU color quantity, CU pixel variance, CU edge directionality distribution, etc.) and decoded video side information (such as CU partitions, modes, residual, etc.) are jointly analyzed. Accordingly, fast CU mode decisions and CU partitions bypass / termination heuristics are designed. Compared with SCM-4.0 official release, the proposed fast transcoding scheme can achieve an average of 48% re-encoding complexity reduction over JCT-VC screen content testing sequences with less than 2.14% marginal BD-Rate increase under SCC common testing conditions for All-Intra (AI) configuration.","","Electronic:978-1-4673-9961-6; POD:978-1-4673-9962-3","10.1109/ICIP.2016.7533152","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533152","Fast Mode Decision (FMD);High Efficiency Video Coding (HEVC);Machine Learning (ML);Screen Content Coding (SCC);Video Transcoding (VTC)","Decision support systems;Radio frequency","decision making;feature extraction;image colour analysis;learning (artificial intelligence);statistical analysis;transcoding;video coding","AI configuration;All-Intra configuration;CU color quantity;CU edge directionality distribution;CU mode decisions;CU partition bypass heuristics;CU partition termination heuristics;CU pixel variance;CU statistical features;HEVC standard;JCT-VC screen content testing sequences;SCC common testing conditions;SCC extension;SCM;coding unit statistical features;decoded video side information;high efficiency video coding standard;intracoding preprocessing module;machine learning;marginal BD-Rate;official SCC test model software;reencoding complexity reduction;screen content coding extension;screen content transcoding","","","","18","","","25-28 Sept. 2016","","IEEE","IEEE Conference Publications"
"Drug-target interaction prediction with hubness-aware machine learning","K. Buza","Brain Imaging Center, Research Center for Natural Sciences, Hungarian Academy of Sciences, Budapest, Hungary","2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI)","20160709","2016","","","437","440","Prediction of interactions between drugs and pharmacological targets is an important task for which various machine learning techniques have been applied recently. Although hubness-aware machine learning techniques are among the most promising recently developed approaches, they have not been used for the prediction of drug-target interactions before. In this paper, we extend the Bipartite Local Model (BLM), one of the most prominent approaches for drug-target interaction prediction. In particular, we propose to use a hubness-aware regression technique, ECkNN, as local model. Furthermore, we propose to represent drugs and targets in the similarity space. In order to assist reproducibility of our work as well as comparison to published results, we perform experiments on widely used publicly available real-world drug-target interaction datasets. The results show that our approach is competitive and, in many cases, superior to state-of-the-art drug-target prediction techniques.","","Electronic:978-1-5090-2380-6; POD:978-1-5090-2381-3; USB:978-1-5090-2379-0","10.1109/SACI.2016.7507416","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507416","","Biochemistry;Computational intelligence;Drugs;Error correction;Informatics;Predictive models;Training","drugs;learning (artificial intelligence);medical computing;pharmaceutical industry;regression analysis","BLM;ECkNN;bipartite local model;drug-target interaction prediction;hubness-aware machine learning technique;hubness-aware regression technique;pharmacological targets;similarity space","","","","","","","12-14 May 2016","","IEEE","IEEE Conference Publications"
"Machine-Learning Indoor Localization with Access Point Selection and Signal Strength Reconstruction","Y. K. Cheng; H. J. Chou; R. Y. Chang","Res. Center for Inf. Technol. Innovation, Taipei, Taiwan","2016 IEEE 83rd Vehicular Technology Conference (VTC Spring)","20160707","2016","","","1","5","Indoor localization technique is a key enabling technology for the future Internet of things (IoT) paradigm. Improving the precision of indoor localization will expand the horizon of indoor IoT applications. In this paper, we propose an enhanced machine-learning indoor localization scheme which incorporates access point (AP) selection and the proposed signal strength reconstruction to enhance robustness in noisy environments. The proposed signal strength reconstruction scheme estimates/reconstructs the received signal strength indicator (RSSI) values of the nonselected APs from those of the selected APs to increase the size of the feature space for enhanced noise robustness. The proposed concept can be applied to various machine-learning frameworks. Simulation results demonstrate improved precision yielded by the proposed method in conjunction with support vector regression (SVR), ensemble SVR, and artificial neural network (ANN) models, as compared to these machine- learning techniques alone.","","Electronic:978-1-5090-1698-3; POD:978-1-5090-1699-0","10.1109/VTCSpring.2016.7504333","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504333","","Internet of things;Predictive models;Received signal strength indicator;Training;Training data","RSSI;indoor radio;learning (artificial intelligence);neural nets;regression analysis;support vector machines;telecommunication computing","ANN model;AP selection;Internet of things paradigm;IoT paradigm;RSSI;SVR;access point selection;artificial neural network model;enhanced machine-learning indoor localization scheme;enhanced noise robustness;noisy environment;received signal strength indicator;robustness enhancement;signal strength reconstruction scheme;support vector regression","","","","","","","15-18 May 2016","","IEEE","IEEE Conference Publications"
"A syllabus on data mining and machine learning with applications to cybersecurity","A. Epishkina; S. Zapechnikov","National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Russia","2016 Third International Conference on Digital Information Processing, Data Mining, and Wireless Communications (DIPDMWC)","20160804","2016","","","194","199","Big data analytics are very fruitful for solving problems in cybersecurity. We have analyzed modern trends in intelligent security systems research and practice and worked out a syllabus for a new university course in the area of data mining and machine learning with applications to cybersecurity. The course is for undergraduate and graduate students studying the cybersecurity. The main objective of the course is to provide students with fundamental concepts in data mining (in particular, mining frequent patterns, associations and correlations, classification, cluster analysis, outlier detection), machine learning (including neural networks, support vector machines etc.) and related issues, e.g. the basics of multidimensional statistics. Contrary to the traditional data mining and machine learning courses we illustrate course topics by cases from the area of cybersecurity including botnet detection, intrusion detection, deep packet inspection, fraud monitoring, malware detection, phishing detection, active authentication. We note that our course has great potential for development.","","CD-ROM:978-1-4673-9378-2; Electronic:978-1-4673-9379-9; POD:978-1-4673-9380-5","10.1109/DIPDMWC.2016.7529388","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529388","associative rules;classification;clustering;data mining;educational and methodical maintenance;information security;information technology;outlier detection;specialists training","Big data;Computer security;Correlation;Data analysis;Data mining;Information security","Big Data;computer science education;data mining;educational courses;further education;learning (artificial intelligence);security of data","active authentication;big data analytics;botnet detection;course syllabus;cybersecurity;data mining;deep packet inspection;fraud monitoring;graduate students;intelligent security systems;intrusion detection;machine learning;malware detection;multidimensional statistics;phishing detection;undergraduate students;university course","","","","","","","6-8 July 2016","","IEEE","IEEE Conference Publications"
"Machine learning based adaptive flow classification for optically interconnected data centers","N. Viljoen; H. Rastegarfar; Mingwei Yang; J. Wissinger; M. Glick","Netronome Systems, 2903 Bunker Lane, Santa Clara, CA 95054, USA","2016 18th International Conference on Transparent Optical Networks (ICTON)","20160825","2016","","","1","4","We optimize flow placement for a hybrid network implementing an adaptive neural network classifier. We predict elephant flows with high accuracy on anonymized university network traffic. We also demonstrate the capability to perform highly complex actions at 40 Gbps using less than 5% of co-processor capacity. This shows that it is possible to implement intelligent actions such as a neural network in a data center using fully programmable NICs without handicapping the server CPU.","","Electronic:978-1-5090-1467-5; POD:978-1-5090-1468-2; USB:978-1-5090-1466-8","10.1109/ICTON.2016.7550294","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550294","circuit-switched;networks;optical interconnects","Classification algorithms;Computer networks;Computer vision;Image motion analysis;Mice;Neural networks;Optical fiber networks","computer centres;intelligent networks;learning (artificial intelligence);network interfaces;neural nets;optical interconnections;telecommunication traffic","adaptive neural network classifier;anonymized university network traffic;co-processor capacity;elephant flow prediction;flow placement optimization;fully-programmable NIC;hybrid network;intelligent actions;machine learning-based adaptive flow classification;optically interconnected data centers","","","","","","","10-14 July 2016","","IEEE","IEEE Conference Publications"
"A service oriented QoS architecture targeting the smart grid world & machine learning aspects","C. Chrysoulas; M. Fasli","School of Computer Science & Electronic Engineering, University of Essex, Essex, United Kingdom","2016 International Multidisciplinary Conference on Computer and Energy Science (SpliTech)","20160901","2016","","","1","6","Dynamic selection of services and by extension of service providers are vital in today's liberalized market of energy. On the other hand it is equally important for Service Providers to spot the one QoS Module that offers the best QoS level in a given cost. Type of service, response time, throughput, availability and cost, consist a basic set of attributes that should be taken into consideration when building a concrete Grid network. In the proposed QoS architecture Prosumers request services based on the aforementioned set of attributes. The Prosumer requests the service through the QoS Module. It is then the QoS Module that seeks the Service Provider that best fits the needs of the client.","","Electronic:978-953-290-063-7; POD:978-1-5090-1661-7","10.1109/SpliTech.2016.7555923","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555923","Machine Learning;Mining;QoS;Service Oriented Architecture;Smart Grid","Computer architecture;Contracts;Databases;Quality of service;Service-oriented architecture;Smart grids;Time factors","grid computing;learning (artificial intelligence);quality of service;service-oriented architecture","Grid network;QoS module;dynamic services selection;liberalized energy market;machine learning aspects;service oriented QoS architecture;service provider extension;services request;smart grid world","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology","B. Sinha; M. Garg; S. Chandra","Department of Electronics and Information Technology, New Delhi, India","2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)","20160718","2016","","","415","420","Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.","","Electronic:978-1-5090-0082-1; POD:978-1-5090-0083-8","10.1109/ICCTICT.2016.7514617","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514617","Deep neural network;IndoWordNet electronic database;Normalized web distance;Ontology;Relation classification;Semantic Web;Support vector machine","Databases;Information retrieval;Neural networks;Ontologies;Semantic Web;Semantics;Support vector machines","information retrieval;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence);semantic Web","Indian languages;domain specific ontology;information classification;machine learning evaluation techniques;multilingual IndoWordNet database matching;natural language based electronic database;natural language processing techniques;normalized Web distance similarity method;ontology generation;relational information extraction;semantic Web technology","","","","","","","11-13 March 2016","","IEEE","IEEE Conference Publications"
"Machine learning model for temporal pattern recognition","C. Inibhunu; C. McGregor","University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, Ontario, Canada L1H 7K4","2016 IEEE EMBS International Student Conference (ISC)","20160709","2016","","","1","4","Temporal abstraction and data mining are two research fields that have tried to synthesis time oriented data and bring out an understanding on the hidden relationships that may exist between time oriented events. In clinical settings, having the ability to know the hidden relationships on patient data as they unfold could help save a life by aiding in detection of conditions that are not obvious to clinicians and healthcare workers. Understanding the hidden patterns is a huge challenge due to the exponential search space unique to time-series data. In this paper, we propose a temporal pattern recognition model based on dimension reduction and similarity measures thereby maintaining the temporal nature of the raw data.","","Electronic:978-1-5090-0935-0; POD:978-1-5090-0936-7","10.1109/EMBSISC.2016.7508614","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508614","","Data mining;Data models;Feature extraction;Market research;Monitoring;Time series analysis","data mining;learning (artificial intelligence);medical computing;pattern recognition;time series","data mining;dimension reduction;exponential search space;hidden patterns;machine learning model;similarity measures;synthesis time;temporal abstraction;temporal pattern recognition;time oriented events;time series data","","","","","","","29-31 May 2016","","IEEE","IEEE Conference Publications"
"Solar radiation forecast with machine learning","X. Shao; S. Lu; H. F. Hamann","IBM T. J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598, U.S.A.","2016 23rd International Workshop on Active-Matrix Flatpanel Displays and Devices (AM-FPD)","20160818","2016","","","19","22","Renewable energy forecasting becomes increasingly important as the contribution of solar/wind power production to the electrical power grid constantly increases. Significant improvement in forecasting accuracy has been demonstrated by developing more sophisticated solar irradiance forecasting models using statistics and/or numerical weather predictions. In this presentation, we report the development of a machine-learning based multi-model blending approach for statistically combing multiple meteorological models to improve the accuracy of solar power forecasting. The system leverages upon multiple existing physical models for prediction including numerous atmospheric and cloud prediction models based on satellite imagery as well as numerical weather prediction (NWP) products.","","Electronic:978-4-9908753-0-5; POD:978-1-5090-1258-9; USB:978-4-9908753-1-2","10.1109/AM-FPD.2016.7543604","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7543604","","Atmospheric modeling;Biological system modeling;Computational modeling;Numerical models;Predictive models;Weather forecasting","atmospheric radiation;atmospheric techniques;learning (artificial intelligence);solar power;weather forecasting","machine learning;meteorological models;multimodel blending approach;numerical weather prediction;solar power forecasting;solar radiation forecast","","","","","","","6-8 July 2016","","IEEE","IEEE Conference Publications"
"Design and application of machine learning algorithm computer in connect6 of computer games system","G. Wu; J. Tao","Educational Administration office, Jianghan University, Wuhan 430056","2016 Chinese Control and Decision Conference (CCDC)","20160808","2016","","","4279","4282","The paper introduces briefly some of the key structures of the computer games system in connect6. And it mainly provides a further optimization for the current existing computer games system in connect6. The machine learning algorithm can increase the adaptive width adjustment in the MTD (f) algorithm and the self-learning method by TD_BP algorithm in the evaluation function which could strengthen the “thinking” ability on the computer games system in connect6. The advanced and modified algorithm is proved to be practical and applicative by experimentations and tests of computer games system in connect6 provided in this paper.","","CD-ROM:978-1-4673-9713-1; Electronic:978-1-4673-9714-8; POD:978-1-4673-9715-5","10.1109/CCDC.2016.7531734","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7531734","Computer Games;Connect6;Evaluation Function;Machine Learning Algorithm","Algorithm design and analysis;Computers;Games;Heuristic algorithms;Learning systems;Machine learning algorithms;Search engines","computer games;learning (artificial intelligence)","MTD (f) algorithm;TD_BP algorithm;adaptive width adjustment;computer games system;connect6;evaluation function;machine learning algorithm computer;self-learning method","","","","","","","28-30 May 2016","","IEEE","IEEE Conference Publications"
"Machine learning-based mobile threat monitoring and detection","W. G. Hatcher; D. Maloney; W. Yu","Department of Computer and Information Sciences, Towson University, Maryland, USA 21252","2016 IEEE 14th International Conference on Software Engineering Research, Management and Applications (SERA)","20160721","2016","","","67","73","Mobile device security must keep up with the increasing demand of mobile users. Smartphones are every day becoming connected to more devices and services, interacting with the growing Internet of things. Every new service, and connection, creates a new pathway for intrusion and data theft. Each intrusion can yield further opportunities for breaches of corporate and enterprise infrastructure, and significant cost. In our study, we propose a mobile security platform that combines our developed security web server, analysis module, and Android OS application, with the Google Cloud Messaging service for queued and targeted device messaging. In the cloud, the developed LAMP (Linux, Apache, MySQL, PHP) server sends, receives, and stores data from a connected device via the corresponding Android OS application. The data consists of system information for device identification, and application data to be distributed to the analysis module for malicious content to be extracted and identified. The analysis module, utilizing the Weka software, performs both static and dynamic analyses to detect Android malware, simultaneously providing rapid and intuitive security with predictive capabilities. The server additionally provides device status visualization and manual security operations.","","Electronic:978-1-5090-0809-4; POD:978-1-5090-0810-0","10.1109/SERA.2016.7516130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516130","Android Security;Machine Learning;Malware;Static and Dynamic Detection","Androids;Cloud computing;Humanoid robots;Malware;Mobile communication;Security;Servers","Android (operating system);Internet of Things;authorisation;cloud computing;invasive software;learning (artificial intelligence);mobile computing;program diagnostics;smart phones","Android OS;Android malware;Apache;Google cloud messaging service;Internet of things;LAMP;Linux;MySQL;PHP serve;Weka software;data theft;dynamic analysis;intrusion detection;machine learning;mobile device security;mobile threat detection;mobile threat monitoring;security Web server;smartphones;static analysis","","","","","","","8-10 June 2016","","IEEE","IEEE Conference Publications"
"VENCE: A new machine learning method enhanced by ontological knowledge to extract summaries","J. A. Motta; L. Capus; N. Tourigny","Computer Sciences Department, Laval University, Qu&#x00E9;bec (QC), Canada","2016 SAI Computing Conference (SAI)","20160901","2016","","","61","70","Obtaining extractive summaries by using functions induced from a training set continue to be a great challenge in the domain of the automatic text summary. This paper presents the VENCE method based on this approach and improves the quality of the abduced functions, using semantic relations of the words (attributes) of the training set that are fetched from a ontology to be inserted in this set. The choice of this training set is reinforced with the optimization of the space of attributes by means of statistical techniques, as well as with the introduction of the Jaccard index, calculated from considering a manual summary that is extracted from the corpus of the chosen documents. The VENCE method is explained in details as well as the different experiments conducted to propose an optimal process. Its application to a text document corpus highlighted its efficiency. The results obtained are very satisfactory for the assessment of discriminating power of the abduced classification function as well as for the quality of summaries produced.","","Electronic:978-1-4673-8460-5; POD:978-1-4673-8461-2","10.1109/SAI.2016.7555963","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7555963","Jaccard index;information content;semi-supervised machine learning","Classification algorithms;Computers;Indexes;Machine learning algorithms;Ontologies;Optimization;Training","learning (artificial intelligence);ontologies (artificial intelligence);optimisation;pattern classification;statistical analysis;text analysis","Jaccard index;VENCE method;automatic text summary;classification function;machine learning method;ontological knowledge;optimization;semantic relations;statistical analysis;summary extraction;text document corpus","","","","","","","13-15 July 2016","","IEEE","IEEE Conference Publications"
"Identifying Rare Diseases from Behavioural Data: A Machine Learning Approach","H. MacLeod; S. Yang; K. Oakes; K. Connelly; S. Natarajan","Sch. of Inf. & Comput., Indiana Univ., Bloomington, IN, USA","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)","20160818","2016","","","130","139","Rare diseases are hard to identify and diagnose. Our goal is to use self-reported behavioural data to distinguish people with rare diseases from people with more common chronic illnesses. To this effect, we adapt a state of the art machine learning algorithm to make this classification. We find that using this method, and an appropriate set of questions, we can accurately identify people with rare diseases.","","Electronic:978-1-5090-0943-5; POD:978-1-5090-0944-2","10.1109/CHASE.2016.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545826","","Diseases;Drugs;Internet;Medical diagnostic imaging;Sociology;Statistics","diseases;learning (artificial intelligence);medical diagnostic computing","behavioural data;chronic illnesses;machine learning approach;rare disease diagnosis;rare disease identification","","","","","","","27-29 June 2016","","IEEE","IEEE Conference Publications"
"Egg size classification on Android mobile devices using image processing and machine learning","R. Waranusast; P. Intayod; D. Makhod","Department of Electrical and Computer Engineering, Faculty of Engineering, Naresuan University, Phitsanulok, Thailand","2016 Fifth ICT International Student Project Conference (ICT-ISPC)","20160725","2016","","","170","173","Chicken eggs are a common ingredient in human food, used as an ingredient in almost every food culture worldwide. Judging the size, and therefore weight, of an egg is often important in many food recipes. This paper proposes an image processing algorithm for classifying eggs by size from an image displayed on an Android device. A coin of known size is used in the image as a reference object. The coin's radius and the egg's dimensions are automatically detected and measured using image processing techniques. Egg sizes are classified based on their features computed from the measured dimensions using a support vector machine (SVM) classifier. The experimental results show the measurement errors in egg dimensions were low at 3.1% and the overall accuracy of size classification was 80.4%.","","CD-ROM:978-1-5090-1123-0; Electronic:978-1-5090-1125-4; POD:978-1-5090-1126-1","10.1109/ICT-ISPC.2016.7519263","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7519263","classification;egg size;image processing;svm","Androids;Classification algorithms;Humanoid robots;Image segmentation;Instruction sets;Size measurement;Weight measurement","Android (operating system);diameter measurement;farming;image classification;learning (artificial intelligence);mobile computing;support vector machines","Android mobile devices;SVM classifier;chicken egg size classification;coin radius measurement;egg dimension measurement;image processing;machine learning;support vector machine classifier","","","","","","","27-28 May 2016","","IEEE","IEEE Conference Publications"
"Instant answering for health care system by machine learning approach","E. Moses; P. Melwin; S. Vigneshwari","Faculty of Computing, Sathyabama University, Chennai, Tamil Nadu, India","2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)","20160804","2016","","","1","7","Health care system's instantaneous answering method using machine-learning process has hindered the cross-system operability and the inter-user reusability. The scheme consists of two naturally reinforced components of database, namely Local Repository and Web. Primarily the user can access the local-repository database with simple keywords for the health data from the local repository. In other cases, if the data is unavailable in the local repository, the searching mechanism is directly carried on to the web database. Stack of PDF oriented medical resources are implemented in web, where it searches with the lexical similarities. The user can also directly report the medical issues with the medical experts in the case, where the medical data is hard to locate within the stack of PDF.","","DVD:978-1-5090-1276-3; Electronic:978-1-5090-1277-0; POD:978-1-5090-1278-7","10.1109/ICCPCT.2016.7530344","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530344","Local Repository;Machine Learning;Question Answering;Web;e-Health","Biomedical imaging;Blogs;Computers;Databases;Encoding;Medical services;Terminology","Internet;health care;learning (artificial intelligence);question answering (information retrieval)","PDF oriented medical resources;Web database;cross-system operability;health care system;instant answering;interuser reusability;local repository database;machine learning","","","","","","","18-19 March 2016","","IEEE","IEEE Conference Publications"
"A personalized recommender system using Machine Learning based Sentiment Analysis over social data","M. Ashok; S. Rajanna; P. V. Joshi; Sowmya Kamath S","Department of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore, India","2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)","20160714","2016","","","1","6","Social Media platforms are already an indispensable part of our daily lives. With its constant growth, it has contributed to superfluous, heterogeneous data which can be overwhelming due to its volume and velocity, thus limiting the availability of relevant and required information when a particular query is to be served. Hence, a need for personalized, fine-grained user preference-oriented framework for resolving this problem and also, to enhance user experience is increasingly felt. In this paper, we propose a such a social framework, which extracts user's reviews, comments of restaurants and points of interest such as events and locations, to personalize and rank suggestions based on user preferences. Machine Learning and Sentiment Analysis based techniques are used for further optimizing search query results. This provides the user with quicker and more relevant data, thus avoiding irrelevant data and providing much needed personalization.","","Electronic:978-1-4673-7918-2; POD:978-1-4673-7919-9","10.1109/SCEECS.2016.7509354","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509354","Machine Learning;Natural Language Processing;Personalized Search Engine;Sentiment Analysis","Algorithm design and analysis;Feature extraction;Machine learning algorithms;Media;Sentiment analysis;Silicon;Support vector machines","learning (artificial intelligence);query processing;recommender systems","fine-grained user preference-oriented framework;heterogeneous data;machine learning based sentiment analysis;personalization;personalized recommender system;search query;social data;social framework;social media platforms;user experience;user preferences","","","","","","","5-6 March 2016","","IEEE","IEEE Conference Publications"
"Speeding up distributed machine learning using codes","K. Lee; M. Lam; R. Pedarsani; D. Papailiopoulos; K. Ramchandran","EECS at UC Berkeley, United States","2016 IEEE International Symposium on Information Theory (ISIT)","20160811","2016","","","1143","1147","Distributed machine learning algorithms that are widely run on modern large-scale computing platforms face several types of randomness, uncertainty and system “noise.” These include stragglers<sup>1</sup>, system failures, maintenance outages, and communication bottlenecks. In this work, we view distributed machine learning algorithms through a coding-theoretic lens, and show how codes can equip them with robustness against this system noise. Motivated by their importance and universality, we focus on two of the most basic building blocks of distributed learning algorithms: data shuffling and matrix multiplication. In data shuffling, we use codes to reduce communication bottlenecks: when a constant fraction of the data can be cached at each worker node, and n is the number of workers, coded shuffling reduces the communication cost by up to a factor Θ(n) over uncoded shuffling. For matrix multiplication, we use codes to alleviate the effects of stragglers, also known as the straggler problem. We show that if the number of workers is n, and the runtime of each subtask has an exponential tail, the optimal coded matrix multiplication is Θ(log n) times faster than the uncoded matrix multiplication or the optimal task replication scheme.","","Electronic:978-1-5090-1806-2; POD:978-1-5090-1807-9","10.1109/ISIT.2016.7541478","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7541478","","Algorithm design and analysis;Convergence;Data models;Distributed databases;Encoding;Machine learning algorithms;Robustness","computational complexity;distributed algorithms;learning (artificial intelligence);matrix multiplication","coding-theoretic lens;data shuffling;distributed machine learning algorithms;exponential tail;matrix multiplication;straggler problem;subtask runtime;system noise;uncoded matrix multiplication;uncoded shuffling","","1","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Heuristic model to improve Feature Selection based on Machine Learning in Data Mining","J. Majumdar; A. Mal; S. Gupta","Department of CSE, ASET, Amity University, Noida, India","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","20160709","2016","","","73","77","Data Mining and Machine Learning is one of the most popular research areas in computer science that is relevant in today's world of unfathomable data. To keep up with the rising size of data, there arises a need to quickly extract knowledge from data sources to aid data analysis research and improve industry and market needs. Primary Data Mining algorithms like k-means, Apriori, PageRank etc. are used today, but Machine Learning techniques can enhance the same by learning from the complex patterns. This paper focuses on the various existing approaches where Machine Learning algorithms have been used to improve data classification and pattern recognition in Data Mining especially for Feature Selection. It compares and contrasts the existing techniques and finds out the best one among them. Further, the paper proposes a heuristic approach to theoretically overcome most of the limitations in existing algorithms.","","CD-ROM:978-1-4673-8202-1; Electronic:978-1-4673-8203-8; POD:978-1-4673-8204-5","10.1109/CONFLUENCE.2016.7508050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508050","Data Mining;Fuzzy;Knowledge Discovery;Machine Learning","Algorithm design and analysis;Classification algorithms;Data mining;Decision trees;Machine learning algorithms;Support vector machines;Training","data mining;feature extraction;learning (artificial intelligence);pattern classification","data analysis;data classification;data mining;feature selection;heuristic model;machine learning;pattern recognition","","","","","","","14-15 Jan. 2016","","IEEE","IEEE Conference Publications"
"Daleel: Simplifying cloud instance selection using machine learning","F. Samreen; Y. Elkhatib; M. Rowe; G. S. Blair","School of Computing and Communications, Lancaster University, United Kingdom","NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium","20160704","2016","","","557","563","Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.","","Electronic:978-1-5090-0223-8; POD:978-1-5090-0224-5","10.1109/NOMS.2016.7502858","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502858","Cloud computing;Machine learning","Actuators;Cloud computing;Computer architecture;Decision making;Knowledge based systems;Planning;Quality of service","cloud computing;decision making;learning (artificial intelligence)","Daleel;adaptive deployment policy;available cloud service offerings;cloud environment;cloud market;customer demands;customer-specific analytics;decision making;machine learning technique;pricing model;public cloud infrastructure;simplifying cloud instance selection","","","","29","","","25-29 April 2016","","IEEE","IEEE Conference Publications"
"Gaze prediction using machine learning for dynamic stereo manipulation in games","G. A. Koulieris; G. Drettakis; D. Cunningham; K. Mania","Technical University of Crete, INRIA","2016 IEEE Virtual Reality (VR)","20160707","2016","","","113","120","Comfortable, high-quality 3D stereo viewing is becoming a requirement for interactive applications today. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this locally, around the object the user is gazing at. The main challenge is thus to develop a gaze predictor in the demanding context of real-time, heavily task-oriented applications such as games. Our key observation is that player actions are highly correlated with the present state of a game, encoded by game variables. Based on this, we train a classifier to learn these correlations using an eye-tracker which provides the ground-truth object being looked at. The classifier is used at runtime to predict object category - and thus gaze - during game play, based on the current state of game variables. We use this prediction to propose a dynamic disparity manipulation method, which provides rich and comfortable depth. We evaluate the quality of our gaze predictor numerically and experimentally, showing that it predicts gaze more accurately than previous approaches. A subjective rating study demonstrates that our localized disparity manipulation is preferred over previous methods.","","Electronic:978-1-5090-0836-0; POD:978-1-5090-0837-7","10.1109/VR.2016.7504694","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7504694","Gaze Prediction;Perception;Stereo Grading","Context;Games;Gaze tracking;Manipulator dynamics;Real-time systems;Stereo image processing;Three-dimensional displays","computer games;gaze tracking;interactive systems;learning (artificial intelligence);solid modelling;stereo image processing","dynamic disparity manipulation method;dynamic stereo manipulation;eye-tracker;game variables;gaze prediction;high-quality 3D stereo viewing;interactive application;machine learning;task-oriented application","","","","","","","19-23 March 2016","","IEEE","IEEE Conference Publications"
"Two Machine Learning Approaches for Short-Term Wind Speed Time-Series Prediction","R. Ak; O. Fink; E. Zio","Chair on Systems Science and the Energetic Challenge, European Foundation for New Energy-Electricit&#233; de France, &#201;cole Centrale Paris, Ch&#x00E2;tenay-Malabry, France","IEEE Transactions on Neural Networks and Learning Systems","20160715","2016","27","8","1734","1747","The increasing liberalization of European electricity markets, the growing proportion of intermittent renewable energy being fed into the energy grids, and also new challenges in the patterns of energy consumption (such as electric mobility) require flexible and intelligent power grids capable of providing efficient, reliable, economical, and sustainable energy production and distribution. From the supplier side, particularly, the integration of renewable energy sources (e.g., wind and solar) into the grid imposes an engineering and economic challenge because of the limited ability to control and dispatch these energy sources due to their intermittent characteristics. Time-series prediction of wind speed for wind power production is a particularly important and challenging task, wherein prediction intervals (PIs) are preferable results of the prediction, rather than point estimates, because they provide information on the confidence in the prediction. In this paper, two different machine learning approaches to assess PIs of time-series predictions are considered and compared: 1) multilayer perceptron neural networks trained with a multiobjective genetic algorithm and 2) extreme learning machines combined with the nearest neighbors approach. The proposed approaches are applied for short-term wind speed prediction from a real data set of hourly wind speed measurements for the region of Regina in Saskatchewan, Canada. Both approaches demonstrate good prediction precision and provide complementary advantages with respect to different evaluation criteria.","2162-237X;2162237X","","10.1109/TNNLS.2015.2418739","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091914","Extreme learning machines (ELMs);multilayer perceptron (MLP);multiobjective genetic algorithms (MOGAs);prediction intervals (PIs);short-term wind speed prediction;wind power production","Artificial neural networks;Neurons;Prediction algorithms;Sociology;Statistics;Training;Wind speed","genetic algorithms;learning (artificial intelligence);multilayer perceptrons;power engineering computing;power markets;smart power grids;time series;wind power plants","Canada;European electricity markets;Regina;Saskatchewan;energy consumption;energy grids;extreme learning machines;flexible intelligent power grids;machine learning approaches;multilayer perceptron neural network training;multiobjective genetic algorithm;nearest neighbors approach;prediction intervals;short-term wind speed time-series prediction;sustainable energy distribution;sustainable energy production;wind power production;wind speed measurement","","3","","51","","20150422","Aug. 2016","","IEEE","IEEE Journals & Magazines"
