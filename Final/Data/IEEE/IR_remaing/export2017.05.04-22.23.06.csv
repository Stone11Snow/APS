"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6516342,6516440,6516405,6514329,6514202,6514318,6514296,6514368,6514192,6496920,6515027,6513057,6513199,6511948,6510325,6511443,6511678,6511862,6511958,6511943,6511934,6511610,6511912,6511946,6511675,6511915,6511608,6511707,6511911,6511897,6511416,6511659,6511654,6511460,6511924,6511947,6511467,6511704,6511919,6512345,6511893,6511926,6511459,6511875,6459497,6165264,6507945,6508165,6506812,6508227,6508268,6508205,6507481,6508206,6508184,6508366,6505123,6505384,6505339,6503205,6505176,6505391,6503396,6505260,6503350,6505157,6505198,6505049,6152103,6165295,6504844,6498468,6498400,6424760,6498527,6424746,6424755,6498455,6498395,6495116,6495298,6495344,6496042,6495152,6496573,6495346,6495860,6495342,6496535,6495245,6495003,6476738,6405375,6490263,6493196,6490260,6493202,6491804,6485469,6488359",2017/05/04 22:23:06
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"IMAGESEER: NASA IMAGEs for Science, Education, Experimentation and Research","J. Le Moigne; T. G. Grubb; B. C. Milner","Software Engineering Division, NASA GSFC, Greenbelt, MD, USA","IEEE Geoscience and Remote Sensing Magazine","20130418","2013","1","1","44","58","NASA Earth and space images have traditionally been difficult to mine by non-remote sensing researchers; often only available in specialized, non-generic formats, they also do not always provide sufficient context for users unfamiliar with the NASA domain to understand their content and their challenges. This paper describes a new database and its associated website, called IMAGESEER (IMAGEs for Science, Education, Experimentation and Research), that seeks to address these issues. Through a graphical web site for browsing and downloading data, IMAGESEER provides a widely accessible database of NASA-centric, easy to read image data for teaching or validating new Image Processing algorithms. Although other NASA image databases exist, none is focused on the goal of providing validation of new and old image processing algorithms. The first IMAGESEER prototype includes a representative sampling of NASA multispectral and hyperspectral images from several Earth Science instruments, along with a few small tutorials. Image processing techniques are represented with techniques such as cloud detection, image registration, and classification. For each technique, corresponding data are selected from different geographic regions representing different image features (e.g., mountains, urban, water coastal, and agriculture areas). After geo-registration, these images are available in simple common formats such as GeoTIFF and raw formats, along with associated benchmark data.","2473-2397;24732397","","10.1109/MGRS.2013.2244694","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504844","","Database systems;Image processing;NASA;Remote sensing;Satellite communication;Space technology","Web sites;cloud computing;geophysical image processing;graphical user interfaces;image classification;image registration;information retrieval systems","Earth Science instruments;IMAGESEER;IMAGEs for Science, Education, Experimentation and Research;NASA image databases;cloud detection;geographic regions;graphical Web site;hyperspectral images;image classification;image processing algorithms;image registration;multispectral images","","0","","22","","","March 2013","","IEEE","IEEE Journals & Magazines"
"Abstract: Digitization and Search: A Non-Traditional Use of HPC","L. Diesendruck; L. Marini; R. Kooper; M. Kejriwal; K. McHenry","Nat. Center for Supercomput. Applic., Univ. of Illinois at Urbana-Champaign, Champaign, IL, USA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","20130411","2012","","","1460","1461","We describe our efforts to provide a form of automated search of handwritten content for digitized document archives. To carry out the search we use a computer vision technique called word spotting. A form of content based image retrieval, it avoids the still difficult task of directly recognizing text by allowing a user to search using a query image containing handwritten text and ranking a database of images in terms of those that contain more similar looking content. In order to make this search capability available on an archive three computationally expensive pre-processing steps are required. We augment this automated portion of the process with a passive crowd sourcing element that mines queries from the systems users in order to then improve the results of future queries. We benchmark the proposed framework on 1930s Census data, a collection of roughly 3.6 million forms and 7 billion individual units of information.","","Electronic:978-0-7695-4956-9; POD:978-1-4673-6218-4","10.1109/SC.Companion.2012.259","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496042","Big Data;Digitization;Indexing Text","","computer vision;content-based retrieval;document image processing;image retrieval;information retrieval systems;parallel processing;text detection;visual databases","Census data;HPC;automated search;computer vision technique;content based image retrieval;digitized document archives;handwritten content;handwritten text;image database ranking;passive crowd sourcing element;query image;query mining;text recognition;word spotting","","0","","2","","","10-16 Nov. 2012","","IEEE","IEEE Conference Publications"
"A comparative analysis of feature selection algorithms on classification of gene microarray dataset","J. Jeyachidra; M. Punithavalli","Department of Computer Science and Applications, Periyar Maniammai University, Thanjavur, Tamilnadu, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","1088","1093","Analysis of gene expression is important in many fields of biological research in order to retrieve the required information. As the time advances, the illness in general and cancer in particular have become more and more complex and complicated, in detecting, analyzing and curing. Cancer research is one of the major research areas in the medical field. Accurate prediction of different tumor types has great value in providing better treatment and toxicity minimization on the patients. To minimize it, the data mining algorithms are important tool and the most extensively used approach to classify gene expression data and plays an important role for cancer classification. One of the major challenges is to discover how to extract useful information from datasets. This research is based on recent advances in the machine learning based microarray gene expression data analysis with three feature selection algorithms.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508165","","Accuracy;Algorithm design and analysis;Cancer;Classification algorithms;Error analysis;Gene expression;Indexes","cancer;data mining;feature extraction;genetics;information retrieval;lab-on-a-chip;learning (artificial intelligence);medical computing;minimisation;patient treatment;pattern classification;toxicology;tumours","biological research;cancer classification;data mining algorithms;feature selection algorithms;gene microarray dataset;information extraction;machine learning;microarray gene expression data analysis;patient treatment;toxicity minimization;tumor prediction","","1","","19","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Enhanced public auditability & secure data storage in cloud computing","T. K. Chakraborty; A. Dhami; P. Bansal; T. Singh","Department of Computer Science & Engineering, Motilal Nehru National Institute of Technology Allahabad, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","101","105","Cloud computing is the most envisioned paradigm shift in the computing world. Its services are being applied in several IT scenarios. This unique platform has brought new security issues to contemplate. This paper proposes a homomorphic encryption scheme based on the Elliptic curve cryptography. It implements a provable data possession scheme to support dynamic operation on data. The application of proof of retrievability scheme provisioned the client to challenge integrity of the data stored. The notion of a third party auditor (TPA) is considered, who verifies and modifies the data on behalf of the client. Data storage at the server is done using a Merkle hash tree (MHT) accomplishing faster data access. This proffered scheme not only checks the data storage correctness but also identifies misbehaving servers. The initial results demonstrate its effectiveness as an improved security system for data storage compared to the existing ones in most prospects.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514202","Cloud computing;Data security;Elliptic curve cryptography (ECC)","Cloud computing;Elliptic curve cryptography;Elliptic curves;Memory;Servers","cloud computing;data integrity;information retrieval;public key cryptography;storage allocation","IT scenarios;MHT;Merkle hash tree;TPA;cloud computing;data access;data integrity;data storage correctness checking;data storage security improvement;dynamic data operation;elliptic curve cryptography;homomorphic encryption scheme;misbehaving server identification;provable data possession scheme;public auditability enhancement;retrievability scheme proof;third party auditor","","1","","24","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Enhancing LOD Complex Query Building with Context","R. Brandão; P. Maio; N. Silva","Knowledge Eng. & Decision Support Res. Center, Polytech. of Porto, Porto, Portugal","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","522","529","Open ontology-described repositories are becoming very common in the web and in enterprises. These repositories are well-suited to answer complex queries, but in order to fully exploit their potential, the queries should be written in a user-demand basis, and not in a traditional static approach by software developers. Hence, the users are required (i) to know the underlying ontology(ies) and/to (ii) write formal queries. Yet, the users often lack such requirements. In this paper we first describe the observations made during manual complex querying process and present a systematization of the users' support wish list for building complex queries. Based on this systematization we propose an extended set of functionalities for a user-supporting system. Finally, we demonstrate their application in a walk-through example and their implementation within a prototype.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511934","complex questions;knowledge management;ontology","","ontologies (artificial intelligence);query processing;question answering (information retrieval)","LOD complex query building;complex query answering;complex querying process;complex support wish list systematization;formal queries;linked-open data repositories;ontology-described repositories;software developers;user demand basis;user-supporting system","","1","","18","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Intensity comparison based compact descriptor for mobile visual search","S. i. Na; K. d. Lee; S. j. Lee; S. k. Je; W. g. Oh","Creative Content Research Lab. ETRI Daejeon, Rep. of Korea","The 19th Korea-Japan Joint Workshop on Frontiers of Computer Vision","20130328","2013","","","103","106","In this paper, we proposed intensity comparison based compact descriptor for mobile visual search. For practical mobile applications, the low complexity and the descriptor size are more preferable, and many algorithms such as SURF, CHoG, and PCA-SIFT have been proposed. However, these approaches focused on not the feature description but the extraction time and the size of the feature. This paper suggests feature description method based on simple intensity comparison with considering descriptor size and extraction speed. Experimental results show that the proposed method has comparable performance to SURF with similar complexity and 20 times much smaller size.","","Electronic:978-1-4673-5621-3; POD:978-1-4673-5620-6","10.1109/FCV.2013.6485469","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6485469","feature descriptor;image matching","Computer vision;Conferences;Equations;Feature extraction;Mobile communication;Pattern recognition;Visualization","computational complexity;feature extraction;information retrieval;mobile computing;transforms","CHoG algorithm;PCA-SIFT algorithm;SURF algorithm;descriptor size;feature description method;feature extraction speed;feature extraction time;feature size;intensity comparison-based compact descriptor;mobile applications;mobile visual search;scale invariant feature transform","","0","","21","","","Jan. 30 2013-Feb. 1 2013","","IEEE","IEEE Conference Publications"
"Social Recommendation Based on Multi-relational Analysis","J. Chen; G. Chen; H. Zhang; J. Huang; G. Zhao","Sch. of Software Eng., South China Univ. of Technol., Guangzhou, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","2","","471","477","Social recommendation methods, often taking only one kind of relationship in social network into consideration, still faces the data sparsity and cold-start user problems. This paper presents a novel recommendation method based on multi-relational analysis: first, combine different relation networks by applying optimal linear regression analysis, and then, based on the optimal network combination, put forward a recommendation algorithm combined with multi-relational social network. The experimental results on Epinions dataset indicate that, compared with existing algorithms, can effectively alleviate data sparsity as well as cold-start issues, and achieve better performance.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511610","multi-relation social network;regression analysis;social recommendation","","information retrieval;recommender systems;regression analysis;social networking (online)","Epinions dataset;cold-start user problem;data sparsity;multirelational analysis;multirelational social network;optimal linear regression analysis;optimal network combination;recommendation algorithm;relation network;social recommendation method","","2","","9","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Web contents recognition and seamless movement in multiscreen environment","Sangmin Park; Hyeontaek Oh; Sanghong An; Jinhong Yang; Hyojin Park; Junkyun Choi; Jung Il Gu","Department of Electrical Engineering, KAIST, Korea","2013 15th International Conference on Advanced Communications Technology (ICACT)","20130328","2013","","","1049","1053","Nowadays, more and more users are surfing web pages using smart devices, such as smart phone, tablet PC, and smart TV. Those devices have some limitations, so that users want to utilize multiple devices to get improved user experience. Multi-screen environment involves web contents sharing, moving, controlling, and managing, and implementing multi-screen web page incurs several problems, which are distinct from conventional web page implementation. Implementing multi-screen environment is not a problem of a single browser, but of multiple browsers, so it requires some kind of browser to browser communication. Actually there are many ways to achieve the multi-screen communication, but we have tried to use only web standard based techniques. Besides implementation, we pick out and describe technical issues and tactics, and also multi-screen scenarios are investigated to see how multi-screen environment can be utilized usefully.","1738-9445;17389445","Electronic:978-89-968650-1-8; POD:978-1-4673-3148-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488359","content recognition;multiscreen;seamless movement;session control","Browsers;HTML;Protocols;Servers","Internet;Web sites;information retrieval;online front-ends;screens (display)","Web content recognition;Web standard based techniques;Web surfing;browser-to-browser communication;multiscreen Web page;multiscreen communication;smart devices","","0","","11","","","27-30 Jan. 2013","","IEEE","IEEE Conference Publications"
"When and How Using Structural Information to Improve IR-Based Traceability Recovery","A. Panichella; C. McMillan; E. Moritz; D. Palmieri; R. Oliveto; D. Poshyvanyk; A. De Lucia","Univ. of Salerno, Fisciano, Italy","2013 17th European Conference on Software Maintenance and Reengineering","20130415","2013","","","199","208","Information Retrieval (IR) has been widely accepted as a method for automated traceability recovery based on the textual similarity among the software artifacts. However, a notorious difficulty for IR-based methods is that artifacts may be related even if they are not textually similar. A growing body of work addresses this challenge by combining IR-based methods with structural information from source code. Unfortunately, the accuracy of such methods is highly dependent on the IR methods. If the IR methods perform poorly, the combined approaches may perform even worse. In this paper, we propose to use the feedback provided by the software engineer when classifying candidate links to regulate the effect of using structural information. Specifically, our approach only considers structural information when the traceability links from the IR methods are verified by the software engineer and classified as correct links. An empirical evaluation conducted on three systems suggests that our approach outperforms both a pure IR-based method and a simple approach for combining textual and structural information.","1534-5351;15345351","Electronic:978-0-7695-4948-4; POD:978-1-4673-5833-0","10.1109/CSMR.2013.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498468","Empirical studies;Traceability Link Recovery","Accuracy;Context;Educational institutions;Indexes;Medical services;Software;Vectors","information retrieval;pattern classification;program diagnostics","automated traceability recovery;candidate link classification;information retrieval-based traceability recovery;software artifact;source code;structural information;textual similarity","","8","","14","","","5-8 March 2013","","IEEE","IEEE Conference Publications"
"A Survival Modeling Approach to Biomedical Search Result Diversification Using Wikipedia","X. Yin; J. X. Huang; Z. Li; X. Zhou","Sch. of Comput. Sci., Beihang Univ., Beijing, China","IEEE Transactions on Knowledge and Data Engineering","20130419","2013","25","6","1201","1212","In this paper, we propose a survival modeling approach to promoting ranking diversity for biomedical information retrieval. The proposed approach concerns with finding relevant documents that can deliver more different aspects of a query. First, two probabilistic models derived from the survival analysis theory are proposed for measuring aspect novelty. Second, a new method using Wikipedia to detect aspects covered by retrieved documents is presented. Third, an aspect filter based on a two-stage model is introduced. It ranks the detected aspects in decreasing order of the probability that an aspect is generated by the query. Finally, the relevance and the novelty of retrieved documents are combined at the aspect level for reranking. Experiments conducted on the TREC 2006 and 2007 Genomics collections demonstrate the effectiveness of the proposed approach in promoting ranking diversity for biomedical information retrieval. Moreover, we further evaluate our approach in the Web retrieval environment. The evaluation results on the ClueWeb09-T09B collection show that our approach can achieve promising performance improvements.","1041-4347;10414347","","10.1109/TKDE.2012.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152103","Survival modeling;biomedical IR;diversity;rerank;thesaurus","Bioinformatics;Electronic publishing;Encyclopedias;Genomics;Internet","Web sites;document handling;genomics;information retrieval;medical information systems","ClueWeb09-T09B collection;TREC;Web retrieval environment;Wikipedia;biomedical information retrieval;biomedical search result diversification;genomics collections;probabilistic models;ranking diversity;retrieved documents;survival analysis theory;survival modeling approach","","3","","42","","20120214","June 2013","","IEEE","IEEE Journals & Magazines"
"A Hybrid Selection Method of Audio Descriptors for Singer Identification in North Indian Classical Music","S. Deshmukh; S. G. Bhirud","IT Dept., GHRCEM, Pune, India","2012 Fifth International Conference on Emerging Trends in Engineering and Technology","20130408","2012","","","224","227","Singer identification is most important application of Music information retrieval. The process starts with identifying first the audio descriptors then using these feature vectors as input to further classification using Gaussian Mixture Model or Hidden Markov Model as classifiers to identify the singer. The process becomes chaotic if all audio descriptors are used for finding the feature vector, instead if the audio descriptors are selected with respect to the application then the process becomes comparatively simple. In this paper we propose a Hybrid method of selecting correct audio descriptors for the identification of singer of North Indian Classical Music. First only strong (primary) audio descriptors are released on the system in forward pass and the classification impact is to be recorded. Then only selecting the top few audio descriptors having largest impact on the singer identification process are selected and rest are eliminated in the backward pass. Then selecting and releasing all the less significant audio descriptors from the groups that had maximum impact on singer identification process increases the success of correctly identifying the singer. The method reduces substantially the large number of audio descriptors to few, important audio descriptors. The selected audio descriptors are then fed as input to further classifiers.","2157-0477;21570477","Electronic:978-0-7695-4884-5; POD:978-1-4799-0276-7","10.1109/ICETET.2012.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495245","Audio descriptors;GMM;HMM;North Indian Classical Music","","Gaussian processes;audio signal processing;hidden Markov models;information retrieval;music;speaker recognition","Gaussian Mixture Model;Hidden Markov Model;North Indian classical music;audio descriptors;hybrid selection method;music information retrieval;singer identification","","0","","12","","","5-7 Nov. 2012","","IEEE","IEEE Conference Publications"
"Malicious URL Detection Based on Kolmogorov Complexity Estimation","H. K. Pao; Y. L. Chou; Y. J. Lee","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","380","387","Malicious URL detection has drawn a significant research attention in recent years. It is helpful if we can simply use the URL string to make precursory judgment about how dangerous a Web site is. By doing that, we can save efforts on the Web site content analysis and bandwidth for content retrieval. We propose a detection method that is based on an estimation of the conditional Kolmogorov complexity of URL strings. To overcome the incomputability of Kolmogorov complexity, we adopt a compression method for its approximation, called conditional Kolmogorov measure. As a single significant feature for detection, we can achieve a decent performance that can not be achieved by any other single feature that we know. Moreover, the proposed Kolmogorov measure can work together with other features for a successful detection. The experiment has been conducted using a private dataset from a commercial company which can collect more than one million unclassified URLs in a typical hour. On average, the proposed measure can process such hourly data in less than a few minutes.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.258","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511912","Kolmogorov complexity;blacklist;compression;entropy;malicious URL","","Web sites;computational complexity;data compression;information retrieval;invasive software","Kolmogorov complexity incomputability;Web site content analysis;commercial company;compression method;conditional Kolmogorov complexity estimation;conditional Kolmogorov measure;content retrieval bandwidth;dangerous Web sites;malicious URL detection;private dataset;unclassified URL string","","3","","14","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Performance enhancement of NUMA multiprocessor systems with on-demand memory migration","V. K. Mishra; D. A. Mehta","IIT Indore, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130425","2013","","","40","43","The quality of the scheduling has a strong impact on the overall application performance because of process and data affinities. However, this issue is now becoming critical due to the variable memory access latencies in NUMA (Non-Uniform Memory Access) architecture, because in NUMA architecture local data access being significantly faster than remote access, then data locality emerges as a critical criterion for scheduling threads and processes, and it becomes important to be able migrate memory together with their accessing tasks. To perform memory migration, we present memory migration on-demand policy to enable automatic dynamic migration of pages with low cost when they are actually accessed by a task. We use PTE flag setup with the help of madvise system call and the corresponding Copy-on-Touch code added in the page-fault handler which allocates the specific page near the accessing task.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6506812","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6506812","NUMA;memory affinity;memory migration;multiprocessor","Heuristic algorithms;Instruction sets;Linux;Load management;Memory architecture;Memory management","fault tolerant computing;information retrieval;performance evaluation;processor scheduling","NUMA architecture local data access;NUMA multiprocessor systems;NUMA performance enhancement;PTE flag setup;automatic dynamic migration;copy-on-touch code;madvise system call;nonuniform memory access architecture;on-demand memory migration;page-fault handler;scheduling quality;variable memory access latencies","","1","","12","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Extracting Web News Using Tag Path Patterns","G. Wu; X. Wu","Sch. of Comput. Sci. & Inf. Eng., Hefei Univ. of Technol., Hefei, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","588","595","How to accurately extract the content of Web news is a popular and significant issue in Web Intelligence. Many Web news sites have similar structures and layout styles, and there are potential correlations between Web content layouts and tag path patterns. Compared with other extraction features, such as HTML tags, literal words and visual features, a tag path pattern not only addresses content segments well, but also has an advantage in the generalization. However, can we accurately extract Web news using only tag path patterns? Motivated by this problem, we propose a PPWIE extraction model. We design an extraction algorithm WEtr using self-defined tag path patterns, and then define a special tag path pattern called the distinguishing tag path pattern. In addition, to tackle the NPC-hard problem in path pattern mining, we propose a polynomial-time (ln|n|+1)-approximation algorithm MPM, in which n indicates the scale of positive samples. Our experiments show that our integration method WEtr+MPM in PPWIE can achieve better performance with more than 98% of precision, recall and the F-score on real world datasets.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511946","Distinguishing Tag Path Pattern;Pattern Mining;Web Information Extraction;Web News","","Web sites;approximation theory;computational complexity;data mining;information retrieval","F-score value;NPC-hard problem;PPWIE extraction model;WEtr extraction algorithm design;WEtr+MPM integration method;Web Intelligence;Web content layouts;Web news content extraction features;Web news sites;content segments;distinguishing tag path pattern;path pattern mining;polynomial-time approximation algorithm;precision value;recall value;self-defined tag path patterns","","2","","17","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"A Hierarchical Triangle-Level Culling Technique for Tile-Based Rendering","C. C. Hsiao; S. L. Chu","Dept. of Inf. & Comput. Eng., Chung Yuan Christian Univ., Chungli, Taiwan","2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming","20130415","2012","","","119","125","Current 3D graphics rendering relies on tens of thousand triangles to generate realistic images on screen. As the number of triangles increases, no specific order for their input sequence exists, thus, many triangles that do not contribute to a final image must still be rasterized and shaded. This study proposes a triangle-level hierarchical culling technique for tile-based rendering. With a novel hardware efficient technique focuses on the depth and coverage relationships among triangles, the invisible triangles can now be culled right after geometry stage. Intended advantages include: cull invisible triangles earlier, reduce storage pressure, reduce triangle and list data accesses (from external memory) during rendering. The results show the proposed mechanism culls 32.99% of triangles before rasterization. In addition, about 15% of storage requirements and external memory transfer are reduced as well.","2168-3034;21683034","Electronic:978-0-7695-4898-2; POD:978-1-4673-4566-8","10.1109/PAAP.2012.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424746","3D Graphics Hardware;depth test;hierarchical culling;triangle-level culling;z-test","Bandwidth;Benchmark testing;Graphics processing units;Rendering (computer graphics);Tiles;Vectors","computational geometry;information retrieval;realistic images;rendering (computer graphics)","3D graphics rendering;data access;geometry stage;hardware efficient technique;hierarchical triangle level culling technique;realistic image generation;screen;tile-based rendering","","0","","14","","","17-20 Dec. 2012","","IEEE","IEEE Conference Publications"
"Matching Relevance Features with Ontological Concepts","Y. Shen; Y. Li; Y. Xu; X. Tao","Sci. & Eng. Fac., Queensland Univ. of Technol., Brisbane, QLD, Australia","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","190","194","In order to comprehend user information needs by concepts, this paper introduces a novel method to match relevance features with ontological concepts. The method first discovers relevance features from user local instances. Then, a concept matching approach is developed for matching these features to accurate concepts in a global knowledge base. This approach is significant for the transition of informative descriptor and conceptional descriptor. The proposed method is elaborately evaluated by comparing against three information gathering baseline models. The experimental results shows the matching approach is successful and achieves a series of remarkable improvements on search effectiveness.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.194","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511675","Concept Matching;Global Knowledge Base;Local Instance;Relevance Feature","","feature extraction;information needs;information retrieval;knowledge based systems;ontologies (artificial intelligence);pattern matching","concept matching approach;conceptional descriptor;global knowledge base;informative descriptor;ontological concepts;relevance feature discovery;relevance feature matching;user information needs;user local instances","","0","","7","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Context Aware Named Entity Disambiguation","I. Laek; P. Vojtá","Fac. of Inf. Technol., Czech Tech. Univ. in Prague, Prague, Czech Republic","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","402","408","Recently, named entity recognition tools tend to disambiguate recognized named entities on a very detailed level. Instead of elementary types (e.g. Person or Location), they assign concrete identifiers, trying to distinguish even different entities having same name and type (e.g. cities with the same name in different countries). We introduce a novel method for this kind of named entity disambiguation exploiting structural dependencies of recognized entities. We analyse the co-occurrence of disambiguated entities in the backing knowledge base and use this information to improve results of existing named entity disambiguation approaches. A model for co-occurrence representation is proposed and evaluated based on a dataset that we mine from Wikipedia.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.96","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511915","Linked Data;Named Entity Recognition and Disambiguation;Text Annotation","","Web sites;data mining;information retrieval;knowledge based systems;text analysis;ubiquitous computing","Wikipedia;backing knowledge base;context aware named entity disambiguation;dataset mining;disambiguated entity co-occurrence representation;named entity recognition tools;structural dependencies","","0","","17","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"An enhanced personal photo recommendation system by fusing contextual and textual features on mobile device","Y. Tian; W. Wang; X. Gong; X. Que; J. Ma","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; Beijing, P.R. China, 100876","IEEE Transactions on Consumer Electronics","20130404","2013","59","1","220","228","As a main means to record scene in personal daily life, personal photos convey high-level semantic information (e.g., who, what, when, where) of an activity user engaged in. Different from other information retrieval tasks, personal photo recommendation depends on the measure of activity relevancy which is implicitly embedded in photos. Spurred by this observation, an enhanced recommendation approach by fusing both contextual and textual features is proposed. First, contextual relevancy is incrementally refined with an enhanced temporal and spatial clustering method respectively. Second, textual similarity of photo annotations is calculated using WordNet to augment the activity relevancy. Third, a fuzzy decision based multi-criteria ranking algorithm i.e., Preference Ranking Organization Method of Enrichment Evaluation (PROMETHEE) is adopted to make recommendations when giving an entry photo. A prototype has been developed on mobile device to illustrate this concept. Experiment results on a real dataset which contains 10,827 photos collected from 50 volunteers during 12 months demonstrate that our approach is more accurate than traditional schemes.","0098-3063;00983063","","10.1109/TCE.2013.6490263","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490263","Activity Relevancy;Context;Fuzzy Decision;Recommendation;Textual","Clustering algorithms;Context;Educational institutions;Indexes;Laboratories;Mobile handsets;Semantics","information retrieval;pattern clustering;recommender systems;sensor fusion;text analysis","PROMETHEE algorithm;WordNet;activity relevancy;contextual feature;contextual relevancy;feature fusion;fuzzy decision based multicriteria ranking algorithm;information retrieval task;mobile device;personal photo recommendation system;photo annotation;preference ranking organization method of enrichment evaluation;recommendation approach;semantic information;spatial clustering method;temporal clustering method;textual feature;textual similarity","","6","","17","","","February 2013","","IEEE","IEEE Journals & Magazines"
"Developing a Cloud Energy-Saving and Case-Based Reasoning Information Agent with Web Service Techniques","S. Y. Yang","Dept. of Comput. & Commun. Eng., St. John's Univ., Taipei, Taiwan","2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming","20130415","2012","","","178","185","Web service techniques are presented herein for supporting a cloud energy-saving and case-based reasoning information agent. Not only can it explore related technologies to establish a Web service platform, but it can also study how to construct cloud interactive diagrams to employ Web service techniques for extensively and seamlessly integrating energy-saving and a case-based reasoning information agent on the Internet. The preliminary system development, display and corresponding comparisons show that the research results can not only sketch the feasibility of the proposed architecture but also are highly successful. Furthermore, the system evaluation shows around 40% of the data queries can be answered by the proposed system, which can effectively alleviate the overloading problem usually associated with a backend server, and its correct rate of data solutions is around 85.1%, which is a very efficient improvement in data query response.","2168-3034;21683034","Electronic:978-0-7695-4898-2; POD:978-1-4673-4566-8","10.1109/PAAP.2012.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424755","Case-Based Reasoning;Cloud Information Systems;Energy-saving Information Systems;Web Services","Cloud computing;Cognition;Data mining;Monitoring;Production;Semantics","Web services;case-based reasoning;cloud computing;energy conservation;multi-agent systems;power aware computing;query processing;question answering (information retrieval)","Internet;Web service;Web service technique;backend server;case-based reasoning;cloud energy saving;cloud interactive diagram;data query answering;data query response;information agent;overloading problem","","0","","14","","","17-20 Dec. 2012","","IEEE","IEEE Conference Publications"
"A Personalized Recommendation Algorithm Based on Approximating the Singular Value Decomposition (ApproSVD)","X. Zhou; J. He; G. Huang; Y. Zhang","GUCAS-VU Joint Lab. for Social Comput. & E-Health Res., Grad. Univ. of Chinese Acad. of Sci., Beijing, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","2","","458","464","Personalized recommendation is, according to the user's interest characteristics and purchasing behavior, to recommend information and goods to users in which they may be interested. With the rapid development of Internet technology, we have entered the era of information explosion, where huge amounts of information are presented at the same time. On one hand, it is difficult for the user to discover information in which he is most interested, on the other hand, general users experience difficult in obtaining information which very few people browse. In order to extract information in which the user is interested from a massive amount of data, we propose a personalized recommendation algorithm based on approximating the singular value decomposition (SVD) in this paper. SVD is a powerful technique for dimensionality reduction. However, due to its expensive computational requirements and weak performance for large sparse matrices, it has been considered inappropriate for practical applications involving massive data. Finally, we present an empirical study to compare the prediction accuracy of our proposed algorithm with that of Drineas's LINEARTIMESVD algorithm and the standard SVD algorithm on the Movie Lens dataset, and show that our method has the best prediction quality.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511608","experimental evaluation;personalization;recommendation syste;singular value decomposition","","Internet;approximation theory;consumer behaviour;information retrieval;purchasing;recommender systems;singular value decomposition;sparse matrices","ApproSVD;Internet technology;dimensionality reduction;information discover;information extraction;personalized recommendation algorithm;prediction quality;purchasing behavior;singular value decomposition approximation;sparse matrices;user interest characteristics","","2","","19","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Enabling the Next Generation Learning Environment","B. Hirsch; A. Al-Rubaie; D. Wang; C. Guttmann; J. W. P. Ng","Etisalat BT Innovation Center, Khalifa Univ. of Sci., Technol. & Res., Abu Dhabi, United Arab Emirates","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","352","356","As more and more services that universities traditionally offer are provided through or underpinned by systems made of computers and networks, the role of these systems become more important. Previously independent functions can now be combined, or easily exchange data and information, hence offering new opportunities. In order to leverage and exploit these new possibilities, these services must be built to access each other's data and functionalities in an open, secure and simple manner. This paper describes a smart learning platform that provides the backbone to the University of the Future for the next generation campus environment.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511707","intelligent campus;learning platform;smart education","","computer aided instruction;educational institutions;information retrieval;security of data","data access;data exchange;information exchange;next generation learning environment;smart learning platform;universities","","1","","16","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"A New Vision-Based Method for Extracting Academic Information from Conference Web Pages","P. Wang; M. Zhou; Y. You; X. Zhang","Sch. of Comput. Sci. & Eng., Southeast Univ., Nanjing, China","2012 IEEE 24th International Conference on Tools with Artificial Intelligence","20130411","2012","1","","976","981","This paper proposes a new vision-based method for extracting academic information from conference Web pages. The main contributions include: (1) An new vision-based page segmentation algorithm is proposed to improve the result of classical VIPS algorithm. This algorithm can divide pages into text blocks. (2) All text blocks are classified as 10 categories according to vision features, keyword features and text content features. The initial classification results have 75% precision and 67% recall. (3) The context information of text blocks are employed to repair and refine initial classification results, which are improved to 96% precision and 98% recall. Finally, academic information is extracted from classified text blocks. Our experimental results on real-world datasets show that the proposed method is effective and efficient for extracting academic information from conference Web pages.","1082-3409;10823409","Electronic:978-0-7695-4915-6; POD:978-1-4799-0227-9","10.1109/ICTAI.2012.138","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495152","Web information extraction;Web page segmentation;bayesian network classifier","Classification algorithms;Data mining;Feature extraction;Noise;Semantics;Web pages","Web sites;educational administrative data processing;information retrieval;text analysis","academic information extraction;classical VIPS algorithm;conference Web pages;keyword features;text blocks;text content features;vision-based method;vision-based page segmentation algorithm","","0","","23","","","7-9 Nov. 2012","","IEEE","IEEE Conference Publications"
"Document Classification by Computing an Echo in a Very Simple Neural Network","C. Brouard","LIG, AMA Team, UPMF - Grenoble2, Grenoble, France","2012 IEEE 24th International Conference on Tools with Artificial Intelligence","20130411","2012","1","","735","741","In this paper we present a new classification system called ECHO. This system is based on a principle of echo and applied to document classification. It computes the score of a document for a class by combining a bottom-up and a top-down propagation of activation in a very simple neural network. This system bridges a gap between Machine Learning methods and Information Retrieval since the bottom-up and the top-down propagations can be seen as the measures of the specificity and exhaustivity which underlie the models of relevance used in Information Retrieval. The system has been tested on the Reuters 21578 collection and in the context of an international challenge on large scale hierarchical text classification with corpus extracted from Dmoz and Wikipedia. Its comparison with other classification systems has shown its efficiency.","1082-3409;10823409","Electronic:978-0-7695-4915-6; POD:978-1-4799-0227-9","10.1109/ICTAI.2012.104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495116","classification;neural network;relevance models","Calibration;Context;Electronic publishing;Encyclopedias;Learning systems;Support vector machines","information retrieval;learning (artificial intelligence);neural nets;pattern classification;text analysis","Dmoz;ECHO classification system;Reuters 21578 collection;Wikipedia;bottom-up propagation;document classification;exhaustivity measure;hierarchical text classification;information retrieval;machine learning method;specificity measure;top-down propagation;very simple neural network","","0","","13","","","7-9 Nov. 2012","","IEEE","IEEE Conference Publications"
"NE-Rank: A Novel Graph-Based Keyphrase Extraction in Twitter","A. Bellaachia; M. Al-Dhelaan","Comput. Sci. Dept., George Washington Univ., Washington, DC, USA","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","372","379","The massive growth of the micro-blogging service Twitter has shed the light on the challenging problem of summarizing a collection of large number of tweets. This paper attempts to extract topical key phrases that would represent topics in tweets. Due to the informality, noise, and short length of tweets, such research is nontrivial. We tackle such challenges with extensive preprocessing approach. Followed by, introduction of new features that improve topical key phrase extraction in Twitter. We start by proposing a novel unsupervised graph-based keyword ranking method, called NE-Rank, that considers word weights in addition to edge weights when calculating the ranking. Then we introduce a new approach of leveraging hash tags when extracting key phrases. We have conducted a set of experiments showing the potential of both approaches with 16% to 39% improvement for NE-Rank and 20% improvement for hash tag enhanced extraction.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.82","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511911","Graph-based Ranking;Hashtag;Keyphrase Extraction;NE-Rank;PageRank;TextRank;Twitter","","graph theory;information retrieval;social networking (online)","NE-rank;Twitter;edge weights;extensive preprocessing approach;graph-based keyphrase extraction;hash tag enhanced extraction;micro-blogging service;topical key phrase extraction;tweets;unsupervised graph-based keyword ranking method;word weights","","4","","23","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Improving data access efficiency by using a tagless access buffer (TAB)","A. Bardizbanyan; P. Gavin; D. Whalley; M. Själander; P. Larsson-Edefors; S. McKee; P. Stenström","Chalmers University of Technology","Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","20130408","2013","","","1","11","The need for energy efficiency continues to grow for many classes of processors, including those for which performance remains vital. Data cache is crucial for good performance, but it also represents a significant portion of the processor's energy expenditure. We describe the implementation and use of a tagless access buffer (TAB) that greatly improves data access energy efficiency while slightly improving performance. The compiler recognizes memory reference patterns within loops and allocates these references to a TAB. This combined hardware/software approach reduces energy usage by (1) replacing many level-one data cache (L1D) accesses with accesses to the smaller, more power-efficient TAB; (2) removing the need to perform tag checks or data translation lookaside buffer (DTLB) lookups for TAB accesses; and (3) reducing DTLB lookups when transferring data between the L1D and the TAB. Accesses to the TAB occur earlier in the pipeline, and data lines are prefetched from lower memory levels, which result in a small performance improvement. In addition, we can avoid many unnecessary block transfers between other memory hierarchy levels by characterizing how data in the TAB are used. With a combined size equal to that of a conventional 32-entry register file, a four-entry TAB eliminates 40% of L1D accesses and 42% of DTLB accesses, on average. This configuration reduces data-access related energy by 35% while simultane-ously decreasing execution time by 3%.","","Electronic:978-1-4673-5525-4; POD:978-1-4673-5524-7","10.1109/CGO.2013.6495003","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495003","energy;memory hierarchy;strided access","Arrays;Educational institutions;Hardware;Indexes;Prefetching;Registers;Resource management","cache storage;energy conservation;hardware-software codesign;information retrieval;program compilers","DTLB lookup;L1D access;TAB;block transfers;compiler;data access energy efficiency;data translation lookaside buffer lookup;energy usage reduction;entry register file;hardware-software approach;level-one data cache access;memory hierarchy levels;memory reference patterns;processor energy expenditure;tag checks;tagless access buffer","","1","","18","","","23-27 Feb. 2013","","IEEE","IEEE Conference Publications"
"Development of an environment for information reuse on a hospital information system","T. Takemura; N. Kume; K. Okamoto; T. Kuroda; H. Yoshihara","Grad. Sch. of Appl. Inf., Univ. of Hyogo, Kobe, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1384","1387","An environment of information reuse on hospital information system have been discussed and some hospitals have introduced data warehouses or similar systems. In Kyoto University Hospital, we have a lot of requests from hospital colleagues that want to data of the hospital information system for clinical research or promotion of streamlining. Therefore, we introduced a environment of database users use freely, and a web service server in order to reuse data of the hospital information system. Consequently, users could do clinical researches and clinical practices on the same environment.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505339","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505339","FileMaker;Web service;information reuse;template","","Web services;data warehouses;hospitals;information retrieval;medical information systems","Kyoto University Hospital;Web service server;clinical practice;clinical research;data reuse;data warehouse;database environment;hospital information system;information reuse;streamlining promotion","","0","","11","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"MePPM- Memory efficient prediction by partial match model for web prefetching","C. D. Gracia; S. Sudha","Department of CSE, National Institute of Technology, Tiruchirapalli, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","736","740","The proliferation of World Wide Web and the immense growth of Internet users and services requiring high bandwidth have increased the response time of the users substantially. Thus, users often experience long latency while retrieving web objects. The popularity of web objects and web sites show a considerable spatial locality that makes it possible to predict future accesses based on the previous accessed ones. This infact has motivated the researchers to devise new prefetching techniques in web so as to reduce the user perceived latency. Most of the research works are based on the standard Prediction by Partial Match model and its derivates such as the Longest Repeating Sequence and the Popularity based model that are built into Markov predictor trees using common surfing patterns. These models require lot of memory. Hence, in this paper, memory efficient Prediction by Partial Match models based on Markov model are proposed to minimize memory usage compared to the standard Prediction models and its derivatives.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514318","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514318","Access pattern;Longest Repeating Sequence;Markov Model;Memory Efficient model;Prediction by Partial Match","Computational modeling;Hidden Markov models;Markov processes;Memory management;Predictive models;Prefetching;Vegetation","Internet;Markov processes;Web sites;information retrieval;pattern matching","Internet services;Internet users;Markov model;Markov predictor trees;MePPM;Web object retrieval;Web prefetching;Web sites;World Wide Web;longest repeating sequence;memory efficient prediction;partial match model;popularity based model;standard prediction by partial match model;surfing patterns","","2","","9","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Dynamically Adaptive User Profiling for Personalized Recommendations","M. A. Zeb; M. Fasli","Sch. of Comput. Sci. & Electron. Eng., Univ. of Essex, Colchester, UK","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","604","611","Modelling user interests for time sensitive domains like RSS feeds and spontaneous social media has been a vibrant research activity in recent times. Although numerous efforts have been invested in to the personalisation of dynamic web content, the voluminous and diversified production of continuous online information still poses significant research challenges. In this paper, we propose a profiling mechanism that learns the user access patterns in a dynamic environment like RSS feeds. Main goal of the proposed mechanism is the retrieval optimisation and personalised recommendation of RSS feeds in close to real time. The mechanism allocates personalized time windows based on the learned access patterns and tries to minimize the chances of time-sensitive information from being missed by the user using a delay minimisation algorithm based on Non-homogenous Poisson Process. The mechanism acquires implicit feedback from the user interaction to calculate potential recommendations. The experiments conducted prove the significance of the mechanism in terms of optimised information retrieval and faster adaptation process where it clearly outperforms the other mechanisms in the literature on these properties.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.247","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511948","adaptation;personalization;recommendation;retrieval optimization;user profiling","","Internet;feedback;information retrieval;optimisation;real-time systems;recommender systems;stochastic processes","continuous online information;delay minimisation algorithm;diversified production;dynamic Web content;dynamic environment;dynamically adaptive user profiling;implicit feedback;learned access patterns;nonhomogenous Poisson process-based delay minimisation algorithm;optimised information retrieval;personalised RSS feeds recommendation;personalized recommendations;personalized time windows;retrieval optimisation;time sensitive domains;time-sensitive information;user access patterns;user interaction;user interest modelling;vibrant research activity","","0","","16","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"A new touchscreen application to retrieve speech information efficiently","A. Imai; N. Tazawa; T. Takagi; T. Tanaka; T. Ifukube","NHK (Japan Broadcasting Corporation) Science and Technology Research Laboratories, Tokyo, Japan. Also with Research Center for Advanced Science and Technology, The University of Tokyo, Tokyo, Japan","IEEE Transactions on Consumer Electronics","20130404","2013","59","1","200","206","In this paper, we describe an adaptive speech rate control technology for ultrafast listening that is equivalent to skimming. Nowadays, listening to audio books on mobile devices is quite common. Therefore, in the future we will obtain ever-increasing amounts of information through speech instead of conventional printed materials. People read books at various levels of detail from close reading to skimming. Although a similar feature to skimming is required to efficiently obtain information from audio sources, there is no tool equivalent to skimming for audio playback. Thus, we have developed a new speech rate conversion method to efficiently obtain information from audio sources with very fast replay. This algorithm will help not only sighted people to enjoy audio books but also visually impaired people because almost all of their information is obtained from speech. Thus, the implementation of this technology on special audio players for visually impaired people as a new replay function is expected to be useful. Moreover, this technology should be useful for all audio book listeners, not only people with limited sight.<sup>1</sup>","0098-3063;00983063","","10.1109/TCE.2013.6490260","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490260","audio book;high-speed speech;speechrate conversion;visually impaired","Algorithm design and analysis;Educational institutions;Electronic mail;Equations;Materials;Speech;Visualization","handicapped aids;information retrieval;publishing;speech processing;touch sensitive screens","adaptive speech rate control technology;audio book;audio playback;audio player;printed material;replay function;skimming;speech information retrieval;speech rate conversion method;touchscreen application;visually impaired people","","1","","14","","","February 2013","","IEEE","IEEE Journals & Magazines"
"Scheduling Parallel Data Retrievals for Non-consecutive Broadcast in MIMO Wireless Networks","P. He; H. Shen; H. Tian","Sch. of Comput. & Inf. Technol., Beijing Jiaotong Univ., Beijing, China","2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming","20130415","2012","","","218","223","Wireless data broadcast is usually used to disseminate the public information to considerable clients in wireless environment. Based on the development of mobile device, some mobile devices can simultaneously use multiple antennae to process some operations. In this paper, we commit to study data retrieval scheduling problem in wireless data broadcast, which mobile device has multiple antennae and the data items are non-consecutively broadcasted in multiple parallel channels. In order to improve the performance of downloading requested data items, we propose a scheme of scheduling parallel data retrievals for non-consecutive broadcast to decrease access latency and energy consumption. In this scheme, we utilize a weight of channel through two balancing factors to choose the suitable channel for data retrieval. We analyze the performance of proposed scheme and the other schemes. Evaluation results prove that the proposed scheme has the better performance.","2168-3034;21683034","Electronic:978-0-7695-4898-2; POD:978-1-4673-4566-8","10.1109/PAAP.2012.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424760","nonconsecutive broadcast;parallel data retrieval;wireless networks","Antennas;Broadcasting;Energy consumption;MIMO;Mobile handsets;Switches;Wireless communication","MIMO communication;antenna arrays;broadcast channels;information dissemination;information retrieval;mobile handsets;radio data systems;radio networks;scheduling;wireless channels","MIMO wireless network;balancing factor;mobile device;multiantenna;nonconsecutive broadcast;parallel channel;parallel data retrieval scheduling;public information dissemination;wireless data broadcast;wireless environment","","2","","21","","","17-20 Dec. 2012","","IEEE","IEEE Conference Publications"
"FoCUS: Learning to Crawl Web Forums","J. Jiang; X. Song; N. Yu; C. Y. Lin","Inf. Process. Center, Univ. of Sci. & Technol. of China, Beijing, China","IEEE Transactions on Knowledge and Data Engineering","20130419","2013","25","6","1293","1306","In this paper, we present Forum Crawler Under Supervision (FoCUS), a supervised web-scale forum crawler. The goal of FoCUS is to crawl relevant forum content from the web with minimal overhead. Forum threads contain information content that is the target of forum crawlers. Although forums have different layouts or styles and are powered by different forum software packages, they always have similar implicit navigation paths connected by specific URL types to lead users from entry pages to thread pages. Based on this observation, we reduce the web forum crawling problem to a URL-type recognition problem. And we show how to learn accurate and effective regular expression patterns of implicit navigation paths from automatically created training sets using aggregated results from weak page type classifiers. Robust page type classifiers can be trained from as few as five annotated forums and applied to a large set of unseen forums. Our test results show that FoCUS achieved over 98 percent effectiveness and 97 percent coverage on a large set of test forums powered by over 150 different forum software packages. In addition, the results of applying FoCUS on more than 100 community Question and Answer sites and Blog sites demonstrated that the concept of implicit navigation path could apply to other social media sites.","1041-4347;10414347","","10.1109/TKDE.2012.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165295","EIT path;ITF regex;URL pattern learning;URL type;forum crawling;page classification;page type","Crawlers;Indexes;Layout;Message systems;Navigation;Software packages;Training","Web sites;information retrieval;pattern classification;software packages","FoCUS;URL types;URL-type recognition problem;annotated forums;blog sites;community question-and-answer sites;entry pages;forum crawler under supervision;forum software packages;forum threads;implicit navigation paths;robust page type classifiers;social media sites;supervised Web-scale forum crawler;thread pages","","1","","","","20120306","June 2013","","IEEE","IEEE Journals & Magazines"
"An Empirical Study of Bugs in Machine Learning Systems","F. Thung; S. Wang; D. Lo; L. Jiang","Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore","2012 IEEE 23rd International Symposium on Software Reliability Engineering","20130404","2012","","","271","280","Many machine learning systems that include various data mining, information retrieval, and natural language processing code and libraries are used in real world applications. Search engines, internet advertising systems, product recommendation systems are sample users of these algorithm-intensive code and libraries. Machine learning code and toolkits have also been used in many recent studies on software mining and analytics that aim to automate various software engineering tasks. With the increasing number of important applications of machine learning systems, the reliability of such systems is also becoming increasingly important. A necessary step for ensuring reliability of such systems is to understand the features and characteristics of bugs occurred in the systems. A number of studies have investigated bugs and fixes in various software systems, but none focuses on machine learning systems. Machine learning systems are unique due to their algorithm-intensive nature and applications to potentially large-scale data, and thus deserve a special consideration. In this study, we fill the research gap by performing an empirical study on the bugs in machine learning systems. We analyze three systems, Apache Mahout, Lucene, and OpenNLP, which are data mining, information retrieval, and natural language processing tools respectively. We look into their bug databases and code repositories, analyze a sample set of bugs and corresponding fixes, and label the bugs into various categories. Our study finds that 22.6% of the bugs belong to the algorithm/method category, 15.6% of the bugs belong to the non-functional category, and 13% of the bugs belong to the assignment/initialization category. We also report the relationship between bug categories and bug severities, the time and effort needed to fix the bugs, and bug impacts. We highlight several bug categories that deserve attention in future research.","1071-9458;10719458","Electronic:978-0-7695-4888-3; POD:978-1-4673-4638-2","10.1109/ISSRE.2012.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405375","","Software reliability","data mining;information retrieval;learning (artificial intelligence);natural language processing;program debugging;software reliability","Apache Mahout;Internet advertising systems;Lucene;OpenNLP;algorithm-intensive code;algorithm-intensive nature;bug categories;bug databases;code repositories;data mining;information retrieval;machine learning systems;natural language processing code;recommendation systems;search engines;software engineering tasks;software mining;system reliability","","9","","42","","","27-30 Nov. 2012","","IEEE","IEEE Conference Publications"
"A Double-Ranking Strategy for Long-Tail Product Recommendation","M. Zhang; N. Hurley; W. Li; X. Xue","Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","282","286","In this paper we attempt to retrieve the items in the long-tail for top-N recommendation. That is, to recommend products that the end-user likes, but that are not generally popular, which has been getting more and more notice lately. By analysing the existing issue of current recommendation algorithms, a strategy is proposed that succeeds in maintaining recommendation accuracy while reducing the concentration of the recommendation on popular items in the system. Evaluating on the publicly available Movie lens and Yahoo! datasets, the results show the recommendation algorithm proposed in this work retrieves items in the users' relatively unpopular tastes without losing the performance in their popular tastes, which ultimately results in a better overall accuracy for the system.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511897","long-tail;popularity;top-N recommendation","","information retrieval;recommender systems","Movie lens;Yahoo datasets;double-ranking strategy;item retrieval;long-tail product recommendation;top-N recommendation","","1","","6","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"A new distributed name disambiguation system based on MapReduce","Liu Pengfei; Ge sheng","School of Computer Science and Engineering, Beihang University, Beijing 100191, China","2012 IEEE 14th International Conference on Communication Technology","20130502","2012","","","550","554","Social network search is a kind of vertical search based on the information polymerization of social network data. And name disambiguation is one of the vital issues in social network search. With the explosive growth of the information, effectively deal with name disambiguation in massive data scenario becomes an important issue. To tackle this issue, we combine the MapReduce model and document clustering algorithm to propose a distributed method for name disambiguation. And then present a distributed name disambiguation system. This system runs on the Hadoop platform, and makes the name disambiguation parallelized by dividing the document clustering task to a couple of maps and reduces. In addition, we evaluated our system in terms of expansibility and accuracy.","","Electronic:978-1-4673-2101-3; POD:978-1-4673-2100-6","10.1109/ICCT.2012.6511416","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511416","MapReduce;document cluster;name disambiguation;vector space model","","data mining;document handling;information retrieval;parallel processing;pattern clustering;social networking (online)","Hadoop platform;MapReduce model;distributed name disambiguation system;document clustering algorithm;information polymerization;social network data","","0","","11","","","9-11 Nov. 2012","","IEEE","IEEE Conference Publications"
"Encrypted phrase searching in the cloud","S. Zittrower; C. C. Zou","Dept. of Electrical Engineering & Computer Science, University of Central Florida, Orlando, United States","2012 IEEE Global Communications Conference (GLOBECOM)","20130422","2012","","","764","770","As cloud computing is increasing in popularity, it is difficult to both maintain privacy in datasets while still providing adequate retrieval and searching procedures. This paper introduces a novel approach in the field of encrypted searching that allows both encrypted phrase searches and proximity ranked multi-keyword searches to encrypted datasets on untrusted cloud. By storing encrypted keyword-location data along with specially truncated encrypted keyword indexes in a relational database, we are able to allow for a full range of search features in our encrypted searches, something that has never been accomplished before. Furthermore, our approach permits the encrypted corpus and index to both be stored on cloud data servers. We modify currently available open-source search engine software to complete a prototype and provide results from experiments on a large scale real-world dataset that has more than half a million documents.","1930-529X;1930529X","Electronic:978-1-4673-0921-9; POD:978-1-4673-0920-2; USB:978-1-4673-0919-6","10.1109/GLOCOM.2012.6503205","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6503205","","","cloud computing;cryptography;data privacy;information retrieval;public domain software;relational databases","cloud computing;cloud data servers;encrypted corpus;encrypted dataset privacy;encrypted keyword-location data storage;encrypted phrase search;open-source search engine software;proximity ranked multikeyword search;relational database;retrieval procedures;searching procedures;truncated encrypted keyword indexes;untrusted cloud","","7","","21","","","3-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Aiding web crawlers; projecting web page last modification","A. Anjum; A. Anjum","Ecole Polytechnique, University of Nantes, Rue Christian Pauc - BP50609 - 44306, cedex 3 - France","2012 15th International Multitopic Conference (INMIC)","20130502","2012","","","245","252","Due to colossal amount of data on the Web, Web archivists typically make use of Web crawlers for automated collection. The Internet Archive is the largest organization based on a crawling approach in order to maintain an archive of the entire Web. The most important requirement of a Web crawler, specially when they are used for Web archiving, is to be aware of the date (and time) of last modification of a Web page. This strategy has various advantages, most important of them include i) presentation of an up-to-date version of a Web page to the end user ii) ease of adjusting the crawl rate that allows future retrieval of a Web page's version at a given date, or to compute its refresh rate. The typical way for this modification information of a Web page, that is, to use the Last-Modified: HTTP header, unfortunately does not provide correct information every time. In this work, we discuss various techniques that can be used to determine the date of last modification of a Web page with the help of experiments. This will help in adjusting the crawl rate for a specific page and also helps in presenting users with up to date information and thus allowing future versioning of a Web page more meticulous.","","Electronic:978-1-4673-2252-2; POD:978-1-4673-2249-2","10.1109/INMIC.2012.6511443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511443","HTTP response headers;Web Archive;Web Crawlers;World Wide Web","","Internet;information retrieval","HTTP header;Internet archive;Web archiving;Web crawler;Web page last modification projection;Web page retrieval;Web page versioning;crawling approach;hypertext transfer protocol;refresh rate","","1","","9","","","13-15 Dec. 2012","","IEEE","IEEE Conference Publications"
"A Misuse Pattern for Retrieving Data from a Database Using SQL Injection","E. B. Fernandez; E. Alder; R. Bagley; S. Paghdar","Dept. of Electr. & Comput. Eng. & Comput. Sci., Florida Atlantic Univ., Boca Raton, FL, USA","2012 ASE/IEEE International Conference on BioMedical Computing (BioMedCom)","20130516","2012","","","127","131","SQL injection attacks represent a serious threat to any database-driven site and they are one of the most frequent types of attacks. We present here a misuse pattern for retrieving data from a database using SQL injection, which describes the essential and typical characteristics of this type of attack. A misuse pattern describes from the point of view of the attacker, how a type of attack or misuse is performed (what units it uses and how), looks at the selection of the methods available to the attacker, analyzes the way of stopping the attack, and describes how to trace the attack once it has happened by appropriate collection and observation of forensic data.","","Electronic:978-0-7695-4938-5; POD:978-1-4673-5495-0","10.1109/BioMedCom.2012.27","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516440","SQL injection attacks;computer security;data security breach;forensics;misuse pattern;security pattern","","SQL;database management systems;digital forensics;information retrieval","SQL injection attacks;data retrieval;database-driven site;forensic data;misuse pattern","","1","","17","","","14-16 Dec. 2012","","IEEE","IEEE Conference Publications"
"Access to the DARIAH Bit Preservation Service for Humanities Research Data","D. Tonne; J. Rybicki; S. E. Funk; P. Gietz","Karlsruhe Inst. of Technol., Karlsruhe, Germany","2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","20130415","2013","","","9","15","Sustainable management of large amounts of research data is gaining in importance for research projects all over the world. The European project DARIAH aims to address this topic for the arts and humanities community. The DARIAH Bit Preservation, as a part of an archiving system for the arts and humanities, allows for a high performance, sustainable, and distributed storage of research data as basis of virtual research environments. A great challenge in designing such a service is to provide a standardized, consistent yet easy-to-use API for accessing the data that remains stable even if backend technology changes over time. As a solution, this paper presents the RESTful API of the DARIAH Bit Preservation which includes an administrative extension, and which is secured by an Authentication and Authorization Infrastructure (AAI) based on SAML. An exemplary implementation illustrates that the API offers distributed access by usage of the HTTP protocol and is able to handle a high number of files. Data transfer rates of up to 45 MB/s were achieved for uploading large files in the local network.","1066-6192;10666192","Electronic:978-1-4673-5321-2; POD:978-1-4799-1709-9","10.1109/PDP.2013.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498527","","Authentication;Authorization;Java;Open source software;Servers;Uniform resource locators;Virtualization","application program interfaces;art;authorisation;computer network security;distributed processing;file organisation;hypermedia;information retrieval;information retrieval systems;records management;transport protocols","AAI;DARIAH bit preservation service;Digital Research Infrastructure for the Arts and Humanities;European project;HTTP protocol;RESTful API;SAML;administrative extension;archiving system;arts;authentication and authorization infrastructure;data access;data transfer rates;distributed access;distributed research data storage;easy-to-use API;file handling;file uploading;humanities research data;local network;sustainable management;virtual research environments","","2","","29","","","Feb. 27 2013-March 1 2013","","IEEE","IEEE Conference Publications"
"Notice of Violation of IEEE Publication Principles<BR>DAta accountability in cloud using reliable log files forwarding","R. N. Heames; P. Sudhakar","MKCE, Karur - 639 113, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","126","130","Notice of Violation of IEEE Publication Principles<BR><BR>""Data Accountability in Cloud Using Reliable Log Files Forwarding""<BR>by R.N. Heames, P. Sudhakar<BR>in the Proceedings of the 2013 International Conference on Information Communication and Embedded Systems, February 2013<BR><BR> After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<BR><BR>This paper is a duplication of the original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.<BR><BR>Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:<BR><BR>""Ensuring Distributed Accountability for Data Sharing in the Cloud""<BR>by Smitha Sundareswaran, Anna C. Squicciarini, and Dan Li<BR>in the IEEE Transactions on Dependable and Secure Computing, Vol 9, No 4, August 2012 pp. 556-568<BR><BR>Cloud computing is receiving a great deal of attention, both in publications and among users, from individuals at home to the government. The cloud makes it possible for you to access your information from anywhere at any time. While a traditional computer setup requires you to be in the same location as your data storage device, the cloud takes away that step. The cloud removes the need for you to be in the same physical location as the hardware that stores your data. A major feature of the cloud services is that user data are usually processed remotely in unknown machines that users do not own or operate. Data handling in the cloud goes through a complex and dynamic hierarchical service chain which does not exist in conventional environments. This can be addressed by a novel approach, namely Cloud Information Accountability (CIA) f- amework. In this every access to the data are correctly and automatically logged. Log files should be sent back to their data owners periodically to inform them of the current usage of their data. More importantly, log files should be retrievable any time by their data owners when needed regardless the location where the files are stored. The end user is allowed to access the data as per their access privileges which they specifies while registering to access the data in the cloud and authentication is provided and accessed data is verified with the original data in the cloud server.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508227","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508227","","Access control;Authentication;Cloud computing;Cryptography;Distributed databases;Java;Servers","authorisation;cloud computing;data handling;file organisation;information retrieval","CIA framework;authentication;cloud computing;cloud information accountability framework;cloud server;cloud services;complex dynamic hierarchical service chain;data access;data accountability;data handling;data storage device;file storage;information access;log files;physical location;reliable log file forwarding;user data processing","","1","","8","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Separable extraction of concealed data and compressed image","C. Rengarajaswamy; K. Vel Murugan","Department of Electronics and Communication Engineering, Oxford Engineering College, UK","2013 International Conference on Emerging Trends in VLSI, Embedded System, Nano Electronics and Telecommunication System (ICEVENT)","20130411","2013","","","1","5","A novel technique for Separable Reversible Data Hiding on Encrypted and transform - based compressed image is proposed. Reversible Data Hiding is the method of hiding data inside a cover file, so that both the data and the cover file could be recovered lossless at the receiver. The transmitter side of such systems involves a cover image, additional data, encryption key and a data hiding key The original image is first encrypted, compressed and then embedded with the secret data. The receiver thus needs to decrypt and extract the hidden information. Important aspects of data hiding include minimizing distortion and improving embedding capacity. Capacity improvement is achieved via compression. Thus compressed image would accumulate more secret data. Amount of secrecy could be further enhanced by compression techniques. Compression could be either lossy or lossless. Also, this method of RDH could be implemented either using a non-separable receiver or a separable receiver. This paper focuses on separable reversible data hiding on encrypted and compressed images.","","Electronic:978-1-4673-5301-4; POD:978-1-4673-5300-7","10.1109/ICEVENT.2013.6496573","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496573","DCT;RDH;capacity;data hiding key;lossy compression;separable receiver","Cryptography;Data mining;Discrete cosine transforms;Generators;Image coding;Wavelet coefficients","cryptography;data compression;data encapsulation;image coding;information retrieval","RDH;cover file;cover image;data hiding key;distortion minimization;embedding capacity;encrypted-based compressed image;encryption key;hidden information decryption;hidden information extraction;lossless compression;lossy compression;nonseparable receiver;secret data;separable receiver;separable reversible data hiding technique;transform-based compressed image;transmitter side","","4","","10","","","7-9 Jan. 2013","","IEEE","IEEE Conference Publications"
"Travel with Words: An Innovative Vision on Travelling","F. M. Zanzotto; A. G. Tudorache","Dept. of Enterprise Eng., Univ. of Rome Tor Vergata, Rome, Italy","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","107","111","Modern travelers wish to explore the world at their own pace, following their own expectations and cultural interests. The travel industry should adapt to this new market environment. A large part of cultural travels is based on history and historical events, and books are a near endless repository of such facts. Moreover, novels are a friendlier gateway to culture than handbooks or reference books. And readers often grow the necessity to see, feel, and taste the imaginary world they are reading about in their own language. By leveraging on books and on modern information and communication technologies, we envision a new way to define travels based on the ""Cultural Travel Clouds"". This paper discusses this vision.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.231","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511659","adaptive systems;adaptive travel experience;travel with words","","cultural aspects;electronic publishing;history;information retrieval;natural language processing;portals;travel industry","cultural interests;cultural travel clouds;historical events;history;information and communication technologies;market environment;travel industry","","0","","26","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"The Retrieval of Important News Stories by Influence Propagation among Communities and Categories","Y. F. Lin; H. Y. Kao","Dept. of Comput. Sci. & Inf. Eng., Nat. Cheng Kung Univ., Tainan, Taiwan","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","32","39","Nowadays, people receive information of the news stories not only from newspapers but also from online news websites. They search important news stories in order to know what happen today. However, it is hard to browse all the news stories published on a day. It is necessary to identify which news stories are more newsworthy on the specific day. In this paper, we investigate how to automatically identify the importance of news stories for different news categories on a specific day by utilizing the influence propagation among communities and news categories. In particular, we build an influence propagation model which consists of three features: category relevance, bloggers' attention and bursty influence. Based on this influence propagation model, we propose a Cross-Category Social Influence Propagation (C-SIP) approach for scoring the importance of news stories on a specific day. We evaluate our approach by using the judgment of Story Ranking Task in TREC 2010 Blog Track. The experiment shows our approach attains a prominent performance in the retrieval of important news stories and gets 9.94% improvement over the best performance of participating systems in TREC 2010 Blog Track.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.236","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511862","Blog;Influence propagation;Information Bursty;News story distillation","","Web sites;information retrieval","bursty influence;category relevance;cross category social influence propagation;important news stories retrieval;influence propagation model;news category;online news Web sites;story ranking task","","0","","31","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Sparse Learning-to-Rank via an Efficient Primal-Dual Algorithm","H. Lai; Y. Pan; C. Liu; L. Lin; J. Wu","Sun Yat-sen University, Guangzhou","IEEE Transactions on Computers","20130429","2013","62","6","1221","1233","Learning-to-rank for information retrieval has gained increasing interest in recent years. Inspired by the success of sparse models, we consider the problem of sparse learning-to-rank, where the learned ranking models are constrained to be with only a few nonzero coefficients. We begin by formulating the sparse learning-to-rank problem as a convex optimization problem with a sparse-inducing ℓ<sub>1</sub> constraint. Since the ℓ<sub>1</sub> constraint is nondifferentiable, the critical issue arising here is how to efficiently solve the optimization problem. To address this issue, we propose a learning algorithm from the primal dual perspective. Furthermore, we prove that, after at most O(1/ε) iterations, the proposed algorithm can guarantee the obtainment of an ε-accurate solution. This convergence rate is better than that of the popular subgradient descent algorithm. i.e., O(1/ε<sup>2</sup>). Empirical evaluation on several public benchmark data sets demonstrates the effectiveness of the proposed algorithm: 1) Compared to the methods that learn dense models, learning a ranking model with sparsity constraints significantly improves the ranking accuracies. 2) Compared to other methods for sparse learning-to-rank, the proposed algorithm tends to obtain sparser models and has superior performance gain on both ranking accuracies and training time. 3) Compared to several state-of-the-art algorithms, the ranking accuracies of the proposed algorithm are very competitive and stable.","0018-9340;00189340","","10.1109/TC.2012.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165264","Fenchel duality;Learning-to-rank;ranking algorithm;sparse models","Accuracy;Computational modeling;Machine learning algorithms;Optimization;Prediction algorithms;Support vector machines;Vectors","convex programming;gradient methods;information retrieval;learning (artificial intelligence)","convex optimization problem;dense model learning;information retrieval;learned ranking models;learning algorithm;nonzero coefficients;performance gain;primal-dual algorithm;ranking accuracy improvement;sparse learning-to-rank problem;sparse-inducing l<sub>1</sub> constraint;sparsity constraints;subgradient descent algorithm","","13","","34","","20120306","June 2013","","IEEE","IEEE Journals & Magazines"
"Biomedical Hot Points Discovering with Data Mining Approach","D. Xu; F. Zhu","Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China","2012 Fourth International Symposium on Information Science and Engineering","20130411","2012","","","279","281","PubMed can retrieval biomedical literatures and get the number of publications. It can't help us analyze the trends and the relationships among the data we got. Our research is finding out keywords from 2007 to 2011 in biomedicine and then analyzing them. Here we list 33 keywords and get their numbers. Then we used trendlines in Excel to observe the trends of the keywords so that we can divide all of them into five types, which can stand for the whole fields in a way and help researchers study well.","2160-1283;21601283","Electronic:978-0-7695-4951-4; POD:978-1-4673-5680-0","10.1109/ISISE.2012.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495346","Biomedicine;E-utilities;Keyword;PubMed;Trend","","data mining;information retrieval;medical information systems","Excel trendlines;PubMed;biomedical hot points;biomedical literature retrieval;biomedicine;data mining;keyword","","0","","8","","","14-16 Dec. 2012","","IEEE","IEEE Conference Publications"
"Efficient schema extraction from large XML documents","Y. Zhang; H. Zhou; J. Liu; Z. Liang; P. Duan","School of Software, Yunnan University; Key Laboratory of Software Engineering of Yunnan Province Kunming, China","2012 5th International Conference on BioMedical Engineering and Informatics","20130504","2012","","","1255","1260","Although the presence of a schema enables many optimizations for operations on XML documents, several studies have shown that many XML documents in practice either do not refer to a schema, or refer to a syntactically incorrect one. It is therefore of utmost importance to provide tools and techniques that can automatically generate XML Schema Definitions from sets of sample documents. While previous work in this area has mostly focused on the method based on regular expressions, we consider its many inadequacies. We provide a theoretically complete algorithm that always infers the correct XSDs when a sufficiently large corpus of XML documents is available. In addition, XTree impressively minimizes the necessary time and main memory to extract the schema. Our approach features several advantages over known techniques: XTree scales to very large documents (beyond 1 GB) both in time and memory consumption; it is able to extract a general, complete, correct, minimal, and readable schema for complex documents; it detects elements appear as a sequence or choice. Experiments confirm these features and properties.","","Electronic:978-1-4673-1184-7; POD:978-1-4673-1183-0","10.1109/BMEI.2012.6513057","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513057","Automatic schema extraction;Large XML documents;XML;XML schema","","XML;information retrieval;storage management;tree data structures","XSD;XTree;automatic XML schema definition generation;complete algorithm;eXtensible Markup Language;large XML document corpus;memory consumption;schema extraction;time consumption","","0","","13","","","16-18 Oct. 2012","","IEEE","IEEE Conference Publications"
"User and Item Modeling Methods Using Customer Reviews towards Recommender System Based on Personal Values","S. Hattori; Y. Takama","Grad. Sch. of Syst. Design, Tokyo Metropolitan Univ., Tokyo, Japan","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","83","86","This paper proposes user and item modeling methods towards recommender systems based on personal values. Marketing fields have been taking notice of personal values, because that such values are significantly related to user preference. While existing recommender systems usually employ user preference of items to make recommendations, proposed method focuses on users' personal values, which mean value judgments that show what attributes users put a high priority. The influence of each attribute on item evaluation is determined based on correspondence between ratings for item and the attribute. The proposed method is applied to actual review data, of which results supports the assumption that different users put high priorities on different attributes.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511654","item modeling;personal values;recommender system;user modeling","","information retrieval;recommender systems;user modelling","customer review;item evaluation;item modeling;marketing;recommender system;review data;user attribute;user modeling;user personal value;user preference","","0","","10","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Multiple leaf nodes based parse trees for News Fetching Systems","K. Sattar; S. Iqbal","School of Electrical Engineering and Computer Science(SEECS), National University of Sciences and Technology (NUST), Islamabad, Pakistan","2012 15th International Multitopic Conference (INMIC)","20130502","2012","","","265","268","With the growth of internet people are shifting from their paper based news system to online news system. These online news websites are very useful for the people who have internet access and they can easily read and search their required news through home/office computer or cell phone without wasting their time. To search a particular news, many searching algorithms and News Fetching Systems (NFS) are available which return the results in less than a second. But how much correct information they are giving from a huge data is still a critical area, where we need some efficient algorithms to give us more optimized results. We propose a Multiple Leaf Nodes based Parse Trees for NFS where all the leaf nodes are commonly used alternative words for a specific word. We evaluate our results for a randomly selected webpage and compare with the simple parse tree system. Analysis shows that our proposed algorithm can enhance the performance of any NFS.","","Electronic:978-1-4673-2252-2; POD:978-1-4673-2249-2","10.1109/INMIC.2012.6511460","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511460","Multiple Leaf Nodes Parse Tree;News Fetching Systems (NFS);News Searching;Parse Tree","","Internet;information retrieval;trees (mathematics)","Internet access;NFS;cell phone;home computer;multiple leaf nodes;news fetching system;news reading;news searching;office computer;online news system;paper based news system;parse tree system;searching algorithm","","1","","10","","","13-15 Dec. 2012","","IEEE","IEEE Conference Publications"
"Flexible access control for outsourcing personal health services in cloud computing using hierarchical attribute set based encryption","V. Kandasamy; E. Papitha","Dept. of Inf. Technol., Loyola Inst. of Technol., Chennai, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","569","571","Online personal health record (PHR) enables patients to manage their own medical records in a centralized way, which greatly facilitates the storage, access and sharing of personal health data. With the emergence of cloud computing, it is attractive for the PHR service providers to shift their PHR applications and storage into the cloud, in order to enjoy the elastic resources and reduce the operational cost. However, by storing PHRs in the cloud, the patients lose physical control to their personal health data, which makes it necessary for each patient to encrypt her PHR data before uploading to the cloud servers. Under encryption, it is challenging to achieve fine-grained access control to PHR data in a scalable and efficient way by using FADE. For each patient, the PHR data should be encrypted so that it is scalable with the number of users having access. Also, since there are multiple owners (patients) in a PHR system and every owner would encrypt her PHR files using a different set of cryptographic keys, it is important to reduce the key distribution complexity in such multi-owner settings. Existing cryptographic enforced access control schemes are mostly designed for the single-owner scenarios. In order to realize scalable, flexible, and fine-grained access control of outsourced data in cloud computing, in this research, we propose hierarchical attribute-set-based encryption (HASBE) by extending cipher text-policy attribute-set-based encryption (ASBE) with a hierarchical structure of users. The proposed scheme not only achieves scalability due to its hierarchical structure, but also inherits flexibility and fine-grained access control in supporting compound attributes of ASBE.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508268","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508268","Hierarchical attribute Cloud computing;Personal Health Record;data security","Access control;Ciphers;Cloud computing;Encryption;Medical services;Servers","authorisation;cloud computing;cryptography;information retrieval;medical information systems;outsourcing","HASBE;PHR applications;PHR data encryption;PHR files;PHR service providers;PHR storage;cipher text-policy attribute set-based encryption;cloud computing;cloud servers;cryptographic keys;elastic resources;fine-grained access control;hierarchical attribute set-based encryption;hierarchical user structure;key distribution complexity reduction;medical record management;online personal health record;operational cost reduction;outsourced data;personal health data access;personal health data sharing;personal health data storage","","2","","5","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Self-Organising Semantic Resource Discovery for Pervasive Systems","G. Stevenson; J. Ye; S. Dobson; M. Viroli; S. Montagna","Sch. of Comput. Sci., Univ. of St. Andrews, St. Andrews, UK","2012 IEEE Sixth International Conference on Self-Adaptive and Self-Organizing Systems Workshops","20130415","2012","","","181","186","The pervasive computing vision encompasses scenarios where services are delivered to users as a result of opportunistic encounters between their personal devices and computational resources embedded in their surrounding environment. The decentralised and dynamic nature of such environments complicates service provision, providing no setting for a conventional orchestrator to manage the resource discovery process. This paper proposes a novel approach to resource discovery, employing nature-inspired patterns to manage the search for and retrieval of information across a dynamic arrangement of devices. We show how the results of fuzzy matching based on semantic resource descriptions can be incorporated at the pattern level to route only the best matched resources to a requestor, and how application context extrinsic to the matching algorithm may augment this process.","","Electronic:978-0-7695-4895-1; POD:978-1-4673-5153-9","10.1109/SASOW.2012.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498400","bio-inspired;resource discovery;semantic matching","","fuzzy set theory;information retrieval;pattern matching;resource allocation;self-adjusting systems;ubiquitous computing","application context;computational resource;decentralised environment;dynamic environment;fuzzy matching;information retrieval;information search management;matching algorithm;nature-inspired pattern;opportunistic encounter;personal device;pervasive computing;pervasive system;resource discovery process management;self-organising semantic resource discovery;semantic resource description;service delivery;service provision","","0","","22","","","10-14 Sept. 2012","","IEEE","IEEE Conference Publications"
"Quantitative common sense estimation system and its application to automatic membership function generation","Y. Igawa; M. Hagiwara","Department of Information and Computer Science Keio University Yokohama, 223-8522 Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1335","1340","Systems capable of autonomous thinking and estimation have been required to cope with unknown situations. One of the important issues is knowledge, especially common sense, acquisition. This paper proposes new quantitative common sense estimation methods and applies them to an automatic membership function generation system. The proposed system estimates threshold values corresponding to large and small for various kinds of object-attribute sets to make membership functions. Here, the proposed system tries to relate each object and the impression. Two methods are proposed in this paper. The method-1 obtains data from top 1,000 snippets by Web search and estimates the global and local tendencies by clustering. The method-2 uses the number of hits in Web search together with parts of the results obtained by the method-1. In addition, several techniques are devised to eliminate unnecessary information from the retrieved Web pages. We carried out evaluation experiments: the effectiveness of the proposed methods has been shown and effectiveness of the combined method is indicated.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505176","Web;knowledge acquisition;membership function;quantitative common sense","","Internet;information retrieval;pattern clustering","Web page retrieval;Web search;automatic membership function generation system;object-attribute;quantitative common sense estimation system;tendency clustering","","2","","17","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Improving QA performance through semantic reformulation","M. Ramprasath; S. Hariharan","","2012 Nirma University International Conference on Engineering (NUiCONE)","20130404","2012","","","1","4","Crushing of textual information is available in electronic form on internet. As a result finding the answer to user query essential in natural Language processing, information retrieval and question answering. Semantic based question reformulation is frequently used in question answering system to retrieve answer from large document collection. The goal of this paper is to find useful and standard reformulation pattern, which can be used in our question answering (QA) system to find exact candidate answer. In this paper we used TREC-8, TREC-9, and TREC-10 collection as training set. Different types of question and corresponding answer can use from TREC collection. The QA system will automatically extract the pattern from sentences retrieved from search engine. With help of word net it will check syntactic tags, semantic relation between question and answer pair. Next weight can assigned to each extracted pattern according to is length, the distance between keyword and the level of semantic similarity between the extracted question and answer. The proposed systems vary from most former other reformulation learning system.","2375-1282;23751282","Electronic:978-1-4673-1719-1; POD:978-1-4673-1720-7","10.1109/NUICONE.2012.6493196","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493196","Natural language processing;Question answering system;question reformulation","","natural language processing;query processing;question answering (information retrieval);search engines;text analysis","QA system performance improvement;TREC-10 collection;TREC-8 collection;TREC-9 collection;automatic pattern extraction;document collection;information retrieval;natural Language processing;pattern distance;pattern length;question answer pair;question answering system;search engine;semantic relation;semantic similarity;semantic-based question reformulation;sentence retrieval;standard reformulation pattern;syntactic tags;textual information crushing;training set;user query;weight assignment","","0","","20","","","6-8 Dec. 2012","","IEEE","IEEE Conference Publications"
"A New Architecture of an Intelligent Agent-Based Crawler for Domain-Specific Deep Web Databases","Y. Li; Y. Wang; E. Tian","Sch. of Comput. Sci. & Technol., Xidian Univ., Xi'an, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","656","663","A key problem of retrieving, integrating and mining rich and high quality information from massive Deep Web Databases (WDBs) online is how to automatically and effectively discover and recognize domain-specific WDBs' entry points, i.e., searchable forms, in the Web. It has been a challenging task because domain-specific WDBs' forms with dynamic and heterogeneous properties are very sparsely distributed over several trillion Web pages. Although significant efforts have been made to address the problem and its special cases, more intelligent and effective solutions remain to be further explored. In this paper, a new architecture of an intelligent agent-based crawler (iCrawler) for domain-specific Deep Web databases has been proposed to address the limitations of the existing methods. The iCrawler, based on intelligent learning agents and domain ontology, and a series of novel and effective strategies, including a two-step page classifier, a link scoring strategy, etc, can improve the performance of the existing methods. Experiments of the iCrawler over a number of real Web pages in a set of representative domains have been conducted and the results show that the iCrawler outperforms the existing domain-specific Deep Web Form-Focused Crawlers (FFCs) in terms of the harvest rate, coverage rate and time performance.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511958","Coverage Rate;Deep Web Databases (WDBs);Form-Focused Crawlers (FFCs);Harvest Rate","","Internet;data mining;database management systems;information retrieval;learning (artificial intelligence);multi-agent systems;pattern classification","FFC;Web pages;coverage rate;domain ontology;domain-specific WDB entry point discovery;domain-specific WDB entry point recognition;domain-specific deep Web database;domain-specific deep Web form-focused crawler;dynamic property;harvest rate;heterogeneous property;iCrawler;information integration;information mining;information retrieval;intelligent agent-based crawler;intelligent learning agents;link scoring strategy;searchable forms;time performance;two-step page classifier","","0","","20","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"A group recommendation system with a multi-modal user interface","M. Yukawa; Y. Hayashi; H. Ogawa; V. V. Kryssanov","Graduate School of Information Science and Engineering, Ritsumeikan University, 1-1-1, Nojihigashi, Kusatsu, Shiga, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","2158","2163","This paper describes a proposed recommendation system and multi-modal interface. In this study, the system aims to provide a solution to need for people with specific skills by recommending dependable people having both needed knowledge and discussion skills. The user has opportunities to quickly reach effective solutions through discussion with people recommended by this system. A procedure using a multi-agent system uses data obtained through a social network represented as architecture of relations. The recommendation system in this study is implemented in this algorithm, supplemented with additional information to describe the comprehensive level of trust between members of the discussion, ratings which are retrieved from those member's evaluations. For some situations, for example, when the user does not agree with the recommendation and wants to update or alter the recommended data, a function has been implemented with which the user can readily coordinate groups. As a way of providing complex information, the multimodal interface represents them with a haptic metaphor for easy understanding and quick manipulation. The use of haptic force feedback displays in the virtual space makes it possible for the user to understand more easily than methods using only the visual modality.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505391","Group Recommendation;haptic metaphor;solution support system","","information retrieval;multi-agent systems;recommender systems;social networking (online);user interfaces","group recommendation system;haptic force feedback;haptic metaphor;multiagent system;multimodal user interface;social network;visual modality","","0","","7","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Processing of unstructured data for information extraction","V. A. Ingle","Department of Computer Science and IT, Dr. B.A.M. University, Aurangabad","2012 Nirma University International Conference on Engineering (NUiCONE)","20130404","2012","","","1","4","Unstructured data are those that have no predetermined form or structure and are full of textual data. It does not fit well into relational tables. Most enterprise data today can actually be considered unstructured. Typical unstructured systems include emails, reports, contracts, transcripts of telephone conversations, and other communications. Web pages also contain links and references to External, often unstructured content such as images, XML files, animations and databases. This paper focuses on extracting features in html pages by using tokenization and Non matrix factorization. Classification of text is done using bag of words approach. The workbench is dataset collected in university domain web pages.","2375-1282;23751282","Electronic:978-1-4673-1719-1; POD:978-1-4673-1720-7","10.1109/NUICONE.2012.6493202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493202","Information Extraction;NMF;Text Mining;Tokenization;Unstructured data","","Web sites;XML;information retrieval;text analysis;text detection","HTML pages;XML files;animations;contracts;databases;emails;enterprise data;feature extraction;images;information extraction;nonmatrix factorization;relational tables;reports;telephone conversation transcripts;textual data;tokenization;university domain Web pages;unstructured content;unstructured data processing;unstructured systems","","0","","18","","","6-8 Dec. 2012","","IEEE","IEEE Conference Publications"
"Predicting Best Responder in Community Question Answering Using Topic Model Method","T. Zhao; C. Li; M. Li; S. Wang; Q. Ding; L. Li","Sch. of Software, Tsinghua Univ., Beijing, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","457","461","Community question answering (CQA) services provide an open platform for people to share their knowledge and have attracted great attention for its rapidly increasing popularity. As the more knowledge people provided are shared in CQA, how to use the historical knowledge for solving new questions has become a crucial problem. In this paper, we investigate the problem as predicting best responders for new questions and tackle the problem from two perspectives, one is from the asker of the new question, and the other is from the question itself. We propose two supervised topic models, Asker-Responder Topic Model (ARTM) and Question-Responder Topic Model (QRTM) for both two perspectives by tracking people's answering history as background knowledge. Our experiments show that the two supervised topic models can effectively predict best responders for new questions in CQA without any additional works and have significant improvement over the baseline method.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511924","Asker-Responder Topic Model;Community Question Answering;Question-Responder Topic Model;Supervised Topic Model","","knowledge management;question answering (information retrieval);social networking (online)","ARTM;CQA;QRTM;asker-responder topic model;background knowledge;best responder prediction;community question answering services;historical knowledge;knowledge sharing;people answering history;question-responder topic model;supervised topic model","","0","","10","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Recommending Inexperienced Products via Learning from Consumer Reviews","F. Wang; L. Chen","Dept. of Comput. Sci., Hong Kong Baptist Univ., Hong Kong, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","596","603","Most products in e-commerce are with high cost (e.g., digital cameras, computers) and hence less likely experienced by users (so they are called ""inexperienced products""). The traditional recommender techniques (such as user-based collaborative filtering and content-based methods) are thus not effectively applicable in this environment, because they largely assume that the users have prior experiences with the items. In this paper, we have particularly incorporated product reviews to solve the recommendation problem. We first studied how to utilize the reviewer-level weighted feature preferences (as learnt from their written product reviews) to generate recommendations to the current buyer, followed by exploring the impact of Latent Class Regression Models (LCRM) based cluster-level feature preferences (that represent the common preferences of a group of reviewers). Motivated by their respective advantages, a hybrid method that combines both reviewer-level and cluster-level preferences is introduced and experimentally compared to the other methods. The results reveal that the hybrid method is superior to the other variations in terms of recommendation accuracy, especially when the current buyer states incomplete feature preferences.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.209","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511947","Latent Class Regression Model;Recommender system;inexperienced products;product reviews;weighted feature preferences","","electronic commerce;information retrieval;pattern clustering;purchasing;recommender systems;regression analysis","LCRM based cluster-level feature preference;consumer reviews;e-commerce;hybrid method;incomplete feature preferences;inexperienced product recommendation;latent class regression model;product reviews;recommendation accuracy;recommendation problem;reviewer-level preferences;reviewer-level weighted feature preferences;written product reviews","","1","","27","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"FSMRank: Feature Selection Algorithm for Learning to Rank","H. J. Lai; Y. Pan; Y. Tang; R. Yu","School of Information Science and Technology, Sun Yat-Sen University, Guangzhou, China","IEEE Transactions on Neural Networks and Learning Systems","20130405","2013","24","6","940","952","In recent years, there has been growing interest in learning to rank. The introduction of feature selection into different learning problems has been proven effective. These facts motivate us to investigate the problem of feature selection for learning to rank. We propose a joint convex optimization formulation which minimizes ranking errors while simultaneously conducting feature selection. This optimization formulation provides a flexible framework in which we can easily incorporate various importance measures and similarity measures of the features. To solve this optimization problem, we use the Nesterov's approach to derive an accelerated gradient algorithm with a fast convergence rate <i>O</i>(1/<i>T</i><sup>2</sup>). We further develop a generalization bound for the proposed optimization problem using the Rademacher complexities. Extensive experimental evaluations are conducted on the public LETOR benchmark datasets. The results demonstrate that the proposed method shows: 1) significant ranking performance gain compared to several feature selection baselines for ranking, and 2) very competitive performance compared to several state-of-the-art learning-to-rank algorithms.","2162-237X;2162237X","","10.1109/TNNLS.2013.2247628","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6476738","Accelerated gradient algorithm;feature selection;generalization bound;learning to rank","Feature extraction;Joints;Machine learning algorithms;Optimization;Prediction algorithms;Training;Vectors","algorithm theory;computational complexity;convex programming;information retrieval;learning (artificial intelligence)","FSMRank;Rademacher complexity;accelerated gradient algorithm;fast convergence rate;feature selection algorithm;flexible framework;generalization bound;joint convex optimization formulation;learning to rank algorithm;optimization problem;public LETOR benchmark dataset;ranking errors","","7","","39","","20130308","June 2013","","IEEE","IEEE Journals & Magazines"
"Vocabulary of Quranic Concepts: A semi-automatically created terminology of Holy Quran","T. Mukhtar; H. Afzal; A. Majeed","Department of Computer Software Engineering, National University of Sciences and Technology (NUST), Islamabad, Pakistan","2012 15th International Multitopic Conference (INMIC)","20130502","2012","","","43","46","The identification and organization of terminology is the foremost step while organizing the domain knowledge for any domain as it is the terms and their inter-relationships that define the conceptual knowledge base. Quran, comprising the divine words of wisdom has been considered and used as prime source of knowledge and guidance for Muslims throughout the world for fourteen centuries. The concepts/topics discussed in Quran have been organized/indexed by many scholars which are used by Muslims who use them to search for guidance regarding various issues of daily life. In current era of information technology, various search services for Quranic topics are available online. They mostly use the terminologies (concepts hierarchy) manually built by scholars. In our work, we have used a semi-automatic approach to identify important concepts/topics from six English translations of Quran, and organized them into a hierarchical structure, named as Vocabulary of Quranic Concepts (VQC). CNC Value method of term recognition is used to identify significant concepts, which are then manually analyzed by domain expert, and are then organized into a hierarchy using the term-head principle. Due to extreme sensitivity of this work, complete automation of system is avoided and outcomes at all steps are manually analyzed. Currently, we have developed a vocabulary from translation of only second chapter of Quran (Al-Bakara). VQC is available at: https://sites.google.com/a/mcs.edu.pk/codteem/projects/qwn.","","Electronic:978-1-4673-2252-2; POD:978-1-4673-2249-2","10.1109/INMIC.2012.6511467","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511467","Natural Language Processing;Vocabulary of Quranic Concepts","","humanities;information retrieval;natural language processing","C-NC value method;English translations;Muslims;Quranic topics;VQC;domain knowledge;information technology;search services;semiautomatically created holy Quran terminology;term recognition;term-head principle;vocabulary of Quranic concepts","","2","","9","","","13-15 Dec. 2012","","IEEE","IEEE Conference Publications"
"Information Forensics: An Overview of the First Decade","M. C. Stamm; M. Wu; K. J. R. Liu","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, USA","IEEE Access","20130510","2013","1","","167","200","In recent decades, we have witnessed the evolution of information technologies from the development of VLSI technologies, to communication and networking infrastructure, to the standardization of multimedia compression and coding schemes, to effective multimedia content search and retrieval. As a result, multimedia devices and digital content have become ubiquitous. This path of technological evolution has naturally led to a critical issue that must be addressed next, namely, to ensure that content, devices, and intellectual property are being used by authorized users for legitimate purposes, and to be able to forensically prove with high confidence when otherwise. When security is compromised, intellectual rights are violated, or authenticity is forged, forensic methodologies and tools are employed to reconstruct what has happened to digital content in order to answer who has done what, when, where, and how. The goal of this paper is to provide an overview on what has been done over the last decade in the new and emerging field of information forensics regarding theories, methodologies, state-of-the-art techniques, major applications, and to provide an outlook of the future.","2169-3536;21693536","","10.1109/ACCESS.2013.2260814","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515027","Anti-forensics;information forensics;multimedia fingerprint;tampering detection","Computer Security;Data privacy;Digital forensics;Feature extraction;Fingerprint recognition;Forensics;Forgery;Information processing;Intellectual property;Multimedia communication;Very large scale integration","VLSI;authorisation;information retrieval;information technology;multimedia systems;standardisation;ubiquitous computing","VLSI technologies;authorized users;coding schemes;communication infrastructure;digital content;forensic methodologies;information forensics;information technologies;intellectual rights;multimedia compression;multimedia content retrieval;multimedia content search;multimedia devices;networking infrastructure;standardization;technological evolution","","49","","179","","","2013","","IEEE","IEEE Journals & Magazines"
"GPU-accelerated Burrow-Wheeler Transform for genomic data","Z. Zhao; J. Yin; W. Xiong","Sch. of Comput. Sci., Nat. Univ. of Defense Technol., Changsha, China","2012 5th International Conference on BioMedical Engineering and Informatics","20130504","2012","","","889","892","As the core of FM-index and compressed suffix array, the Burrows-Wheeler Transform (BWT) plays a key role in indexing genomic sequence data for pattern search. It is more space-efficient than suffix tree and suffix array and also has good searching performance. However, computing a BWT from a sequence in efficient space and time is challenging. In this article, we present a fast algorithm for computing BWT which leverages the computing power of graphics processing unit (GPU). It is based on a blockwise suffix sorting method which builds BWT block-by-block and uses a GPU to accelerate the suffix sorting procedures. Our GPU-accelerated algorithm only needs small memory space and experiments show that it is about 16 times faster than the original CPU implementation for computing BWT of human whole genome.","","Electronic:978-1-4673-1184-7; POD:978-1-4673-1183-0","10.1109/BMEI.2012.6513199","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513199","Burrows-Wheeler Transform;GPU;suffix array","","data compression;genomics;graphics processing units;indexing;information retrieval;sequences;sorting;wavelet transforms","BWT;Burrows-Wheeler transform;FM index;GPU accelerated algorithm;blockwise suffix sorting method;compressed suffix array;genomic sequence data indexing;graphics processing unit;memory space;pattern search","","0","","17","","","16-18 Oct. 2012","","IEEE","IEEE Conference Publications"
"An approach for accessing data from hidden web using intelligent agent technology","L. Singh; D. K. Sharma","Department of CEA, GLA University, Mathura, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","800","805","There is large amount of information available on web, which is hidden from users. This is because such information is not able to be accessed or indexed by traditional search engines. These search engines are only able to crawl information by following hypertext links. The forms which require login or any authorization process can be ignored by them. Hidden web refers to that deepest part of the Web which is not available for traditional Web crawlers. Obtaining the content from Hidden web is a challenging task. Today many web sites are containing pages that are dynamic in nature. This dynamic nature of web pages creates a problem for retrieving information for traditional web crawlers. The effort done to solve the given problem is discussed in brief. Then, a comparative study among the earlier defined architecture, considering various parameters, is also shown. By analyzing above methods a framework is proposed which uses an intelligent agent technology for accessing the hidden web.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514329","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514329","Hidden Web;Hidden Web Crawling;Hidden Web Databases;Intelligent Agent Technology","Crawlers;Databases;Feature extraction;Filling;Intelligent agents;Learning (artificial intelligence);Search engines","Web sites;authorisation;hypermedia;indexing;information retrieval;search engines;software agents","Web crawlers;Web sites;authorization process;data access approach;hidden Web;hypertext links;information crawling;information retrieval problem;intelligent agent technology;search engines","","0","","17","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Leveraging Crowd Knowledge for Software Comprehension and Development","L. Ponzanelli; A. Bacchelli; M. Lanza","REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","2013 17th European Conference on Software Maintenance and Reengineering","20130415","2013","","","57","66","Question and Answer (Q&A) services, such as Stack Overflow, rely on a community of programmers who post questions, provide and rate answers, to create what is termed ""crowd knowledge"". As a consequence, these services archive voluminous and potentially useful information to help developers to solve programming-specific issues. Programmers tap into this crowd knowledge through web browsers. This requires them to step out of their integrated development environments (IDE), formulate a query, inspect the returned results and manually port the solution back to the IDE. We present an integrated and largely automated approach to assist programmers who want to leverage the crowd knowledge of Q&A services. We give a form to our approach by implementing Seahawk, an Eclipse plugin. Seahawk automatically formulates queries from the current context in the IDE, and presents a ranked and interactive list of results. Seahawk lets users identify individual discussion pieces and import code samples through simple drag & drop. Users can also link Stack Overflow discussions and source code persistently. We performed an evaluation of Seahawk, with promising results.","1534-5351;15345351","Electronic:978-0-7695-4948-4; POD:978-1-4673-5833-0","10.1109/CSMR.2013.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498455","Q&amp;A services;recommendation systems","Engines;Java;Navigation;Search engines;Software;User interfaces;XML","application program interfaces;query processing;question answering (information retrieval);software engineering","IDE;Seahawk Eclipse plugin;Web browser;crowd knowledge;integrated development environment;query formulation;question-and-answer service;software comprehension;software development;stack overflow service","","14","","32","","","5-8 March 2013","","IEEE","IEEE Conference Publications"
"Cooperative Caching Scheme for Content Oriented Networking","R. Liu; H. Yin; X. Cai; G. Zhu; L. Yu; Q. Fan; J. Xu","Dept. of Electronics and Information Engineering, Huazhong University of Science and Technologyu, Wuhan, PR China","IEEE Communications Letters","20130501","2013","17","4","781","784","In this letter, we propose a novel cooperative caching scheme for the next-generation Internet - content oriented networking (CON), trying to minimize the content access delay for mobile users. We formulate the caching problem as a mixed integer programming model and propose a heuristic solution based on Lagrangian relaxation. Simulation results show that this scheme can greatly reduce content access delay.","1089-7798;10897798","","10.1109/LCOMM.2013.020513.121680","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459497","Lagrangian relaxation;Mobile Internet;content delivery;content oriented networking;cooperative caching","Cooperative caching;Delay;Internet;Mobile communication;Mobile computing;Multimedia communication;Next generation networking","Internet;cache storage;cooperative communication;information retrieval;integer programming;next generation networks","CON;Lagrangian relaxation-based heuristic solution;caching problem;content access delay;content oriented networking;cooperative caching scheme;mixed integer programming model;mobile users;next-generation Internet","","6","","12","","20130211","April 2013","","IEEE","IEEE Journals & Magazines"
"A stand-alone and SMS-based approach for authentication using mobile phone","S. Indu; T. N. Sathya; V. Saravana Kumar","Veltech Multitech Dr. Rangarajan Dr. Sakunthala Eng. Coll., Chennai, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","140","145","A secure network partially depends on user authentication and unfortunately authentication schemes used at present are not utterly secure. Most of the systems today rely on static passwords to verify the user's identity. This paper describes a method of implementing two factor authentication using mobile phones. The proposed system involves using a mobile phone as a software token for One Time Password generation. OTP algorithm powered with user's unique identifications like International Mobile Equipment Identification and Subscriber Identification Module; makes a finite alphanumeric token valid for a session and for a single use. The generated One Time Password is valid for only a short user defined period of time and is generated by factors that are unique to both, the user and the mobile device itself. Additionally, an SMS-based mechanism is implemented as both a backup mechanism for retrieving the password and as a possible mean of synchronization.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508205","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508205","One Time Password (OTP);Personal Identification Number(PIN);Short Message Service(SMS)","Authentication;Databases;Hardware;Mobile communication;Mobile handsets;Servers;Software","authorisation;computer network security;electronic messaging;information retrieval;mobile computing;mobile handsets","International Mobile Equipment Identification;OTP algorithm;SMS-based approach;backup mechanism;finite alphanumeric token;mobile device;mobile phone;one time password generation;password retrieval;secure network;short message service;software token;stand-alone approach;static passwords;subscriber identification module;two-factor authentication;unique identification;user identity verification","","2","","14","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Optimizing content retrieval delay for LT-based distributed cloud storage systems","Haifeng Lu; Chuan Heng Foh; Yonggang Wen; Jianfei Cai","School of Computer Engineering, Nanyang Technological University, Singapore 639798","2012 IEEE Global Communications Conference (GLOBECOM)","20130422","2012","","","1920","1925","Among different setups of cloud storage systems, fountain-codes based distributed cloud storage system provides reliable online storage solution through placing coded content fragments into multiple storage nodes. Luby Transform (LT) code is one of the popular fountain codes for storage systems due to its efficient recovery. However, to ensure high success decoding of fountain codes based storage, retrieval of additional fragments is required, and this requirement introduces additional delay, which is critical for content retrieval or downloading applications. In this paper, we show that multiple-stage retrieval of fragments is effective to reduce the content-retrieval delay. We first develop a delay model for various multiple-stage retrieval schemes applicable to our considered system. With the developed model, we study optimal retrieval schemes given the success decodability requirement. Our numerical results demonstrate that the content-retrieval delay can be significantly reduced by optimally scheduling packet requests in a multi-stage fashion.","1930-529X;1930529X","Electronic:978-1-4673-0921-9; POD:978-1-4673-0920-2; USB:978-1-4673-0919-6","10.1109/GLOCOM.2012.6503396","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6503396","","","cloud computing;information retrieval;scheduling;storage management;transforms","LT-based distributed cloud storage system;Luby transform;content fragment;content retrieval delay optimization;decodability requirement;fountain-codes based distributed cloud storage system;fragment retrieval;multiple-stage retrieval scheme;online storage solution;scheduling packet","","0","","15","","","3-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Indoor Localization and Guidance Using Portable Smartphones","O. A. Hammadi; A. A. Hebsi; M. J. Zemerly; J. W. P. Ng","Electr. & Comput. Eng. Dept., Khalifa Univ. of Sci., Technol. & Res., Sharjah, United Arab Emirates","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","337","341","Indoor guidance is becoming a significant issue with the increasing number of buildings. This paper describes an Android based indoor map guidance system that assists and guides visitors inside public buildings (e.g. schools, shopping malls, airports, museums, exhibition centers). It utilizes NFC (Near Field Communication) technology and QR (Quick Response) Codes, which are low cost, to determine the location as well as to provide navigation within the buildings. Also, it provides a variety of helpful features such as finding destination, calculating shortest path, storing car parking location, giving feedback to building management, entering surveys for restaurants and coffee shops, finding nearest toilet and making donation. In addition, the system is bilingual and available in both English and Arabic versions. The developed system relies on a server that contains its web server, map server and spatial database. For wide accessibility, the whole system is developed using open source and freely-available software. For example, (a) the Android SDK is used to develop the client interface, (b) the Apache server is used for the web server, (c) Google sketch up and Quantum GIS are used to draw the floor plans, (d) Postgre SQL/PostGIS is used for spatial database to store the drawn floor plans, and (e) MapServer (MS4W) is used for map server to retrieve and draw the stored floor plans from the spatial database. Thus far, the developed mobile application has been fully evaluated and validated for use in a smart campus environment, which has been encapsulated in a test case study delineated herein.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.262","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511704","Dijkstra;NFC;QR Codes;iCampus;indoor navigation;smartphones","","Internet;Linux;buildings (structures);geographic information systems;information retrieval;mobile computing;navigation;public domain software;smart phones;user interfaces;visual databases","Android SDK;Android based indoor map guidance system;Apache server;Google sketchup;MapServer;NFC technology;PostGIS;PostgreSQL;QR Codes;Quantum GIS;Web server;bilingual system;client interface;floor plans;freely-available software;indoor localization;navigation;near field communication;open source software;portable smartphones;public buildings;quick response codes;smart campus environment;spatial database","","7","","11","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Integrating WSN with web services for patient's record management using RFID","M. Ananthi; M. R. Sumalatha","Department of Information Technology, Sairam Engineering College, Anna University, Chennai, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","605","609","Web service supports interoperability for collecting, storing, manipulating and retrieving data from heterogeneous environments. The Wireless Sensor Network is a resource-constrained device, which is low cost, low power and small in size and used in various applications such as industrial control & monitoring, environmental sensing, health care, etc. The main intent is to design a middleware that hides the complexity of accessing the sensor network environment and developing an application for sensor web enablement. This concept holds great importance because integrating wireless sensor network into IP-based systems are still a challenging issue. It is very important to collect patient's details during the emergency period. To create a web service to manage patient's personal data with the help of Radio Frequency Identification Tag (RFID), Web Service is dedicated to collecting, storing, manipulating, and making available clinical information. Context-aware services are needed for searching information more accurately and to produce highly impeccable output.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514296","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514296","Context-aware;RFID;Semantic Web Service (SWS);Sensor Web Enablement (SWE);Service-Oriented Middleware (SOM)","Medical services;Radiofrequency identification;Semantics;Service-oriented architecture;Wireless sensor networks","IP networks;Web services;information retrieval;medical information systems;middleware;open systems;radiofrequency identification;records management;ubiquitous computing;wireless sensor networks","IP-based systems;RFID tag;WSN;Web services;clinical information;context-aware services;data collection;data manipulation;data retrieval;data storage;emergency period;heterogeneous environments;information search;interoperability;middleware design;patient detail collection;patient personal data management;patient record management;radio frequency identification tag;resource-constrained device;sensor Web enablement;wireless sensor network","","3","","18","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Adaptive Data Transfers that Utilize Policies for Resource Sharing","J. Gu; D. Smith; A. L. Chervenak; A. Sim","Lawrence Berkeley Nat. Lab., Berkeley, CA, USA","2012 SC Companion: High Performance Computing, Networking Storage and Analysis","20130411","2012","","","547","555","With scientific data collected at unprecedented volumes and rates, the success of large scientific collaborations requires that they provide distributed data access with improved data access latencies and increased reliability to a large user community. The goal of the ADAPT (Adaptive Data Access and Policy-driven Transfers) project is to develop and deploy a general-purpose data access framework for scientific collaborations that provides fine-grained and adaptive data transfer management and the use of site and VO policies for resource sharing. This paper presents our initial design and implementation of an adaptive data transfer framework. We also present preliminary performance measurements showing that adaptation and policy improve network performance.","","Electronic:978-0-7695-4956-9; POD:978-1-4673-6218-4","10.1109/SC.Companion.2012.78","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495860","adaptation;data transfers;passive performance monitoring;policy;resource sharing","","data handling;information retrieval","ADAPT project goal;VO policies;adaptive data access-and-policy-driven transfers project;data access latency improvement;data access reliability improvement;distributed data access;fine-grained adaptive data transfer management;network performance improvement;resource sharing policies;scientific collaborations;scientific data collection;user community","","3","","23","","","10-16 Nov. 2012","","IEEE","IEEE Conference Publications"
"SmartContent: A Self-Protecting and Context-Aware Active Content","A. E. Tchao; G. D. M. Serugendo","CUI, Univ. de Geneve, Carouge, Switzerland","2012 IEEE Sixth International Conference on Self-Adaptive and Self-Organizing Systems Workshops","20130415","2012","","","151","156","The development of communications systems in general, and the Internet in particular, has given billions of people the opportunity to connect and share content with audiences to which they would otherwise never have had access to. Nowadays, anyone can publish and share content, whether personal or not, on the Internet. In addition, the ubiquitous ness of mobile devices makes it possible to access content anywhere, at anytime on different platforms. All of this often leads to situations of potential intentional or unintentional misuse of contents as well as privacy problems. Traditional solutions for these problems such as Digital Rights Management have proven not to be appropriate because they rely heavily on costly and centralized external systems or infrastructure. In this paper, we propose Smart Content, a novel approach for content protection and privacy. Smart Content acts autonomously and embeds with the content the notion of context and policy. This article presents the general model of Smart Content and an example implementation.","","Electronic:978-0-7695-4895-1; POD:978-1-4673-5153-9","10.1109/SASOW.2012.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498395","adaptability;context;negative selection;policy;self-protection","","Internet;content management;data privacy;information retrieval;mobile computing;security of data","Internet;SmartContent;communication system;content access;content misuse;content privacy;content protection;content publishing;content sharing;context-aware active content;digital rights management;mobile device ubiquitousness;privacy problem;self-protecting active content","","0","1","13","","","10-14 Sept. 2012","","IEEE","IEEE Conference Publications"
"Effective Use of QR Codes in Religious Tourism","S. Alshattnawi","Comput. Sci. Dept., Yarmouk Univ., Irbid, Jordan","2012 International Conference on Advanced Computer Science Applications and Technologies (ACSAT)","20130516","2012","","","497","501","Quick Response (QR) code is a two dimensional codes that is read and accessed via mobile devices equipped with camera and QR reader. Most smart phones have a camera and the QR reader is easy to install and is available for different platforms. Therefore, the simplicity of this type of code makes it effective in many fields. This paper presents the idea of using QR codes combined with a mobile telephone in religious tourism. Generally, the religious ritual has a set of ordered activities which visitors must follow exactly. QR codes will help the visitors to follow the ordered steps and help them to find information quickly without requiring them to search in any printed brochure. We choose the pilgrimage to the holy city, Mecca in Saudi Arabia, as an example to apply our idea.","","Electronic:978-0-7695-4959-0; POD:978-1-4673-5832-3","10.1109/ACSAT.2012.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516405","QR codes;Religious tourism;smart phone;tourist guide","","cameras;information retrieval;mobile computing;smart phones;travel industry","Mecca;QR code;QR reader;Saudi Arabia;camera;holy city;mobile devices;mobile telephone;printed brochure search;quick response code;religious tourism;two dimensional codes","","1","","17","","","26-28 Nov. 2012","","IEEE","IEEE Conference Publications"
"Recommendation of optimized web pages to users using Web Log mining techniques","R. Bhushan; R. Nath","Department of Computer Science & Application, Kurukshetra University, Haryana, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","1030","1033","Now a days, user rely on the web for information, but the currently available search engines often gives a long list of results, much of which are not always relevant to the user's requirement. Web Logs are important information repositories, which record user activities on the search results. The mining of these logs can improve the performance of search engines, since a user has a specific goal when searching for information. Optimized search may provide the results that accurately satisfy user's specific goal for the search. In this paper, we propose a web recommendation approach which is based on learning from web logs and recommends user a list of pages which are relevant to him by comparing with user's historic pattern. Finally, search result list is optimized by re-ranking the result pages. The proposed system proves to be efficient as the pages desired by the user, are on the top in the result list and thus reducing the search time.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514368","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514368","Ranking etc.;Web log;recommendations;web usage mining","Computer architecture;Conferences;Data mining;IP networks;Navigation;Search engines;Web pages","Web sites;data mining;information retrieval;learning (artificial intelligence);recommender systems;search engines","Web log mining technique;information repository;learning;optimized Web page recommendation;result page reranking;search engines;search result list;user historic pattern","","2","","10","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Schema recognition using modified weighted direction index histogram for medical document retrieval systems","H. Kajiwara; H. Kawanaka; K. Yamamoto; H. Takase; S. Tsuruoka","Graduate School of Engineering, Mie University, Tsu, Mie, 514-8507 Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","803","806","This paper discusses a schema recognition method considering annotations to improve recognition accuracy of the previous method. Previously, we studied document image recognition methods for medical/clinical documents and their applications. In addition, we also discussed the method to recognize schemas in the document, because they had important key information for medical document retrieval. However, annotations made a feature vector change drastically, as a result, reduction of the recognition accuracy was caused. In this paper, annotations were automatically detected, and the feature vectorconsidering them was generated. We conducted evaluation experiments using actual schemas used in Mie University Hospital. After experiments, the recognition accuracy of the proposed method was 96%.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505384","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505384","","","document image processing;image recognition;image retrieval;information retrieval;medical information systems;vectors","clinical document;document image recognition;feature vector;medical document retrieval system;schema recognition;weighted direction index histogram","","0","","10","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"A hypothesis-generating support system using medical records for clinical knowledge acquisition","K. Okamoto; H. Tanaka; T. Takemura; N. Kume; T. Kuroda; H. Yoshihara","Div. of Med. Inf. Technol. & Adm. Planning, Kyoto Univ. Hosp., Kyoto, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1130","1133","The authors developed a support system for finding hypotheses about clinical knowledge from medical records accumulated electronically in hospital information systems. In order to support the development of various hypotheses, we matched a variety of medical and clinical information with each case and utilized the information comprehensively. This information includes clinical documents, examination information, medical practice information, and other minor factors. The system uses coverage-oriented views showing targeted medical records, in whole or in part. We selected appropriate methods to provide coverage-oriented views of natural language data, numerical data, and coded data, making use of snippets, graphs, and tag clouds, respectively. In addition, users can narrow the search results utilizing keywords, ranges of values, and labels of codes, respectively. The coverage-oriented views and the narrowing methods utilizing the views make the retrieval system interactive. We predicted that users could find hypotheses by grasping features of targeted cases using coverage-oriented views based on interactive retrieval. We evaluated the developed prototype in terms of its potential for helping users find hypotheses about clinical knowledge and we received positive responses from doctors.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505123","Clinical knowledge;Coverage-oriented views;Hypothesis-finding;Interactive retrieval;Medical records","","computer graphics;decision support systems;information retrieval systems;knowledge acquisition;medical information systems","clinical document;clinical information;clinical knowledge acquisition;coded data;coverage-oriented view;examination information;graphs;hospital information system;hypothesis-generating support system;information utilization;medical information;medical practice information;medical records;natural language data;numerical data;retrieval system;snippets;tag cloud","","0","","5","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Mining Data from National Center for Biotechnology Information System","W. Shao; J. Yang","Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China","2012 Fourth International Symposium on Information Science and Engineering","20130411","2012","","","273","275","The National Center for Biotechnology Information (NCBI) is a division of the National Library of Medicine (NLM) at the National Institutes of Health (NIH). Furthermore it is a big system of bioinformatics, advancing science and health by providing access to biomedical and genomic information. It provides researchers with a variety of biology databases, data retrieval resources, data analysis resources and so on which are of great importance. This paper summarizes the common functions of NCBI to help make fully use of NCBI for biology research.","2160-1283;21601283","Electronic:978-0-7695-4951-4; POD:978-1-4673-5680-0","10.1109/ISISE.2012.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495344","NCBI;bioinformatics;biology database;biology research","","bioinformatics;data analysis;data mining;database management systems;genomics;information retrieval","NCBI;NIH;NLM;National Center For Biotechnology Information System;National Institutes of Health;National Library of Medicine;bioinformatics;biology databases;biology research;biomedical information;data analysis resources;data mining;data retrieval resources;genomic information","","0","","8","","","14-16 Dec. 2012","","IEEE","IEEE Conference Publications"
"ALP: An authentication and leak prediction model for Cloud Computing privacy","M. Farhatullah","Department of CSE, Dr.L.Bullayya College of Engineering for Women, Visakhapatnam, India","2013 3rd IEEE International Advance Computing Conference (IACC)","20130513","2013","","","48","51","The current paper is an attempt to integrate privacy preservation with privacy leak detection in the context of text mining. This may be considered as extending the Storage as a Service feature of Cloud Computing wherein the user in the role of content creator submits the documents to be stored. There exists a facility to cluster these documents based on the concept-based mining algorithm. The clustered documents are usually available in the form a tree. When a user in the role of a subscriber requests to access a document, this request for access will have to go through an authentication procedure based on the Leakage Free Redactable Signature Scheme. Access control information is being maintained in the form of a Cloud user access control list. A privacy detection leak module which detects privacy leaks depending upon the pattern of previous privacy leaks is also being proposed. This information is then used to update the cloud user access control list and users responsible for privacy leaks are prevented from accessing the cloud service. The current document being requested by the user together with the information from the access control list is used to decide which part of the redacted trees have to be made available to the user as a response. Thus this combination of this authentication procedure and privacy leak detection can be used to ensure the privacy of the sensitive information stored by the user in the cloud.","","Electronic:978-1-4673-4529-3; POD:978-1-4673-4527-9","10.1109/IAdCC.2013.6514192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514192","Authentication;Cloud Computing;Pattern Recognition;Privacy","Authentication;Cloud computing;Computational modeling;Leak detection;Pattern recognition;Predictive models;Privacy","cloud computing;data mining;data privacy;digital signatures;information retrieval;storage management;text analysis;tree data structures","ALP;access control information;authentication and leak prediction model;cloud computing privacy;cloud service access;cloud user access control list;concept-based mining algorithm;content creator;document access;document storage;leakage free redactable signature scheme;privacy leak detection;privacy leak detection module;privacy preservation;sensitive information privacy;storage as a service feature;subscriber requests;text mining;tree","","1","","10","","","22-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"The mobile environment of EHR browsing verified on tablet terminal","N. Kume; Y. Hirayama; N. Ohboshi; K. Okamoto; T. Takemura; K. Araki; H. Yoshihara","Div. of Med. Inf., Technol. & Adm. Planning, Kyoto Univ. Hosp., Kyoto, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1374","1377","Recently, medical records can be accessible from Electric Health Record(EHR) in some regions such as e-maiko.net. EHR provides patient's records in patient's hands even though the authorized access requires the information literacy. Therefore, this research aims to provide an environment of short-step EHR access. The authors propose the EHR design to separate the data handling layer and the visualization layer. By the construction of the web access data interface and the visualization of the device optimized viewer, the system can select the visual interface depends on the situation and the viewer devices. Especially, the environment was implemented on the e-maiko.net. Also, the iPad application was provided for the experiment of the optimized visualization. The experiment was carried out with 9 subjects who are the expert of computers, are the novice of tablet terminals. The experiment compaired the perfomance of the access time to find a document between the conventional web interface and the iPad viewer interface. The result of experiment suggested that the iPad viewer makes the EHR access much more easier than the conventional web interface. Also, the result of interviews, the half of the subjects pointed out the security concerns to use mobile devices to access EHR. On the other hand, the rest of the subjects insisted the benefit of short-step access rather than the security issue. For the conclusion, the proposed system will be acceptable as an EHR browsing environment if the system is enough explained about the security issue.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505260","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505260","Asynchronous Communication;Electronic Health Record;Medical Markup Language(MML);Web Service;iPad","","Web services;authorisation;data handling;data visualisation;information retrieval;medical information systems;mobile computing;mobile handsets;notebook computers;user interfaces","EHR browsing environment;EHR design;Web access data interface;Web access data visualization;access time perfomance;authorized access;data handling layer;data visualization layer;device optimized viewer;e-maiko.net;electric health records;iPad application;iPad viewer interface;information literacy;medical records;mobile environment;patient records;short-step EHR access;tablet terminals;visual interface","","0","","5","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"A Comparison Study for Novelty Control Mechanisms Applied to Web News Stories","A. Verheij; A. Kleijn; F. Frasincar; F. Hogenboom","Econometric Inst., Erasmus Univ. Rotterdam, Rotterdam, Netherlands","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","431","436","In this paper we evaluate several novelty control mechanisms for ranking Web news articles depicting a story. These mechanisms rank individual news items based on a novelty measure using as context the items which have been previously reviewed. An evaluation within the Hermes news personalization framework is performed for pair wise and non-pair wise novelty control mechanisms based on various distance measures and vector-based news representations. On average, the most effective distance measure is Kullback-Leibler and the best performing news representation vector uses named entities.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511919","Novelty control;Web news;ranking","","Internet;electronic publishing;information retrieval;vectors","Hermes news personalization framework;Kullback-Leibler distance measure;Web news article ranking;Web news stories;distance measures;named entities;nonpairwise novelty control mechanism;pairwise novelty control mechanisms;vector-based news representations","","1","","29","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Novel dynamic shadow approach for fault tolerance in mobile agent systems","R. Hans; R. Kaur","Department of Information Technology, D.A.V. Institute of Engineering and Technology, Jalandhar, Punjab, India","2012 6th International Conference on Signal Processing and Communication Systems","20130425","2012","","","1","6","Mobile agents are the processes that can migrate from one machine of a system to another machine in order to satisfy requests made by their clients. Since mobile agent moves from one server to another server in an itinerary it is easily prone to various faults like server crash or agent crash, which makes fault tolerance one of the main issues for reliability in information retrieval applications. In this paper we have also proposed a new fault tolerance approach to tackle the problem of server crash by creating a cloned copy of the original agent. The clone follows the original agent in the itinerary so that in case of failure of original agent, it can be recovered from its clone. We have also added checkpointing, so as to limit the roll back of the agent in case of failure of both original agent and its clone. We have implemented the proposed approach on the Aglets mobile agent system and evaluated in terms of parameters such as total trip time, successful migration time, checkpoint time. The results show the improvement in the performance by improving total trip time.","","Electronic:978-1-4673-2393-2; POD:978-1-4673-2392-5; USB:978-1-4673-2391-8","10.1109/ICSPCS.2012.6507945","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507945","Mobile agents;checkpointing;fault tolerance;rollback recovery;serve crash failure","","checkpointing;client-server systems;fault tolerant computing;information retrieval;mobile agents","Aglets mobile agent system;agent crash;agent roll back;checkpoint time;checkpointing;client request satisfaction;dynamic shadow approach;failure;fault tolerance;information retrieval application;migration time;reliability;rollback recovery;server crash;trip time","","1","","17","","","12-14 Dec. 2012","","IEEE","IEEE Conference Publications"
"The pragmatics information extraction based on BP neural network","D. Liu; M. Jiang","School of Humanities and Social Sciences, Lab. of Computational Linguistics, Tsinghua University, Beijing, China","2012 IEEE 11th International Conference on Signal Processing","20130404","2012","2","","1256","1259","This article describes a method that uses the BP neural network to extract the pragmatics information from the conversational corpus. Then cluster the texts based on the pragmatics information matrix that generated by the BP neural network. And, compare with the original term-document matrix, the pragmatics information matrix improve the clustering results obviously.","2164-5221;21645221","Electronic:978-1-4673-2197-6; POD:978-1-4673-2196-9","10.1109/ICoSP.2012.6491804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6491804","BP neural network;conversational corpus;pragmatics;text clustering","","backpropagation;information retrieval;matrix algebra;natural language processing;neural nets;pattern clustering;text analysis","BP neural network;nature language processing;pragmatics information extraction;pragmatics information matrix;text clustering","","0","","12","","","21-25 Oct. 2012","","IEEE","IEEE Conference Publications"
"An enhanced pre-processing technique for web log mining by removing web robots","P. Nithya; P. Sumathi","Manonmaniam Sundaranar University, Tirunelveli, Tamil Nadu, India","2012 IEEE International Conference on Computational Intelligence and Computing Research","20130502","2012","","","1","4","Nowadays, internet becomes useful source of information in day-to-day life. It creates huge development of World Wide Web in its quantity of interchange and its size and difficulty of websites. Web Usage Mining (WUM) is one of the main applications of data mining, artificial intelligence and so on to the web data and forecast the user's visiting behaviors and obtains their interests by investigating the samples. Since WUM directly involves in large range of applications, such as, ecommerce, e-learning, Web analytics, information retrieval etc. Weblog data is one of the major sources which contain all the information regarding the users visited links, browsing patterns, time spent on a particular page or link and this information can be used in several applications like adaptive web sites, modified services, customer summary, pre-fetching, generate attractive web sites etc. There are several problems related with the existing web usage mining approaches. Existing web usage mining algorithms suffer from difficulty of practical applicability. So, a novel research is necessary for the accurate prediction of future performance of web users with rapid execution time. WUM consists of preprocessing, pattern discovery and pattern analysis. Log data is characteristically noisy and unclear. Hence, preprocessing is an essential process for effective mining process. In this paper, a novel pre-processing technique is proposed by removing local and global noise and web robots. Anonymous Microsoft Web Dataset and MSNBC.com Anonymous Web Dataset are used for estimating the proposed preprocessing technique.","","Electronic:978-1-4673-1344-5; POD:978-1-4673-1342-1","10.1109/ICCIC.2012.6510325","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510325","Content Path Set;Data Cleaning;Path Completion;Preprocessing;Travel Path set","","Internet;Web sites;data analysis;data mining;information retrieval;pattern recognition","Anonymous Microsoft Web Dataset;Internet;MSNBC.com Anonymous Web Dataset;WUM;Web analytics;Web data;Web log mining;Web robot removal;Web usage mining algorithm;World Wide Web;adaptive Web sites;artificial intelligence;browsing pattern;customer summary;data mining;e-commerce;e-learning;enhanced preprocessing technique;global noise removal;information retrieval;information source;local noise removal;log data preprocessing;mining process;pattern analysis;pattern discovery;prefetching;user interest;user visiting behavior forecasting;visited links","","2","","21","","","18-20 Dec. 2012","","IEEE","IEEE Conference Publications"
"Community detection in an integrated Internet of Things and social network architecture","S. Misra; R. Barthwal; M. S. Obaidat","School of Information Technology, Indian Institute of Technology Kharagpur - 721302, West Bengal, India","2012 IEEE Global Communications Conference (GLOBECOM)","20130422","2012","","","1647","1652","In this paper, we propose a community detection scheme in an integrated Internet of Things (IoT) and Social Network (SN) architecture. The paper takes a graph mining approach to solve the problem in complex network of IoT and SN. A number of pieces of research literature exist on community detection in SNs; however, no work specifically on integrated IoT and SN architecture addresses this issue. The existing community detection approaches have not considered things into account. We propose the scheme, Community Detection in an Integrated IoT and SN (CDIISN) in which we divide the nodes/actors in complex networks into basic nodes and IoT nodes, and execute the community detection algorithm. We consider two nodes to be in a community, only if the nodes are at most one hop apart and have at least two mutual friends. The smallest community in our case is a subgraph with a cycle of length four. In our approach, a node can be part of multiple communities, and it works well for weighted graphs. Once communities are extracted, we use an access control scheme, based on which access to nodes is provided. This approach of community detection in an integrated environment would find tremendous use in the future, because in the case of any search operation performed by any node, the results obtained intra-community are more relevant than inter-community.","1930-529X;1930529X","Electronic:978-1-4673-0921-9; POD:978-1-4673-0920-2; USB:978-1-4673-0919-6","10.1109/GLOCOM.2012.6503350","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6503350","Clustering;Community detection;Internet of things;social network","","Internet of Things;authorisation;complex networks;data mining;information retrieval;network theory (graphs);social networking (online)","CDIISN;Internet of Things;access control scheme;community detection algorithm;community detection in an integrated IoT and SN;community extraction;complex network problem;graph mining;search operation;social network architecture;subgraph;weighted graphs","","8","","11","","","3-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Automatic keyword annotation using newspapers","T. Takada; M. Arai; T. Takagi","Department of Computer Science, Meiji University, 1-1-1 Higashimita, Tama-Ku, Kawasaki-Shi, Kanagawa, 214-8571, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1752","1756","An index term that affects the precision of retrieval is important for a retrieval system. Retrieval systems usually use a method that automatically extracts a search word from a document, but a method that gives keywords as search words manually is also necessary for newspapers. Therefore, we propose the Nikkei annotator system, which is based on the model of the human brain and learns patterns of past keyword annotation and automatically outputs keywords that are preferred by us, and verify the superiority of this system by comparing it with comparable approaches.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505157","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505157","Confabulation-Theory;Keyword annotation;The Nikkei","","document handling;information retrieval systems","Nikkei annotator system;index term;keyword annotation;newspaper;retrieval system;search word extraction","","0","","9","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Extracting Top Hot Hits in Biomedicine from PubMed","Y. Zhou","Sch. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China","2012 Fourth International Symposium on Information Science and Engineering","20130411","2012","","","266","269","PubMed is a database used to retrieval biomedical literatures. It, however, can't help us find out the hot topics. Nowadays, there are some researchers working on the comparative analysis about different tools as same as PubMed, and some other ones bend themselves to data mining on one special field. Our research is finding out the hot topics by analyzing the numbers of publications of some keywords. Here we referred to Time Magazine to list 34 keywords, used E-utilities to get the numbers of publications from PubMed automatically. By analyzing these numbers we can then conclude the hot topics in recent years.","2160-1283;21601283","Electronic:978-0-7695-4951-4; POD:978-1-4673-5680-0","10.1109/ISISE.2012.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495342","Biomedicine;E-utilities;Information extraction;PubMed;hot topics","","data mining;electronic publishing;information retrieval;medical information systems","PubMed;Time Magazine;biomedical literature retrieval;biomedicine;data mining;database;e-utilities;hot topic;keyword;publication;top hot hit extraction","","1","","15","","","14-16 Dec. 2012","","IEEE","IEEE Conference Publications"
"Dynamic acquisition and Retrieval Tool (DART) for comet sample return","M. Badescu; R. Bonitz; A. Ganino; N. Haddad; P. Walkemeyer; P. Backes; L. Shiraishi; E. Kulczycki; N. Aisen; C. M. Dandino; B. S. Cantrell; W. Gallagher; J. Shevin","Jet Propulsion Laboratory, 4800 Oak Grove Dr., Pasadena, CA 91109, USA","2013 IEEE Aerospace Conference","20130513","2013","","","1","12","The 2011 Decadal Survey for planetary science released by the National Research Council of the National Academies identified Comet Surface Sample Return (CSSR) as one of five high priority potential New Frontiers-class missions in the next decade. The main objectives of the research described in this publication are: develop a concept for an end-to-end system for collecting and storing a comet sample to be returned to Earth; design, fabricate and test a prototype Dynamic Acquisition and Retrieval Tool (DART) capable of collecting 500 cc sample in a canister and ejecting the canister with a predetermined speed; identify a set of simulants with physical properties at room temperature that suitably match the physical properties of the comet surface as it would be sampled. We propose the use of a DART that would be launched from the spacecraft to impact and penetrate the comet surface. After collecting the sample, the sample canister would be ejected at a speed greater than the comet's escape velocity and captured by the spacecraft, packaged into a return capsule and returned to Earth. The DART would be composed of an inner tube or sample canister, an outer tube, a decelerator, a means of capturing and retaining the sample, and a mechanism to eject the canister with the sample for later rendezvous with the spacecraft. One of the significant unknowns is the physical properties of the comet surface. Based on new findings from the recent Deep Impact comet encounter mission, we have limited our search of solutions for sampling materials to materials with 10 to 100 kPa shear strength in loose or consolidated form. As the possible range of values for the comet surface temperature is also significantly different than room temperature and testing at conditions other than the room temperature can become resource intensive, we sought sample simulants with physical properties at room temperature similar to the expected physical properties of the comet surface material. The chose- DART configuration, the efforts to identify a test simulant and the properties of these simulants, and the results of the preliminary testing will be described in this paper.","1095-323X;1095323X","Electronic:978-1-4673-1813-6; POD:978-1-4673-1812-9","10.1109/AERO.2013.6496920","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496920","","Actuators;Iris;Optical devices","artificial satellites;asteroids;comets;data acquisition;information retrieval;planetary satellites;planetary surfaces;shear strength","DART;Earth;comet surface material;comet surface sample return;comet surface temperature;decelerator;deep impact comet encounter mission;dynamic acquisition and retrieval tool;end-to-end system;outer tube;planetary science;pressure 10 kPa to 100 kPa;return capsule;sample canister;sampling material;shear strength;spacecraft","","1","","9","","","2-9 March 2013","","IEEE","IEEE Conference Publications"
"Elasticat: A load rebalancing framework for cloud-based key-value stores","X. Qin; W. Wang; W. Zhang; J. Wei; X. Zhao; T. Huang","Institute of Software, Chinese Academy of Sciences","2012 19th International Conference on High Performance Computing","20130425","2012","","","1","10","The problem of load rebalancing is an important issue for cloud-based key-value stores. However, the new virtualization environment and the store's stateful feature make this classical issue more challenging. In this paper, we build a new load rebalancing framework for cloud-based key-value stores, namely ElastiCat. It can be used for auto reconfiguring the store system with minimal costs and no disruption to the availability of the service. To evaluate and minimize the rebalancing costs, we firstly build two interference-aware prediction models to predict the data migration time and performance impact for each action using statistical machine learning and then create a cost model to strike a right balance between them. A cost-aware rebalancing algorithm is designed to utilize the cost model and balance rate to create a rebalancing plan and guide the choice of possible rebalancing actions. To maintain the availability of storage service, we propose a lightweight piggy-back based data access protocol. Finally, we demonstrate the effectiveness of the framework as well as the cost model using YCSB.","","Electronic:978-1-4673-2371-0; POD:978-1-4673-2372-7","10.1109/HiPC.2012.6507481","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507481","VM interference;key-value store;load rebalancing","","access protocols;cloud computing;information retrieval;learning (artificial intelligence);resource allocation;virtualisation","ElastiCat;YCSB;cloud-based key-value stores;cost-aware rebalancing algorithm;data migration time;interference-aware prediction models;lightweight piggy-back based data access protocol;load rebalancing framework;load rebalancing problem;rebalancing plan;statistical machine learning;storage service;virtualization environment","","2","","21","","","18-22 Dec. 2012","","IEEE","IEEE Conference Publications"
"A study on template extraction","S. Pushpa; D. Kanagalatchumy","Dept. of IT, Sri Manakula Vinayagar Eng. Coll., Puducherry, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","109","115","The World Wide Web is a vast and rapidly growing source of information. Most of the information consumers are based on the World Wide Web for their exploration and the information is in the form of unstructured and so it's tough to handle. Nevertheless, there are many webpages which are having the structured data. Structured data is the format or pattern which is called as template. Some templates are considered destructive, since they worsen the performance and accuracy of the applications due to the irrelevant terms. Thus Template detection techniques are used to raise the working of search engines, clustering and classification of web documents. In this paper we survey some of the algorithms for extracting templates from different web pages in an efficient manner. First, the documents are clustered based on their similarity and thus templates from each cluster are extracted concurrently. Our intent is to present different techniques for the fast and accurate performances in extracting templates.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508206","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508206","MDL principle;Template extraction;clustering techniques","Algorithm design and analysis;Clustering algorithms;Data mining;HTML;Uniform resource locators;Web pages","Internet;Web sites;document handling;information retrieval;pattern classification;pattern clustering;search engines","Web document classification;Web document clustering;Web pages;World Wide Web;information consumers;information source;search engines;structured data;template detection technique;template extraction","","0","","6","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Fuzzy query processing method based on computing with Words methodology","P. Shamoi; A. Inoue; A. Akzhalova","Department of Computer Science, Kazakh-British Technical University, Almaty, Kazakhstan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","564","570","Database queries involving imprecise or fuzzy categories are an evolving area of academic research nowadays. In this paper we propose a novel approach to process queries purely expressed in natural language and demonstrate the effectiveness of using the concepts of Computing with Words in it. Currently, the system can process natural queries of single English sentences and retrieve the corresponding records. What is more, it allows to rank the output tuples by their importance. Such type of interface is much more natural for users, since they are free of using exotic crisp phrases that all standard query mechanisms provide.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505198","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505198","computing with words;fuzzy query;fuzzy sets;natural query","","fuzzy set theory;information retrieval;natural language interfaces;natural language processing;query processing","English sentences;database queries;exotic crisp phrases;fuzzy query processing method;natural language;record retrieval;standard query mechanisms;word methodology","","0","","14","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Accessing the required information from an applications in cloud through SMS","O. Pandithurai; N. suganya; S. Naresh; J. K. Deva; B. Suresh","Anna university, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","385","388","On necessity the cloud computing offers a flexibility in offering a function of resources. This proposal introduces to an new era for accessing information in the cloud computing with the help of mobile phone using the SMS (Short message Service). This would provide the facility to use of the system's cumulative resources, discarding the needs to specifically assign hardware to a task. The objective is to achieve connection between cloud and mobile with a null percent help of browsers. Until this moment accessing internet could be achieved only with the help browsers. In this paper, an alternative approach is proposed, in which on requisition by user he/she could receive global data with the help of SMS. The users could receive the requested information at a cost of just a single SMS. Another advantage in this method is that information could be obtained from anywhere even with the help of non browsers supporting mobiles.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508184","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508184","","Browsers;Cloud computing;Libraries;Mobile communication;Mobile handsets;Servers","cloud computing;electronic messaging;information retrieval;mobile computing;resource allocation","Internet access;SMS;cloud computing;cumulative resources;hardware assignment;information access;mobile phone;resource function;short message service","","0","","8","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Location Explorer with information services: A mobile application to deliver location-based web services","S. P. Ma; W. T. Lee; C. H. Kuo","Department of Computer Science and Engineering, National Taiwan Ocean University, Keelung, Taiwan","2013 International Symposium on Next-Generation Electronics","20130502","2013","","","283","286","In this paper, we propose an approach to delivering location-based information (LBI) and location-based web services (LBWS) to users based on users' past behaviors. The service usage patterns mined from the user's historical location visiting records and service utilization records are used to suggest relevant LBI and LBWS. Furthermore, the social network services are also adopted to enhance the usability of recommendation functions. For illustrating the feasibility of the proposed approach, a software system, called Location Explorer with Information Services (Lewis), is designed and implemented to realize the features of personalized location-based service delivery and web service utilization.","","Electronic:978-1-4673-3037-4; POD:978-1-4673-3036-7","10.1109/ISNE.2013.6512345","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6512345","","Computer architecture;Facebook;Information services;Mobile radio mobility management;Web services","Web services;data mining;information retrieval;mobile computing;recommender systems;social networking (online)","LBI;LBWS;Lewis;Location Explorer with Information Services;Web service utilization;location-based Web service delivery;location-based information delivery;mobile application;personalized location-based service delivery;recommendation function;service usage pattern mining;service utilization record;social network service;software system;user historical location visiting record;user past behavior","","0","","11","","","25-26 Feb. 2013","","IEEE","IEEE Conference Publications"
"A novel approach: Voice enabled interface with intelligent voice response system to navigate mobile devices for visually challenged people","R. T. F. Michael; B. RajaKumar.; S. Swaminathan.; M. Ramkumar.","SRC, SASTRA Univ., Kumbakonam, India","2013 International Conference on Emerging Trends in VLSI, Embedded System, Nano Electronics and Telecommunication System (ICEVENT)","20130411","2013","","","1","4","Now day's usage mobile devices are increasing in numbers. It can provide a foundation for improving the communication, learning and teaching environments. However, not all potential users have the capabilities that allow them to use the existing methodologies. To improve the search and navigation of various application and features of mobile devices we have proposed a model that entails the hands-free voice navigation system will be helpful for the visually challenged people. This model provides the opportunity to operate the mobile devices without using the keypad.","","Electronic:978-1-4673-5301-4; POD:978-1-4673-5300-7","10.1109/ICEVENT.2013.6496535","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496535","Tools for voice navigation;Voice enabled search;visually-disabled users","Computational modeling;Electronic publishing;Information services;Internet;Navigation;Speech;Speech recognition","handicapped aids;information retrieval;mobile computing;speech-based user interfaces","communication improvement;hands-free voice navigation system;intelligent voice response system;learning environments;mobile device features;mobile device navigation;search improvement;teaching environments;visually challenged people;voice enabled interface","","0","","13","","","7-9 Jan. 2013","","IEEE","IEEE Conference Publications"
"A Website Content Analysis Approach Based on Keyword Similarity Analysis","S. Kohli; S. Kaur; G. Singh","Comput. Sci., Birla Inst. of Technol., Noida, India","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","254","257","Content is most important element on any website. Keyword based reports, obtained from web analytics tools, provide insight to content usability of website. In this paper a tool 'Keyword Similarly Measure Tool'(KSMT) is presented which can be used to optimize the Keyword based report by combining the similar keyword that have relevant meaning and get a closer and concise picture. The aim is to improve the data accuracy and overcome limitation of similar keywords being vastly separated in the report. This way the methodology also provides holistic view of the data for similar keywords, by combining the matrices like bounce-rate, visits for the similar keywords and hence aim to provide a collective view and content analysis. The methodology also provides a way to compare & analyze Keywords with 'Suggested Keyword' provided by the user. The KSMT tool is developed using Perl, Apache and flex.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.212","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511893","Bounce rate;Keyword based Report;LCS (Longest Common Subsequence);Traffic Metric-Keyword;Web Analytics","","Web sites;content management;information analysis;information retrieval","Apache;KSMT tool;Keyword based report;Perl;Web analytic tool;Website content analysis approach;content usability;data accuracy;flex;keyword similarity analysis;keyword similarly measure tool;suggested keyword'","","3","","13","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Keyword Proximity Search over Large and Complex RDF Database","Z. Niu; H. T. Zheng; Y. Jiang; S. T. Xia; H. Q. Li","Grad. Sch. at Shenzhen, Tsinghua Univ., Shenzhen, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","467","471","In this paper, we propose a keyword proximity search approach that can be applied to large and complex RDF database. We model RDF database as undirected data graph, construct three indexes for each data graph, only one index need be loaded into memory. Keyword graph is defined as search result, keyword tree and minimal keyword tree are proposed as middle structures for Keyword graph extraction, and we present a link join operation based algorithm to retrieve Keyword trees in this paper. We employ a technique of keyword node pruning to accelerate keyword tree retrieval and define a scoring function to rank search results. In experiments, our approach achieves both high efficiency and high accuracy, outperforms the existing approaches.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.219","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511926","RDF;link join;node pruning;proximity","","database management systems;graph theory;information retrieval;tree data structures","complex RDF database;keyword graph extraction;keyword node pruning;keyword proximity search;keyword tree retrieval;link join operation-based algorithm;minimal keyword tree;undirected data graph","","0","","10","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Computing Semantic Relatedness Based on Search Result Analysis","J. Duan; J. Zeng","Sch. of Econ., Fudan Univ., Shanghai, China","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","3","","205","209","Automatically computing the semantic relatedness of two words is an essential step for many tasks in natural language processing, including information retrieval. Previous approaches to computing semantic relatedness used statistical techniques or lexical resources. We propose Searcher Result Analysis (SRA), a novel method that captures related text from search engine by issuing proper queries. Inferring the relatedness is then based on word occurrences in certain number of pages. Compared with the previous state of the art, using SRA to computing semantic relatedness based on Wikipedia can achieve competitive results with no need to maintain a local copy of remote resources. It is also shown that the correctness can be further improved by selecting proper knowledge resources or corpora for SRA.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511678","search engine;searcher result analysis;semantic relatedness","","information retrieval;natural language processing;resource allocation;search engines;search problems","SRA;Wikipedia;automatic computing;information retrieval;knowledge resources;lexical resources;natural language processing;remote resources;search engine;searcher result analysis;semantic relatedness computing;statistical techniques","","0","","13","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Sentiment analysis and classification based on textual reviews","K. Mouthami; K. N. Devi; V. M. Bhaskaran","Dept of CSE, Kongu Engineering College, Perundurai, Erode, Tamil Nadu, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","271","276","Mining is used to help people to extract valuable information from large amount of data. Sentiment analysis focuses on the analysis and understanding of the emotions from the text patterns. It identifies the opinion or attitude that a person has towards a topic or an object and it seeks to identify the viewpoint underlying a text span. Sentiment analysis is useful in social media monitoring to automatically characterize the overall feeling or mood of consumers as reflected in social media toward a specific brand or company and determine whether they are viewed positively or negatively on the web. This new form of analysis has been widely adopted in customer relation management especially in the context of complaint management. For automating the task of classifying a single topic textual review, document-level sentiment classification is used for expressing a positive or negative sentiment. So analyzing sentiment using Multi-theme document is very difficult and the accuracy in the classification is less. The document level classification approximately classifies the sentiment using Bag of words in Support Vector Machine (SVM) algorithm. In proposed work, a new algorithm called Sentiment Fuzzy Classification algorithm with parts of speech tags is used to improve the classification accuracy on the benchmark dataset of Movies reviews dataset.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508366","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508366","Sentiment Fuzzy Classification;Sentiment analysis;Support Vector Machine;Term weighting;Text classification;opinion mining;parts of speech tags","Accuracy;Classification algorithms;Motion pictures;Sentiment analysis;Support vector machines;Text categorization","customer relationship management;data mining;information retrieval;social networking (online);support vector machines;text analysis","Bag of words;SVM algorithm;benchmark dataset;complaint management;customer relation management;data mining;document level classification;document-level sentiment classification;emotion analysis;emotion understanding;movies reviews dataset;multitheme document;sentiment analysis;sentiment fuzzy classification algorithm;single topic textual review classification;social media monitoring;speech tags;support vector machine;text patterns;textual reviews;valuable information extraction","","10","","19","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Automatic Bug Assignment Using Information Extraction Methods","R. Shokripour; Z. M. Kasirun; S. Zamani; J. Anvik","Fac. of Comput. Sci. & Inf. Technol., Univ. of Malaya, Kuala Lumpur, Malaysia","2012 International Conference on Advanced Computer Science Applications and Technologies (ACSAT)","20130516","2012","","","144","149","The number of reported bugs in large open source projects is high and triaging these bugs is an important issue in software maintenance. As a step in the bug triaging process, assigning a new bug to the most appropriate developer to fix it, is not only a time-consuming and tedious task. The triager, the person who considers a bug and assigns it to a developer, also needs to be aware of developer activities at different parts of the project. It is clear that only a few developers have this ability to carry out this step of bug triaging. The main goal of this paper is to suggest a new approach to the process of performing automatic bug assignment. The information needed to select the best developers to fix a new bug report is extracted from the version control repository of the project. Unlike all the previous suggested approaches which used Machine Learning and Information Retrieval methods, this research employs the Information Extraction (IE) methods to extract the information from the software repositories. The proposed approach does not use the information of the bug repository to make decisions about bugs in order to obtain better results on projects which do not have many fixed bugs. The aim of this research is to recommend the actual fixers of the bugs. Using this approach, we achieved 62%, 43% and 41% accuracies on Eclipse, Mozilla and Gnome projects, respectively.","","Electronic:978-0-7695-4959-0; POD:978-1-4673-5832-3","10.1109/ACSAT.2012.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516342","Bug Assignment;File Activity Histories;Information Extraction;Named Entity Recognition","","information retrieval;program debugging;public domain software;software maintenance","Eclipse projects;Gnome projects;IE methods;Mozilla projects;automatic bug assignment;bug repository;bug triaging process;developer activity;fixed bugs;information extraction methods;information retrieval methods;machine learning;open source projects;reported bugs;software maintenance;software repository;tedious task;time-consuming task;version control repository","","2","","21","","","26-28 Nov. 2012","","IEEE","IEEE Conference Publications"
"Clustering and retrieval method of immunological memory cell in clonal selection algorithm","T. Ichimura; S. Kamada","Faculty of Management and Information Systems, Prefectural University of Hiroshima, 1-1-71, Ujina-Higashi, Minami-ku, Hiroshima, 734-8558, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","20130422","2012","","","1351","1356","The clonal selection principle explains the basic features of an adaptive immune response to a antigenic stimulus. It established the idea that only those cells that recognize the antigens are selected to proliferate and differentiate. This paper explains a computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. Antibodies generated by the clonal selection algorithm are clustered in some categories according to the affinity maturation, so that immunological memory cells which respond to the specified pathogen are created. Experimental results to classify the medical database of Coronary Heart Disease databases are reported. For the dataset, our proposed method shows the 99.6% classification capability of training data.","","Electronic:978-1-4673-2743-5; POD:978-1-4673-2742-8","10.1109/SCIS-ISIS.2012.6505049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505049","","","data analysis;database management systems;information retrieval;medical computing;pattern clustering","adaptive immune response;affinity maturation;antibodies;antigenic stimulus;clonal selection algorithm;clustering method;coronary heart disease databases;dataset;immunological memory cell;medical database;retrieval method;specified pathogen","","2","","24","","","20-24 Nov. 2012","","IEEE","IEEE Conference Publications"
"Improved user RTSE experience on the web through fast retrieval of social media content","N. Z. Quazilbash; S. M. H. Qadri; S. Khoja","Faculty of Computer Science, Institute of Business Administration, Institute of Business Administration, Karachi, Pakistan","2012 15th International Multitopic Conference (INMIC)","20130502","2012","","","260","263","Web 2.0 applications like Twitter and Facebook are producing real-time content continuously, at a very rapid pace, courtesy the growing interest of the masses in the social media. Real-time search engines (RTSE), which have the capacity to search social media content, like Topsy, Bing etc are currently in their initial stage, due to low coverage on web, lack of full access to APIs of social media websites and inconvenient user experience. In this paper, two issues related to fast retrieval of real-time content are presented; first issue is indexing and second being handling of real-time content spikes. Indexing has been discussed as a technique that causes overhead in terms of time consumed to display the results to the user and result spikes are discussed in context of bulk data being produced on the social media and its availability and readability to the user. This has greatly enhanced not only our learning in this area but also reduced the delays in generation of result substantially. Finally, based on these issues, current RTSEs are analyzed and recommendations of a framework are proposed. Standard practices of current RTSEs are introduced, and a framework for proposed recommendations is described in order to improve the efficiency of the system. The framework revolves around the use of indexes by the real-time search engines. It also discusses that how can indexes be avoided to increase the efficiency and throughput of the system for increased user satisfaction.","","Electronic:978-1-4673-2252-2; POD:978-1-4673-2249-2","10.1109/INMIC.2012.6511459","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511459","real-time search engines;search engine indexes;search engines;social media","","indexing;information retrieval;search engines;social networking (online)","API;Bing;Facebook;Topsy;Twitter;Web 2.0 applications;bulk data;fast social media content retrieval;indexing;real-time content;real-time content spikes;real-time search engines;social media Websites;user RTSE experience;user satisfaction","","1","","19","","","13-15 Dec. 2012","","IEEE","IEEE Conference Publications"
"Research on Interactive Information Visualization System in Special Academic Discussion","J. Chen; Y. Li; J. Song","Sch. of Comput. & Inf. Eng., Yibin Univ., Yibin, China","2012 Fourth International Symposium on Information Science and Engineering","20130411","2012","","","59","63","This electronic puts forward some methods of interactive information visualization and Interactive Visualization, which based on the view of traditional inactive information can hardly identification. An information visualizing of academic researches system is realized by XML and java used of FreeChar and Prefuse. The system allows the user to do mutual operating, support the visual retrieval and analysis, Running results indicated that improving the accessibility of mutual information by the special academic seminars has better expansibility.","2160-1283;21601283","Electronic:978-0-7695-4951-4; POD:978-1-4673-5680-0","10.1109/ISISE.2012.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6495298","Interactive information;Visualization;special academic seminars","","Java;XML;computer aided instruction;data visualisation;information retrieval;interactive systems","FreeChar;Java;Prefuse;XML;academic discussion;academic researches system;academic seminar;inactive information;interactive information visualization system;visual retrieval","","0","","9","","","14-16 Dec. 2012","","IEEE","IEEE Conference Publications"
"An Intelligent System for Retrieving Economic Information from Corporate Websites","J. Domenech; B. de la Ossa; A. Pont; J. A. Gil; M. Martinez; A. Rubio","Univ. Politec. de Valencia, Valencia, Spain","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","573","578","The prompt availability of up-to-date economic indicators is crucial to monitor the economy and to steer the design of policies for promoting business innovation and raising firm competitiveness. Economic indicators usually suffer important lags since they are commonly obtained from official databases or from interviews to a sample of agents, thus limiting the representative ness and usefulness of the information. In a context in which the presence of companies in the World Wide Web is almost an obligation to succeed, corporate websites are connected, in some way, to the firm economic activity. On the basis of this relation, this paper proposes an intelligent system that analyzes corporate websites to produce web indicators related to the economic activity of the firms. This system has been successfully implemented and applied to infer company size characteristics from data gathered from corporate websites. Our results show that relatively large companies provide web content in a foreign language and use proprietary web servers.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.92","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511943","","","Web sites;content management;economic indicators;information retrieval","Web content;Web indicators;Web servers;World Wide Web;business innovation;company size characteristics;corporate Websites;economic information retrieval;economy monitoring;firm competitiveness;firm economic activity;intelligent system;up-to-date economic indicators","","0","","16","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Automatic Extraction of Blog Post from Diverse Blog Pages","C. H. Chang; J. M. Chen","Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Chungli, Taiwan","2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology","20130502","2012","1","","129","136","Blog post extraction is essential for researches on blogosphere. In this paper, we address the issue of extracting blog posts from diverse blog pages, which aims at automatically and precisely finding the location of each blog post. Most of the previous researches focused on extracting main content from news pages, but the problem becomes more complex when one turns to blog pages. Our research is based on the combination of maximum scoring subsequence [11] and text-to-tag ratio [18] to develop algorithms that are suitable for blog pages. The first method that we propose is PTR Scoring, which combines postto-tag ratio with maximum scoring subsequence. The second method is CRF Scoring, which applies Conditional Random Field to train a sequence labeling model and use maximum scoring subsequence to improve the accuracy of extraction. The experimental results show that CRF Scoring achieves the best F-Measure at 91.9% compared with other methods.","","Electronic:978-0-7695-4880-7; POD:978-1-4673-6057-9","10.1109/WI-IAT.2012.25","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6511875","blog post extraction;maximum sequence;sequence labeling","","Web sites;information retrieval","CRF scoring;F-Measure;PTR scoring;automatic blog post extraction;blog pages;blogosphere;conditional random field;main content extraction;maximum scoring subsequence combination;post-to-tag ratio;sequence labeling model;text-to-tag ratio","","0","","20","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
