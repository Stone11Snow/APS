"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6628618,6626237,6629320,6628789,6628673,6628851,6629233,6630533,6628298,6628671,6630316,6628697,6630318,6630294,6628745,6628716,6626252,6630341,6630368,6630380,6630370,6628753,6625446,6623786,6624462,6622882,6622978,6624001,6624009,6626124,6622702,6627816,6627843,6624010,6624014,6624034,6623777,6624015,6624012,6623021,6620151,6620160,6621779,6621352,6621899,6620381,6620154,6621427,6621343,6620165,6620782,6621359,6622607,6621360,6620115,6622608,6620163,6620119,6618750,6617511,6616420,6618357,6618344,6618782,6617439,6618710,6618223,6616917,6616134,6617867,6618429,6616486,6616417,6618450,6617431,6616467,6614282,6615353,6614347,6612983,6614216,6615379,6613882,6614046,6607806,6610119,6607613,6606598,6607505,6606731,6608985,6611439,6612290,6607639,6611330,6610467,6609742,6607877,6606603,6605824",2017/05/04 22:21:39
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"SIP: A Simple Tool for Inspecting and Evaluating WSDL Specifications","M. Beron; G. Montejano; D. Riesco; P. R. Henriques; N. Debnath","Inf. Technol. Dept., Univ. Nac. de San Luis, San Luis, Argentina","2013 10th International Conference on Information Technology: New Generations","20130930","2013","","","14","19","Understanding a web application is not a simple task. It implies being available to interpret both their descriptions and several systems that really implement the web services. This interpreting process gathers much information and this information must be properly shown for simplifying the understanding. The descriptions of web services are an important starting point to begin to comprehend a web services. They are significant because they offer hints about the web services complexity. The web services complexity can be measured at high level abstraction considering the number of: Services, Ports, Operations, Bindings, etc. In order to carry out the task previously mentioned, the source code of web service description must be analyzed and the information must be properly displayed to the user. In this paper SIP, a Simple Inspection Tool, is described. SIP applies compilation techniques for extracting information from web services and then it computes a set of simple metrics. These metrics are used for providing an idea about web service complexity from its description. Furthermore, SIP generates visualizations based on charts, texts and source codes to help to comprehend WSDL descriptions.","","Electronic:978-0-7695-4967-5; POD:978-1-4673-5960-3","10.1109/ITNG.2013.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614282","Metric;Static Analysis;WSDL;Web Services","Complexity theory;Inspection;Measurement;Ports (Computers);Visualization;Web services;XML","Web services;computational complexity;formal specification;information retrieval;program compilers;program visualisation;software metrics;source coding;specification languages;text analysis","SIP;WSDL specification evaluation;WSDL specification inspection;Web service complexity measure;chart;compilation technique;information extraction;simple inspection tool;simple metrics;source code;text analysis;visualization generation","","0","","20","","","15-17 April 2013","","IEEE","IEEE Conference Publications"
"Sensitive keyword spotting for voice alarm systems","C. Zhu; Q. J. Kong; L. Zhou; G. Xiong; F. Zhu","Dongguan Research Institute of CASIA, Cloud Computing Center, Chinese Academy of Sciences, Dongguan, China","Proceedings of 2013 IEEE International Conference on Service Operations and Logistics, and Informatics","20130926","2013","","","350","353","Keyword spotting is the task of identifying the occurrences of certain desired keywords in an arbitrary speech signal. Keyword spotting has many applications. One of them is emergency voice alarm systems, like command control and smart monitoring ATM etc. This paper presents a sensitive keyword spotting system applied to voice alarm occasions. By sensitive keyword retrieval, emergency warning systems can automatically monitor and follow up the situation, find the crisis occurs, and then give the alarm. Automatically retrieving and tracking for the particular occasion can not only save a lot of manpower and material resources, but is also more efficient. So the study of this subject has great reality and application value.","","Electronic:978-1-4799-0530-0; POD:978-1-4799-0528-7","10.1109/SOLI.2013.6611439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611439","automatic tracking;keyword spotting;smart monitoring;voice alarm systems","Acoustics;Alarm systems;Feature extraction;Hidden Markov models;Speech;Speech recognition;Viterbi algorithm","alarm systems;emergency services;information retrieval;speech recognition","arbitrary speech signal;emergency voice alarm systems;emergency warning systems;sensitive keyword retrieval;sensitive keyword spotting system","","0","","11","","","28-30 July 2013","","IEEE","IEEE Conference Publications"
"How to effectively use topic models for software engineering tasks? An approach based on Genetic Algorithms","A. Panichella; B. Dit; R. Oliveto; M. Di Penta; D. Poshynanyk; A. De Lucia","University of Salerno, Fisciano (SA), Italy","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","522","531","Information Retrieval (IR) methods, and in particular topic models, have recently been used to support essential software engineering (SE) tasks, by enabling software textual retrieval and analysis. In all these approaches, topic models have been used on software artifacts in a similar manner as they were used on natural language documents (e.g., using the same settings and parameters) because the underlying assumption was that source code and natural language documents are similar. However, applying topic models on software data using the same settings as for natural language text did not always produce the expected results. Recent research investigated this assumption and showed that source code is much more repetitive and predictable as compared to the natural language text. Our paper builds on this new fundamental finding and proposes a novel solution to adapt, configure and effectively use a topic modeling technique, namely Latent Dirichlet Allocation (LDA), to achieve better (acceptable) performance across various SE tasks. Our paper introduces a novel solution called LDA-GA, which uses Genetic Algorithms (GA) to determine a near-optimal configuration for LDA in the context of three different SE tasks: (1) traceability link recovery, (2) feature location, and (3) software artifact labeling. The results of our empirical studies demonstrate that LDA-GA is able to identify robust LDA configurations, which lead to a higher accuracy on all the datasets for these SE tasks as compared to previously published results, heuristics, and the results of a combinatorial search.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606598","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606598","Genetic Algoritms;Latent Dirichlet Allocation;Textual Analysis in Software Engineering","Accuracy;Context;Genetic algorithms;Labeling;Natural languages;Software;Software engineering","genetic algorithms;information retrieval;natural language processing;software engineering;text analysis","IR methods;LDA configurations;LDA-GA;SE tasks;feature location;genetic algorithms;information retrieval methods;latent Dirichlet allocation;natural language documents;natural language text;near-optimal configuration;software artifact labeling;software data;software engineering tasks;software textual retrieval and analysis;source code;topic modeling technique;traceability link recovery","","47","","38","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"Enhancing Diversity-Accuracy Technique on User-Based Top-N Recommendation Algorithms","W. Premchaiswadi; P. Poompuang; N. Jongswat; N. Premchaiswadi","Grad. Sch. of Inf. Technol. in Bus., Siam Univ., Bangkok, Thailand","2013 IEEE 37th Annual Computer Software and Applications Conference Workshops","20130923","2013","","","403","408","In this paper we demonstrate how each item in top-N recommendation list has an impact on total diversity of the list in recommender systems. We proposed a new recommendation ranking method, namely ""Total Diversity Effect Ranking"", based on the total diversity effect of each item. Typically, diversity and accuracy in recommender systems are conversely. The finding shows that the proposed ranking method can guarantee the improvement of diversity quality in top-N recommendations. We proposed two new hybrid ranking approaches to make them more balance and designed experiments to evaluate their performance in comparison with the standard ranking approaches. To make results of our experiments more reliable, we apply the 5-fold cross validation technique to the MovieLens dataset provided by the GroupLens Research Project. In an evaluation process, we suggest using harmonic mean to measure the quality of recommendation approaches. The findings show that the proposed approaches give better performance than the standard approaches.","","Electronic:978-1-4799-2159-1; POD:978-1-4799-2160-7","10.1109/COMPSACW.2013.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605824","Harmonic Mean of Diversity and Precision;Hybrid Recommendation Ranking;Recommender Systems;Total Diversity Effect","Accuracy;Diversity reception;Equations;Harmonic analysis;Prediction algorithms;Recommender systems;Standards","information retrieval;recommender systems","5-fold cross validation technique;MovieLens dataset;diversity-accuracy technique;harmonic mean;hybrid ranking approaches;recommendation ranking method;recommender systems;total diversity effect ranking;user-based top-N recommendation algorithms","","0","","13","","","22-26 July 2013","","IEEE","IEEE Conference Publications"
"PreTrans: Reducing TLB CAM-search via page number prediction and speculative pre-translation","J. Xue; M. Thottethodi","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN 47907-2035","International Symposium on Low Power Electronics and Design (ISLPED)","20131015","2013","","","341","346","The need for fast address translation within tight time constraints (before L1 tag check but after effective address computation) imposes many design constraints. The freedom from such constraints can potentially lead to lower TLB energy costs. In this paper, we observe that (1) data accesses commonly use base-displacement addressing modes in which the effective address is computed as the sum of a base and a displacement, and (2) the effective page numbers are predictable once the base address is known. Further, it is easy to cache address translations alongside the predicted page numbers thus enabling speculative address translation that can filter accesses to the TLB. The two observations enable our PreTrans design in which (a) a speculative translation is available based solely on the base address, and (b) the translation is available simultaneously with the effective (virtual) address. PreTrans replaces most of the energy-expensive CAM-lookups for TLB access with RAM lookups, which translates to significant power improvements in the TLB.","","Electronic:978-1-4799-1235-3; POD:978-1-4799-1233-9","10.1109/ISLPED.2013.6629320","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629320","Power;Prediction;Speculation;TLB","Accuracy;Benchmark testing;Computer architecture;Delays;Organizations;Program processors;Registers","cache storage;information retrieval;random-access storage;table lookup","CAM-lookups;L1 tag check;PreTrans;RAM lookups;TLB CAM-search;TLB energy costs;base-displacement addressing modes;cache address translations;data access;page number prediction;speculative address translation;speculative pre-translation;time constraints;virtual address","","1","","22","","","4-6 Sept. 2013","","IEEE","IEEE Conference Publications"
"Multilingual learning objects indexing and retrieving based on ontologies","H. Achour; M. Zouari","Higher Institute of Management of Tunis, University of Tunis, Tunisia","2013 World Congress on Computer and Information Technology (WCCIT)","20131003","2013","","","1","6","Learning, teaching and training processes are increasingly relying on the Web. Different types of educational web applications are indeed available and provide to learners and trainers a considerable number of online educational resources. In this paper, we tackle the problem of learning objects indexing and retrieving in online learning environments, and propose to use an ontology-based description model, whose main objective is to improve access, search and reuse of learning resources. The ontological concepts and relations that we propose provide a means of guided navigation within an e-Learning system resources. They also allow performing multilingual search in these resources. An implementation of this model in a multilingual learning portal, where we used the proposed ontologies to perform trilingual search within a database of Arabic, French and English learning resources, is described.","","Electronic:978-1-4799-0462-4; POD:978-1-4799-0461-7","10.1109/WCCIT.2013.6618710","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618710","e-learning metadata standards;multilingual learning objects indexation;multilingual learning objects search;onto-terminological-resource;ontology","Electronic learning;Indexing;Navigation;Ontologies;Search problems;Semantics;Standards","Internet;computer aided instruction;indexing;information retrieval;natural language processing;ontologies (artificial intelligence);portals","Arabic learning resources;English learning resources;French learning resources;World Wide Web;database;e-Learning system resources;educational Web applications;guided navigation;learning process;multilingual learning objects indexing;multilingual learning objects retrieval;multilingual learning portal;multilingual search;online educational resources;online learning environments;ontological concepts;ontology-based description model;teaching process;training process;trilingual search","","2","","22","","","22-24 June 2013","","IEEE","IEEE Conference Publications"
"Search Space Reduction for Holistic Ligature Recognition in Urdu Nastalique Script","A. El-Korashy; F. Shafait","","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","1125","1129","This paper addresses the problem of holistic recognition of printed ligatures in Nastalique writing style of the Urdu language. The main difficulty of the recognition process lies in the large number of classes/ligatures (17,000 different possible ligatures in our Urdu text data). This large number of classes not only limits the efficiency (run-time) of the recognition algorithms, but also makes it difficult to use state-of-the-art classifiers - like Random Forests - that can only handle up to a few hundred classes. Nearest neighbor classifiers scale up well to tackle such large-scale classification problems, however their poor run-time efficiency poses a major obstacle. In this paper, we investigate two strategies for improving the efficiency (reducing the search space) of nearest neighbor based classification of Urdu ligatures. The first approach uses spectral hashing to resort to approximate nearest neighbor classification. The second approach is based on the idea of hierarchical classification to partition the search space based on the number of characters in a ligature. Experiments using spectral hashing show that the search space of nearest neighbor comparison can be reduced by about 50% without a loss in recognition accuracy. Further experiments demonstrate that the Random Forest classifier can be reliably used as the first stage classifier to distinguish one-character ligatures from multiple-character ligatures in a hierarchical classification scheme. We hope that the ideas presented in this paper would build the foundations for practical large-scale ligature classification systems not only for Nastalique, but also for other Urdu and Arabic scripts.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.228","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628789","Character Recognition;Nearest Neighbor classification;Urdu Nastalique","Accuracy;Context;Histograms;Shape;Text recognition;Training;Vectors","character recognition;decision trees;information retrieval;natural language processing;pattern classification;text analysis","Arabic scripts;Nastalique writing style;Urdu Nastalique script;Urdu language;Urdu text data;hierarchical classification;holistic ligature recognition;large-scale classification problems;large-scale ligature classification systems;nearest neighbor classifiers;nearest neighbor-based classification efficiency improvement;printed ligatures;random forest classifier;recognition algorithms;search space partitioning;search space reduction;state-of-the-art classifiers;uses spectral hashing","","3","","12","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Demo paper: Savant - A Semantic Video Parsing and Tagging system","H. T. Nguyen; Hai Wei; S. Sheorey; H. Trinh","UtopiaCompression Corporation, Los Angeles, CA, USA","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","20131003","2013","","","1","2","We demonstrate a surveillance video tagging and search system that transforms input live raw video streams into semantic metadata structures through automatic object detection, tracking, classification and event recognition. The metadata generated through online video analysis and stored in spatial databases enable various types of user queries on the semantic content of archived video. They also allow for browsing and navigation through the video database effectively and efficiently. The system works with different application domains and camera setups.","","Electronic:978-1-4799-1604-7; POD:978-1-4799-1603-0; USB:978-1-4799-1602-3","10.1109/ICMEW.2013.6618223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618223","semantic video tagging;video search and retrieval;video surveillance","Cameras;Context;Graphical user interfaces;Object detection;Semantics;Streaming media;Tagging","image classification;information retrieval systems;meta data;object recognition;object tracking;spatial data structures;video retrieval;video streaming;video surveillance;visual databases","Savant;archived video semantic content;automatic object classification;automatic object detection;automatic object tracking;event recognition;input live raw video streams;online video analysis;semantic metadata structures;semantic video parsing system;semantic video tagging system;spatial databases;surveillance video search system;surveillance video tagging system;user queries","","0","","","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Integrating Visual and Textual Cues for Query-by-String Word Spotting","D. Aldavert; M. Rusiñol; R. Toledo; J. Lladós","Dept. Cienc. de la Computacio, Univ. Autonoma de Barcelona, Bellaterra, Spain","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","511","515","In this paper, we present a word spotting framework that follows the query-by-string paradigm where word images are represented both by textual and visual representations. The textual representation is formulated in terms of character n-grams while the visual one is based on the bag-of-visual-words scheme. These two representations are merged together and projected to a sub-vector space. This transform allows to, given a textual query, retrieve word instances that were only represented by the visual modality. Moreover, this statistical representation can be used together with state-of-the-art indexation structures in order to deal with large-scale scenarios. The proposed method is evaluated using a collection of historical documents outperforming state-of-the-art performances.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628673","bag-of-visual-words;handwritten word spotting;latent semantic analysis;query-by-string","Computational modeling;Computer vision;Hidden Markov models;Histograms;Pattern recognition;Vectors;Visualization","document image processing;image representation;information retrieval;statistical analysis","bag-of-visual-words scheme;historical document extraction;indexation structures;query-by-string word spotting framework;statistical representation;sub-vector space;textual cues;textual query;textual representation;visual cues;visual modality;visual representations;word images;word instances","","15","","16","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"3D CAD model retrieval based on GA-ACO","Bo Ding; Zheng Zhang; Xiao-yang Yu; Yun-bin He","College of Computer Science and Technology, Harbin University of Science &Technology, 150080, China","Ifost","20131003","2013","2","","36","41","To reuse 3D CAD models more efficiently, a 3D CAD model retrieval approach which combines Genetic Algorithm (GA) and Ant Colony Optimization (ACO) is proposed. A CAD model is represented by feature dependency graph (FDG). The similar sub-graphs in the corresponding FDGs of different models are obtained according to polymorphic combinatorial GA and ACO. GA is adopted to obtain suboptimal solutions. The pheromone of ACO is initialized according to the suboptimal solutions, and then a further search among the suboptimal solutions is operated for the better solution. And finally, the optimal solutions of the product design can be searched. The hybrid approach is accomplished in convergence efficiency and solution precision. Our experimental results show that, the algorithm can seek the modeling containing detail characteristics and satisfy user's personalized need.","","Electronic:978-1-4799-0933-9; POD:978-1-4799-0932-2","10.1109/IFOST.2013.6616917","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616917","3D CAD model retrieval;ant colony algorithm;feature dependency graph;genetic algorithm","Adaptation models;Convergence;Design automation;Fasteners;Lead;Solid modeling;Three-dimensional displays","CAD;ant colony optimisation;convergence;genetic algorithms;graph theory;information retrieval;solid modelling","3D CAD model retrieval;FDG;GA-ACO;ant colony optimization;convergence efficiency;feature dependency graph;genetic algorithm;pheromone;polymorphic combinatorial GA;product design;subgraphs;suboptimal solutions","","0","","15","","","June 28 2013-July 1 2013","","IEEE","IEEE Conference Publications"
"Retrieving and analyzing mobile apps feature requests from online reviews","C. Iacob; R. Harrison","Department of Computing and Communication Technologies, Oxford Brookes University, Oxford, United Kingdom","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","41","44","Mobile app reviews are valuable repositories of ideas coming directly from app users. Such ideas span various topics, and in this paper we show that 23.3% of them represent feature requests, i.e. comments through which users either suggest new features for an app or express preferences for the re-design of already existing features of an app. One of the challenges app developers face when trying to make use of such feedback is the massive amount of available reviews. This makes it difficult to identify specific topics and recurring trends across reviews. Through this work, we aim to support such processes by designing MARA (Mobile App Review Analyzer), a prototype for automatic retrieval of mobile app feature requests from online reviews. The design of the prototype is a) informed by an investigation of the ways users express feature requests through reviews, b) developed around a set of pre-defined linguistic rules, and c) evaluated on a large sample of online reviews. The results of the evaluation were further analyzed using Latent Dirichlet Allocation for identifying common topics across feature requests, and the results of this analysis are reported in this paper.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624001","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624001","Online reviews;feature requests;mobile apps","Context;Feature extraction;Measurement;Mobile communication;Pragmatics;Prototypes;Resource management","Internet;computational linguistics;information retrieval;mobile computing;natural language processing","MARA design;latent Dirichlet allocation;linguistic rules;mobile app review analyzer;mobile app reviews;mobile apps feature request analysis;mobile apps feature request retrieval;online reviews","","30","1","7","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Automatic video programming using LSA and audio transcript","A. Outtagarts; A. Mbodj","Alcatel-Lucent Bell Labs, Nozay, France","2013 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","20131007","2013","","","1","5","In this paper, we present a new approach of automatic video programming (or editing). Our approach uses text processing, statistic analysis and information retrieval techniques. Using Latency Semantic Analysis (LSA) method, audio transcription texts and constraints of the composed video duration, our algorithm selects the video sequences and chains them to create automatically a new video. Our system allows the user to create a video composition by setting its duration.","2155-5044;21555044","Electronic:978-1-4673-6047-0; POD:978-1-4673-6046-3","10.1109/BMSB.2013.6621779","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621779","LSI;automatic indexing;text mining;text processing;video;video composition","Large scale integration;Multimedia communication;Programming;Semantics;Streaming media;Video sequences;XML","image sequences;information retrieval;semantic networks;statistical analysis;text analysis;video retrieval;word processing","LSA method;audio transcript;audio transcription text processing;automatic video programming;information retrieval technique;latency semantic analysis method;statistical analysis;video composition;video sequence","","0","","18","","","5-7 June 2013","","IEEE","IEEE Conference Publications"
"ICDAR 2013 Competition on Book Structure Extraction","A. Doucet; G. Kazai; S. Colutto; G. Mühlberger","Univ. of Normandy - Unicaen, Caen, France","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","1438","1443","This paper summarizes the 3rd Book Structure Extraction competition that was run at the ICDAR 2013. Its goal is to evaluate and compare automatic techniques for deriving structure information from digitized books, which could then be used to aid navigation inside the books. More specifically, the task that participants are faced with is to construct hyper linked tables of contents for a collection of 1,000 digitized books. This paper reviews the setup of the competition, the book collection used in the task, and the measures used for the evaluation. The main novelty of the 2013 competition is that we were able to rely on an external provider for the ground truthing phase, hence granting the consistency of the evaluation. In addition, this permitted to nearly double the number of annotated books from the 1,040 books annotated in 2009 and 2011 to over 2,000 books. The paper further presents the result performance of the 6 participating research teams, and briefly summarizes their approaches.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628851","","Data mining;Educational institutions;Navigation;Optical character recognition software;Organizations;Portable document format;XML","electronic publishing;information retrieval","ICDAR 2013 competition;annotated books;book collection;book navigation;book structure extraction;digitized books;ground truthing phase;hyper linked tables;international conference on document analysis and recognition;structure information","","1","","18","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Space-efficient read indexing and retrieval based on compressed de Bruijn graphs","M. Almutairy; J. Fish; C. T. Brown","Computer Science and Engineering, Michigan State University, East Lansing, 48824, USA","2013 IEEE 3rd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)","20131015","2013","","","1","1","We introduce a short-read indexing and retrieval scheme based on a compressed de Bruijn graph data structure.","","Electronic:978-1-4799-0716-8; POD:978-1-4799-0715-1","10.1109/ICCABS.2013.6629233","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629233","","Assembly;Bioinformatics;Computer science;Data structures;Electronic mail;Indexing","biology computing;data structures;graph theory;indexing;information retrieval","compressed de Bruijn graph data structure;short-read indexing scheme;short-read retrieval scheme;space-efficient read indexing;space-efficient read retrieval","","0","","3","","","12-14 June 2013","","IEEE","IEEE Conference Publications"
"Mining disease associated biomarker networks from PubMed","Z. Huang","School of Information Science and Technology, Drexel University, Philadelphia, USA","2013 7th International Conference on Systems Biology (ISB)","20131010","2013","","","15","18","Disease related biomarker discovery is the critical step to realize the future personalized medicine and has been an important research area. With exponential growing of biomedical knowledge deposited in PubMed database, it is now an essential step to mine PubMed for biomarker-disease associations to support the laboratory research and clinical validation. We constructed list of human diseases that are most frequently associated with biomarker in literatures by text mining. Top ranked neurology diseases were then used to extract associated genes from PubMed using context sensitive information retrieval methods. Associated genes were then integrated into pathways and subject to network biomarker analysis. Our approach identifies both known and potential biomarkers for 3 neurodegenerative diseases.","2164-2389;21642389","Electronic:978-1-4799-1389-3; POD:978-1-4799-1387-9; USB:978-1-4799-1388-6","10.1109/ISB.2013.6623786","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623786","biological network;biomarker;disease-gene association;semantic;text mining","Biological system modeling;Diseases;Manuals;Proteins;Unified modeling language","data mining;diseases;genetics;genomics;information retrieval;medical information systems;neurophysiology","PubMed database;biomarker discovery;biomarker network analysis;biomarker-disease association;biomedical knowledge;clinical validation;context sensitive information retrieval method;gene association extraction;laboratory research;neurodegenerative disease;personalized medicine;text mining","","0","","15","","","23-25 Aug. 2013","","IEEE","IEEE Conference Publications"
"Towards real-time music auto-tagging using sparse features","Y. H. Yang","Research Center for IT Innovation, Academia Sinica, Taiwan","2013 IEEE International Conference on Multimedia and Expo (ICME)","20130926","2013","","","1","6","Unsupervised feature learning algorithms such as sparse coding and deep belief networks have been shown a viable alternative to hand-crafted feature design for music information retrieval. Nevertheless, such algorithms are usually computationally expensive. This paper investigates techniques to accelerate sparse feature extraction and music classification. To study the trade-off between computational efficiency and accuracy, we compare state-of-the-art, dense audio features with sparse features computed using 1) sparse coding with a random dictionary, 2) randomized clustering forest, and 3) an extension of randomized clustering forest to temporal signals. For classifier training and prediction, we compare support vector machines with linear or non-linear kernel functions. We conduct evaluation on music auto-tagging for 140 genre/style tags using a subset of 7,799 songs of the CAL10k data set. Our result leads to an 11-fold speed increase with 3.45% accuracy loss comparing to dense features. With the proposed sparse features, the feature extraction and auto-tagging operations can be finished in 1 second per song, with 0.1302 tagging accuracy in mean average precision.","1945-7871;19457871","Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8","10.1109/ICME.2013.6607505","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607505","Unsupervised feature learning;music auto-tagging;randomized clustering forest;sparse coding","Abstracts;Lead;Mel frequency cepstral coefficient","feature extraction;information retrieval;music;support vector machines","CAL10k data set;auto-tagging operations;classifier prediction;classifier training;computational efficiency;deep belief networks;dense audio features;hand-crafted feature design;mean average precision;music classification;music information retrieval;nonlinear kernel functions;random dictionary;randomized clustering forest;real-time music auto-tagging;sparse coding;sparse feature extraction;sparse features;support vector machines;temporal signals;unsupervised feature learning algorithms","","7","","37","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Key-Region Detection for Document Images -- Application to Administrative Document Retrieval","H. Gao; M. Rusiñol; D. Karatzas; J. Lladós; T. Sato; M. Iwamura; K. Kise","Dept. Cienc. de la Computacio, Univ. Autonoma de Barcelona, Bellaterra, Spain","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","230","234","In this paper we argue that a key-region detector designed to take into account the special characteristics of document images can result in the detection of less and more meaningful key-regions. We propose a fast key-region detector able to capture aspects of the structural information of the document, and demonstrate its efficiency by comparing against standard detectors in an administrative document retrieval scenario. We show that using the proposed detector results to a smaller number of detected key-regions and higher performance without any drop in speed compared to standard state of the art detectors.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628618","Administrative Document Image Retrieval;MSER;Region Detection","Computer vision;Detectors;Feature extraction;Indexing;Layout;Text analysis;Transforms","document image processing;information retrieval","administrative document retrieval;document images;key-region detection;special characteristics;structural information","","1","","23","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Recent advances in affective and semantic media applications at the BBC","J. Eggink; Y. Raimond","British Broadcasting Corporation (BBC), Research and Development, United Kingdom","2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)","20131003","2013","","","1","4","In this paper, we give an overview of recent BBC R&D work on automated affective and semantic annotations of BBC archive content, covering different types of use-cases and target audiences. In particular, after giving a brief overview of manual cataloguing practices at the BBC, we focus on mood classification, sound effect classification and automated semantic tagging. The resulting data is then used to provide new ways of finding or discovering BBC content. We describe two such interfaces, one driven by mood data and one driven by semantic tags.","2158-5873;21585873","Electronic:978-1-4799-0833-2; POD:978-1-4799-0831-8","10.1109/WIAMIS.2013.6616134","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616134","","Accuracy;Feature extraction;Mood;Semantics;TV;Tagging;Vectors","cataloguing;classification;content management;information retrieval systems","BBC archive content;affective media application;automated affective annotation;automated semantic tagging;manual cataloguing practice;mood classification;mood data;semantic annotation;semantic media application;sound effect classification","","0","","12","","","3-5 July 2013","","IEEE","IEEE Conference Publications"
"Content activity based short-cut routing in Content centric Networks","Tao Liu; Ming Tian; Dongnian Cheng","Nat. Digital Switching Syst. Eng. Technol. R&D Center, Zhengzhou, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","20130930","2013","","","479","482","Content-centric Networking (CCN) is a novel paradigm for content distribution with name based routing. However, the basic CCN routing scheme is not efficient for content retrieving and resource utilization. Based on the announcement of cached content in routers, we present a new content routing fashion called shout-cut routing, which enables routers to perceive neighbor's caching information and perform best routing decision.","2327-0586;23270586","Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4","10.1109/ICSESS.2013.6615353","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615353","ant colony optimization;content activity;content-centric networks;named data networking;shout-cut routing","Face;Indexes;Lead;Multimedia communication","computer networks;content management;information retrieval;resource allocation;telecommunication network routing","CCN routing scheme;content activity-based short-cut routing;content centric networks;content distribution;content retrieval;name-based routing;neighbor caching information;resource utilization;routing decision","","0","","8","","","23-25 May 2013","","IEEE","IEEE Conference Publications"
"A conceptual indexing approach for Arabic texts","A. C. Mazari; H. Aliane; Z. Alimazighi","University of M&#x00E9;d&#x00E9;a M&#x00E9;d&#x00E9;a, Algeria","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","1","Classical IR systems are often based on lexical matching using approaches that rely on purely statistical methods founded on distributions of keywords to calculate the similarity between the query and the documents of the corpus. The relevance of a document according to a query is based on the similarity of vocabulary and not according to the thematic similarity between both. Indeed, a document selected by the IRS relevant shares the same words with the query. Among the research that tried to overcome the limitations of conventional IR, there is an approach called (Concept-Based Information Retrieval) that is characterized by the notion of conceptual space in which documents and queries are represented by opposition to the space of simple words used in conventional IR. The use of external knowledge such as WordNet was the subject of several works in IR, often geared towards improving the precision (WSD: Word-Sense Disambiguation, Conceptual Indexing) and/or improving the recall (query expansion).","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616420","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616420","","Compounds;Context;Frequency measurement;Indexing;Semantics;Statistical analysis;Vocabulary","indexing;information retrieval;text analysis","Arabic texts;WordNet;classical IR systems;concept based information retrieval;conceptual indexing;conceptual space;document;lexical matching;query expansion;simple words;statistical methods;thematic similarity;word sense disambiguation","","0","","","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"Social insect inspired approach for identification and dynamic tracking of news stories on the Web","Š. Sabo; P. Návrat","Faculty of Informatics and Information Technologies, Slovak University of Technology, Bratislava, Slovakia","2013 World Congress on Nature and Biologically Inspired Computing","20131003","2013","","","226","231","In this paper we present an approach to identification and tracking of currently unfolding news stories extracted from the news articles published on the Web. Our approach utilizes a set of social insect inspired agents to retrieve articles from the Web and identify popular terms, referred to as story words, relevant to the ongoing news stories. This allows for a dynamic approach that reflects the changes in article space as new stories unfold and new articles are added. Subsequently a graph representation of the article space is constructed, that contains retrieved articles and identified story words interconnected by edges according to relationships of relevance identified between elements of the graph. Stories are then extracted from the constructed graph by using Louvain method, commonly used to identify communities within modular graphs. Using this approach we have been able to identify news stories in a stream of articles retrieved from the Web with precision of 75.56%, with best precision generally achieved for recent news stories described by popular story words.","","Electronic:978-1-4799-1415-9; POD:978-1-4799-1413-5","10.1109/NaBIC.2013.6617867","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617867","beehive metaphor;community detection;news stories;social insect;topic detection and tracking","Insects;Training","Internet;electronic publishing;graph theory;information retrieval;social networking (online);text analysis","Louvain method;Web articles;article retrieval;article space;community identification;dynamic news story tracking;graph representation;modular graphs;news articles;news story identification;popular term identification;social insect inspired agents;story word identification","","1","","11","","","12-14 Aug. 2013","","IEEE","IEEE Conference Publications"
"Real-time urban traffic information extraction from GPS tracking of a bus fleet","E. Denaxas; S. Mpollas; D. Vitsios; C. Zolotas; D. G. Bleris; G. M. Spanos; N. P. Pitsianis","Department of Electrical and Computer Engineering, Aristotle University, Thessaloniki, Greece","2013 IEEE Symposium on Computational Intelligence in Vehicles and Transportation Systems (CIVTS)","20130926","2013","","","58","63","We present a novel system named Speed-O to estimate in real time the average speed of traffic on every section of urban roads in the city of Thessaloniki, Greece. Speed-O processes the telemetry data reported by the fleet of more than 600 public transportation buses. The system is solving for the mean speed at different temporal and spatial scales much finer than the raw telemetry data. Speed-O is comprised of a large and sparse model of the road system, the bus routes, and the fleet dynamics together with a fast solver that renders the model solution. The solver is multi-layer and iterative. It exploits the physical properties of the problem to operate efficiently in realtime on a multicore processor. We demonstrate the Speed-O system using the Google Maps API as an interactive map that refreshes every minute with the average speed displayed as color-coded road sections.","","Electronic:978-1-4673-5913-9; POD:978-1-4673-5912-2","10.1109/CIVTS.2013.6612290","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612290","","Equations;Global Positioning System;Linear systems;Mathematical model;Roads;Vehicles","Global Positioning System;geographic information systems;information retrieval;real-time systems;traffic information systems","GPS tracking;Google Maps API;Greece;Speed-O system;Thessaloniki;bus fleet;color-coded road sections;interactive map;public transportation buses;real-time urban traffic information extraction;spatial scales;telemetry data;temporal scales","","1","","17","","","16-19 April 2013","","IEEE","IEEE Conference Publications"
"Emotion tracking in music using continuous conditional random fields and relative feature representation","V. Imbrasaitė; T. Baltrušaitis; P. Robinson","Computer Laboratory, University of Cambridge, UK","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","20131003","2013","","","1","6","Digitization of how people acquire music calls for better music information retrieval techniques, and dimensional emotion tracking is increasingly seen as an attractive approach. Unfortunately, the majority of models we still use are borrowed from other problems that do not suit emotion prediction well, as most of them tend to ignore the temporal dynamics present in music and/or the continuous nature of Arousal-Valence space. In this paper we propose the use of Continuous Conditional Random Fields for dimensional emotion tracking and a novel feature vector representation technique. Both approaches result in a substantial improvement on both rootmean-squared error and correlation, for both short and long term measurements. In addition, they can both be easily extended to multimodal approaches to music emotion recognition.","","Electronic:978-1-4799-1604-7; POD:978-1-4799-1603-0; USB:978-1-4799-1602-3","10.1109/ICMEW.2013.6618357","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618357","Arousal-Valence space;acoustic features;continuous emotions;feature representation;machine learning","Correlation;Emotion recognition;Feature extraction;Mathematical model;Measurement uncertainty;Training;Vectors","emotion recognition;feature extraction;information retrieval;music;signal representation;statistical analysis","arousal-valence space;continuous conditional random fields;dimensional emotion tracking;long term measurements;multimodal approach;music emotion recognition;music information retrieval techniques;relative feature vector representation technique;root mean-squared error;short term measurements;temporal dynamics","","3","","25","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"An Effective Fuzzy Keyword Search Scheme in Cloud Computing","H. Tuo; M. Wenping","State Key Lab. of Integrated Service Networks, Xidian Univ., Xi'an, China","2013 5th International Conference on Intelligent Networking and Collaborative Systems","20131015","2013","","","786","789","With the development of cloud computing, individuals and enterprises stored their data in the cloud for great flexibility and economic savings. Many keyword search schemes over encrypted data with privacy preserving were proposed, however, few schemes cloud efficiently support the similarity search. In this paper, we investigate the issue on fuzzy search over cloud data, then by using technique of filter, we improve a efficient keyword search scheme to achieve fuzzy searching with low cost, which is suit for practical cloud computing.","","Electronic:978-0-7695-4988-0; POD:978-1-4673-6368-6","10.1109/INCoS.2013.150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630533","bilinear map;bloom filter;cloud cmoputing;fuzzy keyword serach","Cloud computing;Encryption;Keyword search;Public key;Servers","cloud computing;fuzzy set theory;information retrieval","cloud computing;filter technique;fuzzy keyword search scheme;similarity search","","5","","10","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Ontology-based trace retrieval","Y. Li; J. Cleland-Huang","School of computer science and technology, Wuhan University of Technology, Hubei, China","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","30","36","In automated requirements trace retrieval, an ontology can be used as an intermediary artifact to identify relationships that would not be recognized by standard information retrieval techniques. However, ontologies must be carefully constructed to fit the needs of the project. In this paper we present a technique for incorporating information from general and domain-specific ontologies into the tracing process. Our approach applies the domain ontology at the phrase level and then uses a general ontology to augment simple term matching in order to deduce relationships between individual terms weighted according to the relative importance of the phrase in which they occur. The combined weights are used to compute the overall similarity between a source and target artifact in order to establish a candidate trace link. We experimentally evaluated our approach against the standard Vector Space Model (VSM) and show that a domain ontology combined with generalized ontology returned greatest improvements in trace accuracy.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620151","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620151","Ontology;Retrieval;Similarity Score;Syntax Tree;Traceability","Artificial neural networks;Electronics packaging;Ontologies;Semantics;Standards","information retrieval;ontologies (artificial intelligence);software engineering","automated requirements trace retrieval;domain-specific ontologies;general ontologies;intermediary artifact;ontology-based trace retrieval;phrase level;source artifact;standard information retrieval techniques;target artifact;term matching;trace accuracy improvement","","0","","19","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"A discriminative model approach for suggesting tags automatically for Stack Overflow questions","A. K. Saha; R. K. Saha; K. A. Schneider","University of Saskatchewan, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","73","76","Annotating documents with keywords or `tags' is useful for categorizing documents and helping users find a document efficiently and quickly. Question and answer (Q&A) sites also use tags to categorize questions to help ensure that their users are aware of questions related to their areas of expertise or interest. However, someone asking a question may not necessarily know the best way to categorize or tag the question, and automatically tagging or categorizing a question is a challenging task. Since a Q&A site may host millions of questions with tags and other data, this information can be used as a training and test dataset for approaches that automatically suggest tags for new questions. In this paper, we mine data from millions of questions from the Q&A site Stack Overflow, and using a discriminative model approach, we automatically suggest question tags to help a questioner choose appropriate tags for eliciting a response.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624009","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624009","Machine learning;automatic tagging;discriminative model","Accuracy;Prediction algorithms;Predictive models;Support vector machines;Tagging;Training;Vectors","Web sites;data mining;document handling;question answering (information retrieval)","Q&A site;Stack Overflow questions;automatic question categorization;automatic question tagging;automatic tag suggestion;data mining;discriminative model approach;document annotation;document categorization;document keywords;document tags;question-and-answer sites;test dataset;training dataset","","4","","4","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Intelligent loading of code modules based on the users' historical operating log in GIS software","H. Li; Y. Gao; H. Yu; L. Liu; X. Guo","Institute of Remote Sensing and Geographical Information Systems, Peking University, Beijing, PR China","2013 21st International Conference on Geoinformatics","20131010","2013","","","1","4","The intelligent loading is a technology which is widely used in GIS Software. The paper designed a method which is based on the users' preference that is extracted and analysed from users' historical operating records. A matrix is used for describing the relation between code modules and users' behavior. Two vectors are used to formalize the users' behavior in the dimension of code modules. The two vectors record the information to judge whether the code module needs to be loaded or not. By a series of the calculation, the code modules which are very likely used by the project are selected. Through this method, GIS Software is loaded more efficient.","2161-024X;2161024X","Electronic:978-1-4673-6228-3; POD:978-1-4673-6226-9","10.1109/Geoinformatics.2013.6626124","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626124","Code Modules;Geographic Information System (GIS);Intelligent Loading;Users' Prefermance","Algorithm design and analysis;Geographic information systems;Heuristic algorithms;Loading;Software;Software algorithms;Vectors","geographic information systems;information retrieval;matrix algebra;recommender systems","GIS software;code modules;historical operating records;intelligent loading;matrix method;user behavior","","0","","13","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"An alternative algorithm for wave information extraction from X-band nautical radar images","W. Huang; E. W. Gill","Faculty of Engineering and Applied Science, Memorial University, St. John's, NL, Canada, A1B 3X5","IET International Radar Conference 2013","20131010","2013","","","1","5","A new algorithm is presented to extract ocean wave information from X-band nautical radar images. Based on the iterative least-squares (LS) current method in which the samples of the image spectra are classified as the contributions from fundamental, first-order harmonic, and higher harmonics waves or noise, the proposed algorithm uses the classified fundamental and first-order harmonic wave components for wave spectra and parameter retrieval directly. Unlike the previous wave analysis in which a band-pass filter is required subsequently to eliminate the non-wave contributions after current velocity is obtained, the proposed algorithm simplifies the wave retrieval without such a bandpass filter. Harmonic wave components are also accounted for in the wave retrieval process. This algorithm is applied to both simulated and field radar images, and results from both types of data show that this modified wave algorithm is effective.","","Electronic:978-1-84919-603-1","10.1049/cp.2013.0298","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624462","Nautical radar images;wave spectrum","","geophysical image processing;image denoising;image retrieval;information retrieval;iterative methods;least squares approximations;marine engineering;marine radar;ocean waves;oceanographic techniques;radar computing;radar imaging","first-order harmonic wave components;fundamental harmonic wave components;higher harmonics waves;image spectra;iterative least-squares current method;nonwave contribution elimination;ocean wave information extraction;parameter retrieval;wave retrieval process;wave spectra;x-band nautical radar images","","0","","","","","14-16 April 2013","","IET","IET Conference Publications"
"Towards an eye-tracking enabled IDE for software traceability tasks","B. Walters; M. Falcone; A. Shibble; B. Sharif","Department of Computer Science and Information Systems, Youngstown State University, Ohio USA 44555","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","51","54","The paper presents iTrace, an eye-tracking plug-in for the Eclipse IDE. The premise is to use developers' eye gaze as input to traceability tasks such as generating links between various artifacts. The design, architecture, and current state of iTrace is described. Support for a variety of traceability tasks such as link retrieval, link evolution, link visualization, and empirical studies are also discussed. An initial link generation heuristic using iTrace is presented with plans for future evaluation.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620154","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620154","Eclipse plug-in;Eye-tracking;traceability","Calibration;Electronic mail;Java;Layout;Software;Unified modeling language;Visualization","data visualisation;information retrieval;programming environments;software engineering","empirical analysis;eye gaze;eye-tracking enabled Eclipse IDE;eye-tracking plug-in;iTrace architecture;iTrace design;iTrace state;link evolution;link generation heuristic;link retrieval;link visualization;software traceability tasks","","4","","11","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"Supporting cultural emotional browsing for museums: The versoverdi APP","M. Roccetti; G. Marfia; C. Bertuccioli; A. Marcomini; M. Zanichelli; A. Varni","Department of Computer Science and Engineering, Alma Mater Studiorum - University of Bologna, Mura A. Zamboni 7, 40127, Italy","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","20131003","2013","","","1","5","Among the infinite options that are only limited by one's creativity, many different applications are flourishing in the context of cultural heritage. In fact, while cultural heritage is often hard to understand or even only to reach, technology is today providing all with a powerful access key to many of its very different instances. Such key can be based on traditional means of communication and explanation (e.g., audio guides), or also ambitiously exploit novel ways of representation and visualization (e.g., augmented reality renderings). Our contribution in this area is that of designing and implementing a mobile app that supports the navigation of Giuseppe Verdi-related cultural sites. Unlike any other app, ours proposes an innovative human-computer interaction scheme that supports mood-based browsing of cultural information across multiple museums at once. In practice, our app is capable of determining a user's mood and, based on such information, of providing visit suggestions as well as accessibility to content that matches the given mood state.","","Electronic:978-1-4799-1604-7; POD:978-1-4799-1603-0; USB:978-1-4799-1602-3","10.1109/ICMEW.2013.6618429","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618429","Emotional browsing;cultural heritage museums;human-computer interfaces","Cultural differences;Emotion recognition;Mobile communication;Mood;Mouth;Planets;Shape","Web sites;cultural aspects;data visualisation;history;human computer interaction;information retrieval;mobile computing;museums","Giuseppe Verdi-related cultural sites;VersoVerdi application;cultural emotional browsing;cultural heritage;human computer interaction scheme;information representation;information visualization;mobile application;mood-based browsing;museum;navigation","","3","","4","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"An ontology toolkit for problem domain concept location in program comprehension","N. R. Carvalho","Department of Informatics, University of Minho, Campus de Gualtar - 4710-057 Braga, Portugal","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1415","1418","Programmers are able to understand source code because they are able to relate program elements (e.g. modules, objects, or functions), with the real world concepts these elements are addressing. The main goal of this work is to enhance current program comprehension by systematically creating bidirectional mappings between domain concepts and source code. To achieve this, semantic bridges are required between natural language terms used in the problem domain and program elements written using formal programming languages. These bridges are created by an inference engine over a multi-ontology environment, including an ontological representation of the program, the problem domain, and the real world effects program execution produces. These ontologies are populated with data collected from both domains, and enriched using available Natural Language Processing and Information Retrieval techniques.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606731","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606731","","Conferences;Data mining;Engines;Natural languages;Ontologies;Software maintenance","formal languages;inference mechanisms;information retrieval;natural language processing;ontologies (artificial intelligence);programming languages;reverse engineering","bidirectional mapping;formal programming languages;inference engine;information retrieval techniques;multiontology environment;natural language processing;natural language terms;ontological representation;ontology toolkit;problem domain concept location;program comprehension;program elements;program execution;semantic bridges;source code","","2","","27","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"An intelligent state machine towards task-oriented search support","N. Y. Yen; Q. Zhao; Y. Liu; J. C. Tsai","School of Computer Science and Engineering The University of Aizu Aizu-Wakamatsu, Japan","2013 IEEE International Conference on Cybernetics (CYBCO)","20131003","2013","","","46","50","Researchers tend to agree that an increasing quantity of data has caused the complexity and difficulty for information discovery, management and reuse. An essential factor relates to the increasing channels for information sharing. Finding information, especially those meaningful or useful one, that meets ultimate task of user becomes harder then it is used to be. In this research, issues concerning the use of user-generated contents for individual search support are investigated. In order to make efficient use of user-generated contents, an intelligent state machine, as a hybridization of graph model and petri-net model (i.e. Document Sensitive Petri-Net), is proposed. It is utilized to clarify the vague usage scenario between user-generated contents, such as discussions, posts, etc., and to identify correlations and experiences within them. As a practical contribution, an interactive search algorithm that generates potential solutions for individual is implemented. The feasibility of this research is demonstrated by a series of experiments and empirical studies with around 320,000 user-generated contents collected from the Internet and 180 users.","","Electronic:978-1-4673-6469-0; POD:978-1-4673-6468-3","10.1109/CYBConf.2013.6617439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617439","Decision Support;Document Sensitive Petri-Net;Human-Centered;Intelligent State Machine;User-Generated Contents","Conferences;Correlation;Educational institutions;Internet;Peer-to-peer computing;Social network services;User-generated content","Internet;Petri nets;decision making;finite state machines;information retrieval;search problems","Internet;Petri-net model;decision-making support;graph hybridization model;information discovery;information retrieval;information sharing;intelligent state machine;interactive search algorithm;task-oriented search support;user-generated contents","","0","","11","","","13-15 June 2013","","IEEE","IEEE Conference Publications"
"User Participation in Social Networks: The Case of Balatarin, an Online Persian-Language Social Bookmarking System","A. Rostami","Dept. of Comput. Sci., Social Media & Web Technol., Linnaeus Univ., Va&#x0308;xjo&#x0308;, Sweden","2013 10th International Conference on Information Technology: New Generations","20130930","2013","","","445","449","The raise of social Web brought new ways of disseminating information, news and published content over the Web. Social news aggregator Web sites such as Digg and Reddit are already known for their role in promoting user-based content by utilizing story submission and voting system. Balatarin, the first and one of the most famous news aggregator in Persian language, is a social book marking Web site, which is developed by combining the underlying idea of Reddit, Digg, News vine and Del.icio.us. Although Balata in is among popular and influential Web sites in the Middle East, there has not been any analytical research on it. Here, we studied the user activity to see whether it is dominated by few users' participation, or social incidents can motivate users and influence their contribution. For our study, we collected data representing user activity over the course of three years. We carried out some computational analysis on user participation under social circumstances, such as blockade and election in Iran. The evaluation of the results shows that such social factors can motivate users to break the tyranny of the minority submitters by contributing more.","","Electronic:978-0-7695-4967-5; POD:978-1-4673-5960-3","10.1109/ITNG.2013.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614347","Balatarin.com;news aggregation;social media;social network analysis;user participation","Crawlers;Data mining;Databases;Media;Nominations and elections;Social network services;Web sites","data mining;information retrieval;social networking (online)","Balatarin;Del.icio.us;Digg;Iran;Middle East;News vine;Reddit;blockade;computational analysis;election;information dissemination;news dissemination;online Persian-language social bookmarking system;published content dissemination;social Web;social book marking Web site;social circumstance;social factors;social network;social news aggregator Web site;story submission;user activity;user participation;user-based content;voting system","","0","","8","","","15-17 April 2013","","IEEE","IEEE Conference Publications"
"Extracting candidate terms from medical texts","I. Bentounsi; Z. Boufaida","Lire Laboratory, Department of Software Technology and Information, System, Universtity of Constantine 2, Constantine, Algeria","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","4","In this paper, we present a new method for the construction of domain ontology from texts in the medical field. In our method, NLP tools are not used since our goal is to reduce the amount of noise and the number of filters applied. This allows us to manage the control of semantic quality of Candidate Terms (CT). The proposed method is based on the technique of semantic annotation for controlled terminology extraction via semantic resources, which we supplement with a Coloring Strategy Identification (CSI) method allowing the identification of medical terms and an extraction of CT according to their appearance in the text. In CSI, we exploit the resulting metadata for semantic annotation via some rules. Moreover, we apply a method of categorization by prototype model to overcome perceived silence. The result of this extraction is structured in XML representing the hierarchy of concepts that composes our medical ontology.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616486","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616486","coloration;document in XML;filters;knowledge extraction;noise;ontology;semantic annotation","Noise;Ontologies;Semantics;Syntactics;Terminology;Unified modeling language;XML","XML;information retrieval;medical computing;meta data;ontologies (artificial intelligence);programming language semantics;quality control;text analysis","XML;candidate term extraction;coloring strategy identification;controlled terminology extraction;medical domain ontology;medical term identification;medical text;metadata;noise reduction;prototype model;semantic annotation;semantic quality control;semantic quality management;semantic resource","","1","","14","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"WαSH-ing visual repositories: Searching Europeana using local features","C. Varytimidis; K. Rapantzikos; S. Kollias","National Technical University of Athens, Greece","2013 18th International Conference on Digital Signal Processing (DSP)","20131010","2013","","","1","6","Museums, libraries, national archives and art galleries deal with visual objects that must be made accessible to a wide variety of experts or non-experts like researchers, art lovers or interested people. The ability to identify objects sharing some aspect of visual similarity can be useful when trying to trace historical influences or when looking for further examples of paintings, sculptures or other cultural objects appealing to their taste. In this direction, we use our recent detector [1] for an image retrieval task on a subset of Europeana's<sup>1</sup> content. The detector produces distinctive features by grouping sampled image edges according to proper shape stability measures. We evaluate the detector by integrating it into the Visual Search Engine for Europeana (Vieu)<sup>2</sup> tool.","1546-1874;15461874","Electronic:978-1-4673-5807-1; POD:978-1-4673-5806-4","10.1109/ICDSP.2013.6622702","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622702","","Computer vision;Detectors;Europe;Feature extraction;Image edge detection;Shape;Visualization","art;edge detection;history;image retrieval;information retrieval;libraries;museums;search engines","Europeana (Vieu)2 tool;WαSH-ing visual repository;art gallery;cultural objects;detector;historical influences;image retrieval task;library;museums;national archives;paintings;sampled image edges;sculptures;searching Europeana;shape stability measures;visual objects;visual search engine;visual similarity","","0","","21","","","1-3 July 2013","","IEEE","IEEE Conference Publications"
"Correlation-based Feature Analysis and Multi-Modality Fusion framework for multimedia semantic retrieval","Hsin-Yu Ha; Yimin Yang; F. C. Fleites; Shu-Ching Chen","School of Computing and Information Sciences, Florida International University, 1200 SW 8th Street, Miami, 33199, USA","2013 IEEE International Conference on Multimedia and Expo (ICME)","20130926","2013","","","1","6","In this paper, we propose a Correlation based Feature Analysis (CFA) and Multi-Modality Fusion (CFA-MMF) framework for multimedia semantic concept retrieval. The CFA method is able to reduce the feature space and capture the correlation between features, separating the feature set into different feature groups, called Hidden Coherent Feature Groups (HCFGs), based on Maximum Spanning Tree (MaxST) algorithm. A correlation matrix is built upon feature pair correlations, and then a MaxST is constructed based on the correlation matrix. By performing a graph cut procedure on the MaxST, a set of feature groups are obtained, where the intra-group correlation is maximized and the inter-group correlation is minimized. Finally, one classifier is trained for each of the feature groups, and the generated scores from different classifiers are fused for the final retrieval. The proposed framework is effective because it reduces the dimensionality of the feature space. The experimental results on the NUSWIDE-Lite data set demonstrate the effectiveness of the proposed CFA-MMF framework.","1945-7871;19457871","Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8","10.1109/ICME.2013.6607639","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607639","feature correlation;fusion;maximum spanning tree;multi-modality;multimedia semantic retrieval","Algorithm design and analysis;Correlation;Feature extraction;Multimedia communication;Semantics;Training;Visualization","correlation methods;information retrieval;matrix algebra;multimedia computing;semantic Web;trees (mathematics)","CFA method;CFA-MMF framework;HCFG;MaxST algorithm;NUSWIDE-lite data set;correlation matrix;correlation-based feature analysis;feature pair correlations;feature space;graph cut procedure;hidden coherent feature groups;inter-group correlation;intra-group correlation;maximum spanning tree algorithm;multimedia semantic concept retrieval;multimedia semantic retrieval;multimodality fusion framework","","1","","19","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Mining the multilingual terminology from the web","F. Sadat","University of Quebec in Montreal (UQAM), Department of Computer Science, 201 President Kennedy, Montreal, QC, H2X 3Y7, Canada","2013 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","20131010","2013","","","41","45","Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and exploit the idea of using multilingual Web-based encyclopaedias such as Wikipedia as comparable corpora for multilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and texts. The next step will be using the linguistics information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statistics-based and linguistic-based approaches were applied on Japanese-French-English languages. These evaluations showed a real open improvement and good quality of the extracted term candidates for building or enriching multilingual ontologies, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.","1555-5798;15555798","Electronic:978-1-4799-1501-9; POD:978-1-4799-1500-2; USB:978-1-4799-1499-9","10.1109/PACRIM.2013.6625446","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625446","Web application;lexical database;natural language processing","Electronic publishing;Encyclopedias;Internet;Ontologies;Terminology","Internet;Web sites;data mining;dictionaries;encyclopaedias;information filtering;information retrieval systems;ontologies (artificial intelligence);text analysis","Japanese-French-English languages;Web;Wikipedia link information;combined statistics-based-linguistic-based approaches;cross-language information retrieval system;dictionaries;information filtering;language pairs;multilingual Web-based encyclopaedias;multilingual linguistic resources;multilingual ontologies;multilingual terminology extraction;multilingual terminology mining;parallel corpora;source query;text domains","","0","","22","","","27-29 Aug. 2013","","IEEE","IEEE Conference Publications"
"Content extraction of Quran Interpretation (Tafseer) books for digital content creation and distribution","M. Menacer; A. Arbaoui","NOOR Research Centre, Taibah University, College Computer Science & Engineering, Al Madinah, Saudi Arabia","2013 World Congress on Computer and Information Technology (WCCIT)","20131003","2013","","","1","3","This paper presents concepts for relevant content extract of known and prominent Quran interpretation (Tafseer) books. The extracted content can be used efficiently in building, creating and distributing digital and multimedia content on Quran, Tafseer and Islamic issues. Due to the uniqueness of Quran and most renown Tafseer books, extracting relevant information in a structured manner and with accuracy is a quite delicate matter, because of the important and sensitive issues being dealt with. Natural Language processing techniques for automatic information retrieval and extraction are not reliable and desirable approach in this case, due to the level of inaccuracy and objectivity involved, which is not tolerated for such highly referenced books for muslims. The aim of this paper is to propose a systematic approach into extracting and collecting the most relevant information in a structured manner from Tafseer books that are useful for academic purposes as well as for general use. Al Asfahani Tafseer book, `Mufradat fi Gharib al-Quran', has been chosen as in this case. Building more digital content details of the book would allow for better search as well as further development into related authoring and indexing. Overall concepts of the content extraction approach is presented in this paper with the different phases involved.","","Electronic:978-1-4799-0462-4; POD:978-1-4799-0461-7","10.1109/WCCIT.2013.6618750","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618750","Content distribution;Content extraction;Digital content creation;Quran;Quran Sciences;Tafseer books;multimedia","Authentication;Buildings;Data mining;Databases;Educational institutions;Internet;Multimedia communication","electronic publishing;information retrieval;multimedia computing;natural language processing","Al Asfahani Tafseer book;Islamic issues;Mufradat fi Gharib al-Quran;Muslims;Quran interpretation Tafseer books;Quran issues;Tafseer issues;automatic information extraction;automatic information retrieval;content extraction;content extraction approach;digital content creation;digital content details;digital content distribution;distributing digital content;multimedia content;natural language processing techniques","","0","","7","","","22-24 June 2013","","IEEE","IEEE Conference Publications"
"Semantic-based sound retrieval by ERP in rapid serial auditory presentation paradigm","L. Jiang; B. Cai; S. Xiao; Y. Wang; W. Chen; X. Zheng","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20130926","2013","","","2788","2791","“Semantic gap” is the major bottleneck of semantic-based multimedia retrieval technique in the field of information retrieval. Studies have shown that robust semantic-based image retrieval can be achieved by single-trial visual evoked event related potential (ERP) detection. However, the question remains whether auditory evoked ERP can be utilized to achieve semantic-based sound retrieval. In this paper, we investigated this question in the rapid serial auditory presentation (RSAP) paradigm. Eight BCI-naïve participants were instructed to perform target detection in RSAP sequences with the vocalizations of 8 familiar animals as sound stimuli, and we compared ERP components and single-trial ERP classification performance between two conditions, the target was a predefined specific one, and the targets were different but belonged to the same semantic category (i.e., semantic-based sound retrieval). Although the amplitudes of ERP components (e.g., N2 and P3) and classification performance decreased a little due to the difficulty of the semantic-based sound retrieval tasks, the best two participants still achieved the area under the receive operating characteristic curve (AUC) of single-trial ERP detection more than 0.77. It suggested that semantic-based sound retrieval by auditory evoked ERP was potentially feasible.","1094-687X;1094687X","Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3","10.1109/EMBC.2013.6610119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610119","","Animals;Electrodes;Electroencephalography;Multimedia communication;Object detection;Scalp;Semantics","acoustic signal processing;audio signal processing;auditory evoked potentials;bioacoustics;brain-computer interfaces;information retrieval;medical signal detection;semantic networks;sensitivity analysis;signal classification","AUC;BCI-naive participant;ERP component amplitudes;RSAP sequences;area under the receive operating characteristic curve;auditory evoked ERP;information retrieval;rapid serial auditory presentation paradigm;semantic-based image retrieval;semantic-based multimedia retrieval technique;semantic-based sound retrieval task;single-trial ERP classification performance;single-trial ERP detection;single-trial visual evoked event related potential detection;sound stimuli;target detection;vocalization","","0","","18","","","3-7 July 2013","","IEEE","IEEE Conference Publications"
"Full ceramic bearing fault diagnosis using LAMSTAR neural network","J. M. Yoon; D. He; B. Qiu","Department of Mechanical and Industrial Engineering, The University of Illinois at Chicago, USA","2013 IEEE Conference on Prognostics and Health Management (PHM)","20131007","2013","","","1","9","In this paper, an integrated full ceramic bearing fault diagnostic system developed with acoustic emission (AE) sensors and a large memory storage and retrieval (LAMSTAR) artificial neural network (ANN) is presented. LAMSTAR is a newly developed and US patented neural network algorithm. The performance of the diagnostic system is compared with those implemented with other types of fault classification algorithms using laboratory seeded fault test data. The presented diagnostic system with LAMSTAR network achieved over 93% individual fault detection accuracies along with over 96% overall accuracy.","","Electronic:978-1-4673-5723-4; POD:978-1-4673-5721-0","10.1109/ICPHM.2013.6621427","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621427","AE sensors;ANN;LAMSTAR;acoustic emission sensors;artificial neural network;fault diagnosis;full ceramic bearing","Artificial neural networks;Sensors","acoustic emission;ceramics;condition monitoring;fault diagnosis;information retrieval;machine bearings;mechanical engineering computing;neural nets;pattern classification;signal classification","AE sensors;LAMSTAR ANN;acoustic emission sensors;diagnostic system performance;fault classification algorithms;integrated full ceramic bearing fault diagnostic system;laboratory-seeded fault test data;large-memory storage-and-retrieval artificial neural network","","0","","15","","","24-27 June 2013","","IEEE","IEEE Conference Publications"
"Emulation-Based Fault Effect Analysis for Resource Constrained, Secure, and Dependable Systems","N. Druml; M. Menghin; D. Kroisleitner; C. Steger; R. Weiss; A. Krieg; H. Bock; J. Haid","Inst. for Tech. Inf., Graz Univ. of Technol., Graz, Austria","2013 Euromicro Conference on Digital System Design","20131015","2013","","","337","344","Testing hardware and software components regarding their fault detection and fault handling capabilities is of vital importance. However, considering the fact that security systems are built using several distributed hardware components (e.g., reader/smart card authentication system), testing each component individually is insufficient. Because novel system-wide multi-fault attack campaigns can be conducted, fault propagation as well as fault handling of the entire system must be regarded. State-of-the-art emulation-based fault analysis approaches neglect this system aspect as well as the fault impact on power dissipation and power supply. Here, we present a novel analysis methodology that characterizes the behavior of complete systems during the design phase, in terms of fault handling, power dissipation, and power supply. Emulation-based techniques are applied to provide cycle accurate analysis information of the system-under-test in real time. The presented approach is of importance when it comes to test resource constrained, dependable, and high secure system designs. We demonstrate the application of this approach by means of a reader/smart card authentication system. Furthermore, we show how system level-based multi-fault attacks can be emulated and how the resulting system behavior (e.g., power consumption, power supply, information leakage) can be exploited to extract security relevant information.","","Electronic:978-1-4799-2978-8; POD:978-1-4799-2979-5","10.1109/DSD.2013.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628298","Estimation-based Techniques;Fault Emulation;Hardware Emulation;Power Analysis","Circuit faults;Emulation;Hardware;Power demand;Security;Smart cards","information retrieval;power aware computing;program testing;security of data;smart cards;software fault tolerance;terminal emulation","dependable system;distributed hardware component testing;emulation-based fault analysis;fault detection;fault handling;fault impact;fault propagation;information extraction;level-based multifault attack;power dissipation;power supply;reader authentication system;resource constraint test;security system;smart card authentication system;software component testing;system-under-test","","2","","26","","","4-6 Sept. 2013","","IEEE","IEEE Conference Publications"
"Word Semantic Similarity Based on Document's Title","M. S. Hamani; R. Maamri","Dept. of STIC, Univ. of M'sila, M'sila, Algeria","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","43","47","Measuring similarity between words using a search engine based on page counts alone is a challenging task. Search engines consider a document as a bag of words, ignoring the position of words in a document. In order to measure semantic similarity between two given words, this paper proposes a transformation function for web measures along with a new approach that exploits the document's title attribute and uses page counts alone returned by Web search engines. Experimental results on benchmark datasets show that the proposed approach outperforms snippets alone methods, achieving a correlation coefficient up to 71%.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621343","Decision Making;Page Count;Semantic Similarity;Text Mining;Title","Correlation;Correlation coefficient;Engines;Google;Search engines;Semantics;Web search","information retrieval;search engines","Web measures;Web search engines;document title;page counts;search engine;transformation function;word semantic similarity","","0","","14","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"Can we do better in unimodal biometric systems? A novel rank-based score normalization framework for multi-sample galleries","P. Moutafis; I. A. Kakadiaris","Computational Biomedicine Lab, Department of Computer Science University of Houston, 4800 Calhoun Rd. Houston, TX 77004","2013 International Conference on Biometrics (ICB)","20130930","2013","","","1","8","The large amount of research on multimodal systems raises an important question: can we extract additional information from unimodal systems? In this paper, we propose a rank-based score normalization framework that addresses this problem when multi-sample galleries are avail-able. The main idea is to partition the matching scores into subsets and normalize each subset independently. In addition, we present two versions of our framework that: (i) use gallery-based information (i.e., gallery versus gallery scores), and (ii) update available information in an online fashion. We use the theory of Stochastic Dominance to illustrate that the proposed framework can increase the system's performance. Our approach: (i) does not require tuning of any parameters, (ii) can be used in conjunction with any score normalization technique and any integration rule, and (iii) extends the use of W-score normalization to multisample galleries. While our approach is better suited for an Open-set Identification task, we also demonstrate that it can be used for a Verification task. In order to assess the performance of the proposed framework we conduct experiments using the BDCP Face database. Our approach improves the Detection and Identification Rate by 14.87% for Z-score and by 4.82% for W-score.","2376-4201;23764201","Electronic:978-1-4799-0310-8; POD:978-1-4799-0311-5","10.1109/ICB.2013.6612983","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612983","","Biometrics (access control);Databases;Face;Partitioning algorithms;Probes;Robustness;Standards","biometrics (access control);formal verification;information retrieval;stochastic processes","BDCP face database;W-score normalization;gallery-based information;information extraction;integration rule;matching score partition;multimodal system;multisample gallery;open set identification task;rank-based score normalization framework;stochastic dominance theory;unimodal biometric system;verification task","","2","","14","","","4-7 June 2013","","IEEE","IEEE Conference Publications"
"eMontage: An architecture for rapid integration of situational awareness data at the edge","S. Simanta; G. Cahill; E. Morris","Carnegie Mellon Software Eng. Inst., Pittsburgh, PA, USA","2013 1st International Workshop on the Engineering of Mobile-Enabled Systems (MOBS)","20130930","2013","","","7","12","This paper presents an architecture for rapid integration of situational awareness data on mobile handheld devices in resource-constrained, hostile environments. This capability will give users in crisis environments access to relevant data presented on a single screen with a consistent user interface. The framework and architecture discussed here enable the rapid addition of both publicly available and domain-specific data sources. This solution enables users to construct geospatial data mashups. Our architecture for accessing and filtering data from multiple sources provides benefits such as combining data from real-time and historical sources, operating in connected or disconnected modes, supporting individual selection and filtering of data, and integrating data from multiple sources.","","Electronic:978-1-4673-6333-4; POD:978-1-4673-6334-1","10.1109/MOBS.2013.6614216","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614216","","Bandwidth;Computer architecture;Mashups;Mobile communication;Servers;Smart phones","geographic information systems;information retrieval;mobile handsets;user interfaces","consistent user interface;data accessing;data filtering;domain-specific data sources;eMontage;geospatial data mashups;historical sources;mobile handheld devices;publicly available data sources;rapid situational awareness data integration;real-time sources;resource-constrained environments","","1","","8","","","25-25 May 2013","","IEEE","IEEE Conference Publications"
"A cross-standard metadata formatting structure","S. L. Linfoot; T. Coughlin","Founder & CTO of MediaTag Ltd., Leicester, UK","IEEE Transactions on Consumer Electronics","20131015","2013","59","3","550","555","Over the past 10 years, the amount of user-generated content has increased at phenomenal rates. However, due to a lack of common content descriptors, reliable website searches and access to this content is becoming increasingly difficult. There are third party products available that allow metadata descriptors to be created automatically but the formatting may not be compatible with the website database. With so many different standards for metadata, the whole area is becoming, effectively, de-standardized. This paper will present a cross-standard formatting platform that will allow compatibility between standards.","0098-3063;00983063","","10.1109/TCE.2013.6626237","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626237","Metadata;cross-standard;standardization;topology","Filtering;Media;Physical layer;Physiology;Psychology;Semantics;Standards","Web sites;information retrieval;information retrieval systems;meta data","Web site database;cross-standard metadata formatting structure;metadata descriptor","","1","","9","","","August 2013","","IEEE","IEEE Journals & Magazines"
"Fast HMM-Filler Approach for Key Word Spotting in Handwritten Documents","A. H. Toselli; E. Vidal","Univ. Politec. de Valencia, Valencia, Spain","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","501","505","The so-called filler or garbage Hidden Markov Models (HMM) are among the most widely used models for lexicon-free, query by string key word spotting in the fields of speech recognition and (lately) handwritten text recognition. An important drawback of this approach is the large computational cost of the keyword-specific HMM Viterbi decoding process needed to obtain the confidence scores of each word to be spotted. This paper presents a novel way to compute such confidence scores, directly from character lattices produced during a single Viterbi decoding process using only the ""filler"" model (i.e. no explicit keyword-specific decoding is needed). Experiments show that, as compared with the classical HMM-filler approach, the proposed method obtains essentially the same spotting results, while requiring between one and two orders of magnitude less query computing time.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628671","Character Lattice;HMM-Filler Model;Spotting","Computational modeling;Decoding;Handwriting recognition;Hidden Markov models;Indexing;Training;Viterbi algorithm","Viterbi decoding;document image processing;handwriting recognition;hidden Markov models;information retrieval","HMM Viterbi decoding process;character lattices;fast HMM-filler approach;garbage Hidden Markov Models;handwritten documents;handwritten text recognition;query computing time;single Viterbi decoding process;speech recognition;string key word spotting","","11","","13","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Creating openEHR content to different moments of care: Obstetrics emergency scenario","G. M. Bacelar-Silva; R. F. Sousa-Santos; R. J. Cruz-Correia","Center for Research in Health Technologies and Information Systems, Porto, Portugal","Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems","20131010","2013","","","361","364","Introduction: If health information systems used a common reference model to store patient data it would ease health data exchange and achievement of semantic interoperability. Aim: To describe the process and the issues related to developing openEHR content to be used during different moments in the Obstetrics scenario, namely to retrieve patient's data. Methods: It was conducted a search on two major repositories - openEHR and National E-Health Transition Authority (NEHTA) - for archetypes to represent clinical concepts defined in a previous study (Obstetrics Scenario). The better-suited archetypes were chosen to design the templates. Results: 22 clinical concepts were identified and 21 of them could be covered by the archetypes. It was necessary 28 archetypes (23 openEHR / 5 NEHTA) to represent them (13 openEHR / 8 NEHTA). Discussion and Conclusion: This paper demonstrates that it was possible to create an openEHR template to describe the information needs during different moments of an Obstetrics scenario. A possible application is to use them to retrieve patient's data from external systems.","1063-7125;10637125","Electronic:978-1-4799-1053-3; POD:978-1-4799-1052-6","10.1109/CBMS.2013.6627816","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6627816","","Biomedical imaging;Guidelines;Information exchange;Interoperability;Medical services;Semantics","health care;information retrieval;medical information systems;obstetrics;open systems","NEHTA;National E-Health Transition Authority;health care;health data exchange;health information system;obstetrics emergency scenario;openEHR template;patient data retrieval;patient data storage;semantic interoperability","","0","","6","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"A Study on Support System of the Library Reference Service Using Reference Examples and Library Pathfinders","Y. Nakao","Dept. of Libr. Sci., Kyushu Univ., Fukuoka, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","51","55","Reference examples and library pathfinders are conventionally used as separate sources of information in a reference service. This paper attempts to combine reference examples and library pathfinders. To achieve this objective, reference tools were used in processing. The data cooperation model could provide effective support for information searching.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.90","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630316","Research Navi;collaborative reference database;library pathfinder;reference example;reference service;reference tool","Chemistry;Collaboration;Crystals;Databases;Dictionaries;Ice;Libraries","information retrieval;libraries","data cooperation model;information searching;library pathfinders;library reference service;reference examples;reference tools;support system","","0","","11","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Using a clinical document importance estimator to optimize an agent-based clinical report retrieval system","J. H. Patriarca-Almeida; B. Santos; R. J. Cruz-Correia","CINTESIS, Alameda Prof. Hern&#x00E2;ni Monteiro, 4200-319 Porto, Portugal","Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems","20131010","2013","","","469","472","The OPTIM project aims to optimize the graphical user interface of an electronic health record (EHR) by predicting clinical documents' relevance and provide a ranked list of relevant documents for the given user at a certain time. In this paper we will describe part of the system architecture pertaining to report retrieval and relevance assignment, focusing on the optimization of an agent based report retrieval system (MAID) using the webservice layer of the OPTIM project. The prototype of MAID using the optimized retrieval was tested in a simulated environment. In the executed simulations the classifier was able to rate 10 reports per second. Including a report rating in the EHR interface based on clinical relevance calculated by mathematical models can potentially improve the usability of the EHR.","1063-7125;10637125","Electronic:978-1-4799-1053-3; POD:978-1-4799-1052-6","10.1109/CBMS.2013.6627843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6627843","","Databases;Hospitals;Multi-agent systems;Optimization;Prototypes;Web services","Web services;document handling;graphical user interfaces;information retrieval;medical information systems;optimisation","EHR interface;MAID;OPTIM project;agent based clinical report retrieval system optimization;clinical document importance estimator;clinical document relevance;clinical relevance;electronic health record;graphical user interface;mathematical model;relevance assignment;simulated environment;system architecture;webservice layer","","0","","4","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"CENDARI: Establishing a digital ecosystem for historical research","R. Gartner; M. Hedges","Centre for e-Research King's College London London, United Kingdom","2013 7th IEEE International Conference on Digital Ecosystems and Technologies (DEST)","20130926","2013","","","61","65","The CENDARI (Collaborative European Digital Archive Infrastructure) project, which aims to create a research infrastructure for World War I and medieval history, is examined as a example digital ecosystem. The diverse information requirements of these two communities are met by a strategy which combines newly devised approaches to metadata and tools for data integration and ontology development. The ways in which this strategy produces an environment which conforms to the notion of a digital ecosystem is also examined.","2150-4938;21504938","Electronic:978-1-4799-0786-1; POD:978-1-4799-0785-4","10.1109/DEST.2013.6611330","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611330","digital ecosystems;intermediary metadata schemas;metadata;virtual research environments","Communities;Ecosystems;Europe;Libraries;Ontologies;Standards;XML","data integration;history;information retrieval systems;ontologies (artificial intelligence);research and development","CENDARI;World War I;collaborative European digital archive infrastructure;data integration;digital ecosystem;diverse information requirements;historical research;metadata;ontology development;research infrastructure","","0","","14","","","24-26 July 2013","","IEEE","IEEE Conference Publications"
"Data processing and presentation for a personalised, image-driven medical graphical avatar","M. de Ridder; L. Bi; L. Constantinescu; J. Kim; D. D. Feng","Sch. of Inf. Technol., Univ. of Sydney, Sydney, NSW, Australia","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20130926","2013","","","4183","4186","With the continuing digital revolution in the healthcare industry, patients are being confronted with the difficult task of managing their digital medical data. Current personal health record (PHR) systems are able to store and consolidate this data, but they are limited in providing tools to facilitate patients' understanding and management of the data. One reason for this stems from the limited use of contextual information, especially in presenting spatial details such as in volumetric images and videos, as well as time-based temporal data. Further, lack of meaningful visualisation techniques exist to represent the data stored in PHRs. In this paper we propose a medical graphical avatar (MGA) constructed from whole-body patient images, and a navigable timeline of the patient's medical records. A data mapping framework is presented that extracts information from medical multimedia data such as images, video and text, to populate our PHR timeline, while also embedding spatial and textual annotations such as regions of interest (ROIs) that are automatically derived from image processing algorithms. We developed a prototype to process the various forms of PHR data and present the data in a graphical avatar. We analysed the usefulness of our system under various scenarios of patient data use and present preliminary results that indicate that our system performs well on standard consumer hardware.","1094-687X;1094687X","Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3","10.1109/EMBC.2013.6610467","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6610467","","Avatars;Biomedical imaging;Computed tomography;Data visualization;Image segmentation;Medical services;Videos","data handling;data structures;data visualisation;health care;information retrieval;information storage;medical information systems;multimedia databases;visual databases","PHR data;PHR timeline;ROI;data mapping framework;data presentation;data processing;data representation;data storage;digital medical data management;embedding spatial annotation;healthcare industry;image processing algorithm;image-driven medical graphical avatar;information extraction;meaningful visualisation technique;medical multimedia data;patient medical record;personal health record system;personalised-driven medical graphical avatar;regions of interest;standard consumer hardware;textual annotation;time-based temporal data;volumetric images;volumetric videos;whole-body patient image","","1","","29","","","3-7 July 2013","","IEEE","IEEE Conference Publications"
"Document Specific Sparse Coding for Word Retrieval","R. Shekhar; C. V. Jawahar","Centre for Visual Inf. Technol., Int. Inst. of Inf. Technol., Hyderabad, India","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","643","647","Bag of words (BoW) based retrieval is an efficient method to compare the visual similarity between two images. Recognition free methods based on BoW have shown to outperform OCR based methods. We further improve the performance by defining a document specific sparse coding scheme for representing visual words (interest points) in document images. Our method is motivated by the successful use of sparsity in signal representation by exploiting the neighbourhood properties. In addition to providing insights into the design of the coding scheme, we also verify the method on two data sets and compare with the recent methods. We have also developed text query based search solution, and we report performance comparable to image based search.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.132","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628697","Bag of Words;Document Image Retrieval;Sparse Coding","Encoding;Feature extraction;Image coding;Quantization (signal);Vectors;Visualization;Vocabulary","document image processing;image coding;image representation;information retrieval","BoW based retrieval;bag of words based retrieval;document images;document specific sparse coding;recognition free methods;signal representation;text query based search solution;visual similarity","","2","","16","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Clinical Data Mining: Problems, Pitfalls and Solutions","E. Weitschek; G. Felici; P. Bertolazzi","Dept. of Comput. Sci. & Autom., Univ. Roma Tre, Rome, Italy","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","90","94","The wide spread of electronic data collection in medical environments leads to an exponential growth of clinical data extracted from heterogeneous patient samples. Collecting, managing, integrating and analyzing these data are essential activities in order to shed light on diseases and on related therapies. The major issues in clinical data analysis are the incompleteness (missing values), the different adopted measure scales, the integration of the disparate collection procedures. Therefore, the main challenges are in managing clinical data, in discovering patients interactions, and in integrating the different data sources. The final goal is to extract relevant information from huge amounts of clinical data. Therefore, the analysis of clinical data requires new effective and efficient methods to extract compact and relevant information: the interdisciplinary field of data mining, which guides the automated knowledge discovery process, is a natural way to approach the complex task of clinical data analysis. Data mining deals with structured and unstructured data, that are, respectively, data for which we can give a model or not. For example, in clinical contexts it is important to highlight those trials (variables) that are frequent in a particular disease diagnosis. The objective of this work is to study and apply methods to manage and retrieve relevant information in clinical data sets. A practical analysis from real patient data collected from several dementia clinical departments in Italy is reported as example of clinical data mining. The particular field of logic classification, where a data model is computed in form of propositional logic formulas, is investigated for clinical data mining and compared to other techniques, showing that it is a successful approach to compute a compact data model for clinical knowledge discovery.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621352","classification;knowledge extraction;supervised learning","Data analysis;Data mining;Data models;Diseases;Feature extraction;Medical diagnostic imaging","data mining;data models;information retrieval;medical information systems","Italy;clinical data management;clinical data mining;clinical data retrieval;clinical knowledge discovery;compact data model;data model;dementia clinical departments;logic classification;propositional logic formulas;relevant information retrieval","","3","","29","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"Affective retrieval: Multimedia data retrieval based on impression","T. Hochin","Information Science, Kyoto Institute of Technology, Japan","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","20130926","2013","","","2","2","Summary form only given. Affective computing and affective engineering have attracted the interests of researchers in the field of information science. Feelings, sensitivity, impressions, and emotions are treated in affective engineering. Multimedia data including pictures, music pieces, and movies give us some impression. Impression should be considered in the multimedia data retrieval. This means that the similarity of multimedia data includes the factor of impression. In the affective retrieval, the similarity of data depends on the similarity of impression. Additionally, impressive words may be specified in retrieval conditions. For example, we may specify the word “fresh” in retrieving pictures and/or music pieces. In order to realize the affective retrieval, impression must be analyzed, and the degrees of impression must be derived or calculated, and be used. In this talk, the background and the end of affective retrieval are presented. Major approaches to affective retrieval are surveyed. A prototype system of affective retrieval is introduced with the demonstration. This system retrieves multimedia data based on the similarity of impression. Impressive words could be specified in the retrieval. It is also shown that affective retrieval easily enables cross-media retrieval.","","Electronic:978-1-4799-0174-6; POD:978-1-4799-0173-9","10.1109/ICIS.2013.6607806","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607806","","Abstracts;Affective computing;Information science;Motion pictures;Multimedia communication;Multimedia computing;Sensitivity","information retrieval;multimedia systems","affective computing;affective engineering;affective retrieval;cross-media retrieval;impression degree;impression similarity;information science field;multimedia data retrieval;multimedia data similarity","","0","","","","","16-20 June 2013","","IEEE","IEEE Conference Publications"
"GCG: Mining maximal complete graph patterns from large spatial data","G. Al-Naymat","College of Computer Science and Information Technology, University of Dammam, KSA","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","8","Recent research on pattern discovery has progressed from mining frequent patterns and sequences to mining structured patterns, such as trees and graphs. Graphs as general data structure can model complex relations among data with wide applications in web exploration and social networks. However, the process of mining large graph patterns is a challenge due to the existence of large number of subgraphs. In this paper, we aim to mine only frequent complete graph patterns. A graph g in a database is complete if every pair of distinct vertices is connected by a unique edge. Grid Complete Graph (GCG) is a mining algorithm developed to explore interesting pruning techniques to extract maximal complete graphs from large spatial dataset existing in Sloan Digital Sky Survey (SDSS) data. Using a divide and conquer strategy, GCG shows high efficiency especially in the presence of large number of patterns. In this paper, we describe GCG that can mine not only simple co-location spatial patterns but also complex ones. To the best of our knowledge, this is the first algorithm used to exploit the extraction of maximal complete graphs in the process of mining complex co-location patterns in large spatial dataset.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616417","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616417","","Catalogs;Data mining;Earth;Extraterrestrial measurements;Itemsets;Spatial databases;Vectors","data mining;divide and conquer methods;information retrieval;network theory (graphs);social networking (online)","GCG;SDSS;Sloan Digital Sky Survey;Web exploration;connected graph;divide and conquer strategy;frequent pattern mining;grid complete graph;maximal complete graph extraction;maximal complete graph pattern mining;pattern discovery;pruning technique;social network;spatial data;structured pattern mining;subgraph","","0","","16","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"TCIA: An information resource to enable open science","F. W. Prior; K. Clark; P. Commean; J. Freymann; C. Jaffe; J. Kirby; S. Moore; K. Smith; L. Tarbox; B. Vendt; G. Marquez","Sch. of Med., Mallinckrodt Inst. of Radiol., Washington Univ., St. Louis, MO, USA","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20130926","2013","","","1282","1285","Reusable, publicly available data is a pillar of open science. The Cancer Imaging Archive (TCIA) is an open image archive service supporting cancer research. TCIA collects, de-identifies, curates and manages rich collections of oncology image data. Image data sets have been contributed by 28 institutions and additional image collections are underway. Since June of 2011, more than 2,000 users have registered to search and access data from this freely available resource. TCIA encourages and supports cancer-related open science communities by hosting and managing the image archive, providing project wiki space and searchable metadata repositories. The success of TCIA is measured by the number of active research projects it enables (>40) and the number of scientific publications and presentations that are produced using data from TCIA collections (39).","1094-687X;1094687X","Electronic:978-1-4577-0216-7; POD:978-1-4577-0215-0; USB:978-1-4577-0214-3","10.1109/EMBC.2013.6609742","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609742","","Biomedical imaging;Cancer;Educational institutions;Information services;Lungs;Radiology","Web sites;cancer;document image processing;information retrieval systems;medical information systems;meta data;visual databases","TCIA collections;The Cancer Imaging Archive;active research project;cancer research;cancer-related open science communities;image archive hosting;image archive managing;information resource;oncology image data collection;oncology image data curation;oncology image data de-identification;open image archive service;project wiki space;publicly available data;scientific publication;searchable metadata repositories","","3","","23","","","3-7 July 2013","","IEEE","IEEE Conference Publications"
"Exploring activeness of users in QA forums","V. S. Sinha; S. Mani; M. Gupta","IBM Research - New Delhi, India","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","77","80","Success of a Q&A forum depends on volume of content (questions and answers) and quality of content (are the questions asked relevant, answers provided correct etc). Community participation is essential to create and curate content. Since their inception in 2008, stack exchange based forums have been able to engage a large number of users to create a rich repository of good quality questions and answers. In this paper, we wish to investigate the “activeness” of users in the stackexchange network particularly from a perspective of content creation. We also attempt to measure how the forums' incentive mechanism has enabled user's activeness. Further, we investigate how user's have diffused to other parts of the stack exchange network over time, hence bootstrapping new forums.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624010","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624010","","Androids;Awards activities;Communities;Data mining;Electronic mail;Games;Humanoid robots","Web sites;question answering (information retrieval)","QA forums;content creation;content quality;content volume;stack exchange based forums;stack-exchange network;user activeness","","5","","4","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Extracting the semantic content of web pages via repeated structures","Zheng He; Hangzai Luo; Jianping Fan; Xiao Liu","East China Normal University, Shanghai, China","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","20131003","2013","","","1","6","Web pages may carry semantics that are very important to the authors and the readers. Due to many reasons, the authors may insert contents that are irrelevant to the underlying semantics of the page to different positions of the page, such as advertizements, guide bars, links. As a result, it may not lead good effect by using all the data of a web page to model its semantics. In this paper, we propose a framework that can extract the real semantic content from web pages via repeated structures of the HTML data. Our algorithm first detect the real semantic blocks in web pages via repeated structure segmentation, then extracts the real semantic content of the pages from real semantic blocks.","","Electronic:978-1-4799-1604-7; POD:978-1-4799-1603-0; USB:978-1-4799-1602-3","10.1109/ICMEW.2013.6618450","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618450","Repeated Structure;Semantic modeling;Web page","Data mining;Feature extraction;HTML;Nickel;Semantics;Visualization;Web pages","Web sites;hypermedia markup languages;information retrieval","HTML data;Web page semantics model;repeated structure segmentation;semantic block detection;semantic content extraction","","0","","17","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Estimating Feature Terms for Supporting Exploratory Browsing of Twitter Timelines","S. Itokawa; S. Shiramatsu; T. Ozono; T. Shintani","Dept. of Comput. Sci. & Eng., Nagoya Inst. of Technol., Nagoya, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","62","67","In recent years, use of the social networking/micro-blogging service Twitter has spread widely. Twitter users constantly post tweets and connect with an often huge number of other users. We need to optimize viewing a lot of tweets. For users who browse infrequently, many unread tweets appear on their timeline, making it difficult to view. It is also difficult to search the timeline for past tweets using the existing system. To solve these problems, we propose a system that supports exploratory browsing of Twitter timelines. The key element of the proposed system is that it presents feature terms from a specific time period, which enables users to see a relevant overview of their Twitter timelines. The experimental results show that our system can effectively extract feature terms from Twitter timelines within specific time periods.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630318","Twitter;exploratory searching;feature term extraction;natural language processing;user interface","Accuracy;Data mining;Feature extraction;Missiles;Nominations and elections;Schedules;Twitter","feature extraction;information retrieval;social networking (online)","Twitter timelines;exploratory browsing;feature term estimation;feature term extraction;microblogging service;social networking","","1","","9","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"An Efficient Proof of Retrievability with Public Auditing in Cloud Computing","J. Li; X. Tan; X. Chen; D. S. Wong","Sch. of Comput. Sci. & Educ. Software, Guangzhou Univ., Guangzhou, China","2013 5th International Conference on Intelligent Networking and Collaborative Systems","20131015","2013","","","93","98","Cloud Computing moves the application software and databases to the centralized large data centers, where the management of the data and services may not be fully trustworthy. In this work, we study the problem of ensuring the integrity of data storage in Cloud Computing. To reduce the computational cost at user side during the integrity verification of their data, the notion of public verifiability has been proposed. However, the challenge is that the computational burden is too huge for the users to compute the public authentication tags of file blocks. To tackle the challenge, we propose a new cloud storage architecture with two independent cloud servers, that is, the cloud storage server and the cloud audit server, where the latter is assumed to be semi-honest. In particular, we consider the task of allowing the cloud audit server, on behalf of the cloud users, to pre-process the data before uploading to the cloud storage server and later verifying the data integrity. The introduction of cloud audit server eliminates the involvement of user in the auditing and in the pre-processing phases.","","Electronic:978-0-7695-4988-0; POD:978-1-4673-6368-6","10.1109/INCoS.2013.185","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630294","","Cloud computing;Computational modeling;Educational institutions;Memory;Protocols;Security;Servers","cloud computing;computer centres;file servers;information retrieval;storage management","application software;centralized large data centers;cloud audit server;cloud computing;cloud storage architecture;cloud storage server;data management;data storage integrity;database;file blocks;independent cloud servers;public auditing;public authentication tags;public verifiability;retrievability proof","","4","","20","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Computational analysis of thematic blog data for sociological inference mining","V. K. Singh; P. Waila; R. Sadat; R. Piryani; A. Uddin","Department of Computer Science, South Asian University, New Delhi, India","2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI)","20130926","2013","","","293","298","This paper describes our proposed approach for computational analysis of thematic blog data through a novel combine of sophisticated Information Retrieval and Language Processing Techniques. We have implemented algorithms for Topic Modeling, Entity Extraction and Sentiment Classification with a view to draw sociologically relevant inferences from freeform unstructured social media data. Our experimental data comprised of more than 600 blog posts on the broader theme of `Discrimination, Abuse and Sexual Crime against Women' collected during two discrete time periods. We have tried to extract some important inferences from the data such as key persons and organizations mentioned in the data, key themes encountered in the entire data collection, sentiment orientation inherent in the texts and variation in topic trends during the two discrete time periods. The results obtained are very interesting and validate the usefulness of our approach for computational analysis of social media data.","","Electronic:978-1-4673-6400-3; POD:978-1-4673-6399-0; USB:978-1-4673-6398-3","10.1109/SACI.2013.6608985","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608985","Information Extraction;Sentiment Classification;Social Computing;Social Media;Text Analytics","Blogs;Computational modeling;Data mining;Educational institutions;Media;Organizations;Tag clouds","data mining;inference mechanisms;information retrieval;natural language processing;pattern classification;social networking (online);social sciences computing","abuse against women;computational analysis;discrete time periods;discrimination against women;entity extraction;freeform unstructured social media data;inference extraction;information retrieval techniques;language processing techniques;sentiment classification;sentiment orientation;sexual crime against women;social media data;sociological inference mining;sociologically relevant inferences;thematic blog data;topic modeling","","0","","25","","","23-25 May 2013","","IEEE","IEEE Conference Publications"
"Universal Words relationship question-answering from UNL Ontology","K. M. A. Salam; H. Uchida; S. Yamada; T. Nishino","The University of Electro-Communications, Tokyo, Japan","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","20130926","2013","","","423","427","Universal Networking Language (UNL) represents natural language sentences as a semantic network of Universal Words (UWs). UNL Ontology provides the semantic background using relations for each UWs. However UWs are formal and all information provided by the UNL Ontology may not be understandable for human without the UNL specification. Especially we need to ensure the UW dictionary editors can understand the UWs information stored in UNL Ontology. To understand the UWs information, this research propose a question-answering system to provide the relationship information by explaining UWs relations with natural language. Using this system, the editors can easily understand the relationship information stored in UNL ontology.","","Electronic:978-1-4799-0174-6; POD:978-1-4799-0173-9","10.1109/ICIS.2013.6607877","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607877","NLP;Ontology;UNL;Word Semantics","Animals;Computers;Dictionaries;Educational institutions;Natural languages;Ontologies;Semantics","dictionaries;natural language processing;ontologies (artificial intelligence);question answering (information retrieval);specification languages;word processing","UNL ontology;UNL specification;UW dictionary editors;UW information;information storage;natural language sentences;semantic network;universal networking language;universal word relationship question-answering system;universal words","","0","","5","","","16-20 June 2013","","IEEE","IEEE Conference Publications"
"Finding Critical Cells in Web Tables with SRL: Trying to Uncover the Devil's Tease","N. D. Mauro; F. Esposito; S. Ferilli","Dipt. di Inf., Univ. of Bari, Bari, Italy","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","882","886","Tables are extremely important components of documents, because they bear very informative content in a compact and structured way. Being able to understand a table's internal organization would allow to extract and reuse the data they contain. This can be reduced to recognizing critical cells only. Since purely algorithmic approaches are unable to deal with the many different table layouts designed to represent particular kinds of information and/or particular perspectives on them, Machine Learning may represent an effective solution. On one hand, the spatial organization of tables puts a strong emphasis on the relationships among cells, on the other, the extreme variability in style, size, and aims of tables requires flexible approaches. This paper proposes the exploitation of a Statistical Relational Learning approach, that is able to model the complex spatial relationships involved in a table structure, by mixing the power of a relational representation formalism with the flexibility of a statistical learning tool. Experiments on a real-world dataset are reported both for single cell classification and for overall table structure recognition, whose results prove the validity of the proposed approach.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628745","","Accuracy;High definition video;Layout;Probabilistic logic;Text analysis;Training","Internet;information retrieval;learning (artificial intelligence);relational databases;statistical analysis;text analysis","SRL approach;Web tables;compact informative content;complex spatial relationships;critical cell recognition;machine learning;real-world dataset;relational representation formalism;spatial table organization;statistical learning tool flexibility;statistical relational learning approach;structured informative content;table internal organization;table layouts;table structure recognition","","2","","18","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"An approach for OSGi and DPWS interoperability: Bridging enterprise application with shop-floor","S. Iarovyi; J. Garcia; J. L. M. Lastra","Factory Automation Systems and Technologies Lab, Tampere University of Technology, Finland","2013 11th IEEE International Conference on Industrial Informatics (INDIN)","20131010","2013","","","200","205","Scope and importance of the enterprise applications are increasing constantly, as these information systems allow more effective usage of enterprise time and resources. The enterprise applications are retrieving the information about the production processing the data from outer sources. The factory shop-floor devices are among the most important data sources. The exposure of the shop-floor devices information and functionality to information systems is currently being implemented employing Service-Oriented Architecture (SOA) paradigm. Device Profile for Web Services (DPWS) is among the most employed technologies providing the service functionality to devices. The contemporary factory information system is usually a continuously expanding set of dissimilar, independent applications, which are separately retrieving the information about enterprise processes. These applications frequently duplicate information or functionality of each other. The necessity to integrate these applications emerges, once the overlap between applications should be removed. Creation of multiple custom integrations between the applications in constantly developing system, commonly, is not an effective practice. The Message Oriented Middleware (MOM) can simplify and standardize the integration, providing neutral messaging system. Currently MOM and factory information systems are often implemented employing dynamic packaging systems such as OSGi. The approach for interoperability of OSGi and DPWS presented in this article is based on the DPWS adapter function block. This block can be deployed in OSGi system, instantiated and configured for particular integration tasks. This approach aims to introduce the loose connection between the enterprise information system and devices.","1935-4576;19354576","Electronic:978-1-4799-0752-6; POD:978-1-4799-0750-2; USB:978-1-4799-0751-9","10.1109/INDIN.2013.6622882","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622882","DPWS;Enterprise Integration;Factory Automation;Function Blocks;OSGi;SOA;Web Services","Information systems;Interoperability;Java;Production facilities;Service-oriented architecture;Standards","Web services;business data processing;enterprise resource planning;information retrieval;middleware;open systems;operating systems (computers);service-oriented architecture","DPWS;DPWS adapter function block;DPWS interoperability;MOM;OSGi system;SOA;contemporary factory information system;device profile for Web services;dynamic packaging systems;enterprise application;enterprise information system;factory information systems;information retrieval;information systems;message oriented middleware;multiple custom integrations;service-oriented architecture;shop-floor devices information","","4","","22","","","29-31 July 2013","","IEEE","IEEE Conference Publications"
"A reconfigurable open GeoSMS mobile client App design for Android smartphones","G. Heo; G. Yu; L. Di","Center for Spatial Information Science and Systems (CSISS), George Mason University, 4400 University Dr., MSN 6E1, Fairfax, VA 22030, USA","2013 Second International Conference on Agro-Geoinformatics (Agro-Geoinformatics)","20131007","2013","","","153","158","A geotagged Short Message Service (SMS), such as those following Open Geospatial Consortium (OGC) Open GeoSMS (OGS) standard, is useful for many applications that require location based services field work. In OGS specification, a geotag is used as a hyperlink for retrieving detail information of given target at given location. This feature meets the interoperation requirements of time-sensitive, location-aware notification and monitoring services. This feature can give service providers to design more various and useful service scenarios, and these requirements make importantly to develop vendor specific Apps for realizing these service scenarios. A design pattern for OGS mobile client Apps will help to develop the Apps with relatively less effort. This paper focuses on a design for an OGS mobile client App and shows a Connector-Translator-Launcher (CTL) flow for smartphone environments. The CTL flow based mobile client App is based on Model-View-Controller (MVC) model. It applies pluggable modules for connecting geospatial services pointed by each geotag and translating geospatial content retrieved from such services. The App displays details information specified by a geotag following the OGS. The Android-based mobile client App was developed for OGC's OGC Web Services, Phase 8 (OWS-8). It was applied as a mobile tracking client in geospatial web service testbed scenarios for the Observation Fusion (Tracking) thread. Two pluggable modules were developed for the scenario application. One is a connector module for connecting to a track server, and another is a translator module for translating track information for displaying on map views.","","Electronic:978-1-4799-0868-4; POD:978-1-4799-0867-7","10.1109/Argo-Geoinformatics.2013.6621899","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621899","MVC model;OGC;Open GeoSMS;architectural design;mobile client;moving object tracking;reconfiguration;smartphone App","Connectors;Geospatial analysis;Mobile communication;Servers;Smart phones;Standards","Web services;geographic information systems;information retrieval;mobile computing;smart phones","Android smartphones;CTL;MVC model;OGC Web Services;connector translator launcher;detail information retrieval;geospatial Web service;geospatial web service;geotagged short message service;location aware notification;mobile client application;model view controller;monitoring services;observation fusion tracking;open GeoSMS;open geospatial consortium;reconfigurable open GeoSMS mobile client App design;smartphone environments;time-sensitive requirements","","0","","22","","","12-16 Aug. 2013","","IEEE","IEEE Conference Publications"
"Intelligent semantic question answering system","E. Najmi; K. Hashmi; F. Khazalah; Z. Malik","Department of Computer Science Wayne State University Detroit, Michigan, USA","2013 IEEE International Conference on Cybernetics (CYBCO)","20131003","2013","","","255","260","The volume of information available on the World Wide Web and the rate of its growth requires new techniques to handle and organize this data. Ontologies are becoming the pivotal methodology to represent domain-specific conceptual knowledge and hence help in providing solutions for Question Answering (QA) systems. This paper introduces an approach for enhancing the capabilities of QA systems using semantic technologies. We implemented an approach to convert the natural language user queries to Resource Description Framework (RDF) triples and find relevant answers. The experiment results show that the proposed technique works very well for single word answers. We believe that with some modifications this approach can be expanded to a wider scale.","","Electronic:978-1-4673-6469-0; POD:978-1-4673-6468-3","10.1109/CYBConf.2013.6617431","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617431","","Google;Natural languages;Ontologies;Organizations;Resource description framework;Search engines;Semantics","natural language processing;ontologies (artificial intelligence);question answering (information retrieval)","RDF triples;World Wide Web;domain specific conceptual knowledge;intelligent semantic question answering system;natural language user queries;ontologies;resource description framework;semantic technologies","","4","","23","","","13-15 June 2013","","IEEE","IEEE Conference Publications"
"Efficient parallel data retrieval in MIMO wireless data broadcast with replicated data items","Shuli Luan; Ping He; Xiaoguang Mou","Qingdao Agricultural University, China","IET International Conference on Information and Communications Technologies (IETICT 2013)","20131003","2013","","","340","347","Data retrieval problem is a novel research direction about wireless data broadcast. The goal is to find an efficient sequence to download the requested information from public information in wireless communication. The types of data items broadcast in channel are followed: consecutive and no replicated, non-consecutive and no replicated, and nonconsecutive and replicated. The goal of above types is to reduce the energy consumption, but the complexity is introduced. In order to reduce the complexity of wireless data broadcast with non-consecutive and replicated data items, we define MIMO parallel data retrieval problem with replicated data items and prove that it is NP-hard. In this paper, we also design two heuristic algorithms for data retrieval problem in MIMO and MISO wireless data broadcast. The experiment results show that the proposed schemes have better performance in the case of less requested data items.","","Electronic:978-1-84919-653-6","10.1049/cp.2013.0068","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617511","Data retrieval;MIMO;Wireless data broadcast","","MIMO communication;computational complexity;information retrieval;telecommunication computing","MIMO wireless data broadcast;MISO wireless data broadcast;energy consumption;nonconsecutive data items;parallel data retrieval problem;replicated data items;wireless communication","","0","","","","","27-29 April 2013","","IET","IET Conference Publications"
"Automatic Detection of Pseudocodes in Scholarly Documents Using Machine Learning","S. Tuarob; S. Bhatia; P. Mitra; C. L. Giles","Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","738","742","A significant number of scholarly articles in computer science and other disciplines contain algorithms that provide concise descriptions for solving a wide variety of computational problems. For example, Dijkstra's algorithm describes how to find the shortest paths between two nodes in a graph. Automatic identification and extraction of these algorithms from scholarly digital documents would enable automatic algorithm indexing, searching, analysis and discovery. An algorithm search engine, which identifies pseudocodes in scholarly documents and makes them searchable, has been implemented as a part of the CiteSeerX suite. Here, we illustrate the limitations of start-of-the-art rule based pseudocode detection approach, and present a novel set of machine learning based techniques that extend previous methods.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.151","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628716","Algorithm;Classification;Experiment;Machine Learning;Pseudocode","Algorithm design and analysis;Approximation algorithms;Feature extraction;Machine learning algorithms;Portable document format;Software algorithms;Standards","document handling;electronic publishing;indexing;information retrieval;learning (artificial intelligence);search engines","CiteSeer suite;algorithm search engine;automatic algorithm analysis;automatic algorithm discovery;automatic algorithm extraction;automatic algorithm identification;automatic algorithm indexing;automatic algorithm searching;automatic pseudocode detection;computer science;machine learning;pseudocode identification;scholarly articles;scholarly digital documents","","6","","16","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Empirical analysis of multi-labeling algorithms for music emotion annotation","Ja-Hwung Su; Yi-Cheng Tsai; V. S. Tseng","Kainan University, Taiwan","2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","20131003","2013","","","1","6","Music is highly related to human affective feelings with different kinds of emotions may be embedded in a music work simultaneously. Hence, how to extract emotions from music has been a hot topic for music information retrieval over the past few decades. To this end, a considerable number of multi-labeling studies have been conducted on tagging music emotions. In this paper, we conduct a comparative analysis of state-of-the-art methods for music emotion annotation through extensive experimental evaluations. Comparative experiments were performed on real dataset CAL500 with different evaluation metrics. Moreover, to reveal the robustness, the compared algorithms including different domains of annotation ones were examined with simple and complex types of emotions. The experimental results provide the researchers with insightful ideas in algorithm design for emotionalizing music from technical point of view.","","Electronic:978-1-4799-1604-7; POD:978-1-4799-1603-0; USB:978-1-4799-1602-3","10.1109/ICMEW.2013.6618344","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618344","Music emotion;annotation;empirical analysis;multi-labeling;tagging","Abstracts;Frequency modulation;Indexes;Labeling;Multimedia communication;Probabilistic logic;Semantics","emotion recognition;information retrieval;music","empirical analysis;human affective feelings;information retrieval;multilabeling algorithms;music emotion annotation","","0","","19","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Text clustering using statistical and semantic data","A. Benghabrit; B. Ouhbi; H. Behja; B. Frikh","LM2I laboratory, ENSAM, Moulay Isma&#x00EF;l University, Marjane II, B.P. 4024, Mekn&#x00E8;s, Morocco","2013 World Congress on Computer and Information Technology (WCCIT)","20131003","2013","","","1","6","The explosive growth of information stored in unstructured texts created a great demand for new and powerful tools to acquire useful information, such as text mining. Document clustering is one of its the powerful methods and by which document retrieval, organization and summarization can be achieved. However, it represents a challenge when dealing with a big number of data due to high dimensionality of the feature space and to the semantic correlation between features. In this paper, we propose a new sequential document clustering algorithm that uses a statistical and semantic feature selection methods. The semantic process was proposed to improve the frequency mechanism with the semantic relations of the text documents. The proposed algorithm selects iteratively relevant features and performs clustering until convergence. To evaluate its performance, experiments on two corpora have been conducted. The obtained results show that the performance of our algorithm is superior to that obtained by the existing algorithms.","","Electronic:978-1-4799-0462-4; POD:978-1-4799-0461-7","10.1109/WCCIT.2013.6618782","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618782","Text mining;clustering;feature selection methods;performance analysis","Algorithm design and analysis;Clustering algorithms;Convergence;Mutual information;Semantics;Text mining;Vectors","data mining;information retrieval;organisational aspects;pattern clustering;statistical analysis;text analysis","document organization;document retrieval;document summarization;semantic correlation;semantic data;semantic feature selection methods;semantic process;sequential document clustering algorithm;statistical data;statistical feature selection methods;text clustering;text documents;text mining;unstructured texts","","1","","27","","","22-24 June 2013","","IEEE","IEEE Conference Publications"
"The role of artefact corpus in LSI-based traceability recovery","G. Bavota; A. De Lucia; R. Oliveto; A. Panichella; F. Ricci; G. Tortora","University of Sannio, Via Traiano, 82100 Benevento, Italy","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","83","89","Latent Semantic Indexing (LSI) is an advanced method widely and successfully employed in Information Retrieval (IR). It is an extension of Vector Space Model (VSM) and it is able to overcome VSM in canonical IR scenarios where it is used on very large document repositories. LSI has also been used to semi-automatically generate traceability links between software artefacts. However, in such a scenario LSI is not able to overcome VSM. This contradicting result is probably due to the different characteristics of software artefact repositories as compared to document repositories. In this paper we present a preliminary empirical study to analyze how the size and the vocabulary of the repository-in terms of number of documents and terms (i.e., the vocabulary)-affects the retrieval accuracy. Even if replications are needed to generalize our findings, the study presented in this paper provides some insights that might be used as guidelines for selecting the more adequate methods to be used for traceability recovery depending on the particular application context.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620160","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620160","Empirical Studies;Latent Semantic Indexing;Traceability recovery;Vector Space Model","Accuracy;Indexing;Large scale integration;Software;Unified modeling language;Vectors;Vocabulary","database indexing;document handling;information retrieval;program diagnostics","LSI-based traceability recovery;VSM;artefact corpus;canonical IR scenarios;document repositories;information retrieval;latent semantic indexing;semiautomatically traceability link generation;software artefact repositories;traceability recovery;vector space model","","4","","23","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"WIKI-CMR: A web cross modality dataset for studying and evaluation of cross modality retrieval models","Wei Xiong; Shuhui Wang; C. Zhang; Qingming Huang","Schoold of Computer and Control Engineering, University Of Chinese Academy Of Science, China","2013 IEEE International Conference on Multimedia and Expo (ICME)","20130926","2013","","","1","6","With the popularity of Web multimedia data, cross-modality retrieval becomes an urgent and challenging problem. Bridging the semantic gap between different modalities and dealing with abundant data are the main challenges for cross-modality retrieval. A well-designed dataset could provide a platform for developing the state-of-the-art cross-modality retrieval algorithms. However, existing Web cross-modality datasets are small in size, or do not contain the full information, for example, the hyperlink structure. In this paper, we introduce a new Web cross-modality dataset called “WIKI-CMR” by selecting Wikipedia as the reliable and information-rich data resource, and collect data with a smart crawling strategy. This dataset is comprised of 74961 documents with textual paragraphs, images and hyperlinks. All documents are categorized into 11 semantic topics. We point out several challenges on this dataset and use this dataset to evaluate some well-known cross-modality retrieval models.","1945-7871;19457871","Electronic:978-1-4799-0015-2; POD:978-1-4799-0014-5; USB:978-1-4799-0013-8","10.1109/ICME.2013.6607613","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607613","Multimedia;cross-modality;dataset;retrieval","Abstracts;Electronic publishing;Encyclopedias;Internet;Multimedia communication;Robots","Internet;Web sites;hypermedia;information retrieval;multimedia computing","WIKI-CMR;Web cross modality dataset;Web multimedia data;Wikipedia;cross modality retrieval models;data resource;hyperlink structure;semantic gap;semantic topics;smart crawling strategy;state-of-the-art cross-modality retrieval algorithms;textual paragraphs;well-designed dataset","","1","","21","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"An exploratory analysis of mobile development issues using stack overflow","M. Linares-Vásquez; B. Dit; D. Poshyvanyk","Computer Science Department, The College of William and Mary, Williamsburg, VA, USA","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","93","96","Question & answer (Q&A) websites, such as Stack Overflow (SO), are widely used by developers to find and provide answers to technical issues and concerns in software development. Mobile development is not an exception to the rule. In the latest SO dump, more than 400K questions were labeled with tags related to mobile technologies. Although, previous works have analyzed the main topics and trends in SO threads, there are no studies devoted specifically to mobile development. In this paper we used topic modeling techniques to extract hot-topics from mobile-development related questions. Our findings suggest that most of the questions include topics related to general questions and compatibility issues, and the most specific topics, such as crash reports and database connection, are present in a reduced set of questions.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624014","Stack Overflow;mining software repositories;mobile platforms;topic modeling","Analytical models;Androids;Computer crashes;Entropy;Humanoid robots;Mobile communication;Software","Web sites;mobile computing;question answering (information retrieval);software engineering","SO dump;Stack Overflow;compatibility issues;crash reports;database connection;exploratory analysis;mobile development issues;mobile-development related questions;question & answer Websites;topic modeling techniques","","7","","12","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Trace-by-classification: A machine learning approach to generate trace links for frequently occurring software artifacts","M. Wieloch; S. Amornborvornwong; J. Cleland-Huang","School of Computing, DePaul University, Chicago, IL, 60604, USA","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","110","114","Over the past decade the traceability research community has focused upon developing and improving trace retrieval techniques in order to retrieve trace links between a source artifact, such as a requirement, and set of target artifacts, such as a set of java classes. In this Trace Challenge paper we present a previously published technique that uses machine learning to trace software artifacts that recur is similar forms across across multiple projects. Examples include quality concerns related to non-functional requirements such as security, performance, and usability; regulatory codes that are applied across multiple systems; and architectural-decisions that are found in many different solutions. The purpose of this paper is to release a publicly available TraceLab experiment including reusable and modifiable components as well as associated datasets, and to establish baseline results that would encourage further experimentation.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620165","challenge;machine learning;traceability","Educational institutions;Probabilistic logic;Security;Software;Standards;Training;Weight measurement","Java;information retrieval;learning (artificial intelligence);object-oriented programming;pattern classification;program diagnostics;security of data;software architecture;software quality;software reusability","Java classes;TraceLab experiment;architectural-decisions;component modifiability;component reusability;frequently occurring software artifacts;machine learning approach;performance requirement;quality concerns;regulatory codes;security requirement;target artifacts;trace challenge;trace link generation;trace link retrieval;trace retrieval techniques;trace-by-classification;traceability research community;usability requirement","","2","","15","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"The research on topic detection based on multi-models and multi-characteristics","S. x. Zhang; Y. x. Li; X. l. Wang; L. y. Xie","State Grid Inf. & Telecommun. Co., Ltd., Beijing, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","20130930","2013","","","595","598","In this paper, a new approach was proposed for the topic detection, which combined the multi-models and multi-characteristics, entity information similarities were researched as features for support vector machine model (SVM) by us, for example, the content similarity, time similarity and location similarity methods can be proposed respectively, the Bayesian model also can be discussed to obtain the atomic characteristics in this paper. Except this features, the expert knowledge base has been studied to solve the difficult classification problem. The experimental results show that the approach combined the statistical model with expert rule base is effective.","2327-0586;23270586","Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4","10.1109/ICSESS.2013.6615379","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615379","clustering;entity information similarity;feature selection;support vector model","Support vector machine classification;Testing","Bayes methods;expert systems;information retrieval;support vector machines;text analysis","Bayesian model;SVM;content similarity;entity information similarity;expert knowledge base;expert rule base;location similarity;multicharacteristics;multimodels;statistical model;support vector machine model;time similarity;topic detection","","0","","12","","","23-25 May 2013","","IEEE","IEEE Conference Publications"
"Finite length analysis on listing failure probability of Invertible Bloom Lookup Tables","D. Yugawa; T. Wadayama","Department of Computer Science and Engineering, Nagoya Institute of Technology, Nagoya, Japan","2013 IEEE International Symposium on Information Theory","20131007","2013","","","3030","3034","The Invertible Bloom Lookup Tables (IBLT) is a data structure which supports insertion, deletion, retrieval and listing operations of the key-value pair. The IBLT can be used to realize efficient set reconciliation for database synchronization. The most notable feature of the IBLT is the complete listing operation of the key-value pairs based on the algorithm similar to the peeling algorithm for low-density parity check (LDPC) codes. In this paper, we will present a stopping set (SS) analysis for the IBLT which reveals finite length behaviors of the listing failure probability. The key of the analysis is enumeration of the number of stopping matrices of given size. We derived a novel recursive formula useful for computationally efficient enumeration. An upper bound on the listing failure probability based on the union bound accurately captures the error floor behaviors. It will be shown that, in the error floor region, the dominant SS have size 2. We propose a simple modification on hash functions, which are called SS avoiding hash functions, for preventing occurrences of the SS of size 2.","2157-8095;21578095","Electronic:978-1-4799-0446-4; POD:978-1-4799-0444-0","10.1109/ISIT.2013.6620782","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620782","","Arrays;Computers;Parity check codes;Radiation detectors;Synchronization;Upper bound","data structures;database management systems;information retrieval;matrix algebra;parity check codes;probability;set theory;table lookup","IBLT;LDPC;SS occurrence prevention;data structure;database synchronization;deletion operation;error floor behaviors;error floor region;finite length analysis;finite length behaviors;hash function avoidance;insertion operation;invertible bloom lookup tables;key-value pairs;listing failure probability;listing operation;low-density parity check codes;peeling algorithm;recursive formula;retrieval operation;set reconciliation;stopping matrices;stopping set analysis","","0","","13","","","7-12 July 2013","","IEEE","IEEE Conference Publications"
"Identifying temporal relations between main events in new articles","I. Berrazega; R. Faiz","LARODEC, University of Tunis - ISG, 2000 Bardo, Tunisia","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","4","With the expansion of the Web 2.0, daily huge amount of data is produced everywhere, namely new articles. These contents need to be exploited in order to extract relevant information and to build knowledge databases. In this concern, processing the temporal dimension of language and extracting temporal information from electronic news articles is becoming a prominent task. In this concern, we propose an approach for identifying inter-sentential temporal relations between main events from news articles. Our approach is based on a complete linguistic analysis of texts and supervised learning models.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616467","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616467","Classification;Linguistic Analysis;Machine Learning;Natural Language Processing;Temporal Information Extraction;Temporal Relation Identification;Web 2.0","Accuracy;Data mining;Feature extraction;Natural language processing;Pragmatics;Semantics;Syntactics","Internet;information retrieval","Web 2.0;electronic news articles;intersentential temporal relation identification;knowledge databases;linguistic text analysis;supervised learning models;temporal information extraction;temporal language dimension","","0","","13","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"A Linked Data Perspective for Collaboration in Mashup Development","D. Bianchini; V. De Antonellis; M. Melchiori","Dip. di Ing. dell'Inf., Univ. di Brescia, Brescia, Italy","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","128","132","Web mashup is becoming an approach more and more popular for developing Web applications both for general and enterprise purposes. Mashup development is fueled by Web sites, as Programmable Web and Mashape, offering large, ever growing, catalogues of software components accessible through Web APIs. Developing Web mashup applications requires specialized knowledge about Web APIs, technologies and the way to combine them in a meaningful way. This kind of knowledge is often available but distributed among different experts. In this paper we introduce the LINKSMAN (LINKed data Supported MAshup collaboratioN) approach for expert search in enterprise mashup development. The approach is based on integrating knowledge both internal and external to enterprises. The result is then published as linked data set. We show how typical collaboration patterns among mashup developers can be formalized and implemented on this linked data set to support expert search. A prototype application is also described.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621359","Collaborative mashup design;Enterprise 2.0;enterprise linked data;enterprise mashup;expert search;linked data integration","Collaboration;Knowledge engineering;Mashups;Organizations;Resource description framework;Vocabulary","Web services;Web sites;application program interfaces;cataloguing;data integration;groupware;information retrieval;software development management","LINKSMAN approach;Mashape;ProgrammableWeb;Web API;Web Mashup application development;Web site;enterprise mashup development;expert search;knowledge integration;linked data set;linked data supported Mashup collaboration;software component catalogue","","0","","9","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"Expert user discovery in a spontaneous social network an approach using knowledge retrieval","G. Freitas; C. da Costa; J. Barbosa; R. Righi; A. Yamin","Applied Computing Graduate Program - PIPCA, University of Vale do Rio dos Sinos - UNISINOS, S&#x00E3;o Leopoldo, Brazil","2013 Fifth International Conference on Computational Aspects of Social Networks","20131007","2013","","","15","20","Nowadays, we can observe that finding answers on social networks is a hard and time-consuming task. The main contribution of this article is the creation of a model and algorithm that allows users to find answers to their questions using a spontaneous social network called Mingle. This algorithm uses the Mingle ontology-based knowledge base to find expert users. To achieve this, two important steps are taken: Mingle ontology is updated to support user-oriented expertise and a detailed model is created for the given algorithm. This model was created considering other three similar applications and algorithms. Moreover, we used a semantic web framework. In the end, an evaluation using real-life scenarios is applied to evaluate if the created algorithm meets the initial goals.","","Electronic:978-1-4799-1409-8; POD:978-1-4799-1408-1","10.1109/CASoN.2013.6622607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622607","Expert Users;Knowledge Retrieval;Ontologies;Social Networks","Social network services","information retrieval;ontologies (artificial intelligence);social networking (online)","Mingle ontology-based knowledge;expert user discovery;knowledge retrieval;semantic web framework;spontaneous social network","","0","","14","","","12-14 Aug. 2013","","IEEE","IEEE Conference Publications"
"The GHTorent dataset and tool suite","G. Gousios","Software Engineering Research Group, Delft University of Technology, Delft, The Netherlands","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","233","236","During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624034","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624034","GitHub;dataset;repository","Collaboration;Data collection;Data mining;Databases;History;Organizations;Software engineering","application program interfaces;groupware;information resources;information retrieval;software engineering","GHTorent dataset;GitHub;collaboration platform;extensive REST API;high-quality interconnected data retrieval;hosting platform;mirroring platform;tool suite","","21","","3","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Fuzzy tool for efficient searching in information system","B. Walek; J. Bartoš","Dept. of Inf. & Comput., Univ. of Ostrava, Ostrava, Czech Republic","2013 36th International Conference on Telecommunications and Signal Processing (TSP)","20130930","2013","","","15","19","In today's information systems various heterogeneous information are stored. Users usually search in these systems using search forms. In this article, the fuzzy tool for searching in information systems is proposed. The fuzzy tool helps user to find suitable data in cases when he doesn't know exactly all criteria for searching. For definition of searching criteria the linguistic expressions and variables are used. This article covers the main steps and parts of proposed fuzzy tool and the fuzzy tool is verified on specific example.","","Electronic:978-1-4799-0404-4; POD:978-1-4799-0401-3","10.1109/TSP.2013.6613882","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613882","Fuzzy searching;expert system;fuzzy;information system;linguistic expression","Airports;Databases;Information systems;Loading;Natural languages;Pragmatics;XML","computational linguistics;fuzzy set theory;information retrieval;search engines","fuzzy tool;heterogeneous information;information system;linguistic expression;searching criteria","","1","","9","","","2-4 July 2013","","IEEE","IEEE Conference Publications"
"Departures from optimality: Understanding human analyst's information foraging in assisted requirements tracing","N. Niu; A. Mahmoud; Z. Chen; G. Bradshaw","Department of Computer Science and Engineering, Mississippi State University, USA","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","572","581","Studying human analyst's behavior in automated tracing is a new research thrust. Building on a growing body of work in this area, we offer a novel approach to understanding requirements analyst's information seeking and gathering. We model analysts as predators in pursuit of prey - the relevant traceability information, and leverage the optimality models to characterize a rational decision process. The behavior of real analysts with that of the optimal information forager is then compared and contrasted. The results show that the analysts' information diets are much wider than the theory's predictions, and their residing in low-profitability information patches is much longer than the optimal residence time. These uncovered discrepancies not only offer concrete insights into the obstacles faced by analysts, but also lead to principled ways to increase practical tool support for overcoming the obstacles.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606603","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606603","Traceability;information foraging;requirements engineering;study of human analysts","Analytical models;Buildings;Computational modeling;Debugging;Navigation;Software;Software engineering","behavioural sciences computing;formal specification;formal verification;information needs;information retrieval","assisted requirements tracing;automated tracing;human analyst behavior;human analyst information foraging;low-profitability information patches;optimal information forager;optimal residence time;optimality models;rational decision process;traceability information","","6","","45","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"The case for distributed data archival using secret splitting with Percival","T. M. Kroeger; J. C. Frank; E. L. Miller","Sandia National Laboratories, Livermore, CA 94550, USA","2013 6th International Symposium on Resilient Control Systems (ISRCS)","20131010","2013","","","204","209","Most encryption used today obfuscates data behind a secret key or a problem believed to be computationally complex. One can fundamentally think of it as delayed release for a determined adversary. This approach is not well suited for long-term archival of sensitive data. Additionally, issues such as key rotation, and lost or exposed keys, make keeping such archives up to date very difficult. As a result most become static and unable to respond to attacks. Once hacked, such systems offer little to no protection for data privacy and leave open uncertainty about data integrity. Given the increasing frequency of major cyber events these days, it is clear that any secure long-term archive needs to be able to address maintaining data privacy and integrity throughout a compromise event. In spite of these needs, most data archives today still use central storage servers and encryption. In this paper we make the case for secure data archival based on secret splitting and distributed data repositories. We present Percival, one example of a research project focused on long-term data archival using Shamir's secret splitting and distributed data repositories. We examine how this approach can continue secure operations in the presence of adversarial compromise. We discuss how this distributed model significantly increases the attacker's burden by requiring the compromise of many sites. Additionally, this approach increases the resilience to insider threat and provides stronger assurances of data integrity and confidentiality. Finally we discuss current research to create new capabilities that enable blinded search across such an archive.","","CD-ROM:978-1-4799-0501-0; Electronic:978-1-4799-0503-4; POD:978-1-4799-0502-7","10.1109/ISRCS.2013.6623777","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623777","","Authentication;Data privacy;Distributed databases;Encryption;Polynomials","cryptography;data integrity;data privacy;information retrieval systems;records management","Percival;data integrity;data privacy;distributed data archival;encryption;obfuscates data;secret splitting;sensitive data","","1","1","11","","","13-15 Aug. 2013","","IEEE","IEEE Conference Publications"
"An efficient audio fingerprint search algorithm for music retrieval","S. Lee; D. Yook; S. Chang","Dept. of Comput. & Commun. Eng., Korea Univ., Seoul, South Korea","IEEE Transactions on Consumer Electronics","20131015","2013","59","3","652","656","The conventional audio fingerprinting system by Haitsma uses a lookup table to identify the candidate songs in the database, which contains the sub-fingerprints of songs, and searches the candidates to find a song whose bit error rate is the lowest. However, this approach has a drawback that the number of database accesses increases dramatically, especially when the database contains a large number of songs or when a matching sub-fingerprint is not found in the lookup table due to a heavily degraded input signal. In this paper, a novel search method is proposed to overcome these difficulties. The proposed method partitions each song found from the lookup table into blocks, assigns a weight to each block, and uses the weight as a search priority to speed up the search process while reducing the number of database accesses. Various results from our experiment show the significant improvement in search speed while maintaining the search accuracy comparable to the conventional method.","0098-3063;00983063","","10.1109/TCE.2013.6626252","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626252","Audio fingerprint;audio database search;music informationretrieval","Accuracy;Algorithm design and analysis;Audio databases;Bit error rate;Educational institutions;Fingerprint recognition","audio databases;audio signal processing;error statistics;information retrieval;music;search problems;table lookup","audio database;audio fingerprint search algorithm;bit error rate;lookup table;music retrieval;search accuracy;search method;search priority","","2","","10","","","August 2013","","IEEE","IEEE Journals & Magazines"
"Answering questions about unanswered questions of Stack Overflow","M. Asaduzzaman; A. S. Mashiyat; C. K. Roy; K. A. Schneider","Department of Computer Science, University of Saskatchewan, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","97","100","Community-based question answering services accumulate large volumes of knowledge through the voluntary services of people across the globe. Stack Overflow is an example of such a service that targets developers and software engineers. In general, questions in Stack Overflow are answered in a very short time. However, we found that the number of unanswered questions has increased significantly in the past two years. Understanding why questions remain unanswered can help information seekers improve the quality of their questions, increase their chances of getting answers, and better decide when to use Stack Overflow services. In this paper, we mine data on unanswered questions from Stack Overflow. We then conduct a qualitative study to categorize unanswered questions, which reveals characteristics that would be difficult to find otherwise. Finally, we conduct an experiment to determine whether we can predict how long a question will remain unanswered in Stack Overflow.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624015","Stack Overflow;prediction;question-answer","Communities;Data mining;Knowledge discovery;Predictive models;Software;Taxonomy;Time factors","data mining;question answering (information retrieval);software engineering","Stack Overflow services;community-based question answering services;data mining;information seekers;software developers;software engineers;unanswered questions;voluntary services","","21","","9","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Build a Search Engine to Support Doing Research Surveys on SNS","C. Yin; B. Flanagan; S. Hirokawa; Y. Tabata","Res. Inst. for Inf. Technol., Kyushu Univ., Fukuoka, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","183","186","It is very important for any academic researches to do research surveys. In this paper, we proposed an SNS based search engine to support doing research surveys, called SNSearch. Our SNSearch system not only supports analysis of research trends, but also offers learners opportunities to reflect on their search behaviors. By browsing the experts' survey history, students can learn search skills.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630341","Data mining;Doing reasearch suvery;SNS;search eninge","Educational institutions;History;Internet;Market research;Mobile communication;Search engines;Social network services","computer aided instruction;information retrieval;research and development management;search engines;social networking (online)","SNS based search engine;SNSearch system;academic research;learner opportunities;research surveys;search behaviors;search skill learning;social network service","","0","","7","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Parallel Indexing of Large Multi-dimensional Data","K. Funaki; T. Hochin; H. Nomiya; H. Nakanishi; M. Kojima","Dept. of Inf. Sci., Kyoto Inst. of Technol., Kyoto, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","324","329","This paper proposes two methods of indexing a large amount of data in order to resolve the issues about time limitation and size limitation. One is the method that an index is created one by one. When the size of an index reaches the limitation, a new index is created. The other is the method that several indexes are created in advance. Data are inserted into the indexes according to the Round-robin scheme. The performance evaluation experiments show that the proposed one-by-one method provides the best insertion performance and that the proposed methods provide better retrieval performance than the conventional method. In addition, parallel processing to indexes divided by the proposed methods could accelerate the retrieval.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630368","DBMS;Multi-dimensional index;Parallel processing","Informatics","database indexing;information retrieval;multimedia databases;parallel processing;tree data structures","large multidimensional data;multimedia databases;one-by-one method;parallel R-tree;parallel processing;parallel spatial indexing methods;retrieval performance;round-robin scheme","","3","","10","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Sparse phase retrieval: Convex algorithms and limitations","K. Jaganathan; S. Oymak; B. Hassibi","Department of Electrical Engineering, California Institute of Technology, Pasadena, CA 91125","2013 IEEE International Symposium on Information Theory","20131007","2013","","","1022","1026","We consider the problem of recovering signals from their power spectral densities. This is a classical problem referred to in literature as the phase retrieval problem, and is of paramount importance in many fields of applied sciences. In general, additional prior information about the signal is required to guarantee unique recovery as the mapping from signals to power spectral densities is not one-to-one. In this work, we assume that the underlying signals are sparse. Recently, semidefinite programming (SDP) based approaches were explored by various researchers. Simulations of these algorithms strongly suggest that signals upto O(n<sup>1/2- ϵ</sup>) sparsity can be recovered by this technique. In this work, we develop a tractable algorithm based on reweighted ℓ<sub>1</sub>-minimization that recovers a sparse signal from its power spectral density for significantly higher sparsities, which is unprecedented. We also discuss the limitations of the existing SDP algorithms and provide a combinatorial algorithm which requires significantly fewer ”phaseless” measurements to guarantee recovery.","2157-8095;21578095","Electronic:978-1-4799-0446-4; POD:978-1-4799-0444-0","10.1109/ISIT.2013.6620381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620381","Phase Retrieval;Phaseless measurements;Reweighted ℓ<inf>1</inf>-minimization;Semidefinite Programming (SDP)","Correlation;Extraterrestrial measurements;Fourier transforms;Minimization;Phase measurement;Silicon;Sparse matrices","convex programming;information retrieval;mathematical programming;signal processing","SDP-based approaches;combinatorial algorithm;convex algorithms;phaseless measurements;power spectral densities;reweighted l<sub>1</sub>-minimization;semidefinite programming-based approaches;signal recovery;signals mapping;sparse phase retrieval;sparse signal;tractable algorithm","","20","","22","","","7-12 July 2013","","IEEE","IEEE Conference Publications"
"Representing Texts as Contextualized Entity-Centric Linked Data Graphs","A. Freitas; S. ORiain; E. Curry; J. C. P. da Silva; D. S. Carvalho","Digital Enterprise Researh Inst. (DERI), Nat. Univ. of Ireland, Galway, Galway, Ireland","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","133","137","The integration of a small fraction of the information present in the Web of Documents to the Linked Data Web can provide a significant shift on the amount of information available to data consumers. However, information extracted from text does not easily fit into the usually highly normalized structure of ontology-based datasets. While the representation of structured data assumes a high level of regularity, relatively simple and consistent conceptual models, the representation of information extracted from texts need to take into account large terminological variation, complex contextual/dependency patterns, and fuzzy or conflicting semantics. This work focuses on bridging the gap between structured and unstructured data, proposing the representation of text as structured discourse graphs (SDGs), targeting an RDF representation of unstructured data. The representation focuses on a semantic best-effort information extraction scenario, where information from text is extracted under a pay-as-you-go data quality perspective, trading terminological normalization for domain-independency, context capture, wider representation scope and maximization of textual information capture.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621360","Discoruse Graphs;Discourse Representation;Linked Data;Semantic Web","Context;Context modeling;Data mining;Data models;Ontologies;Resource description framework;Semantics","Internet;data structures;information retrieval;ontologies (artificial intelligence);text analysis","RDF representation;SDG;complex contextual patterns;contextualized entity-centric linked data graphs;large terminological variation;linked data Web;ontology-based datasets;pay-as-you-go data quality perspective;semantic best-effort information extraction scenario;structured data;structured discourse graphs;terminological normalization;text representation;unstructured data","","1","","6","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"A model for expert finding based on social network structure and underlying information diffusion network","A. Kardan; S. Mohtaj","Dept. of Computer Engineering, Amir Kabir University of Technology, Tehran, Iran","The 5th Conference on Information and Knowledge Technology","20131007","2013","","","472","477","Social networks can play a crucial role in a range of diffusion of information and knowledge, from the adoption of political opinions and technologies, to educational and learning usages. Thus knowledge sharing is one of the main purposes of people for using these networks. The main problem regarding to using social networks as a knowledge source is lacking of a criterion for determine validity of the diffused knowledge in these networks. Finding experts in social networks and knowing the level of users' knowledge can help to solve this problem by ranking users and validating their posts. In this paper we proposed a model for finding experts in social networks. This model is based on the structure of corresponding network and implicit flow of information in network and proposed according to SNPageRank algorithm.","","Electronic:978-1-4673-6490-4; POD:978-1-4673-6488-1","10.1109/IKT.2013.6620115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620115","expert finding;graph;information cascade;information diffusion;social network","Accuracy;Communication networks;Data mining;Electronic mail;Facebook;Knowledge engineering","information retrieval;social networking (online)","SNPageRank algorithm;expert finding model;information diffusion network;knowledge sharing;knowledge source;social network structure","","0","","18","","","28-30 May 2013","","IEEE","IEEE Conference Publications"
"A new approach to short web document creation based on textual and visual information","M. Zachariasova; P. Kamencay; R. Hudec; M. Benco; S. Matuska","Dept. of Telecommun. & Multimedia, Univ. of Zilina, Zilina, Slovakia","2013 36th International Conference on Telecommunications and Signal Processing (TSP)","20130930","2013","","","788","792","This paper deals with research in area of automatic semantic inclusion of textual and non-textual information of Web documents. The main idea is to create a robust method for extraction of images and textual segments to obtain short web document. Thus, developed method consist of two data types extractions, where both, image and text data extraction are using Document Object Model (DOM) tree. Extracted objects are saved in separated databases followed by the images analysis that defines and describes image object from semantic point of view. Moreover, the semantic descriptions of all modal objects are utilized to short web document creation. We implement our novel method using the Scale Invariant Feature Transform (SIFT) descriptor within a Support Vector Machine (SVM) classifier. Further, in order to obtain a semantic description of objects in static image, the Support Vector Machine (SVM) classification were applied. Finally, semantic inclusion textual and visual information was realized. The developed method has been tested on real and off-line web documents.","","Electronic:978-1-4799-0404-4; POD:978-1-4799-0401-3","10.1109/TSP.2013.6614046","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614046","DOM;Image analysis;SIFT;SVM;Semantic Inclusion of Images and Textual segments","Algorithm design and analysis;Feature extraction;Image analysis;Image segmentation;Semantics;Support vector machines;Training","feature extraction;information retrieval;object recognition;support vector machines;text analysis;text detection;transforms","DOM tree;SIFT descriptor;SVM classifier;automatic semantic inclusion;document object model tree;image analysis;image extraction;object extraction;object semantic description;off-line web document;real web document;scale invariant feature transform;semantic inclusion textual information;semantic inclusion visual information;separated databases;short-Web document creation;static image;support vector machine;text data extraction;textual segments","","0","","12","","","2-4 July 2013","","IEEE","IEEE Conference Publications"
"Finding groups of friends who are significant across multiple domains in social networks","S. K. Tanbeer; F. Jiang; C. K. S. Leung; R. K. MacKinnon; I. J. M. Medina","Department of Computer Science, University of Manitoba, Winnipeg, Canada","2013 Fifth International Conference on Computational Aspects of Social Networks","20131007","2013","","","21","26","Social networking websites such as Facebook, LinkedIn, Twitter, and Weibo have been used for collaboration and knowledge sharing between users. The mining of social network data has become an important topic in data mining and computational aspects of social networks. Nowadays, it is not uncommon for most users in a social network to have many friends and in multiple social domains. Among these friends, some groups of friends are more significant than others. In this paper, we introduce a data mining technique that helps social network users find groups of friends who are significant across multiple domains in social networks.","","Electronic:978-1-4799-1409-8; POD:978-1-4799-1408-1","10.1109/CASoN.2013.6622608","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622608","Applications on social networks;computational aspects of social networks;knowledge discovery and data mining;social computing;social network analysis and mining","Computers;Data mining","data mining;information retrieval;social networking (online)","data mining technique;friends;social network users;social networking Websites","","3","","24","","","12-14 Aug. 2013","","IEEE","IEEE Conference Publications"
"A builder for Adaptable Human Machine Interfaces for mobile devices","O. Neira; A. N. Lee; J. L. M. Lastra; R. S. Camp","Factory Automation Systems and Technologies Lab, Tampere University of Technology, Finland","2013 11th IEEE International Conference on Industrial Informatics (INDIN)","20131010","2013","","","750","755","Adaptive Human Machine Interfaces (HMI) are used to improve the interaction between users and interfaces by taking into account the state of the user and the system. This paper describes a solution that enables the generation of adaptable HMIs applying web application technologies in the Android Operating Systems. The solution is based on an Adaptive Human Machine Interface Engine, and in this article one of the main components, called HMI Builder, is presented. The HMI Builder uses different technologies to retrieve information from the system in order to generate adaptive elements in the HMI. This approach is a flexible solution that can be incorporated in most of the Android devices in the market; including smartphones and tablets. The proposed approach was implemented and tested in a manufacturing scenario by creating adaptive interfaces for different types of user based on the user roles, tasks, the state of the system and the context. Finally, the technical implementation details, result and future work are presented and discussed.","1935-4576;19354576","Electronic:978-1-4799-0752-6; POD:978-1-4799-0750-2; USB:978-1-4799-0751-9","10.1109/INDIN.2013.6622978","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622978","Adaptive interfaces;human machine interface (HMI);mobile devices;mobile interfaces;web interface","Adaptive systems;Androids;Engines;Humanoid robots;Mobile communication;Mobile handsets;Servers","information retrieval;notebook computers;operating systems (computers);smart phones;user interfaces","Android operating systems;HMI Builder;Web application technologies;adaptable HMI;adaptable human machine interfaces;adaptive human machine interface engine;information retrieval;mobile devices;smartphones;system state;tablets;user roles;user tasks","","1","","22","","","29-31 July 2013","","IEEE","IEEE Conference Publications"
"Making sense of online code snippets","S. Subramanian; R. Holmes","School of Computer Science, University of Waterloo, Waterloo, ON, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","85","88","Stack Overflow contains a large number of high-quality source code snippets. The quality of these snippets has been verified by users marking them as solving a specific problem. Stack Overflow treats source code snippets as plain text and searches surface snippets as they would any other text. Unfortunately, plain text does not capture the structural qualities of these snippets; for example, snippets frequently refer to specific API (e.g., Android), but by treating the snippets as text, linkage to the Android API is not always apparent. We perform snippet analysis to extract structural information from short plain-text snippets that are often found in Stack Overflow. This analysis is able to identify 253,137 method calls and type references from 21,250 Stack Overflow code snippets. We show how identifying these structural relationships from snippets could perform better than lexical search over code blocks in practice.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624012","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624012","","Androids;Data mining;Documentation;Educational institutions;Humanoid robots;Java;Libraries","Linux;Web sites;application program interfaces;information retrieval;software quality","Android API;Stack Overflow code snippets;code blocks;high-quality source code snippets;method calls;online code snippets;plain text;snippet structural qualities;surface snippet search;type references","","11","1","9","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Extensive medical data storage with prominent symmetric algorithms on cloud - A protected framework","G. Nalinipriya; R. Aswin Kumar","Saveetha Engineering College, Chennai, India","Smart Structures and Systems (ICSSS), 2013 IEEE International Conference on","20131010","2013","","","171","177","The cloud is an emerging field in the IT. It is a way of delivering IT-enabled services in the form of software, platform and infrastructure. Cloud computing in recent times has become a trend among all walks of life to store and retrieve data from anywhere around the globe. With the rising cost for the data storage devices and data retrieval process; enterprises have moved to centralized storage for cost efficiency. This proves a greater task to the service providers in terms of security and consistency. As shared systems are prone to be attacked, various counter measures have been proposed by experts to secure the stored data. Here the data is being encrypted by the advanced encryption algorithm which is considered to be the most competent in the present security scenario. A symmetric key is generated to the stored file. To further enhance the trust of the service provider the trust certificate of the company is being sent along with the keys to the client. All the communication process is taken place using the Key Exchange protocol. These techniques are performed to ensure the client that the stored data is being secured, integrated and a total control over the route in which the data is being communicated. In this paper, a new efficient technique is proposed which indicates the effectiveness, flexibility of the storage and retrieval process by a generic framework. This framework fills the gap between the security needs and challenges.","","Electronic:978-1-4673-6243-6; POD:978-1-4673-6242-9","10.1109/ICSSS.2013.6623021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623021","Advanced Encryption Standards - New Instructions;Cloud Service Provider;Data Encryption Standards;Diffie Hellman;Message Authentication Code and Cloud Security Alliance group;Message Digest","Companies;Encryption","cloud computing;cryptographic protocols;information retrieval;medical administrative data processing","advanced encryption algorithm;centralized storage;cloud computing;cost efficiency;data retrieval;key exchange protocol;medical data storage;shared systems;symmetric algorithms;trust certificate;ymmetric key","","3","","21","","","28-29 March 2013","","IEEE","IEEE Conference Publications"
"Detecting Location-Based Enumerating Bursts in Georeferenced Micro-Posts","K. Tamura; H. Kitakami","Grad. Sch. of Inf. Sci., Hiroshima City Univ., Hiroshima, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","389","394","Nowadays, a large number of georeferenced micro-posts, i.e., short messages including location information, are posted on social media sites. People transmit and collect information over the Internet through these georeferenced micro-posts, which are usually related to not only personal topics but also local topics and events. Detecting local topics and events in georeferenced micro-posts is beneficial for many different geo-mobile application domains. Burstiness is one of the simplest and most effective criteria for extracting hot topics and events in micro-posts. In this paper, we propose a novel burst detection algorithm for detecting location-based enumerating bursts in georeferenced micro-posts. To evaluate the proposed burst detection algorithm, we used an actual set of georeferenced micro-posts, which are crawling tweets posted on the Twitter site. The experimental results show that our new burst detection algorithm can detect location-based enumerating bursts.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630380","burst detection;enumerating bursts;georeferenced micro-posts;spatiotemporal data;topic detection and tracking","Detection algorithms;Educational institutions;Equations;Mathematical model;Media;Snow;Twitter","Internet;information retrieval;social networking (online);text analysis","Internet;Twitter;geomobile application domain;georeferenced micropost;location-based enumerating burst detection;social media site","","6","","15","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Towards feature-aware retrieval of refinement traces","P. Rempel; P. Mäder; T. Kuschke","Ilmenau Technical University, Department of Software Systems, Germany","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","100","104","Requirements traceability supports practitioners in reaching higher project maturity and better product quality. To gain this support, traces between various artifacts of the software development process are required. Depending on the number of existing artifacts, establishing traces can be a time-consuming and error-prone task. Additionally, the manual creation of traces frequently interrupts the software development process. In order to overcome those problems, practitioners are asking for techniques that support the creation of traces (see Grand Challenge: Ubiquitous (GC-U)). In this paper, we propose the usage of a graph clustering algorithm to support the retrieval of refinement traces. Refinement traces are traces that exist between artifacts created in different phases of a development project, e.g., between features and use cases. We assessed the effectiveness of our approach in several TraceLab experiments. These experiments employ three standard datasets containing differing types of refinement traces. Results show that graph clustering can improve the retrieval of refinement traces and is a step towards the overall goal of ubiquitous traceability.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620163","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620163","TraceLab experiment;graph clustering;requirements traceability;trace retrieval;ubiquitous challenge (GC-U)","Clustering algorithms;Conferences;Large scale integration;Software;Software engineering;Standards;Vectors","formal specification;graph theory;information retrieval;pattern clustering;product quality;project management;software development management;ubiquitous computing","TraceLab experiments;development project;feature-aware retrieval;graph clustering algorithm;product quality;project maturity;refinement trace retrieval;requirements traceability;software development process;ubiquitous traceability","","1","","19","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"Feasibility of Unified Usage of Heterogeneous Databases Storing Private Information","X. Wang; T. Hochin; H. Nomiya","Dept. of Inf. Sci., Kyoto Inst. of Technol., Kyoto, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","337","342","This paper shows the feasibility of the client-server retrieval system enabling users to receive various services with keeping the users' data private. This functionality is inevitable when the users want to receive some services while they want to keep their data private. In this system, the users could store their data into various kinds of database. It is shown that the system could be realized by using Java Database Connectivity (JDBC) in accessing databases in the client computers. The descriptions in using ordinary database management systems including PostgreSQL, MySQL, and SQLite are almost the same, while the description in using Excel files is slightly different from them.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630370","JDBC;databases;learning system;unified usage","Informatics","Java;SQL;client-server systems;data privacy;information retrieval systems","Excel files;JDBC;Java database connectivity;MySQL;PostgreSQL;SQLite;client computers;client-server retrieval system;heterogeneous databases;ordinary database management systems;private information storage;user data private","","3","","12","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Feature Selection for Forensic Handwriting Identification","A. M. M. M. Amaral; C. O. d. A. Freitas; F. Bortolozzi","Dept. of Inf., UniCesumar, Maringa, Brazil","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","922","926","Current paper describes the use of a feature selection technique to reduce the number of features while the goodness set is selected on a framework for forensic handwriting identification. A sequential forward search and an evaluation criterion based on dependency were used to obtain a goodness subset (GS) to improve the identification rate. The accuracy of the system applied to 100 different writers and taking account all features (N = 81) is 58%, whereas the accuracy based on goodness subset (GS) is 80% applied to the same number of writers. The validation of results was verified initially against all the features and later against some empirically set of features. Results are comparable to others in the literature on graphometric features.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.188","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628753","feature selection;forensic handwriting analysis;forensic letter;graphometric feature;writer identification","Accuracy;Current measurement;Data mining;Feature extraction;Forensics;Handwriting recognition","feature extraction;forensic science;handwriting recognition;information retrieval","evaluation criterion;feature selection;forensic handwriting identification;goodness subset;graphometric feature;identification rate improvement;sequential forward search","","2","","27","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Geo-based search engine","G. Y. Ghasemi; L. G. Licayan","Ghaen Technical college, University of Birjand, Ghaen City, Iran","The 5th Conference on Information and Knowledge Technology","20131007","2013","","","495","501","A search engine is a program that searches documents using specific keywords on the World Wide Web and return a list of websites where the keywords are found. It has become a major tool for reaching a growing number of social, political, economic, educational, agricultural and cultural domains represented on the Web. Majority of the search engines return various results in which most of them are unnecessary and unrelated to the geographic location of the web user/surfer. This research project aims to create a search engine that prioritizes and gives emphasis on the geographic location of the end user. Geo-based search engine makes use of the allocated IP addresses to each country, the Country Code Top Level Domain (ccTLD) such as .ph for the Philippines, and the registered place of domain names as the basis of gathering documents and datasets during the web crawling activity.","","Electronic:978-1-4673-6490-4; POD:978-1-4673-6488-1","10.1109/IKT.2013.6620119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620119","Geo search engine;Search engine;World Wide Web","Crawlers;IP networks;Indexing;Search engines;Software;Web pages","Web sites;document handling;geography;information retrieval;search engines","IP addresses;Web crawling activity;Website;World Wide Web;agricultural domain;ccTLD;country code top level domain;cultural domain;document searching;economic domain;educational domain;end user geographic location;geo-based search engine;keywords;political domain;social domains","","1","","12","","","28-30 May 2013","","IEEE","IEEE Conference Publications"
