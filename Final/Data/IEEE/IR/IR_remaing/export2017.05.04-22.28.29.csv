"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=5766764,6152743,6149991,6151562,6150439,6146854,6149571,6147660,6147684,6149593,6147663,6147052,6149082,6147055,6149565,6146851,6148989,6149159,6147007,6146932,6149097,6147607,6149449,6149204,6149339,6149430,6149104,6149090,6149105,6148795,6149080,6149255,6149295,6118328,6144069,6143427,6143435,6144790,6144785,6143440,6143822,6142299,6143518,6144776,6143441,6143514,6142747,6142272,6142868,6142295,6141376,6141220,5611552,6140798,5645623,6081847,6139386,6139376,6138211,5677515,6138212,5677531,6138181,6137409,6137285,6137387,6086533,6131141,6137717,6137375,6137335,6137325,6137365,6137593,6137362,6137330,6137369,6137407,6137284,6137309,6135863,5989824,6137425,6136294,6137514,6133000,6133227,6133071,6132924,6133199,6059860,6059820,6130745,6130680,6059861,6131747,6059874,6132148,6059817,6059825",2017/05/04 22:28:29
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"A survey on informaton extraction using entity relation based methods","S. Geetha; G. S. Anandha Mala; N. Kanya","JNTU Hyderabad, Andhra Pradesh, India","International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011)","20120202","2011","","","882","885","Information extraction (IE) is the process of extracting the essential elements from structured text or knowledge from unstructured text by identifying references to named entities as well as stated relationships between such entities. IE systems can be used to directly untangle abstract knowledge from a text corpus, or to extract concrete information from a set of documents which can further be analyzed with traditional data-mining techniques to discover more general patterns and to identify the relationship between the entities in the Structured Text. It is required to discuss the various Entity Relation Extraction Methods and accuracy of the Extraction process. This paper presents the comparison of different entity relation extraction methods between two entities.","","Electronic:978-9-38043-000-3","10.1049/cp.2011.0491","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143440","Entities;Information Extraction;Patterns;Relationship;Structured Text","","data mining;document handling;information retrieval","IE;data mining techniques;document set;entity relation extraction methods;information extraction;text structure","","0","","","","","20-22 July 2011","","IET","IET Conference Publications"
"A novel automatic text summarization system with feature terms identification","S. Manne; S. M. Z. Pervez; S. S. Fatima","Department of IT, VRSEC, Vijayawada","2011 Annual IEEE India Conference","20120126","2011","","","1","6","With ever growing content on World Wide Web, it has been increasingly difficult for users to search for relevant information. A rough estimation of world's famous search engine Google in year 2010 revealed that the total size of internet has now turned to 2 petabytes. Search engines that are supposed to satisfy user's information need, has too much information to offer than what is required. This problem is referred as information overload. The field of Information Extraction (IE) is offering a huge scope to concise and compact the information enabling the user to decide by mere check at snippets of each link. Automatic text summarization, a subset of IE is an important activity in the analysis of a high volume text documents. In this context, it has been increasingly important to develop information access solutions that can provide an easy and efficient access to users. Automatic summarization systems address information overload problem by producing a summary of related documents that provides an overall understanding of the topic without having to go through every document. In this paper, we propose a feature term based text summarization technique based on the analysis of Parts of Speech Tagging. A new approach of generating summary for a given input document is discussed based on identification and extraction of important sentences in the document. The system obtains the selective terms from the extracted terms and builds qualitative summary with appreciable compression ratio.","2325-940X;2325940X","Electronic:978-1-4577-1109-1; POD:978-1-4577-1110-7","10.1109/INDCON.2011.6139386","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139386","Extractive feature terms;HMM tagger;POS tagging;Term frequency","Feature extraction;Frequency measurement;Hidden Markov models;Natural language processing;Stochastic processes;Tagging;Training","Internet;information retrieval;search engines;text analysis","Google search engine;Internet;World Wide Web;automatic text summarization system;feature terms identification;important sentence extraction;important sentence identification;information access;information extraction;parts-of-speech tagging;text document analysis","","1","","19","","","16-18 Dec. 2011","","IEEE","IEEE Conference Publications"
"A Study on Automatic Extraction of New Terms","X. Zhang; A. C. Fang","Dept. of Chinese, Translation & Linguistics, City Univ. of Hong Kong, Kowloon, China","2011 Fourth International Symposium on Knowledge Acquisition and Modeling","20120123","2011","","","599","602","This research explores to automatically predict new terms based on linguistic features and statistical behaviors of noun phrases during a special period. It integrates both syntactic function value and TF-IDF value into an automatic term extraction system to weight new term candidates. Research questions include: what are the linguistic and statistic properties of new terms during a special period? Will linguistic features contribute to prediction of new terms? And will statistic features like, TFIDF Value contribute to prediction of new terms? Correspondingly, a series of experiments are conducted on medical corpus to examine a group of new terms' distribution properties and syntactic features across two years in comparison. The results show there does exist significant difference between two groups of values. Regardless of this limitation, this research is meaningful as it attempts to realize automation of selection process of new medical terms, which will greatly avoid subjective decisions and reduce experts' workloads.","","Electronic:978-0-7695-4547-9; POD:978-1-4577-1788-8","10.1109/KAM.2011.162","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137717","New term;SF-Value;TFIDF;old term;term extraction","Abstracts;Analysis of variance;Equations;Pragmatics;Syntactics;Terminology;Testing","information retrieval;statistical analysis","TF-IDF value;automatic new term extraction system;linguistic feature;noun phrase behavior;statistical behavior;syntactic function value","","0","1","9","","","8-9 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Genetic Programming Approach to Record Deduplication","M. G. de Carvalho; A. H. F. Laender; M. A. Goncalves; A. S. da Silva","Nokia INdT, Brazil","IEEE Transactions on Knowledge and Data Engineering","20120126","2012","24","3","399","412","Several systems that rely on consistent data to offer high-quality services, such as digital libraries and e-commerce brokers, may be affected by the existence of duplicates, quasi replicas, or near-duplicate entries in their repositories. Because of that, there have been significant investments from private and government organizations for developing methods for removing replicas from its data repositories. This is due to the fact that clean and replica-free repositories not only allow the retrieval of higher quality information but also lead to more concise data and to potential savings in computational time and resources to process this data. In this paper, we propose a genetic programming approach to record deduplication that combines several different pieces of evidence extracted from the data content to find a deduplication function that is able to identify whether two entries in a repository are replicas or not. As shown by our experiments, our approach outperforms an existing state-of-the-art method found in the literature. Moreover, the suggested functions are computationally less demanding since they use fewer evidence. In addition, our genetic programming approach is capable of automatically adapting these functions to a given fixed replica identification boundary, freeing the user from the burden of having to choose and tune this parameter.","1041-4347;10414347","","10.1109/TKDE.2010.234","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5645623","Database administration;database integration.;evolutionary computing and genetic algorithms","Data mining;Databases;Genetic programming;Machine learning;Probabilistic logic;Training","genetic algorithms;information retrieval;replicated databases","computational time;data repositories;database administration;database integration;digital libraries;e-commerce brokers;fixed replica identification boundary;genetic programming;information retrieval;record deduplication;replica removal;replica-free repositories","","20","","33","","20101129","March 2012","","IEEE","IEEE Journals & Magazines"
"Data Decomposition in Biomedical e-Science Applications","Y. Mohammed; S. Shahand; V. Korkhov; A. C. M. Luyf; B. D. C. van Schaik; M. W. A. Caan; A. H. C. van Kampen; M. Palmblad; S. D. Olabarriaga","Dept. of Clinical Epidemiology, Biostat., & Bioinf., Univ. of Amsterdam, Amsterdam, Netherlands","2011 IEEE Seventh International Conference on e-Science Workshops","20120116","2011","","","158","165","As the focus of e-Science is moving toward the forth paradigm and data intensive science, data access remains dependent on the architecture of the used e-Science infrastructure. Such architecture is in general job-driven, i.e., a (grid) job is a sequence of commands that run on the same worker node. Making use of the infrastructure involves having a parallelized application. This is done foremost by data decomposition. In general practice of parallel programming, data decomposition depends on the programmer's experience and knowledge about the used data and the algorithm/application. On the other hand, data mining scientists have an established foundation for data decomposition, automatic decomposition methods are already in use, methodologies and patterns are defined. Our experience in porting biomedical applications to the Dutch e-Science infrastructure shows that the used data decomposition to gain parallelism fit to some degree a subgroup of the data mining decomposition patterns, i.e., object set decomposition. In this paper we discuss porting three biomedical packages to a grid computing environment, two for medical imaging and one for DNA sequencing. We show how the data access of the applications was reengineered around the executables to make use of the parallel capacity of e-Science infrastructure.","","Electronic:978-0-7695-4598-1; POD:978-1-4673-0026-1","10.1109/eScienceW.2011.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130745","Healthgrid;data decomposition;e-infrastructure;grid Computing;legacy application;porting to grid;workflows","Biomedical imaging;Data mining;Diffusion tensor imaging;Educational institutions;Parallel processing;Pipelines","bioinformatics;data mining;grid computing;information retrieval;medical information systems;parallel programming","DNA sequencing;Dutch e-Science infrastructure;automatic data decomposition methods;biomedical e-science applications;biomedical packages;data access;data intensive science;data mining decomposition patterns;grid computing environment;medical imaging;object set decomposition;parallel programming","","0","","34","","","5-8 Dec. 2011","","IEEE","IEEE Conference Publications"
"Efficient validation/verification of a robust DVB-H link layer","O. Eerenberg; P. Wendrich; E. H. M. van Orsouw; P. H. N. de With","Trident Microsystems, The Netherlands","IEEE Transactions on Consumer Electronics","20120123","2011","57","4","1679","1687","The link layer is one of the key differentiating functions in a DVB-H receiver. In order to differentiate between silicon solutions, manufacturers can choose between basic or advanced link-layer implementation concepts, resulting in modest or excellent data retrieval performance, respectively. Hence, objective performance evaluation, which proves the quality of the link layer implementation, is an essential step to validate product robustness. This paper presents techniques for validation and verification tools to evaluate the DVB-H link-layer robustness. The proposed techniques for robustness testing are capable of generating both regular and extreme signal conditions. Furthermore, we also discuss an efficient analysis tool required to inspect (1) the IP-based output data, (2) Service Information and (3) Program Specific Information of the link layer. Simulation results for improved link-layer performance presented by the tools are validated from a newly developed silicon implementation<sup>1</sup>.","0098-3063;00983063","","10.1109/TCE.2011.6131141","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131141","DVB-H;Forward Error Correction;InternetProtocol;Link Layer;MPEG;Multi-Protocol Encapsulation;OSI;Reed-Solomon;Verification/Validation Tools.","Decoding;Digital video broadcasting;Forward error correction;Generators;IP networks;Robustness;Silicon","IP networks;computer network performance evaluation;digital video broadcasting;information retrieval;radio links;radio receivers","DVB-H receiver;IP-based output data;advanced link layer implementation;data retrieval performance;objective performance evaluation;program specific information;robust DVB-H link layer;robustness testing;service information;silicon solutions;validation tools;verification tools","","1","","12","","","November 2011","","IEEE","IEEE Journals & Magazines"
"Building a Natural Language Hindi Speech Interface to Access Market Information","A. Imran; S. K. Kopparapu","TCS Innovation Labs. - Mumbai, Tata Consultancy Services, Mumbai, India","2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics","20120119","2011","","","58","61","It is a well known fact that majority of rural India earns its livelihood from agriculture and farming. Although India is a net exporter of various agricultural products, the farmer who happens to be the primary producer, has remained information poor which puts him at a disadvantage. With little or no knowledge of prices at the markets, farmers have no leverage to negotiate better prices for their produce. Speech based solution can address this issue of market price information availability to farmers. Speech based solutions are increasingly being used for transaction but they are both (a) restricted to menu based type interactions where a series of interactions are required for the transaction to take place and (b) primarily built for the English literate population synonymously urban population. Paradoxically, the benefit of a speech based solution is best reaped by the rural folks speaking their native language (very often non-English) because the other modes of transactions are either not readily available to them or if available difficult to use. In this paper, we develop a natural language Hindi speech interface to enable Hindi speaking population access market prices of commodities.","","Electronic:978-0-7695-4599-8; POD:978-1-4577-2102-1","10.1109/NCVPRIPG.2011.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133000","Hindi Speech Interface;Mandi Bhav Jankari;Natural Language","Accuracy;Acoustics;Engines;Grammar;Natural languages;Speech;Speech recognition","agriculture;information retrieval;marketing data processing;natural language processing;pricing;speech processing;user interfaces","English literate population;Hindi speaking population access market prices;India;agricultural products;farming;market information access;market price information availibility;menu based type interactions;natural language Hindi speech interface;speech based solution;urban population","","2","","9","","","15-17 Dec. 2011","","IEEE","IEEE Conference Publications"
"Dynamic peer-to-peer distributed document clustering and cluster summarization","S. Meena","Department of Information Technology, Rajalakshmi Engineering College, Chennai-602105, India","International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011)","20120202","2011","","","815","819","The main objective of this paper is to provide cluster summarization of huge text document. Mining process includes the sharing of large scale amount of data from various sources, which gets concluded at the mined data. In distributed data mining, adopting aflat node distribution model can affect scalability, modularity, flexibility which are being overcome by using dynamic peer to peer document clustering and cluster summarization. The Dynamic P2P document clustering and cluster summarization (DP2PCS) architecture is based upon bonus words and stigma words. For document clustering applications, the system summarizes the distributed document clusters using a distributed key-phrase extraction algorithm, thus providing interpretation of the clusters. Document summarization is used for fast information retrieval in less time. Compared to existing system the dynamic nature of proposed system facilitates a scalable cluster wherein the peers may join or leave the group at will. The summarization process on an average reduces the original documents content by 63 percentage based on the word count.","","Electronic:978-9-38043-000-3","10.1049/cp.2011.0478","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143427","Distributed data mining;distributed document clustering;document summarization;hierarchical peer-to-peer networks","","data mining;information retrieval;pattern clustering;peer-to-peer computing;text analysis","bonus words;distributed data mining process;distributed key-phrase extraction algorithm;dynamic peer-to-peer distributed document clustering;flat node distribution model;information retrieval;stigma words;text document cluster summarization","","0","","","","","20-22 July 2011","","IET","IET Conference Publications"
"Extraction of Attribute Dependency Graph from Database Applications","K. Liu; H. B. K. Tan; X. Chen","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore, Singapore","2011 18th Asia-Pacific Software Engineering Conference","20120116","2011","","","138","145","Database applications constitute a large portion of the software systems. This paper proposes a novel graph called attribute dependency graph to show the dependencies between attributes in a database application and also the programs involved. We propose an approach to automatically extract the attribute dependency graph through analyzing the source code of database applications. The extracted information can be used in the maintenance process particularly in the impact analysis upon modification of a database application. A tool has been developed to implement the proposed approach for PHP-based database applications. Case studies have also been conducted to demonstrate the use of our proposed approach.","1530-1362;15301362","Electronic:978-0-7695-4609-4; POD:978-1-4577-2199-1","10.1109/APSEC.2011.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130680","attribute dependency graph;extraction;impact analysis;maintenance","Data mining;Databases;Flow graphs;Maintenance engineering;Prototypes;Software systems;US Department of Transportation","database management systems;graph theory;information retrieval","PHP-based database applications;attribute dependency graph extraction;information extraction;maintenance process;software systems;source code","","2","","22","","","5-8 Dec. 2011","","IEEE","IEEE Conference Publications"
"The DIEGO Lab Graph Based Gene Normalization System","R. Sullivan; R. Leaman; G. Gonzalez","Dept. of Biomed. Inf., Arizona State Univ., Tempe, AZ, USA","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","2","","78","83","Gene entity normalization, the mapping of a gene mention in free text to a unique identifier, is one of the primary subtasks in the biomedical information extraction pipeline. Gene entity normalization provides many challenges, specifically with the high ambiguity of gene names and the many-to-many relationship between gene names and identifiers. Drawing inspiration from recent work in word sense disambiguation, this paper presents a gene entity normalization system based on entity relationship graphs. This system creates a concept graph from the possible entities and their relationships within a full-text document, and takes advantage of a node ranking algorithm to rank and score each potential candidate entity. This system is a prototype to represent a specific approach to gene normalization, and the results reflect this. However, this system demonstrates that the relationship graph-based approach, an approach grounded in a theoretical basis, can potentially be useful for gene normalization and possibly for the normalization of various biomedical entities.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147052","","Biological system modeling;Computational linguistics;Data models;Dictionaries;Semantics;Taxonomy;Training","genetics;graph theory;information retrieval;medical computing;natural language processing;text analysis","DIEGO lab graph;biomedical entity;biomedical information extraction pipeline;entity relationship graph;full-text document;gene entity normalization system;gene name;graph-based approach;potential candidate entity;word sense disambiguation","","0","","26","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Automatic Cleaning and Linking of Historical Census Data Using Household Information","Z. Fu; P. Christen; M. Boot","Res. Sch. of Comput. Sci., Australian Nat. Univ. Canberra, Canberra, ACT, Australia","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","413","420","Historical census data captures information about our ancestors. These data contain the social status at a certain point time. They contain valuable information for genealogists, historians, and social scientists. Historical census data can be used to reconstruct important aspects of a particular era in order to trace the changes in households and families. Record linkage across different historical census datasets can help to improve the quality of the data, enrich existing census data with additional information, and facilitate improved retrieval of information. In this paper, we introduce a domain driven approach to automatically clean and link historical census data based on recent developments in group linkage techniques. The key contribution of our approach is to first detect households, and to use this information to refine the cleaned data and improve the accuracy of linking records between census datasets. We have developed a two-step linking approach, which first links individual records using approximate string similarity measures, and then performs a group linking based on the previously detected households. The results show that this approach is effective and can greatly reduce the manual efforts required for data cleaning and linking by social scientists.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137409","Historical census data;data cleaning;domain knowledge;group linking;record linkage","Cleaning;Computer science;Couplings;Educational institutions;Electronic mail;Joining processes;Magnetic heads","demography;history;information retrieval;records management","approximate string similarity measures;automatic cleaning;group linkage techniques;historical census data;household information;information retrieval;record linkage;social status;two-step linking approach","","1","","24","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Geographic locations retrieval from photos","Đ. Petrović; D. Perić; V. Pavlović","","2011 19thTelecommunications Forum (TELFOR) Proceedings of Papers","20120202","2011","","","1422","1425","This document will consider the possibilities and risks of the use of geographic data included in the photos, as well as techniques for discovering such information, which we have used in practice. Geotag embedded in images and video clips provide lot of possibilities, but can be potentially dangerous from the standpoint of preserving privacy. Since the data is visible to more users, it can be assumed that these users are not aware of the existence of such data. The goal of online services, which we have set to provide visitors an easy and simple way to verify whether the photos include data on geographical location.","","Electronic:978-1-4577-1498-6; POD:978-1-4577-1499-3","10.1109/TELFOR.2011.6143822","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143822","","Arrays;Fires;Geology;Global Positioning System;Google;Internet;Twitter","data privacy;geography;information retrieval;information services","Geotag;geographic data;geographic location retrieval;online service;photos;privacy preservation","","0","","8","","","22-24 Nov. 2011","","IEEE","IEEE Conference Publications"
"Artificial intelligence technologies in business and engineering","S. Archana Bai","Dr MGR University, Chennai-95, India","International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011)","20120202","2011","","","856","859","Artificial intelligence (AI) is making its way back into the mainstream of corporate technology, this time at the core of business systems which are providing competitive advantage in all sorts of industries, including electronics, manufacturing, marketing, human resource, financial services software, medicine, entertainment, engineering and communications. Designed to leverage the capabilities of humans rather than replace them, today's AI technology enables an extraordinary array of applications that forge new connections among people, computers, knowledge, and the physical world. Some AI enabled applications are information distribution and retrieval, database mining, product design, manufacturing, inspection, training, user support, surgical planning, resource scheduling, and complex resource management. AI technologies help enterprises reduce latency in making business decisions, minimize fraud and enhance revenue opportunities.","","Electronic:978-9-38043-000-3","10.1049/cp.2011.0486","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143435","Applications of AI;Artificial Intelligence in management-Manufacturing;Artificial intelligence (AI);Engineering Domains of AI;Financial Services;Human Resources;Marketing","","artificial intelligence;commerce;data mining;database management systems;information retrieval;inspection;product design;scheduling","artificial intelligence;business systems;complex resource management;corporate technology;database mining;engineering;information distribution;information retrieval;inspection;manufacturing;product design;resource scheduling;surgical planning;training;user support","","1","","","","","20-22 July 2011","","IET","IET Conference Publications"
"A Willing Events Identification Method","Y. Zhao; W. Zhou; Y. Li; Z. Liu; Y. Zhu; P. Zhu","Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China","2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing","20120202","2011","","","424","431","Events extraction is an important part of information extraction, at present, most events extraction aim at extracting events themselves but do not consider whether the events have already happened. This paper focuses on the events named willing events which have aleady proposed and made sure to happen but not yet happened currently. There exist quite a number of willing events in many areas, and those can provide a lot of information, thus, carries on the extraction is very meaningful. This paper presents a new feature selection method based on Dependency Parsing. Dependency parsing is used to find the syntactic relations among the words expresses willingness, event denoters and other words. Then these features are used in machine learning to classify the events, and finally achieve the identification of willing events. The experiment shows that, aiming at willing events, the features found based on dependency parsing do better than those features used in traditional events identification.","","Electronic:978-0-7695-4580-6; POD:978-1-4577-1976-9","10.1109/iThings/CPSCom.2011.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142299","LWL;dependency parsing;event identification;libSVM;machine learning","Data mining;Feature extraction;Hidden Markov models;Machine learning;Semantics;Support vector machines;Syntactics","feature extraction;information retrieval;learning (artificial intelligence);natural language processing;pattern classification;text analysis","dependency parsing;event classification;event denoter;events extraction;feature selection method;information extraction;machine learning;natural language processing;syntactic relation;text processing;willing events identification method;willingness expressing words","","0","","13","","","19-22 Oct. 2011","","IEEE","IEEE Conference Publications"
"Learning to Rank Using Markov Random Fields","N. Freno; T. Papini; M. Diligenti","Dipt. di Ing. dell'Inf., Univ. of Siena, Siena, Italy","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","2","","257","262","Learning to rank from examples is an important task in modern Information Retrieval systems like Web search engines, where the large number of available features makes hard to manually devise high-performing ranking functions. This paper presents a novel approach to learning-to-rank, which can natively integrate any target metric with no modifications. The target metric is optimized via maximum-likelihood estimation of a probability distribution over the ranks, which are assumed to follow a Boltzmann distribution. Unlike other approaches in the literature like BoltzRank, this approach does not rely on maximizing the expected value of the target score as a proxy of the optimization of target metric. This has both theoretical and performance advantages as the expected value can not be computed both accurately and efficiently. Furthermore, our model employs the pseudo-likelihood as an accurate surrogate of the likelihood to avoid to explicitly compute the normalization factor of the Boltzmann distribution, which is intractable in this context. The experimental results show that the approach provides state-of-the-art results on various benchmarks and on a dataset built from the logs of a commercial search engine.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.157","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147684","","Boltzmann distribution;Computational modeling;Equations;Markov processes;Mathematical model;Measurement;Training","Internet;Markov processes;information retrieval;maximum likelihood estimation;probability;random processes;search engines","BoltzRank;Boltzmann distribution;Markov random field;Web search engine;information retrieval system;learning-to-rank;maximum-likelihood estimation;normalization factor;probability distribution;pseudo-likelihood;target metric","","1","","25","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Development of Bengali screen reader using Festival speech synthesizer","N. P. Narendra; K. S. Rao; K. Ghosh; V. R. Reddy; S. Maity","School of Information Technology, Indian Institute of Technology Kharagpur, Kharagpur - 721302, West Bengal, India","2011 Annual IEEE India Conference","20120126","2011","","","1","4","This paper discusses the development of Bengali screen reader using Festival speech synthesizer. Screen reader is developed with the objective that the visually challenged people can use the computer without any difficulty. The usability of system is checked throughout the development and appropriate modifications are made. Unrestricted Bengali text to speech synthesis (TTS) system which can produce good quality speech in different domains is integrated into the screen reader. Pruning of database is performed to reduce the response time of screen reader. Finally subjective evaluation of screen reader is carried out by the visually challenged people for different applications such as web browsing. Results indicate that the developed system can be used by visually challenged people independently without any external assistance.","2325-940X;2325940X","Electronic:978-1-4577-1109-1; POD:978-1-4577-1110-7","10.1109/INDCON.2011.6139376","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139376","Bengali screen reader;database pruning;unrestricted Bengali TTS","Buildings;Computers;Databases;Speech;Synthesizers;Time factors;Usability","Internet;handicapped aids;human computer interaction;information retrieval;natural language processing;screens (display);speech synthesis","Bengali screen reader development;Bengali text-to-speech synthesis system;Festival speech synthesizer;Web browsing;system usability;visually challenged people","","3","","11","","","16-18 Dec. 2011","","IEEE","IEEE Conference Publications"
"DMS — Improving Web performance","A. Jevremovic; R. Popovic; D. Zivkovic; M. Veinovic; G. Shimic","Fac. of Inf. & Comput, Singidunum Univ., Belgrade, Serbia","2011 19thTelecommunications Forum (TELFOR) Proceedings of Papers","20120202","2011","","","166","169","In this paper we consider the problem of improving Web performance and propose an efficient differencing and merging system (DMS) based on an HTTP protocol extension. To provide for faster information exchange over the Web, the system tries to transfer only computed differences between requested documents and previously retrieved documents from the same site. Analysis and experimental results prove the effectiveness of DMS, but also show bigger processor and memory load on servers and clients. DMS is compatible with most of the existing solutions for improving Web performance. Moreover, SSL security system may be used to provide Web privacy and authenticity. The DMS model is simple to use and can be relatively easily integrated in Web servers and browsers.","","Electronic:978-1-4577-1498-6; POD:978-1-4577-1499-3","10.1109/TELFOR.2011.6143518","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143518","Web performance;differencing;merging","Telecommunications","Internet;data privacy;information retrieval;merging;online front-ends;software performance evaluation","DMS model;HTTP protocol extension;SSL security system;Web browser;Web performance;Web privacy;Web server;authenticity;differencing and merging system;document retrieval;information exchange;memory load","","0","","22","","","22-24 Nov. 2011","","IEEE","IEEE Conference Publications"
"Towards the Discovery of Semantic Relations in Large Biomedical Annotated Corpora","V. N. Romero; S. Kudama; R. B. Llavori","Univ. Jaume I, Castellon, Spain","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","465","469","This paper proposes the application of multidimensional analysis over large semantically annotated biomedical corpora for the identification of relevant abstract relations between the recognized entities. The identification of relations is one of the most challenging issues in information extraction, as they guide the definition of the patterns used during the extraction phase. Multidimensional analysis allows us to define different analysis perspectives with different detail levels over the extracted facts. Among other tasks, users can distinguish discriminative relation patterns from ambiguous ones, detect the most relevant relation patterns and identify clusters of patterns that can refer to the same abstract relation. The proposal has been implemented upon a commercial tool and tested over the CALBC corpus.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.83","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059861","Information Extraction;Multidimensional Analysis;Relation Identification;Semantic Annotation","Bioinformatics;Biomedical measurements;Data mining;Protein engineering;Proteins;Semantics;Unified modeling language","information retrieval;medical computing","CALBC corpus;biomedical annotated corpora;extraction phase;information extraction;multidimensional analysis;relevant abstract relation identification;semantic relation discovery","","0","","15","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"'Knee-Deep in the Data': Practical Problems in Applying the OAIS Reference Model to the Preservation of Computer Games","J. P. McDonough","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1625","1634","The Reference Model for an Open Archival Information System has been extraordinarily influential within the digital preservation community. The Preserving Virtual Worlds project explored the application of the OAIS Reference Model for the preservation of computer games, videogames and electronic literature within a research library setting. This paper identifies practical problems in determining the appropriate range of representation and context information needed to preserve computer games and discusses possible solutions to those problems.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.1","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149082","OAIS;digital preservation;metadata;videogames","Communities;Computational modeling;Computers;Context;Games;Libraries;Ontologies","computer games;information retrieval systems;research libraries","OAIS reference model;Preserving Virtual Worlds project;computer game preservation;digital preservation community;electronic literature;open archival information system;research library;videogames","","0","","24","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Twitter Trending Topic Classification","K. Lee; D. Palsetia; R. Narayanan; M. M. A. Patwary; A. Agrawal; A. Choudhary","Dept. of Electr. Eng. & Comput. Sci., Northwestern Univ., Evanston, IL, USA","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","251","258","With the increasing popularity of microblogging sites, we are in the era of information explosion. As of June 2011, about 200 million tweets are being generated everyday. Although Twitter provides a list of most popular topics people tweet about known as Trending Topics in real time, it is often hard to understand what these trending topics are about. Therefore, it is important and necessary to classify these topics into general categories with high accuracy for better information retrieval. To address this problem, we classify Twitter Trending Topics into 18 general categories such as sports, politics, technology, etc. We experiment with 2 approaches for topic classification, (i) the well-known Bag-of-Words approach for text classification and (ii) network-based classification. In text-based classification method, we construct word vectors with trending topic definition and tweets, and the commonly used tf-idf weights are used to classify the topics using a Naive Bayes Multinomial classifier. In network-based classification method, we identify top 5 similar topics for a given topic based on the number of common influential users. The categories of the similar topics and the number of common influential users between the given topic and its similar topics are used to classify the given topic using a C5.0 decision tree learner. Experiments on a database of randomly selected 768 trending topics (over 18 classes) show that classification accuracy of up to 65% and 70% can be achieved using text-based and network-based classification modeling respectively.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137387","Social Networks;Topic Classification;Twitter","Accuracy;Computational modeling;Data models;Labeling;Machine learning;Twitter","Bayes methods;decision trees;information retrieval;pattern classification;social networking (online);text analysis","C5.0 decision tree learner;Twitter trending topic classification;bag-of-words approach;information explosion;information retrieval;microblogging sites;naive Bayes multinomial classifier;network-based classification;politics;sports;technology;text-based classification method;tf-idf weights;word vectors","","38","","18","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Comparison of Two Methods for Finding Biomedical Categories in Medline","L. Yeganova; W. Kim; D. C. Comeau; W. J. Wilbur","Nat. Libr. of Med., Nat. Inst. of Health, Bethesda, MD, USA","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","2","","96","99","In this paper we describe and compare two methods for automatically learning meaningful biomedical categories in Medline®. The first approach is a simple statistical method that uses part-of-speech and frequency information to extract a list of frequent headwords from noun phrases in Medline. The second method implements an alignment-based technique to learn frequent generic patterns that indicate a hyponymy/hypernymy relationship between a pair of noun phrases. We then apply these patterns to Medline to collect frequent hypernyms, potential biomedical categories. We study and compare these two alternative sets of terms to identify semantic categories in Medline. Our method is completely data-driven.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147055","","Diseases;Feature extraction;Ontologies;Semantics;Statistical analysis;Unified modeling language;Vectors","document handling;information retrieval;learning (artificial intelligence);medical computing;statistical analysis","biomedical categories;frequency information;frequent headwords;frequent hypernyms;medical literature analysis and retrieval system online;medline biomedical categories;noun phrases;part-of-speech information;semantic categories;statistical method;two method comparison","","0","","10","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"New Trends in Security Evaluation of Bayesian Network-Based Malware Detection Models","E. Filiol; S. Josse","Operational Cryptology & Virology Lab., ESIEA Res., Laval, France","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","5574","5583","Statistical methods have been used for a long time as a way to detect viral code. Such a detection method has been called spectral analysis, because it works with statistical distributions, such as bytes, instructions or system calls frequencies spectra. Most statistical classification algorithms can be described as graphical models, namely Bayesian networks. We will first present in this paper an approach of viral detection by means of spectral analysis based on Bayesian networks, through two basic examples of such learning models: naive Bayes and hidden Markov models. Designing a statistical information retrieval model requires careful and thorough evaluation in order to demonstrate the superior performance of new techniques on representative program collections. Nowadays, it has developed into a highly empirical discipline. We will next present information theory based criteria to characterize the effectiveness of spectral analysis models and then discuss the limits of such models.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.450","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149571","Bayesian Network;Hidden Markov Model;Naive Bayes;Spectral analysis","Analytical models;Bayesian methods;Engines;Hidden Markov models;Random variables;Spectral analysis;Training data","Bayes methods;belief networks;hidden Markov models;information retrieval;invasive software;pattern classification;spectral analysis;statistical distributions","Bayesian network-based malware detection models;graphical models;hidden Markov model;information theory based criteria;naive Bayes model;security evaluation;spectral analysis;statistical classification algorithms;statistical distributions;statistical information retrieval model;statistical methods;viral code detection","","1","","23","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"A Large Scale URL Verification Pipeline Using Hadoop","S. Guo; J. Dong","AT&T Interactive, San Francisco, CA, USA","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","159","166","Data quality is a key element for local search and advertising. Inaccurate, out-of-date or missing information causes an unpleasant search experience for users and affects competitiveness of service providers. This paper addresses the problem of evaluating link quality for business listings in local search and online advertising domain. We introduce a novel system where we apply data mining technologies on a Hadoop-based platform to provide an efficient and highly scalable solution for the problem. Due to various reasons, links associated with business listings do not always point to their business websites. Possible noises include parked domain, broken links, third-party advertisers, irrelevant websites etc. To detect above noises and improve link quality, we formulate this problem as a binary classification problem: whether a given URL is the business website of the associated listing. Experiments conducted on real-world data show that our system can verify millions of business listings against about 100 million web pages in a couple of hours with 93% classification accuracy.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137375","classification;cloud computing;data quality;web information extraction","Business;Cities and towns;Data mining;Facsimile;Feature extraction;Internet;Web pages","Web sites;advertising;business data processing;data mining;distributed processing;information retrieval;pattern classification","Hadoop;advertising;binary classification problem;broken links;business Websites;business listings;data mining technologies;data quality;irrelevant Websites;large scale URL verification pipeline;local search;online advertising domain;parked domain;service providers;third party advertisers","","0","","12","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Lightweight Introduction of EAST-ADL2 in an Automotive Software Product Line","A. Leitner; N. Kajtazovic; R. Mader; C. Kreiner; C. Steger; R. Weiß","Inst. for Tech. Inf., Graz Univ. of Technol., Graz, Austria","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","5526","5535","This paper describes the technical aspects of the transition to a software product line approach in the automotive domain. One major challenge is the current existence of two different emerging standards for this domain, AUTOSAR and EAST-ADL2. These potential standards should be borne in mind during the software product line introduction because they may someday become mandatory. In addition, the existing development process should be changed as little as possible, and one final important requirement for the software product line is the implementation of a single point of control to ensure consistency between various development artifacts. To this end, we propose a lightweight introduction of EAST-ADL2 as a documentation tool only as an initial step. This is achieved by extracting structural information from AUTOSAR models and automatically generating the corresponding EAST-ADL2 representation. The automatic generation ensures consistency between AUTOSAR and EAST-ADL2 models. As an important side effect, variability information can be extracted in this transformation step and used to build an EAST-ADL2 compositional variability model. This model can then be mapped to the central domain model and used to configure the EAST-ADL2 documentation to the other development artifacts consistently. In this way, we can accomplish the lightweight introduction of EAST-ADL2 in the development process through the automatic generation and use the generated variability information for configuration from a single control point.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.414","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149565","","Automotive engineering;Computer architecture;Data mining;Hardware;Sensors;Software;Unified modeling language","automotive engineering;information retrieval;product development;production engineering computing;software engineering","AUTOSAR model;EAST-ADL2 compositional variability model;EAST-ADL2 documentation tool;EAST-ADL2 representation;automatic generation;automotive software product line;structural information extraction;transformation step;variability information extraction","","0","","25","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"End-to-End Policy-Based Encryption and Management of Data in the Cloud","S. Pearson; M. C. Mont; L. Chen; A. Reed","Cloud & Security Lab., HP Labs., Bristol, UK","2011 IEEE Third International Conference on Cloud Computing Technology and Science","20120119","2011","","","764","771","This paper introduces and discusses a data management solution to provide accountability within the cloud as well as addressing privacy issues. The central idea is as follows: Customers allow cloud (service) providers to have access to specific data based on agreed policies and by forcing interactions with interchangeable independent third parties called Trust Authorities. The access to data can be as fine-grained as necessary, based on policy definitions, underlying encryption mechanisms (supporting the stickiness of policies to the data) and a related key management approach that allows (sets of) data attribute(s) to be encrypted specifically based on the policy. Access to data is mediated by a Trust Authority that checks for compliance to policies in order to release decryption keys. By these means users can be provided with fine-grained control over access and usage of their data within the cloud, even in public cloud models.","","Electronic:978-0-7695-4622-3; POD:978-1-4673-0090-2","10.1109/CloudCom.2011.118","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133227","cloud;policy enforcement;privacy;sticky policy","Cloud computing;Encryption;ISO standards;Protocols;Public key","cloud computing;cryptography;database management systems;information retrieval;trusted computing","accountability;cloud data management;cloud provider;data access;data attribute;decryption key;end-to-end policy-based encryption;interchangeable independent third party;key management approach;policy definition;privacy issue;public cloud model;trust authority","","5","5","26","","","Nov. 29 2011-Dec. 1 2011","","IEEE","IEEE Conference Publications"
"Collaborating with executable content across space and time","M. Satyanarayanan; V. Bala; G. St. Clair; E. Linke","Carnegie Mellon University, USA","7th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)","20120202","2011","","","528","537","Executable content is of growing importance in many domains. How does one share and archive such content at Internet-scale for spatial and temporal collaboration? Spatial collaboration refers to the classic concept of user collaboration: two or more users who are at different Internet locations performing a task using shared context. Temporal collaboration refers to the archiving of context by one user and use of that context by another user, possibly many years or decades later. The term “shared context” has typically meant shared documents or a shared workspace such as a whiteboard. However, executable content forces us to think differently. Just specifying a standardized data format is not sufficient; one has to accurately reproduce computation. We observe that the precise encapsulation of computing state provided by a virtual machine (VM) may help us solve this problem. We can cope with large VM size through a streaming mechanism that demand fetches memory and disk state during execution. Based on our positive initial experience with VMs for archiving execution state, we propose the creation of Olive, an Internet ecosystem of curated VM image collections.","","Electronic:978-1-936968-36-7; POD:978-1-4673-0683-6; USB:978-1-936968-32-9","10.4108/icst.collaboratecom.2011.247203","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144776","file systems;open source software;operating systems;virtual machine monitors","Grid computing;Internet;Technological innovation","Internet;groupware;image retrieval;information retrieval systems;storage management;virtual machines","Internet ecosystem;Olive;collaboration across space;collaboration across time;content archiving;curated VM image collection;executable content collaboration;memory fetching;shared context;shared documents;shared workspace;spatial collaboration;streaming mechanism;temporal collaboration;user collaboration;virtual machine","","1","","30","","","15-18 Oct. 2011","","IEEE","IEEE Conference Publications"
"Real-Time Seizure Detection Based on EEG and ECG Fused Features Using Gabor Functions","S. Nasehi; H. Pourghassem","Dept. of Electr. Eng., Islamic Azad Univ., Najafabad, Iran","2011 International Conference on Intelligent Computation and Bio-Medical Instrumentation","20120116","2011","","","204","207","Using the scalp electroencephalogram (EEG) to detect seizure onsets that are not associated with rhythmic EEG activity is challenging. In this paper, we illustrate how supplementing the extracted information from the scalp EEG with the extracted information from electrocardiogram (ECG) can improve the detection of these types of seizures. In this scheme, spectral and spatial features are extracted from EEG and ECG signals by Gabor functions. Then a k-nearest neighbour classifier is used to classify the extracted features from seizure and non-seizure EEG-ECG signals. This algorithm can automatically detect the presence of seizures which can be important advance facilitating timely medical intervention. The performance of algorithm is evaluated on 12 records and recognizes 98.31% expert-labeled seizures with a false detection rate of 11.52%.","","Electronic:978-1-4577-1151-0; POD:978-1-4577-1152-7","10.1109/ICBMI.2011.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6131747","ECG;EEG;Gabor functions;K-nearest neighbour;epilepsy;feature extraction;seizure detection","Classification algorithms;Detectors;Electrocardiography;Electroencephalography;Feature extraction;Pediatrics;Scalp","Gabor filters;electrocardiography;electroencephalography;feature extraction;information retrieval;medical signal detection;medical signal processing;seizure;sensor fusion;signal classification","ECG fused feature;EEG fused feature;Gabor functions;electrocardiogram;expert-labeled seizures;false detection rate;information extraction;k-nearest neighbour classifier;medical intervention;real-time seizure detection;scalp electroencephalogram;spatial feature extraction","","7","","16","","","14-17 Dec. 2011","","IEEE","IEEE Conference Publications"
"Text Mining-Supported Information Extraction: An Extended Methodology for Developing Information Extraction Systems","C. Feilmayr","Inst. of Applic. Oriented Knowledge Process. (FAW), Johannes Kepler Univ., Linz, Austria","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","217","221","Information extraction (IE) and knowledge discovery in databases (KDD) are both useful approaches for discovering information in textual corpora, but they have some deficiencies. Information extraction can identify relevant sub-sequences of text, but is usually unaware of emerging, previously unknown knowledge and regularities in a text and thus cannot form new facts or new hypotheses. Complementary to information extraction, emerging data mining methods and techniques promise to overcome the deficiencies of information extraction. This research work combines the benefits of both approaches by integrating data mining and information extraction methods. The aim is to provide a new high-quality information extraction methodology and, at the same time, to improve the performance of the underlying extraction system. Consequently, the new methodology should shorten the life cycle of information extraction engineering because information predicted in early extraction phases can be used in further extraction steps, and the extraction rules developed require fewer arduous test-and-debug iterations. Effectiveness and applicability are validated by processing online documents from the areas of eHealth and eRecruitment.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.79","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059820","(Web-) Information Extraction;Data Mining;Information Extraction Methodology;Machine Learning Algorithms;Text Mining","Data models;Feature extraction;Learning systems;Machine learning;Semantics;Text mining","data mining;information retrieval;information retrieval systems;text analysis","data mining methods;eHealth;eRecruitment;extraction rules;extraction steps;information extraction engineering;information extraction systems;knowledge discovery-in-databases;online document processing;performance improvement;test-and-debug iterations;text mining-supported information extraction","","3","","16","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Visual data mining to discover knowledge patterns from Web navigational trends","N. Neelima; Syeda Farha Shazmeen","Department of Information Technology, Balaji Institute of Technology & Science, Warangal - 506002, A.P., INDIA","2011 International Conference on Recent Trends in Information Systems","20120209","2011","","","117","120","Discovering web navigational trends and understanding data mining results which is useful to web designers and web-based application builders. It is also desirable to interactively investigate web access data and patterns, Visualizing the usage data in the context of the web site structure is of major importance, as it puts web access requests and their connectivity in perspective. The visualization tool we used to represent the data mining functionalities to generate new patterns. Here we present our visual data mining system, WebViz, which allows interactive investigation of web usage data within their structure context, as well as ad-hoc knowledge, by using Webwiz pattern discovery on web navigational behaviour.","","Electronic:978-1-4577-0792-6; POD:978-1-4577-0790-2","10.1109/ReTIS.2011.6146851","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146851","Visual Data Mining;WebVisualization;knowledge pattern","Data visualization;Knowledge engineering;Layout;Navigation;Visualization;Web mining","Internet;Web design;data mining;data visualisation;information retrieval","Web data access;Web designer;Web navigational trends;Web site structure;Web usage data;Web-based application builder;WebViz;Webwiz pattern discovery;ad-hoc knowledge;knowledge pattern discovery;pattern generation;visual data mining;visualization tool","","0","","21","","","21-23 Dec. 2011","","IEEE","IEEE Conference Publications"
"Knowledge and Social Networks in Yahoo! Answers","A. Rechavi; S. Rafaeli","Sagy Center for Internet Res., Univ. of Haifa, Haifa, Israel","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","781","789","This study defines and explores relations between knowledge-seeking and social relationship networks, using data from a popular Q&A social network site. Our theoretical framework draws on Motivation, Common-goods, and Social capital theories to generate an understanding of the interrelationship of the two types of networks. A dataset consisting of 19 months of activity on Q&A Yahoo! Answers provides the basis for answering the following questions: Does a reciprocal relationship exist between the two networks? Is there a positive correlation in either size or growth between the two? Who are their members and actual participants? How does the social capital created by the social activities influence the asker's satisfaction? Finally, can the growth of one network come at the expense of the other? Findings suggest a positive correlation between the knowledge-seeking network and the social network, Social capital does contribute to askers' satisfaction and furthermore, under certain conditions, one network might grow at the expense of the other. Theoretical and practical implications for the mutual dependency of social ties and informational provision are discussed.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.398","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148989","Q&A site;Social netwrok;Yahoo! Answers","Communities;Correlation;Educational institutions;Internet;Knowledge engineering;Social network services;Thumb","question answering (information retrieval);search engines;social networking (online)","Q&A Yahoo Answers;common-goods;informational provision;knowledge-seeking network;motivation;social activities;social capital theories;social relationship networks;social ties mutual dependency","","0","","48","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Public Health Informatics: Increasing Use and Access","R. A. Strum; A. Fruhling; A. Schumaker","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","2745","2751","The purpose of this research was to conduct an assessment in support of the facilitation of access to public health data and information in Nebraska. This was accomplished by surveying public health stakeholders on their data needs and priorities. Appropriate quantitative and qualitative statistical analyses were used to draw sound conclusions on respondents' data needs and priorities. Most commonly-accessed sources of public health information, most needed health data resources, and most important data sets and corresponding data aggregates were identified and explored for opportunities to improve public health practice in Nebraska.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.492","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149159","access;data;health;informatics;information;public;training","Aggregates;Government;Informatics;Information technology;Public healthcare;Training","health care;information needs;information resources;information retrieval;public administration;public information systems;statistical analysis","Nebraska;USA;data aggregates;data needs;health data resources;information access;public health data access;public health informatics;public health practice;public health stakeholders;qualitative statistical analysis;quantitative statistical analysis","","0","","11","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Modeling and Monitoring of Dynamic Processes","Y. Zhang; T. Chai; Z. Li; C. Yang","State Key Laboratory of Synthesis Automation of Process Industry, Northeastern University, Shenyang, China","IEEE Transactions on Neural Networks and Learning Systems","20120206","2012","23","2","277","284","In this paper, a new online monitoring approach is proposed for handling the dynamic problem in industrial batch processes. Compared to conventional methods, its contributions are as follows: (1) multimodes are separated correctly since the cross-mode correlations are considered and the common information is extracted; (2) the expensive computing load is avoided since only the specific information is calculated when a mode is monitored online; and (3) after that, two different subspaces are separated, and the common and specific subspace models are built and analyzed, respectively. The monitoring is carried out in the subspace. The corresponding confidence regions are constructed according to their respective models.","2162-237X;2162237X","","10.1109/TNNLS.2011.2179669","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118328","Common and specific correlations;industrial processes;multimode process monitoring;subspace separation","Batch production systems;Correlation;Furnaces;Monitoring;Principal component analysis;Systematics;Vectors","batch processing (industrial);computerised monitoring;information retrieval;process monitoring","confidence region;cross-mode correlation;dynamic process monitoring;expensive computing load;industrial batch process;information extraction;online monitoring approach;specific subspace model","","33","","37","","20111229","Feb. 2012","","IEEE","IEEE Journals & Magazines"
"An Experimental Study to Investigate the Use of Additional Classifiers to Improve Information Extraction Accuracy","H. H. Lek; D. C. C. Poo","Dept. of Inf. Syst., Nat. Univ. of Singapore, Singapore, Singapore","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","1","","412","415","In this paper, we present an information extraction system and investigate the use of additional classifiers to help improve information extraction performance. We propose a simple idea of training an additional classifier using the same feature configurations on another corpus and then using this new classifier to classify the original dataset. The classification result of this new classifier is then used as a feature to the original classifier. We tested this approach on the CMU seminar announcements and the Austin job posting datasets and obtained results better than all previously reported systems.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.31","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147007","information extraction;maximum entropy;natural-language processing","Accuracy;Data mining;Feature extraction;Seminars;Support vector machines;Testing;Training","information retrieval;pattern classification","Austin job posting dataset;CMU seminar announcements;additional classifier training;dataset classification;feature configuration;information extraction accuracy;information extraction system","","0","","13","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Combining Semantic and Multilingual Search to Databases with Recommender Systems","L. C. Brodeala; M. J. Martin-Bautista; R. J. Gil","Univ. of Granada, Granada, Spain","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","544","548","IR is a difficult task to do when dealing with free text fields of databases. This paper describes, through Seman-Query, how a search system can be combined with a recommender system in order to make it easier. The search part provides a solution to improve the functionality of database search techniques when dealing with IR from the text fields of databases, while the recommender part is responsible with adding documents that are similar to the results.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059874","Seman Query;multilingualism;recommender system;search system;semantic relations","Cats;Collaboration;Databases;Dictionaries;Educational institutions;Recommender systems;Semantics","database management systems;information retrieval;recommender systems","Seman-Query;database search;databases;multilingual search;recommender systems;search system;semantic search","","0","","12","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Performances and Characteristics of DIGRank, Ranking in the Incomplete Networks","X. Niu; L. Li; X. Xiong; D. Tkach; H. Li; K. Xu","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","1182","1187","Page Rank has been widely used in ranking retrieval results on the web, finding the top influential papers in citation networks or detecting valuable users in online social networks. However, in practice, it is usually hard to obtain a complete structure of any above networks to rank nodes. Thus, some researchers have begun to explore how to get estimated ranks efficiently without acquiring the whole network. They have proposed some approximating methods, however, it is difficult to determine which method is the best one or which is suitable to a certain application. In this case, we set experiments in small-world and scale-free generated networks to certify the feasibility and characteristics of four approximating methods. We also use eleven real networks to mention different optimal conditions for these methods. We find the DIG Rank method performs better than other local estimation methods in almost every given sub graph. Besides, Mean field approach method tends to perform well in networks that have low average shortest path length, small amount of nodes with the same low in degree, or weak community structure. Finally, we apply the most versatile method DIG Rank to Sina micro-blog website to precisely classify users in a group as elites, grassroots or mummy users.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.117","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137335","Online Social Network;PageRank;Scale-free;Small-world","Accuracy;Correlation;Electronic mail;Error analysis;Fans;Neodymium;Social network services","Internet;approximation theory;information retrieval;social networking (online)","DIGRank characteristics;approximating methods;incomplete networks;online social networks;ranking retrieval","","0","1","8","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Preliminary Results for Biomedical Word Sense Disambiguation Based on Semantic Clustering","T. Martin-Wanton; R. Berlanga-Llavori; A. Jimeno-Yepes","Univ. Jaume I, Castellon, Spain","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","460","464","Word sense disambiguation (WSD) is an intermediate task within information retrieval and information extraction, attempting to select the proper sense of ambiguous words. Due to the scarcity of training data, knowledge-based and knowledge-lean methods receive attention as disambiguation methods. Knowledge-based methods compare the context of the ambiguous word to the information available in the terminological resource, but their main purpose is not only word sense disambiguation. Knowledge-lean unsupervised methods rely on terms distribution instead of a resource enumerating the possible senses but might be inappropriate when there is a requirement to commit to a terminological resource. In this work, we rely on a Knowledge Resource (KR) which provides both an inventory of concepts and their lexical information. Our aim is to design scalable unsupervised WSD methods for the semantic annotation of large biomedical corpora. More specifically, we present a clustering-based method that takes profit from the KR information encoded in form of kernels. Prelimanary results are compared to state-of-the-art methods for unsupervised WSD.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059860","clustering;word sense disambiguation","Biomedical measurements;Context;Kernel;Knowledge based systems;Semantics;USA Councils;Unified modeling language","information retrieval;medical computing;pattern clustering;unsupervised learning","biomedical word sense disambiguation;information extraction;information retrieval;knowledge resource;knowledge-based method;knowledge-lean unsupervised methods;semantic clustering","","0","","28","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Towards an Approach for Web Surfing in Unison","A. A. Jalbani; S. Abbasi; G. D. Menghwar; A. Yasmin","Inf. Technol. Centre, Sindh Agric. Univ., Tandojam, Pakistan","2011 Developments in E-systems Engineering","20120213","2011","","","267","270","Internet is a the biggest library of the world containing huge information that is not organized properly. Hence searching for exact information in this big library is a difficult, tiring and time consuming job. This is because World Wide Web (WWW) does not include the user during surfing and purely focuses on documents and links among the documents. Therefore, there is no way to surf together with particular reference to the user for required information. To avoid this difficulty, this paper presents a tool based approach, which provides additional facility to surf the web in a unison way.","","Electronic:978-0-7695-4593-6; POD:978-1-4577-2186-1","10.1109/DeSE.2011.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149991","browsing together;mutual interest sharing on the net;netsurf;ng together;sur&#64257","Collaboration;Educational institutions;Internet;Real time systems;Social network services;Unified modeling language;Web sites","Internet;information retrieval","Internet;World Wide Web;document links;exact information search;library;unison Web surfing","","0","","25","","","6-8 Dec. 2011","","IEEE","IEEE Conference Publications"
"Semi-supervised Discriminant Hashing","S. Kim; S. Choi","Dept. of Comput. Sci., Pohang Univ. of Sci. & Technol., Pohang, South Korea","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","1122","1127","Hashing refers to methods for embedding high dimensional data into a similarity-preserving low-dimensional Hamming space such that similar objects are indexed by binary codes whose Hamming distances are small. Learning hash functions from data has recently been recognized as a promising approach to approximate nearest neighbor search for high dimensional data. Most of 'learning to hash' methods resort to either unsupervised or supervised learning to determine hash functions. Recently semi-supervised learning approach was introduced in hashing where pair wise constraints (must link and cannot-link) using labeled data are leveraged while unlabeled data are used for regularization to avoid over-fitting. In this paper we base our semi-supervised hashing on linear discriminant analysis, where hash functions are learned such that labeled data are used to maximize the separability between binary codes associated with different classes while unlabeled data are used for regularization as well as for balancing condition and pair wise decor relation of bits. The resulting method is referred to as semi-supervised discriminant hashing (SSDH). Numerical experiments on MNIST and CIFAR-10 datasets demonstrate that our method outperforms existing methods, especially in the case of short binary codes.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137325","Hashing;regularized discriminant analysis;semisupervised","Binary codes;Decorrelation;Eigenvalues and eigenfunctions;Hamming distance;Training;Training data;Vectors","approximation theory;binary codes;cryptography;information retrieval;learning (artificial intelligence);statistical analysis","CIFAR-10 dataset;Hamming distances;MNIST dataset;binary codes;hash function learning;high dimensional data;labeled data;linear discriminant analysis;nearest neighbor search approximation;semi-supervised discriminant hashing;similarity search;similarity-preserving low-dimensional Hamming space;unlabeled data","","6","","17","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"AQA: Aspect-based Opinion Question Answering","S. Moghaddam; M. Ester","Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","89","96","With the rapid growth of product review forums, discussion groups, and Blogs, it is almost impossible for a customer to make an informed purchase decision. Different and possibly contradictory opinions written by different reviewers can even make customers more confused. In the last few years, mining customer reviews (opinion mining) has emerged as an interesting new research direction to address this need. One of the interesting problem in opinion mining is Opinion Question Answering (Opinion QA). While traditional QA can only answer factual questions, opinion QA aims to find the authors' sentimental opinions on a specific target. Current opinion QA systems suffers from several weaknesses. The main cause of these weaknesses is that these methods can only answer a question if they find a content similar to the given question in the given documents. As a result, they cannot answer majority questions like ""What is the best digital camera?"" nor comparative questions, e.g. ""Does SamsungY work better than CanonX?"". In this paper we address the problem of opinion question answering to answer opinion questions about products by using reviewers' opinions. Our proposed method, called Aspect-based Opinion Question Answering (AQA), support answering of opinion-based questions while improving the weaknesses of current techniques. AQA contains five phases: question analysis, question expansion, high quality review retrieval, subjective sentence extraction, and answer grouping. AQA adopts an opinion mining technique in the preprocessing phase to identify target aspects and estimate their quality. Target aspects are attributes or components of the target product that have been commented on in the review, e.g. 'zoom' and 'battery life' for a digital camera. We conduct experiments on a real life dataset, Epinions.com, demonstrating the improved effectiveness of the AQA in terms of the accuracy of the retrieved answers.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137365","Answer Retrieval;Opinion Mining;Opinion Question Answering;Opinion Retrieval;Question analysis","Batteries;Blogs;Context;Digital cameras;Educational institutions;Speech;Support vector machines","Web sites;customer satisfaction;data mining;question answering (information retrieval)","Epinions.com;answer grouping;aspect based opinion question answering;blogs;customer review mining;discussion groups;high quality review retrieval;informed purchase decision;opinion mining;product review forums;question analysis;question expansion;subjective sentence extraction","","2","","25","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"A robust characterization of audio signals using the level of information content per Chroma","A. Manzo-Martínez; A. Camarena-Ibarrola","Universidad Michoacana de San Nicol&#x00E1;s de Hidalgo, Divisi&#x00F3;n de estudios de postgrado de la Facultad de Ingenier&#x00ED;a El&#x00E9;ctrica, Morelia, Michoac&#x00E1;n, M&#x00E9;xico","2011 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)","20120213","2011","","","212","217","In this paper we propose a new technique to characterize audio-signals. We use Shannon's Entropy to estimate the level of information content per chroma and we show that involving entropy contributes for a more robust audio characterization. A new audio- fingerprint (AFP) based on this feature is proposed in this paper which we have called Entropy-Chroma Fingerprint (ECFP). Two approaches were considered to estimate entropy; the first assumes the spectral coefficients distribute normally, while the second, estimates its probability density function (PDF) with the Parzen Windows Estimation method. We compared the robustness of the ECFP against the Chromagram-Based Audio-Fingerprint (CBFP) which is determined using the Constant Q Transform (CQT). Three thousand and jive hundred AFPs were determined from songs of several genres. A subset of 350 songs were severely degraded and searched for using excerpts of 5 seconds for that matter. The ECFP determined assuming gaussianity on the PDF turned out to be much more robust than the CBFP. The ECFP determined assuming gaussianity is much faster to process than both, the CBFP and the ECFP determined with Parzen Windows and still more robust.","2162-7843;21627843","Electronic:978-1-4673-0753-6; POD:978-1-4673-0752-9; USB:978-1-4673-0751-2","10.1109/ISSPIT.2011.6151562","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6151562","Audio Fingerprints;Chroma Values;Chromagrams;Constant Q Transform;Entropy","","Fourier transforms;audio signal processing;entropy;fingerprint identification;information retrieval;music;probability;spectral analysis","AFP;CBFP;ECFP;Parzen window estimation method;Shannon entropy;audio signal processing;chromagram-based audiofingerprint;constant Q transform;entropy chroma fingerprint;information content per chroma;probability density function;spectral coefficients","","2","","19","","","14-17 Dec. 2011","","IEEE","IEEE Conference Publications"
"Transaction Dialogs Based on Short Messages Understanding","S. Ye; D. a. Liao; Y. Zhou","Sch. of Inf. Technol., Univ. of Changzhou, Changzhou, China","2011 Fourth International Symposium on Knowledge Acquisition and Modeling","20120123","2011","","","117","120","The paper proposed the crucial technique in transaction service system via Short Messages (SMs), which is a novel kind of dialog based on SMs understanding. After determining the domain of a new dialog using SPN, the service agent SMartS extracts information from SMs that are parsed by SM parser, part-of-speech tagger, name entities recognizer and semantic parser in turn, and then replies the prompt which could direct the procedure of dialog. An induction algorithm is used to discover automatically most domain-specific parser rules from the corpus which contains original SMs and the predefined case frame which depicts detailed semantic groups. Finally, a real domain application was provided.","","Electronic:978-0-7695-4547-9; POD:978-1-4577-1788-8","10.1109/KAM.2011.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137593","dialog;message understanding","Accuracy;Business;Data mining;Satellite broadcasting;Semantics;Syntactics;Training","grammars;information retrieval;interactive systems;software agents;speech processing","SM parser;SPN;domain-specific parser rules;entities recognizer;information extraction;part-of-speech tagger;semantic parser;service agent SMartS;short messages;transaction dialogs;transaction service system","","0","","11","","","8-9 Oct. 2011","","IEEE","IEEE Conference Publications"
"Improving Classifier Performance by Autonomously Collecting Background Knowledge from the Web","S. N. Minton; M. Michelson; K. See; S. Macskassy; B. C. Gazen; L. Getoor","InferLink Corp, El Segundo, CA, USA","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","1","","1","6","Many websites allow users to tag data items to make them easier to find. In this paper we consider the problem of classifying tagged data according to user-specified interests. We present an approach for aggregating background knowledge from the Web to improve the performance of a classier. In previous work, researchers have developed technology for extracting knowledge, in the form of relational tables, from semi-structured websites. In this paper we integrate this extraction technology with generic machine learning algorithms, showing that knowledge extracted from the Web can significantly benefit the learning process. Specifically, the knowledge can lead to better generalizations, reduce the number of samples required for supervised learning, and eliminate the need to retrain the system when the environment changes. We validate the approach with an application that classifies tagged Fickr data.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146932","Background Knowledge;Classifiers;Information Extraction;Ontologies;Web Harvesting","Cities and towns;Data mining;Fires;Knowledge engineering;Monitoring;Portals;Training","Web sites;information retrieval;learning (artificial intelligence);pattern classification","Website;autonomous background knowledge collection;background knowledge aggregation;classifier performance improvement;generic machine learning algorithm;knowledge extraction technology;supervised learning;tagged Flickr data;tagged data classification;user-specified interest","","1","","14","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"STARLET: Multi-document Summarization of Service and Product Reviews with Balanced Rating Distributions","G. Di Fabbrizio; A. Aker; R. Gaizauskas","Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","67","74","Reviews about products and services are abundantly available online. However, selecting information relevant to a potential buyer involves a significant amount of time reading user's reviews and weeding out comments unrelated to the important aspects of the reviewed entity. In this work, we present STARLET, a novel approach to multi-document summarization for evaluative text that considers the rating distribution as summarization feature to consistently preserve the overall opinion distribution expressed in the original reviews. We demonstrate how this method improves traditional summarization techniques and leads to more readable summaries.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.158","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137362","A* search;Summarization;evaluative text;multi-ratings prediction","Adaptation models;Data mining;Feature extraction;Measurement;Predictive models;Redundancy;Training","information retrieval;reviews;text analysis","STARLET;balanced rating distribution;evaluative text;multidocument summarization;opinion distribution;product review;service review;summarization feature;user reviews","","2","","33","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Latent topic estimation based on events in a document","R. Kitajimay; I. Kobayashiz","Grad. Sch. of Humanities & Sci., Ochanomizu Univ., Tokyo, Japan","2011 7th International Conference on Natural Language Processing and Knowledge Engineering","20120126","2011","","","289","295","Recently, several latent topic model-based methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, and therefore the relationship between words in a document is not considered. Considering this, we propose a latent topic extraction method which assigns topics to events that represent the relation between words in a document. There are several ways to express events, and the accuracy of estimating latent topics differs depending on the definition of an event. Therefore, we propose several event types and examine which event type works well to estimate latent topics in a document with a common document retrieval task. Furthermore, as an application of our proposed method, we also show a multi-document summarization based on latent topics.","","Electronic:978-1-61284-729-0; POD:978-1-4799-1689-4","10.1109/NLPKE.2011.6138211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138211","","Accuracy","document handling;information retrieval","document retrieval task;event definition;latent topic estimation;latent topic extraction method;multidocument summarization;text analysis","","0","","26","","","27-29 Nov. 2011","","IEEE","IEEE Conference Publications"
"A comparative study of Information Extraction tools used for Biological database","N. Kanya; T. Ravi; S. Geetha","Manonmanium Sundaranar University, Chennai, India","International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011)","20120202","2011","","","886","890","The Internet presents a huge amount of biological data. It is difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible Information Extraction (IE) systems and tool that transform the Web pages into program-friendly structures such as a relational database. This paper made a study on information extraction tools. Which can be used for Biological databases. The tools have been classified based on four categories such as tools for Manually constructed Information Extraction, Supervised Wrapper Induction System, Semi supervised Information Extraction Systems, Unsupervised Information extraction Systems. Finally we made a comparative study on the Information Extraction tools used for Biological database based on the technique used such as scan Pass, Extraction Rules Type, Features used, Learning Algorithm and Tokenization Schemes.","","Electronic:978-9-38043-000-3","10.1049/cp.2011.0492","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143441","Biological database;Information Extraction;Learning Algorithm;Tokenization Schemes","","Internet;Web sites;biology computing;information retrieval;learning (artificial intelligence);relational databases","IE systems;Internet;Web pages;biological database;extraction rules type;information extraction tools;learning algorithm;manually constructed information extraction;program-friendly structures;relational database;relevant data extraction;scan pass;semi supervised information extraction systems;supervised wrapper induction system;tokenization schemes;unsupervised information extraction systems","","0","","","","","20-22 July 2011","","IET","IET Conference Publications"
"Combining Visual and Acoustic Features for Music Genre Classification","M. J. Wu; Z. S. Chen; J. S. R. Jang; J. M. Ren; Y. H. Li; C. H. Lu","Dept. of Comput. Sci., Nat. Tsing Hua Univ., Hsinchu, Taiwan","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","2","","124","129","Music genre classification is a challenging task in the field of music information retrieval. Existing approaches usually attempt to extract features only from acoustic aspect. However, spectrogram also provides useful information because it describes the temporal change of energy distribution over frequency bins. In this paper, we propose the use of Gabor filters to generate effective visual features that can capture the characteristics of a spectrogram's texture patterns. On the other hand, acoustic features are extracted using universal background model and maximum a posteriori adaptation. Based on these two types of features, we then employ SVM to perform the final classification task. Experimental results demonstrate that combining visual and acoustic features can achieve satisfactory classification accuracy on two widely used datasets.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147660","Gabor filters;Gaussian super vectors;genre classification","Feature extraction;Gabor filters;Music;Spectrogram;Vectors;Visualization","Gabor filters;acoustic signal processing;feature extraction;information retrieval;maximum likelihood estimation;music;pattern classification;support vector machines","Gabor filters;SVM;acoustic features;energy distribution;feature extraction;frequency bins;maximum a posteriori adaptation;music genre classification;music information retrieval;spectrogra;universal background model;visual features","","0","","22","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Neural network based supervised rank aggregation","R. Ali; I. Naim","Department of Computer Engineering, A.M.U., Aligarh, U.P., India","2011 International Conference on Multimedia, Signal Processing and Communication Technologies","20120213","2011","","","72","75","Rank Aggregation problem is to find a combined ordering for objects, given a set of rankings obtained from different rankers. Rank aggregation is a technique that combines results of various rankings on the sets of entities (e.g. Documents or web pages of search result) to generate an overall ranking of the entities. In the context of the World Wide Web, Rank aggregation is frequently used in metasearching. In this paper, we discuss the development of a supervised rank aggregation system that is based on “neural networks”. A supervised rank aggregation system provides an aggregation of rankings of entities by learning rules for combining the different individual rankings of the entities on the basis of training data. In case of a metasearch system, the training data may be the user feedback based ranking of the search results. The main contribution of the paper is the formulation of the rank aggregation problem as a function approximation problem. As the multilayer perceptrons are considered as the universal approximator, we use a multilayer perceptron for the supervised rank aggregation. For experimental purpose, we apply this supervised rank aggregation technique to metasearching. We train our metasearch system with the user feedback based ranking of the search results from 7 public search engines for a set of 15 queries. We also evaluate the performance of our trained metasearch system using the feedback from three independent evaluators and find that our system gives a very good performance.","","Electronic:978-1-4577-1107-7; POD:978-1-4577-1105-3","10.1109/MSPCT.2011.6150439","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6150439","metasearching;neural network;rank aggregation;supervised learning","Artificial neural networks;Correlation;Function approximation;Search engines;Training;Training data;Web sites","Internet;information retrieval;meta data;multilayer perceptrons;search engines","World Wide Web;learning rules;metasearching;multilayer perceptrons;neural network;public search engines;supervised rank aggregation;training data;user feedback based ranking;web pages","","0","","16","","","17-19 Dec. 2011","","IEEE","IEEE Conference Publications"
"Scalable Diversified Ranking on Large Graphs","R. H. Li; J. X. Yu","Chinese Univ. of Hong Kong, Hong Kong, China","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","1152","1157","Enhancing diversity in ranking on graphs has been identified as an important retrieval and mining task. Nevertheless, many existing diversified ranking algorithms cannot be scalable to large graphs as they have high time or space complexity. In this paper, we propose a scalable algorithm to find the top-K diversified ranking list on graphs. The key idea of our algorithm is that we first compute the Pagerank of the nodes of the graph, and then perform a carefully designed vertex selection algorithm to find the top-K diversified ranking list. Specifically, we firstly present a new diversified ranking measure, which can capture both relevance and diversity. Secondly, we prove the submodularity of the proposed measure. And then we propose an efficient greedy algorithm with linear time and space complexity with respect to the size of the graph to achieve near-optimal diversified ranking. Finally, we evaluate the proposed method through extensive experiments on four real networks. The experimental results indicate that the proposed method outperforms existing diversified ranking algorithms both on improving diversity in ranking and the efficiency of the algorithms.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.126","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137330","","Algorithm design and analysis;Collaboration;Complexity theory;Diversity reception;Greedy algorithms;Measurement;Social network services","computational complexity;data mining;graph theory;information retrieval;social networking (online)","Pagerank;diversified ranking algorithms;diversity enhancement;large graphs;linear space complexity;linear time complexity;mining task;retrieval task;scalable diversified ranking;social network analysis;top-K diversified ranking list;vertex selection algorithm","","7","","15","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Work Item Tagging: Communicating Concerns in Collaborative Software Development","C. Treude; M. A. Storey","University of Victoria, Victoria","IEEE Transactions on Software Engineering","20120130","2012","38","1","19","34","In collaborative software development projects, work items are used as a mechanism to coordinate tasks and track shared development work. In this paper, we explore how “tagging,” a lightweight social computing mechanism, is used to communicate matters of concern in the management of development tasks. We present the results from two empirical studies over 36 and 12 months, respectively, on how tagging has been adopted and what role it plays in the development processes of several professional development projects with more than 1,000 developers in total. Our research shows that the tagging mechanism was eagerly adopted by the teams, and that it has become a significant part of many informal processes. Different kinds of tags are used by various stakeholders to categorize and organize work items. The tags are used to support finding of tasks, articulation work, and information exchange. Implicit and explicit mechanisms have evolved to manage the tag vocabulary. Our findings indicate that lightweight informal tool support, prevalent in the social computing domain, may play an important role in improving team-based software development practices.","0098-5589;00985589","","10.1109/TSE.2010.91","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5611552","Tagging;articulation work;collaboration;software development;task management;work items.","Collaboration;Data mining;Programming;Software engineering;Tagging","groupware;information retrieval;software development management","collaborative software development project;lightweight informal tool support;social computing mechanism;tag vocabulary;team-based software development practice;work item tagging","","13","","44","","20101028","Jan.-Feb. 2012","","IEEE","IEEE Journals & Magazines"
"Understanding Plagiarism Linguistic Patterns, Textual Features, and Detection Methods","S. M. Alzahrani; N. Salim; A. Abraham","Faculty of Computer Science and Information Systems, Taif University, Alhawiah, Saudi Arabia","IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)","20120216","2012","42","2","133","149","Plagiarism can be of many different natures, ranging from copying texts to adopting ideas, without giving credit to its originator. This paper presents a new taxonomy of plagiarism that highlights differences between literal plagiarism and intelligent plagiarism, from the plagiarist's behavioral point of view. The taxonomy supports deep understanding of different linguistic patterns in committing plagiarism, for example, changing texts into semantically equivalent but with different words and organization, shortening texts with concept generalization and specification, and adopting ideas and important contributions of others. Different textual features that characterize different plagiarism types are discussed. Systematic frameworks and methods of monolingual, extrinsic, intrinsic, and cross-lingual plagiarism detection are surveyed and correlated with plagiarism types, which are listed in the taxonomy. We conduct extensive study of state-of-the-art techniques for plagiarism detection, including character n-gram-based (CNG), vector-based (VEC), syntax-based (SYN), semantic-based (SEM), fuzzy-based (FUZZY), structural-based (STRUC), stylometric-based (STYLE), and cross-lingual techniques (CROSS). Our study corroborates that existing systems for plagiarism detection focus on copying text but fail to detect intelligent plagiarism when ideas are presented in different words.","1094-6977;10946977","","10.1109/TSMCC.2011.2134847","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5766764","Linguistic patterns;plagiarism;plagiarism detection;taxonomy;textual features","Feature extraction;Humans;Natural languages;Plagiarism;Pragmatics;Taxonomy;Writing","computational linguistics;fuzzy set theory;information retrieval;text analysis","CNG;CROSS;FUZZY;SEM;STRUC;STYLE;SYN;VEC;character n-gram based technique;cross-lingual plagiarism detection;extrinsic plagiarism detection;fuzzy based technique;intelligent plagiarism;intrinsic plagiarism detection;literal plagiarism;monolingual plagiarism detection;plagiarism linguistic pattern;semantic based technique;structural based technique;stylometric based technique;syntax based technique;vector based technique","","34","1","142","","20110512","March 2012","","IEEE","IEEE Journals & Magazines"
"Multidiscipline approach to key performance indicator measurements","L. Tovarek","Oakland University, USA","2011 19thTelecommunications Forum (TELFOR) Proceedings of Papers","20120202","2011","","","150","153","Mobile Operators offer usually also Internet Services but because of regulations in some countries they may get their redundant Internet Access from authorized Internet service Providers (ISP) only. For providing good level of service for their customers, the operators sign Service Level Agreements (SLA) with ISP's with Key Performance Indicators (KPI) defined. The paper deals with multidiscipline approach to the KPI measurements respecting technical, psychological and social aspects, pro-activity, reporting, visualizing, and alerting results. Such KPI measuring is useful also for ISP's.","","Electronic:978-1-4577-1498-6; POD:978-1-4577-1499-3","10.1109/TELFOR.2011.6143514","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143514","ISP;KPI;Mobile Operator;SLA;Telecom Operator;multidiscipline","Browsers;Electronic mail;Human factors;Humans;Psychology;Telecommunications;Visualization","Internet;information retrieval;mobile computing;psychology;service-oriented architecture","ISP;Internet service provider;KPI measurement;indicator measurement;key performance indicator;mobile operator;psychological aspect;redundant Internet access;service level agreement;social aspect;technical aspect","","0","","12","","","22-24 Nov. 2011","","IEEE","IEEE Conference Publications"
"The Missing Link: Intention to Produce Online Content Accessible to People with Disabilities by Non-professionals","K. Nahon; I. Benbasat; C. Grange","Inf. Sch., Univ. of Washington, Seattle, WA, USA","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1747","1757","The aim of this paper is to provide a theoretical framework to analyze obstacles, challenges, and incentives which lead non-professional developers to design websites and produce information that are accessible to people with disabilities, and to describe the development of a reliable and validated instrument designed to test it. Results show that intrinsic motivation, self-efficacy, influence of one's close community and the perception that others are responsible to design such websites influence one's behavior in creating accessible content for people with disabilities.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.578","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149097","accessibility;accessible content;digital divides;disability;end-user developers;non-professionals","Communities;Context;Educational institutions;Instruments;Internet;Law;Reliability","Web design;handicapped aids;information retrieval","Web sites design;accessible content;disabled people;intrinsic motivation;missing link;nonprofessional developer;online content;self-efficacy","","1","","39","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Conducting successful studies in Materials Science and Engineering with consideration to a multiplicity of student cohorts and delivery modes","P. Keleher; A. Patil; K. Duan","CQUniversity, Australia","2011 Frontiers in Education Conference (FIE)","20120202","2011","","","F1D-1","F1D-6","Second year students undertaking their studies in Materials Science and Engineering within the Bachelor of Engineering at CQUniversity, Australia, are a diverse group requiring a variety of approaches to enable them to undertake their studies. The university, whilst having a strong regional focussed approach, conducts its Engineering degree offerings over three campuses, at Mackay, Rockhampton and Gladstone, in Queensland, Australia and by flexible mode throughout Australia and internationally. Lecturers are located on each of the campuses and it is their role to oversee the cohort of on-campus students on that particular campus. On-campus, full-time students are traditionally secondary students who have continued with their studies into tertiary education or mature learners who have been in the workforce for some time and return to study to re-skill, up-skill or update their knowledge and skill base. While students who study by a part-time, external (ie. flexible or distance) study mode are practitioners who have a trade qualification or are university graduates wishing to re-skill, up-skill or update their knowledge and skill base and continue working throughout their study. All students have access to course materials, video-streams of lectures, student-student and student-lecturer communication channels via a dedicated course Moodle™ website.","0190-5848;01905848","Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1","10.1109/FIE.2011.6142747","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142747","distance education;lecturer-centric;materials science;residential schools","Australia;Cities and towns;Conferences;Educational institutions;Laboratories;Materials","Web sites;computer aided instruction;distance learning;educational institutions;further education;information retrieval;materials science computing;video streaming","CQUniversity;bachelor of engineering;course Moodle Web site;course material;delivery mode;engineering degree;material engineering;material science;mature learner;on-campus student;regional focussed approach;student cohort;student-lecturer communication channel;tertiary education;video stream","","0","","12","","","12-15 Oct. 2011","","IEEE","IEEE Conference Publications"
"Detecting General Opinions from Customer Surveys","E. A. Stepanov; G. Riccardi","Dept. of Inf. Eng. & Comput. Sci., Univ. of Trento, Trento, Italy","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","115","122","Questionnaire-based surveys and on-line product reviews resemble each other in that they both have user comments and satisfaction ratings. Since a comment might be a general opinion about a product or only one or a set of its attributes, in which case the text might not reflect the rating, surveys and reviews share the problem of pairing free-text comments with these ratings. To train accurate models for automatic evaluation of products from free-text, it is important to distinguish these two kinds of opinions. In this paper we present experiments on detecting general opinions that target a product as a whole, thus, reflect the user sentiments better. The task is different from subjectivity detection, since the goal is to detect generality of an opinion regardless of the rest of the documents being opinionated or not. The task complements feature-based opinion analysis and opinion polarity classification, since it can be applied as a preceding step to both tasks. We show that when used as a classification feature user ratings are not useful in the general opinion detection task. However, they are effective in predicting the polarity of a comment once it is identified as a general opinion.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137369","Classification;Opinion mining;Sentiment analysis","Accuracy;Data mining;Data models;Feature extraction;Motion pictures;Support vector machines;Training","feature extraction;information retrieval;pattern classification;reviews;text analysis","customer survey;feature-based opinion analysis;free-text comments;general opinion detection;generality detection;online product review;opinion polarity classification;product evaluation;questionnaire-based survey;satisfaction rating;user comments;user sentiment","","2","","18","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"SemanticQA: Exploiting semantic associations for cross-document question answering","S. Tartir; I. B. Arpinar; B. McKnight","Faculty of Information Technology Philadelphia University Amman, 19392 Jordan","International Symposium on Innovations in Information and Communications Technology","20120209","2011","","","1","6","As more data is being semantically annotated, it is getting more common that researchers in multiple disciplines rely on semantic repositories that contain large amounts of data in the form of ontologies as a compact source of information. One of the main issues currently facing these researchers is the lack of easy-to-use interfaces for data retrieval, due to the need to use special query languages or applications. In addition, the knowledge in these repositories might not be comprehensive or up-to-date due to several reasons, such as the discovery of new knowledge in the field after the repositories was created. In this paper, we introduce an enhanced version of our SemanticQA system that allows users to query semantic data repositories using natural language questions. If a user question cannot be answered solely from the ontology, SemanticQA detects the failing parts and attempts to answer these parts from web documents and plugs in the answers to answer the whole questions, which might involve a repetition of the same process if other parts fail.","","Electronic:978-1-61284-675-0; POD:978-1-61284-672-9","10.1109/ISIICT.2011.6149593","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149593","","Educational institutions;Engines;Natural language processing;Ontologies;Search engines;Semantics;Web search","document handling;ontologies (artificial intelligence);query languages;question answering (information retrieval);semantic Web","SemanticQA system;Web documents;cross document question answering;ontologies;query languages;semantic annotatation;semantic associations","","0","","18","","","Nov. 29 2011-Dec. 1 2011","","IEEE","IEEE Conference Publications"
"FAARM: Frequent Association Action Rules Mining Using FP-Tree","D. E. Difallah; R. G. Benton; V. Raghavan; T. Johnsten","eXascale Infolab, Univ. of Fribourg, Fribourg, Switzerland","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","398","404","Action rules mining aims to provide recommendations to analysts seeking to achieve a specific change. An action rule is constructed as a series of changes, or actions, which can be made to some of the flexible characteristics of the information system that ultimately triggers a change in the targeted attribute. The existing action rules discovery methods consider the input decision system as their search domain and are limited to expensive and ambiguous strategies. In this paper, we define and propose the notion of action table as the ideal search domain for actions, and then propose a strategy based on the FP-Tree structure to achieve high performance in rules extraction.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.82","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137407","FP-Tree;action rules;action table;association mining;recommendation","Association rules;Atomic clocks;Atomic measurements;Classification algorithms;Information systems;Machine learning algorithms","data mining;information retrieval;information systems;recommender systems;tree data structures","FP-tree structure;frequent association action rules mining;information system;recommendations;rules extraction;search domain","","1","","14","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Organize metadata servers by using quorum system","T. J. Liu; Wei-Chan Wang; Chuan-Mu Tseng","Department of Information Engineering and Computer Science, Feng Chia University, Taichung, Taiwan, R.O.C.","2011 IEEE/SICE International Symposium on System Integration (SII)","20120209","2011","","","1125","1130","It is very important to distribute requests of finding files and to avoid the crash of a single metadata server, so we use a few metadata servers to build a distributed file system. However, this method may cause the inconsistency among metadata and the difficulty of searching metadata. Therefore, we use the quorum system to organize the metadata servers into a distributed file system, and the loading of the file system will be shared on metadata servers as average as possible. Besides, the quorum system not only makes sure of the consistency among metadata but also makes metadata searched easily.","","Electronic:978-1-4577-1524-2; POD:978-1-4577-1523-5","10.1109/SII.2011.6147607","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147607","","Distributed databases;File servers;File systems;Loading;Peer to peer computing;Search problems;Servers","file organisation;file servers;information retrieval;meta data;network operating systems;peer-to-peer computing","distributed file system;file organisation;file sharing;metadata searching;metadata servers;quorum system","","1","","19","","","20-22 Dec. 2011","","IEEE","IEEE Conference Publications"
"ADANA: Active Name Disambiguation","X. Wang; J. Tang; H. Cheng; P. S. Yu","Dept. of Comput. Sci. & Technol., Tsinghua Univ., Beijing, China","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","794","803","Name ambiguity has long been viewed as a challenging problem in many applications, such as scientific literature management, people search, and social network analysis. When we search a person name in these systems, many documents (e.g., papers, web pages) containing that person's name may be returned. It is hard to determine which documents are about the person we care about. Although much research has been conducted, the problem remains largely unsolved, especially with the rapid growth of the people information available on the Web. In this paper, we try to study this problem from a new perspective and propose an ADANA method for disambiguating person names via active user interactions. In ADANA, we first introduce a pairwise factor graph (PFG) model for person name disambiguation. The model is flexible and can be easily extended by incorporating various features. Based on the PFG model, we propose an active name disambiguation algorithm, aiming to improve the disambiguation performance by maximizing the utility of the user's correction. Experimental results on three different genres of data sets show that with only a few user corrections, the error rate of name disambiguation can be reduced to 3.1%. A real system has been developed based on the proposed method and is available online.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137284","Active Learning;Digital Library;Name Disambiguation;Social Network Analysis","Accuracy;Approximation algorithms;Clustering algorithms;Correlation;Educational institutions;Inference algorithms;Web pages","Internet;graph theory;information retrieval;user interfaces","ADANA method;PFG model;active name disambiguation algorithm;active person name disambiguation;active user interactions;name ambiguity;pairwise factor graph model;user correction;utility maximizing","","11","","29","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Extending Attribute Information for Small Data Set Classification","D. C. Li; C. w. Liu","National Cheng Kung University, Tainan","IEEE Transactions on Knowledge and Data Engineering","20120126","2012","24","3","452","464","Data quantity is the main issue in the small data set problem, because usually insufficient data will not lead to a robust classification performance. How to extract more effective information from a small data set is thus of considerable interest. This paper proposes a new attribute construction approach which converts the original data attributes into a higher dimensional feature space to extract more attribute information by a similarity-based algorithm using the classification-oriented fuzzy membership function. Seven data sets with different attribute sizes are employed to examine the performance of the proposed method. The results show that the proposed method has a superior classification performance when compared to principal component analysis (PCA), kernel principal component analysis (KPCA), and kernel independent component analysis (KICA) with a Gaussian kernel in the support vector machine (SVM) classifier.","1041-4347;10414347","","10.1109/TKDE.2010.254","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677515","Classification;feature construction;small data set;support vector machine.","Accuracy;Artificial neural networks;Classification algorithms;Feature extraction;Kernel;Principal component analysis;Support vector machines","Gaussian processes;data handling;fuzzy set theory;information retrieval;pattern classification;principal component analysis;support vector machines","Gaussian kernel;attribute construction approach;attribute information;attribute information extraction;classification-oriented fuzzy membership function;data attributes;data quantity;feature space;kernel independent component analysis;kernel principal component analysis;principal component analysis;similarity-based algorithm;small data set classification;support vector machine classifier","","15","","50","","20101230","March 2012","","IEEE","IEEE Journals & Magazines"
"An Individual's Problem Space and Web Information Searching: A Proposed Study on Mental Organization of Keyword Importance and Efficiency in Everyday Web Information Searching","J. H. A. Chen","Commun. & Inf. Sci., Univ. of Hawai'i at Manoa, Honolulu, HI, USA","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","4583","4592","An individual's problem space has been identified as important in problem solving. A problem space is a person's inner representation of the task after extracting critical components in the external problem task. This paper proposes a study to probe whether there are different problem spaces for efficient and inefficient Web information searchers. The questions will be answered quantitatively using the psychometric scaling method TRICIR (for circular triads) and ANOVA.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.4","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149449","Austronesian;Google;Taiwanese aboriginal;Web searching;keyword;mental organization;problem space","Humans;Organizations;Problem-solving;Search engines;Search problems;Web search","Internet;information retrieval","ANOVA;TRICIR;Web information searching;external problem task;individual problem space;keyword importance;mental organization;person inner representation;problem solving;psychometric scaling method","","0","","36","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Better Organizing Your Contacts: An Empirical Study of an Intelligent Social Contact Management System","D. Yang; B. Guo; D. Zhang","Inst. TELECOM SudParis, Evry, France","2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing","20120202","2011","","","283","290","Human memory is generally poor and often fails in unpredictable ways, sometimes with dire consequences. On social occasions, it usually causes embarrassing situations (e.g., forgetting the name of a friend). Moreover, as the number of contacts increases, people feel difficult to maintain their social contacts with merely memory. Aiming at helping people better manage their social contacts, a powerful social contact management tool named SCM is introduced. It supports the auto-collection of rich contact data and a simple but efficient contact retrieval interface. First, an online survey is carried out with a series of questions about contact management. Based on the survey results, the SCM system is developed. Furthermore, to evaluate the usability and effectiveness of SCM, a user study of contact management is performed which proved SCM is very helpful for contact re-finding. On the other hand, several particular phenomena about social contact management and recall are discovered. The human contact-memorizing pattern is also concluded based on the result of this user study.","","Electronic:978-0-7695-4580-6; POD:978-1-4577-1976-9","10.1109/iThings/CPSCom.2011.87","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142272","Empirical Study;Human Computer Interaction;Human memory Aid;Pervasive Computing;Social Contact Management","Business;Communities;Context;Data mining;Educational institutions;Humans;Meteorology","information retrieval;social sciences computing;user interfaces","contact retrieval interface;human contact memorizing pattern;human memory;intelligent social contact management system;online survey;rich contact data;social occasions","","0","","15","","","19-22 Oct. 2011","","IEEE","IEEE Conference Publications"
"Supervised Lazy Random Walk for Topic-Focused Multi-document Summarization","P. Du; J. Guo; X. Cheng","Inst. of Comput. Technol., Beijing, China","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","1026","1031","Topic-focused multi-document summarization aims to produce a summary given a specific topic description and a set of related documents. It has become a crucial text processing task in many real applications that can help users consume the massive information. This paper presents a novel extractive approach based on supervised lazy random walk (Super Lazy). This approach naturally combines the rich features of sentences with the intrinsic sentence graph structure in a principled way, and thus enjoys the advantages of both the existing supervised and unsupervised approaches. Moreover, our approach can achieve the three major goals of topic-focused multi-document summarization (i.e. relevance, salience and diversity) simultaneously with a unified ranking process. Experiments on the benchmark dataset TAC2008 and TAC2009 are performed and the ROUGE evaluation results demonstrate that our approach can significantly outperform both the state-of-the-art supervised and unsupervised methods.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137309","diversity;relevance;salience;supervised lazy random walk;topic-focused multi-document summarization","Feature extraction;Humans;Lead;Measurement;Support vector machines;Training;Vectors","information retrieval;learning (artificial intelligence);random processes;text analysis","ROUGE evaluation;SuperLazy;benchmark dataset TAC2008;benchmark dataset TAC2009;intrinsic sentence graph structure;supervised lazy random walk;text processing task;topic focused multidocument summarization;unified ranking process","","2","","27","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Exploring Factors Impacting Users' Attitude and Intention towards Social Tagging Systems","H. Allam; J. Blustein; M. Bliemel; L. Spiteri","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","3129","3138","While recent progress has been made in understanding the structure and dynamics of social tagging systems, we know little about the users' underlying motivations for tagging, and how these motivations influence the resulting use of tagging systems. In this article, we propose and empirically validate a conceptual model of key factors that affect users' attitude and intention to use social tagging systems. Our findings highlight three new factors and confirm two previous factors. In addition to Perceived Enjoyment and Perceived Ease-of-use, we introduce Content Generation, Information Retrievability, and Information Re-findability as new dimensions affecting the use of social tagging systems. Our goal is to help researchers, designers, and managers of tagging systems and other social systems on the Web understand how to motivate users to increase their use and hence harvest the collaborative and sharing benefits associated with these tools.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.267","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149204","Collaborative tagging;Social tagging;social bookmarking;technology acceptance model","Atmospheric measurements;Collaboration;Educational institutions;Media;Particle measurements;Tagging","Internet;behavioural sciences computing;information retrieval","World Wide Web;conceptual model;content generation;information re-findability;information retrievability;perceived ease-of-use;perceived enjoyment;social tagging system;user attitude;user intention","","1","","47","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Consumer Perspectives on Quality Attributes in Evaluating Health Websites","D. Tao; C. M. LeRouge; G. Deckard; G. De Leo","St. Louis Univ., St. Louis, MO, USA","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","2675","2684","Healthcare consumers are increasingly turning to the Internet for health information. Health website sponsors and developers are challenged to ensure high quality to satisfy the spectrum of site visitors. Yet, research does not seem to provide needed guidance from the perspective of the healthcare consumer. In response, this study provides a taxonomy of website quality attributes and explores differences in ratings of the importance of 15 quality attributes of health websites from healthcare consumers. Both quantitative and qualitative methods were deployed to collect the data.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.180","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149339","Health websites;Healthcare consumers;Website design;Website evaluation;Website quality","Accuracy;Business;Educational institutions;Internet;Medical services;Visualization","Internet;Web sites;health care;information retrieval","Internet;consumer perspective;health information;healthcare Web site quality attribute;healthcare consumer;qualitative method;quantitative method","","4","","28","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Game-changing interoperability for healthcare: Bringing semantically harmonized clinical information into provider workflows from disparate health information technologies","A. Halevy","Co-Founder, dbMotion, Pittsburgh, USA","2011 8th International Conference & Expo on Emerging Technologies for a Smarter World","20120123","2011","","","1","3","Healthcare suffers from no shortage of data - laboratory results, imaging studies, clinical history, procedures, treatment summaries, etc. But while volume is not an issue, value is. Clinicians face significant obstacles in identifying what information is valuable, what part of it is available, how to access it and how to make it useful rather than simply viewable. In the midst of today's debate of about the future of healthcare, one certainty emerges: The industry must ensure comprehensive, up-to-date patient information is available to the current provider at the immediate point of care. Physicians in the ED or inpatient setting need access to medical information generated in outpatient and primary care facilities. Community providers must be able to view and use relevant information from a patient's hospital stay. Unfortunately, data sets are locked into health information technology (HIT) silos - generated and stored in disparate systems that don't communicate, or are incapable of synthesizing data to make it meaningful and usable. Assaf Halevy will discuss innovative Health Information Exchange (HIE) approaches and tools that deliver valuable information from virtually any HIT system directly into a treating provider's workflow, while semantically harmonizing the data, its structure, content and nomenclature, making it actionable, simple and clear: · The application operates in the background, constantly indentifying the user, the application s/he is running and the patient. On the caregiver's behalf, the application automatically queries the solution's inherent HIE workspace for data the clinician lacks. If relevant information is found, a visual indication appears and the clinician can, in one click from within the current workflow, see an “exception view.” If valuable and required, the clinician can import data into the EHR and incorporate it into clinical decision making. · Significantly, the information is semanticall- harmonized. The solution collects the patient's current clinical picture, and packages it in the preferred nomenclature and structure of the consumer - regardless of originating system, vocabulary or format. Developers and beta users have identified a variety of benefits including: ·Physicians and nurses need not leave the clinical system and workflow they are in to view and use important patient information. The convenient, nonintrusive nature of the functionality saves time, and ensures a complete picture of the patient is considered during diagnosis, care planning and treatment. · Providers are not overwhelmed with data that may or may not be relevant to the current clinical episode because the application focuses on “delta data,” and enables users to access a high-level view from their workflow to determine what is important. · Unobtrusive, embedded GUI enables to clinicians to drill down for detailed, full-screen information as desired. ·Selective, semantic export functionality allows providers to import meaningful, actionable information to their EHR system or other vehicles including wireless devices. ·The solution can offer access to the full clinical picture throughout the continuum of care, over wireless network or internet, and across the boundaries of an organization.","","Electronic:978-1-4577-1591-4; POD:978-1-4577-1592-1","10.1109/CEWIT.2011.6135863","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6135863","","History;Hospitals;Information technology;Medical diagnostic imaging;Semantics","Internet;electronic data interchange;graphical user interfaces;health care;information retrieval;medical information systems;mobile computing;open systems;workflow management software","EHR system;Internet;caregiver;clinical decision making;clinical history;community providers;data structure;disparate health information technology;full-screen information;game-changing interoperability;health information exchange;healthcare;imaging study;laboratory results;medical information access;nurses;outpatient facility;pPhysicians;patient care planning;patient current clinical picture;patient diagnosis;patient hospital stay;patient treatment;primary care facility;provider workflow;selective semantic export functionality;semantically harmonized clinical information;treatment summary;unobtrusive embedded GUI;up-to-date patient information;valuable information identification;wireless device;wireless network","","0","","","","","2-3 Nov. 2011","","IEEE","IEEE Conference Publications"
"Expected answer type construction using analogical reasoning in a question answering task","H. Toba; M. Adriani; R. Manurung","Information Retrieval Laboratory, Faculty of Computer Science, Universitas Indonesia, Indonesia","2011 International Conference on Advanced Computer Science and Information Systems","20120126","2011","","","283","290","In a question answering system (QAS), question analysis component has an important task to determine the expected answer type (EAT) of a given question. Many QAS's rely their question analysis performance on manually developed patterns, such as in Open Ephyra (OE), one of a state of the art freely available QAS. Recently, there are a number of studies which investigated the influence of statistical relational framework to learn question-answer pairs in particular component of a QAS. In this study, we propose an approach that utilizes the intensity of statistical learning of question-answer pairs as a means to develop EAT patterns. In a question analysis experiment setting by using factoid testing questions from QA@CLEF 2008, our result outperforms the accuracy of manually constructed patterns of OE, with 84.17% against 81.67%.","","Electronic:978-979-1421-11-9; POD:978-1-4577-1688-1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140798","","Accuracy;Feature extraction;Gold;Organizations;Testing;Training;Vectors","case-based reasoning;learning (artificial intelligence);question answering (information retrieval);statistical analysis","Open Ephyra;analogical reasoning;expected answer type construction;factoid testing questions;question analysis component;question analysis performance;question answering system;statistical learning;statistical relational framework","","0","","12","","","17-18 Dec. 2011","","IEEE","IEEE Conference Publications"
"The online testing system with explain function as students answer mistake","Wang Xiuying; Zhou Fang","The College of Information Science and Technology, Qingdao University of Science and Technology, 266000, China","2011 IEEE International Symposium on IT in Medicine and Education","20120116","2011","2","","456","460","Aiming at current deficiencies of online testing system, students only know the making wrong, but also doesn't know the whys. In this paper, the online testing system is developed based on ASP.NET and AJAX technologies, which consists of 6 modules such as chapter test, comprehensive test, score query, management of test questions, etc. The online testing system is embedded into VB excellent curriculum website which has already been developed. Through more than one year application, it is demonstrated that system operation is stable, and that the goal that students self-exam the learned chapters, knowledge points, focuses and difficult points is targeted. In particular, the test questions analysis in the management module can promptly provide causes for students' the wrong answers. Therefore, students not only know the hows but also know the whys, which reaps students' affirmation.","","Electronic:978-1-61284-704-7; POD:978-1-61284-701-6","10.1109/ITiME.2011.6132148","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132148","ASP.NET;Ajax;VB;excellent curriculum website;online test","Computers;Databases;Educational institutions;Presses;System analysis and design;Testing","Visual BASIC;Web sites;computer aided instruction;educational courses;question answering (information retrieval)","AJAX technologies;ASP.NET technologies;VB excellent curriculum Web site;comprehensive test;knowledge points;online testing system;score query;test questions analysis","","1","","8","","","9-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Exploiting Total Order Multicast in Weakly Consistent Transactional Caches","P. Ruivo; M. Couceiro; P. Romano; L. Rodrigues","INESC-ID Lisboa, Univ. Tec. de Lisboa, Lisbon, Portugal","2011 IEEE 17th Pacific Rim International Symposium on Dependable Computing","20120119","2011","","","99","108","Nowadays, distributed in-memory caches are increasingly used as a way to improve the performance of applications that require frequent access to large amounts of data. In order to maximize performance and scalability, these platforms typically rely on weakly consistent partial replication mechanisms. These schemes partition the data across the nodes and ensure a predefined (and typically very small) replication degree, thus maximizing the global memory capacity of the platform and ensuring that the cost to ensure replica consistency remains constant as the scale of the platform grows. Moreover, even though several of these platforms provide transactional support, they typically sacrifice consistency, ensuring guarantees that are weaker than classic 1-copy serializability, but that allow for more efficient implementations. This paper proposes and evaluates two partial replication techniques, providing different (weak) consistency guarantees, but having in common the reliance on total order multicast primitives to serialize transactions without incurring in distributed deadlocks, a main source of inefficiency of classical two-phase commit (2PC) based replication mechanisms. We integrate the proposed replication schemes into Infinispan, a prominent open-source distributed in-memory cache, which represents the reference clustering solution for the well-known JBoss AS platform. Our performance evaluation highlights speed-ups of up to 40× when using the proposed algorithms with respect to the native Infinispan replication mechanism, which relies on classic 2PC-based replication.","","Electronic:978-0-7695-4590-5; POD:978-1-4577-2005-5","10.1109/PRDC.2011.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133071","Atomic Multicast;Distributed Memory;Partial Replication;Transactional Memory","Benchmark testing;Context;Distributed databases;Memory management;Protocols;System recovery;Throughput","cache storage;data integrity;distributed memory systems;information retrieval;operating systems (computers);public domain software","Inflnispan;JBoss AS platform;classic 1-copy serializability;classic 2PC-based replication;classical two-phase commit based replication;distributed deadlock;distributed in-memory cache;open-source distributed in-memory cache;reference clustering solution;replica consistency;replication degree;total order multicast;transactional support;weakly consistent transactional cache","","4","1","31","","","12-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Data management support via spectrum perturbation-based subspace classification in collaborative environments","C. Chen; M. L. Shyu; S. C. Chen","Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA","7th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)","20120202","2011","","","67","76","Data management support to enable effective and efficient information sharing in collaborative environments is critical, especially in semantics based search and retrieval. In this paper, a novel spectrum perturbation-based subspace classification is proposed to mine semantics and other useful information from a large-scale dataset by utilizing a lower-dimensional subspace to discriminate different classes of the dataset. Among the existing subspace-based approaches, the principal component (PC) subspace is the most prevailing one and has been well studied. After investigating previous work related to PC subspace, we found that none of them had considered the perturbation on spectrum when building the subspace learning models. However, such perturbation is of certain importance and is able to provide discriminant information that helps improve classification performance by measuring the closeness of each testing data instance towards a subspace model by a closeness score based on the spectrum perturbation. Each testing data instance is assigned to its closest class by searching the smallest closeness score. Experiments are conducted to evaluate our proposed subspace classifier using data sets from three different sources, and the experimental results show that it achieves promising results and outperforms comparative subspace classifiers as well as some other commonly used classifiers.","","Electronic:978-1-936968-36-7; POD:978-1-4673-0683-6; USB:978-1-936968-32-9","10.4108/icst.collaboratecom.2011.247202","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144790","Collaborative environment;Principal component (PC) subspace;classification;closeness score;spectrum perturbation","","data mining;groupware;information retrieval;learning (artificial intelligence);pattern classification;principal component analysis","PC subspace;closeness score;collaborative environment;data management support;discriminant information;information mining;information sharing;large-scale dataset;lower dimensional subspace;principal component subspace;semantic based retrieval;semantic based search;semantic mining;spectrum perturbation based subspace classification;subspace based approach;subspace classifier;subspace learning models;testing data","","2","","29","","","15-18 Oct. 2011","","IEEE","IEEE Conference Publications"
"A fast lossless compression algorithm for Arabic textual images","S. alZahir","Graphics, Image Processing and Multimedia Lab, Computer Science, University of Northern British Columbia, Prince George, BC, V2N 4Z9, Canada","2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)","20120202","2011","","","595","598","In recent years, an unparalleled volume of textual information was transported over the Internet via email, chatting, blogging, twittering, digital libraries, and information retrieval systems. As the volume of text data has exceeded 40% of the total volume of traffic on the Internet, compressing textual data becomes imperative. Many algorithms were introduced and employed for this purpose including Huffman encoding, arithmetic encoding, the Ziv-Lempel family, Dynamic Markov Compression, and Burrow-Wheeler Transform. In this paper, a novel algorithm for compressing textual images is presented. The algorithm comprises of two parts: (i) a fixed-to-variable codebook; and (ii) row and column reduction coding scheme, RCRC. Simulation results on a large number of Arabic textual images show that this algorithm has a compression ratio of approximately 87%, which exceeds published results including those of JBIG2.","","Electronic:978-1-4577-0242-6; POD:978-1-4577-0243-3","10.1109/ICSIPA.2011.6144069","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144069","Arabic text compression;binary image compression;entropy;written text compression","Algorithm design and analysis;Conferences;Entropy;Image coding;Matrix converters;Morphology;Vectors","Huffman codes;Internet;Markov processes;arithmetic codes;data compression;digital libraries;document image processing;electronic mail;information retrieval systems;natural language processing;social networking (online);text analysis","Arabic textual images;Burrow-Wheeler transform;Huffman encoding;Internet traffic;JBIG2;RCRC;Ziv-Lempel family;arithmetic encoding;blogging;chatting;column reduction coding scheme;compression ratio;digital library;dynamic Markov compression;email;fixed-to-variable codebook;information retrieval systems;lossless compression algorithm;row reduction coding scheme;text data;textual data compression;textual image compression;textual information;twittering;unparalleled volume","","0","","10","","","16-18 Nov. 2011","","IEEE","IEEE Conference Publications"
"An Approach to Source-Code Plagiarism Detection and Investigation Using Latent Semantic Analysis","G. Cosma; M. Joy","P.A College, Cyprus","IEEE Transactions on Computers","20120123","2012","61","3","379","394","Plagiarism is a growing problem in academia. Academics often use plagiarism detection tools to detect similar source-code files. Once similar files are detected, the academic proceeds with the investigation process which involves identifying the similar source-code fragments within them that could be used as evidence for proving plagiarism. This paper describes PlaGate, a novel tool that can be integrated with existing plagiarism detection tools to improve plagiarism detection performance. The tool also implements a new approach for investigating the similarity between source-code files with a view to gathering evidence for proving plagiarism. Graphical evidence is presented that allows for the investigation of source-code fragments with regards to their contribution toward evidence for proving plagiarism. The graphical evidence indicates the relative importance of the given source-code fragments across files in a corpus. This is done by using the Latent Semantic Analysis information retrieval technique to detect how important they are within the specific files under investigation in relation to other files in the corpus.","0018-9340;00189340","","10.1109/TC.2011.223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086533","Source-code similarity detection;latent semantic analysis.;similarity investigation tool","Matrix decomposition;Plagiarism;Programming;Semantics;Software","file organisation;information retrieval;security of data;source coding","PlaGate;academia;academics;information retrieval;latent semantic analysis;source-code files;source-code plagiarism detection","","21","","33","","20111122","March 2012","","IEEE","IEEE Journals & Magazines"
"Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries","E. J. Humphrey; A. P. Glennon; J. P. Bello","Music & Audio Res. Lab. (MARL), New York Univ., New York, NY, USA","2011 10th International Conference on Machine Learning and Applications and Workshops","20120209","2011","2","","142","147","Though tags and metadata may provide rich indicators of relationships between high-level concepts like songs, artists or even genres, verbal descriptors lack the fine-grained detail necessary to capture acoustic nuances necessary for efficient retrieval of sounds in extremely large sample libraries. To these ends, we present a flexible approach titled Non-linear Semantic Embedding (NLSE), capable of projecting high-dimensional time-frequency representations of musical instrument samples into a low-dimensional, semantically-organized metric space. As opposed to other dimensionality reduction techniques, NLSE incorporates extrinsic semantic information in learning a projection, automatically learns salient acoustic features, and generates an intuitively meaningful output space.","","Electronic:978-0-7695-4607-0; POD:978-1-4577-2134-2","10.1109/ICMLA.2011.105","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6147663","","Aerospace electronics;Convolution;Instruments;Organizations;Principal component analysis;Semantics;Training","acoustic signal processing;digital libraries;information retrieval;meta data;musical instruments;time-frequency analysis","NLSE;acoustic nuances;dimensionality reduction techniques;extrinsic semantic information;fine-grained detail;high-dimensional time-frequency representations;high-level concepts;large instrument sample library;large sample library;low-dimensional metric space;metadata;musical instrument samples;nonlinear semantic embedding;output space;salient acoustic features;semantically-organized metric space;sound retrieval;verbal descriptors","","4","","11","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"The impact of hyper-threading on processor resource utilization in production applications","S. Saini; H. Jin; R. Hood; D. Barker; P. Mehrotra; R. Biswas","NASA Advanced Supercomputing Division, NASA Ames Research Center, Moffett Field, CA 94035-1000, USA","2011 18th International Conference on High Performance Computing","20120216","2011","","","1","10","Intel provides Hyper-Threading (HT) in processors based on its Pentium and Nehalem micro-architecture such as the Westmere-EP. HT enables two threads to execute on each core in order to hide latencies related to data access. These two threads can execute simultaneously, filling unused stages in the functional unit pipelines. To aid better understanding of HT-related issues, we collect Performance Monitoring Unit (PMU) data (instructions retired; unhalted core cycles; L2 and L3 cache hits and misses; vector and scalar floating-point operations, etc.). We then use the PMU data to calculate a new metric of efficiency in order to quantify processor resource utilization and make comparisons of that utilization between single-threading (ST) and HT modes. We also study performance gain using unhalted core cycles, code efficiency of using vector units of the processor, and the impact of HT mode on various shared resources like L2 and L3 cache. Results using four full-scale, production-quality scientific applications from computational fluid dynamics (CFD) used by NASA scientists indicate that HT generally improves processor resource utilization efficiency, but does not necessarily translate into overall application performance gain.","1094-7256;10947256","Electronic:978-1-4577-1950-9; POD:978-1-4577-1951-6","10.1109/HiPC.2011.6152743","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6152743","Benchmarking;Computational Fluid Dynamics (CFD);Hyper-Threading (HT);Intel Westmere-EP;Intel's Nehalem micro-architecture;Performance Evaluation;Performance Tools;SGI Altix ICE 8400EX;Simultaneous Multi-Threading (SMT)","Bandwidth;Computational fluid dynamics;Hardware;Instruction sets;Performance gain;Phasor measurement units;Radiation detectors","computational fluid dynamics;information retrieval;microprocessor chips;multi-threading;performance evaluation;resource allocation","HT-related issue;NASA scientist;Nehalem microarchitecture;PMU data;Pentium microarchitecture;code efficiency;computational fluid dynamics;data access;functional unit pipeline;hyper-threading;performance monitoring unit;processor resource utilization;production application;production-quality scientific application;resource sharing;single-threading mode;unhalted core cycle","","8","","26","","","18-21 Dec. 2011","","IEEE","IEEE Conference Publications"
"Tracking Mobile Users in Wireless Networks via Semi-Supervised Colocalization","J. J. Pan; S. J. Pan; J. Yin; L. M. Ni; Q. Yang","Facebook, Inc., Palo Alto","IEEE Transactions on Pattern Analysis and Machine Intelligence","20120123","2012","34","3","587","600","Recent years have witnessed the growing popularity of sensor and sensor-network technologies, supporting important practical applications. One of the fundamental issues is how to accurately locate a user with few labeled data in a wireless sensor network, where a major difficulty arises from the need to label large quantities of user location data, which in turn requires knowledge about the locations of signal transmitters or access points. To solve this problem, we have developed a novel machine learning-based approach that combines collaborative filtering with graph-based semi-supervised learning to learn both mobile users' locations and the locations of access points. Our framework exploits both labeled and unlabeled data from mobile devices and access points. In our two-phase solution, we first build a manifold-based model from a batch of labeled and unlabeled data in an offline training phase and then use a weighted k-nearest-neighbor method to localize a mobile client in an online localization phase. We extend the two-phase colocalization to an online and incremental model that can deal with labeled and unlabeled data that come sequentially and adapt to environmental changes. Finally, we embed an action model to the framework such that additional kinds of sensor signals can be utilized to further boost the performance of mobile tracking. Compared to other state-of-the-art systems, our framework has been shown to be more accurate while requiring less calibration effort in our experiments performed on three different testbeds.","0162-8828;01628828","","10.1109/TPAMI.2011.165","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989824","AI applications.;Wireless sensor networks;colocalization;indoor localization;semi-supervised learning","Data models;IEEE 802.11 Standards;Laplace equations;Manifolds;Mobile handsets;Robot sensing systems;Trajectory","collaborative filtering;information retrieval;learning (artificial intelligence);mobile computing;mobility management (mobile radio);pattern clustering;wireless sensor networks","access points;collaborative filtering;graph based semisupervised learning;incremental model;labeled data;machine learning based approach;manifold based model;mobile client;mobile devices;mobile user location;offline training phase;online localization phase;semisupervised colocalization;signal transmitters;two-phase colocalization;unlabeled data;user location data;weighted k-nearest neighbor method;wireless sensor network","","37","1","37","","20110818","March 2012","","IEEE","IEEE Journals & Magazines"
"Do Me a Solid? Information Asymmetry, Liking, and Compliance Gaining Online","N. J. Claes; C. Hurley; M. A. Stefanone","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","4417","4426","As the popularity of interactive social media grows, it is increasingly normal for individuals to reveal significant amounts of personal information online. Although this information is intended to support social networks, it can potentially be misused. We hypothesize that access to routine network site profile information can enable individuals to foster feelings of interpersonal attraction in their communication partner, which should increase the likelihood that their partner complies with requests for help. This study reports on an experiment conducted to assess these relationships. Results show participants who had access to personal information about their conversation partner in zero history dyads were more likely to gain their partner's compliance. Surprisingly, participants who benefited from the information asymmetry incurred a cost as well, as their partners reported liking them less compared to the control condition. Further, those who rated the information as valuable for getting their partner to like them were the least successful at gaining compliance.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149430","compliance gaining;information asymmetry;social networks","Availability;Context;Educational institutions;Electronic mail;Facebook;Internet","information retrieval;personal information systems;social sciences","communication partner;information asymmetry;interactive social media;interpersonal attraction;online personal information access;routine network site profile information access;social network;zero history dyad","","1","","48","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Hedonic and Utilitarian Search for Electronic Word-of-Mouth","E. Poyry; P. Parvinen; J. Salo; H. Blakaj","Sch. of Econ., Aalto Univ., Aalto, Finland","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1797","1806","Online information search is often seen as a highly utilitarian task but consumers' diverse ways of using the web have brought forth more hedonic information search patterns. At the same time, the impact of electronic word-of-mouth (eWOM) on consumer purchase decisions is increasing. The purpose of this study is to investigate the differences between hedonic and utilitarian eWOM search in the light of eWOM utilization. Using survey data from 1660 customers of two travel agencies, the study finds that, unlike utilitarian information search, hedonic information search promotes the utilization of eWOM in buying decisions.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.300","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149104","Online information search;electronic word-of-mouth;hedonism;utilitarianism","Consumer behavior;Context;Economics;Educational institutions;Industries;Internet;Web sites","consumer behaviour;information retrieval","buying decision;eWOM utilization;electronic word-of-mouth;hedonic information search pattern;hedonic search;online consumer behavior;online information search;travel agency;utilitarian information search;utilitarian search","","3","","54","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Using Modified Multivariate Bag-of-Words Models to Classify Physiological Data","P. Ordonez; T. Armstrong; T. Oates; J. Fackler","Comput. Sci. & Electr. Eng. Dept., Univ. of Maryland, Baltimore, MD, USA","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","534","539","In this paper we present two novel multivariate time series representations to classify physiological data of different lengths. The representations may be applied to any group of multivariate time series data that examine the state or health of an entity. Multivariate Bag-of-Patterns and Stacked Bags of-Patterns improve on their univariate counterpart, inspired by the bag-of-words model, by using multiple time series and analyzing the data in a multivariate fashion. We also borrow techniques from the natural language processing domain such as term frequency and inverse document frequency to improve classification accuracy. We introduce a technique named inverse frequency and present experimental results on classifying patients who have experienced acute episodes of hypotension.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.174","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137425","Multivariate Bag-of-Patterns;Stacked Bags-of-Patterns;classification;clincal informatics;multivariate time series","Accuracy;Data visualization;Medical diagnostic imaging;Physiology;Time frequency analysis;Time series analysis;Vectors","information retrieval;medical computing;natural language processing;pattern classification;physiology;time series","inverse frequency;multivariate bag-of-words models;multivariate fashion;multivariate time series representations;natural language processing;physiological data classification;stacked bags of-patterns","","7","","26","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
"Bootstrapping operation-level web service ontology: A bottom-up approach","X. Liu; H. Liu","Department of Computer Science, Rochester Institute of Technology, USA","7th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)","20120202","2011","","","19","26","Ontology is the key ingredient of semantic Web service technologies, which support systematic management of Web services, such as automatic service discovery, service composition, and change management. It is crucial and challenging to reduce the human efforts for developing ontologies. We propose a bottom-up approach that bootstraps operation-level service ontologies from WSDL descriptions. The approach leverages the techniques of information retrieval and machine learning. The relevance and similarity between Web services are measured based on the WSDL descriptions. The process of developing service ontologies consists of two steps. First, we build service ontologies based on the service relevance. We then construct the structure of the service ontologies based on the service similarity. We conduct an empirical study on real Web services to demonstrate the effectiveness of the proposed approach.","","Electronic:978-1-936968-36-7; POD:978-1-4673-0683-6; USB:978-1-936968-32-9","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144785","Ontology;WSDL;Web services","Ontologies;Web services","Web services;information retrieval;learning (artificial intelligence);ontologies (artificial intelligence);semantic Web","WSDL descriptions;automatic service discovery;bootstrapping operation-level Web service ontology;bottom-up approach;change management;information retrieval;machine learning;semantic Web service;service composition;service similarity","","0","","18","","","15-18 Oct. 2011","","IEEE","IEEE Conference Publications"
"Automatic extraction of historical transition in researchers and research topics","S. Hori; M. Murata; M. Tokuhisa; Q. Ma","Graduate School of Engineering, Tottori University, Japan","2011 7th International Conference on Natural Language Processing and Knowledge Engineering","20120126","2011","","","296","299","It is necessary for a researcher to know historical transition in researchers and research topics. Although Web search engines can be used for obtaining such information, collecting the information across a long time period is difficult and laborious. Thus, we proposed a method for automatically extracting historical transition in researchers and research topics by using co-occurrence information. We used an original method in which a concept that co-occurred more often with a certain concept X near the time when the concept X was generated was more likely to be the root of the concept X. We compared our method with the previous method proposed by Kawanaka et al., where transition information on concepts was automatically extracted by analyzing tags that describe concepts in social bookmarks, and we confirmed that our method outperformed their method. The accuracies of the extracted transition information in researchers and research topics in our method were 0.66 and 0.65 respectively, whereas the corresponding accuracies in Kawanaka et al.'s method were 0.25 and 0.61.","","Electronic:978-1-61284-729-0; POD:978-1-4799-1689-4","10.1109/NLPKE.2011.6138212","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138212","automatic;co-occurrence information;derivation;estimation;senior junior relation;transition","Accuracy;Bismuth;Data mining;Educational institutions;Mutual information;Natural language processing;Probability","information retrieval;research and development;search engines;social networking (online)","Web search engines;automatic historical transition extraction;research topics;researchers;social bookmarks;transition information","","1","","7","","","27-29 Nov. 2011","","IEEE","IEEE Conference Publications"
"A parallel approach to context-based term weighting","S. Arora; S. Chakravarty","Dept. of Information Technology, Netaji Subhas Institute of Technology, New Delhi, India","2011 World Congress on Information and Communication Technologies","20120130","2011","","","951","956","Information retrieval and extraction essentially rely on estimating the relevance of words present in a large corpus of documents or text. One of the approaches to measuring relevance is analyzing the importance of words based on their statistical distribution within a document. Quite another approach ensues from their linguistic relevance within a logically perceived context. Literature presents a body of work done employing both statistical as well as contextual approaches. The challenge currently is on enhancing the performance of document analysis and clustering systems. Ever since we witnessed a massive explosion of information and raw data available on the web, their analysis demands more rigorous computations and processing. Given the widely distributed environment as a backbone platform for these systems to operate, there is an urgent need to develop techniques to scale up their performance on multiple processors. We propose a parallelized strategy to estimate the statistical as well as contextual relevance of words, employing master-slave configuration on a cluster of processors. Our parallel algorithm has been successfully tested on a self-made Beowulf cluster comprising ten nodes, showing significant performance improvement over single processor.","","Electronic:978-1-4673-0126-8; POD:978-1-4673-0127-5","10.1109/WICT.2011.6141376","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141376","Amdahl's Law;Cluster computing;Context based Text Classification;Retrieval;TF-IDF","Algorithm design and analysis;Clustering algorithms;Context;Measurement;Pragmatics;Program processors;Switches","information retrieval;parallel algorithms;pattern clustering;statistical distributions;text analysis","Beowulf cluster;clustering system;context-based term weighting;document analysis;document corpus;information extraction;information retrieval;linguistic relevance;parallel algorithm;parallel approach;statistical distribution;text corpus","","0","","15","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Work in progress — Item retrieval system in distributed repositories to generate adaptive assessments supported in items response theory (.IRTT)","P. Yehiry; F. Gloria; B. Silvia; G. Juan","Researcher, Universidad Distrital Francisco Jos&#x00E9; de Caldas","2011 Frontiers in Education Conference (FIE)","20120202","2011","","","T1A-1","T1A-4","The assessment process is one of the most important issues in the learning process and in many case it is the process that defines the instruction sequence because it measures the performance of the student in the educational process. During recent decades the inclusion of TICs in the teaching-learning process have facilitated addressing the diversity of student and teacher features. Technologies to enhance learning have allowed the different ways of learning and teaching that coexist in the educational context to be adapted through user modeling and adaptation processes. Our purpose in this paper is to introduce “. IRTT” (item response theory tool), an adaptive tool based on item response theory to generate assessment according to the student competence level, in the context of a learning management system (LMS), in particular. LRN [2].","0190-5848;01905848","Electronic:978-1-61284-469-5; POD:978-1-61284-468-8; USB:978-1-61284-467-1","10.1109/FIE.2011.6142868","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142868","Distributed Repositories;E-learning;Items Response Theory;LMS","Adaptation models;Adaptive systems;Cities and towns;Conferences;Context;Mathematical model;Unified modeling language","computer aided instruction;distributed processing;educational administrative data processing;human computer interaction;information retrieval systems;teaching;user modelling",".IRTT;LMS;adaptation processes;adaptive assessment generation;adaptive tool;distributed repositories;educational process;instruction sequence;item response theory tool;item retrieval system;learning management system;student competence level;teaching-learning process;user modeling","","0","","12","","","12-15 Oct. 2011","","IEEE","IEEE Conference Publications"
"Behavior Based User Interests Extraction Algorithm","K. Xing; B. Zhang; B. Zhou; Y. Liu","Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China","2011 International Conference on Internet of Things and 4th International Conference on Cyber, Physical and Social Computing","20120202","2011","","","448","452","The most important issue in personalized services is how to build a proper user model for individual user. Extracting user interests is an essential question, which attracts many researchers' investigation. But most of them don't consider enough the history of user behaviors. This paper proposes a method to extract user interests from user behavior history and document information, so called Behavior based User Interests extraction (BUIE) algorithm. By analyzing document information of English scientific literatures with domain ontology, the interest items of the documents can be extracted. Then combining with the user behavior history, the user interest items' weights can be refined. Experimental results show that, BUIE algorithm could get better performance on user interests extraction than those that don't consider user behaviors. The user interests calculated by this method are quite consistent with user's real interests. The different kinds of behaviors' contribution degrees to user interests are analyzed, and it's found that the behaviors of copying, saving, printing are important factors in extracting user interests. The contribution degree of printing is higher than that of saving and copying. But the stability of these three behaviors is less than that of relative browsing time.","","Electronic:978-0-7695-4580-6; POD:978-1-4577-1976-9","10.1109/iThings/CPSCom.2011.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6142295","behavior based user interests extraction;domain ontology;user behavior history","Computer science;Data mining;Delta modulation;Genetic algorithms;History;Ontologies;Printing","information analysis;information retrieval;natural language processing;ontologies (artificial intelligence);user modelling","English scientific literatures;behavior based user interests extraction algorithm;copying behavior;document information analysis;domain ontology;personalized service;printing behavior;saving behavior;user behavior history;user interest extraction;user model","","3","","9","","","19-22 Oct. 2011","","IEEE","IEEE Conference Publications"
"A Framework for Similarity Search of Time Series Cliques with Natural Relations","B. Cui; Z. Zhao; W. H. Tok","Peking University, Beijing","IEEE Transactions on Knowledge and Data Engineering","20120126","2012","24","3","385","398","A Time Series Clique (TSC) consists of multiple time series which are related to each other by natural relations. The natural relations that are found between the time series depend on the application domains. For example, a TSC can consist of time series which are trajectories in video that have spatial relations. In conventional time series retrieval, such natural relations between the time series are not considered. In this paper, we formalize the problem of similarity search over a TSC database. We develop a novel framework for efficient similarity search on TSC data. The framework addresses the following issues. First, it provides a compact representation for TSC data. Second, it uses a multidimensional relation vector to capture the natural relations between the multiple time series in a TSC. Lastly, the framework defines a novel similarity measure that uses the compact representation and the relation vector. We conduct an extensive performance study, using both real-life and synthetic data sets. From the performance study, we show that our proposed framework is both effective and efficient for TSC retrieval.","1041-4347;10414347","","10.1109/TKDE.2010.270","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677531","Time series clique;compact representation;natural relation;similarity search.","Algorithm design and analysis;Databases;Games;Principal component analysis;Search problems;Time series analysis;Trajectory","data analysis;data structures;database management systems;information retrieval;time series","TSC data compact representation;TSC database;data analysts;multidimensional relation vector;natural relations;similarity measure;similarity search;time series cliques;time series retrieval","","3","","26","","20101230","March 2012","","IEEE","IEEE Journals & Magazines"
"Document Clustering via Matrix Representation","X. Wang; J. Tang; H. Liu","Arizona State Univ., Tempe, AZ, USA","2011 IEEE 11th International Conference on Data Mining","20120123","2011","","","804","813","Vector Space Model (VSM) is widely used to represent documents and web pages. It is simple and easy to deal computationally, but it also oversimplifies a document into a vector, susceptible to noise, and cannot explicitly represent underlying topics of a document. A matrix representation of document is proposed in this paper: rows represent distinct terms and columns represent cohesive segments. The matrix model views a document as a set of segments, and each segment is a probability distribution over a limited number of latent topics which can be mapped to clustering structures. The latent topic extraction based on the matrix representation of documents is formulated as a constraint optimization problem in which each matrix (i.e., a document) A<sub>i</sub> is factorized into a common base determined by non-negative matrices L and R<sup>T</sup>, and a non-negative weight matrix M<sub>i</sub> such that the sum of reconstruction error on all documents is minimized. Empirical evaluation demonstrates that it is feasible to use the matrix model for document clustering: (1) compared with vector representation, using matrix representation improves clustering quality consistently, and the proposed approach achieves a relative accuracy improvement up to 66% on the studied datasets, and (2) the proposed method outperforms baseline methods such as k-means and NMF, and complements the state-of-the-art methods like LDA and PLSI. Furthermore, the proposed matrix model allows more refined information retrieval at a segment level instead of at a document level, which enables the return of more relevant documents in information retrieval tasks.","1550-4786;15504786","Electronic:978-0-7695-4408-3; POD:978-1-4577-2075-8","10.1109/ICDM.2011.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137285","Document Clustering;Document Representation;Matrix Representation;Non-Negative Matrix Approximation","Approximation methods;Bismuth;Clustering algorithms;Data mining;Matrix decomposition;Probability distribution;Vectors","Web sites;constraint handling;document handling;information retrieval;matrix algebra;optimisation;pattern clustering;probability","LDA;NMF;PLSI;Web pages;clustering structures;cohesive segments;constraint optimization problem;document clustering;information retrieval tasks;k-means;matrix model;matrix representation;probability distribution;vector space model","","0","2","24","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Building Support for Web Information Gathering Tasks","A. Alhenshiri; C. Watters; M. Shepherd; J. Duffy","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1687","1696","Web information gathering includes tasks in which users attempt to locate, organize, and use information from different sources on the web to satisfy an information need, often over multiple sessions. An example is the case of writing a report or beginning an investigation. This paper presents the results of a research study to explore the difficulties users experience during such tasks. Twenty users participated in the study in which the frequency of specific kinds of activities involved was recorded along with qualitative measures. The results of the study provide insight into the area's most needing support in order to provide guidance for the building of new tools to support users in these tasks. The results indicate that tools are needed to bring information into a final report, facilitate editing, and to support re-finding information in its original context over multiple sessions.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.136","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149090","Web;behavior;gathering;information;keeping;re-finding;search;task;user","Browsers;Context;Educational institutions;Electronic mail;Organizations;Organizing;Web pages","human factors;information networks;information retrieval;search engines","Web browsing;Web information gathering task;information need;information refinding;search tool","","1","","22","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"A Study on Information Recommendation Systems for Continuous Use","H. Shoji; H. Inamura; A. Ogino","Dept. of Ind. & Syst. Eng., Chuo Univ., Tokyo, Japan","2011 Third International Conference on Intelligent Networking and Collaborative Systems","20120119","2011","","","867","871","Most of traditional studies on information recommendation systems aim at recommending highly satisfactory information in a one-time or small number of uses of a system or service, and never take it into account that the user would feel tired of the same system or service after they have used it many times over. This study has modeled the human nature of being tired of the same thing and enabled the measurement of the degree of such boredom. This time, the study has implemented an information recommendation system that has features to (1) visualize the degree of the user's boredom and (2) recommend information that can reduce it. Then, an evaluation experiment was conducted using a system developed. The result of an evaluation experiment shows that the system can get the user less bored and more satisfied when used continuously.","","Electronic:978-0-7695-4579-0; POD:978-1-4577-1908-0","10.1109/INCoS.2011.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132924","boredom;continuuous use;information recommendation","Color;Data models;Floods;Raw materials;Solid modeling;Vectors","information retrieval;recommender systems","continuous use;information recommendation system;user boredom","","0","","4","","","Nov. 30 2011-Dec. 2 2011","","IEEE","IEEE Conference Publications"
"A proposed methodology for Semantic Web implementation","K. U. Danyaro; J. Jaafar; M. S. Liew","Department of Computer & Information Sciences, Universiti Teknologi PETRONAS, Seri Iskandar, 31750 Tronoh, Perak, Malaysia","2011 National Postgraduate Conference","20120123","2011","","","1","6","Semantic Web provides precise, timely, searchable and well-define information across different sources of data. However, there is need of solving the problem of interoperability due to everyday increase of data from different sources. Whereby, the resourceful information or data integration between human and machines are needed. Thus, more precisely, this paper addresses an ultimate problem of integrating and utilizing the accurate data on information systems. The proposed methodology of this paper includes: (1) grounded theory that focuses on particular interaction between human and machines, (2) developing the analysis; the research introduces the Software Development Life Cycle (SDLC) approach, from the development to implementation stages, (3) sources of data; for state-of-the-art research, we propose to use MetOcean (Meteorological and Oceanographic) data, and (4) Research instrument which is to set our research into practice where we will build Semantic Web application using Franz AllegroGraph and use SIMILE (Semantic Interoperability of Metadata and Information in Unlike Environments). Finally, we provide the Ontology model of the for the state-of-the-art research in meteorological and Oceanographic domain.","","Electronic:978-1-4577-1884-7; POD:978-1-4577-1882-3","10.1109/NatPC.2011.6136294","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6136294","MetOcean;Oceanographic and Meteorological Data;Ontology;Semantic Web","Information systems;Ontologies;Organizations;Resource description framework;Semantics;XML","data integration;geophysics computing;information retrieval;meta data;meteorology;oceanography;ontologies (artificial intelligence);open systems;problem solving;semantic Web;software development management;software maintenance","Franz AllegroGraph;MetOcean data;SDLC approach;SIMILE;data integration;grounded theory;information systems;meteorological and ceanographic data;meteorological domain;oceanographic domain;ontology model;problem solving;resourceful information;searchable information;semantic Web implementation;semantic interoperability of metadata and information in unlike environments;software development life cycle approach;state-of-the-art research;well-define information","","0","","34","","","19-20 Sept. 2011","","IEEE","IEEE Conference Publications"
"Relationship between Online Word-of-Mouth Communication and Consumer Behavior","I. Okada","Soka Univ., Soka, Japan","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1807","1813","Online word-of-mouth communication was studied using a model that focuses on consumers' informational behavior in terms of ""information retrieving"" and ""information sending."" This model was previously used in studies of the basic mechanisms of the winner-takes-all phenomenon and consumption concentration. We applied the model to online word-of-mouth communication by assuming heterogeneity across consumers in terms of individual informative actions. Consumers were assumed to communicate with other consumers selectively using one of three policies: random selection, similar-level selection, and higher-level selection. A basic simulation showed that the most effective policy for selecting communication partners depends on the characteristics of the goods under consideration. An agent-based simulation showed that an effective word-of-mouth policy also depends on the characteristics of the goods under consideration. These findings clarify how consumers deal with cognitive limitations in the face of the massive amount of information now available.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.500","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149105","","Analytical models;Blogs;Consumer behavior;Economics;Internet;Memory management;Technological innovation","Internet;cognition;consumer behaviour;information retrieval","agent-based simulation;cognitive limitations;communication partners;consumer behavior;consumers informational behavior;consumption concentration;goods under consideration;higher-level selection;individual informative actions;information retrieving;information sending;online word-of-mouth communication;random selection;similar-level selection;winner-takes-all phenomenon;word-of-mouth policy","","1","","15","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Fusing Blog Opinion Retrieval Results for Better Effectiveness","S. Wu","Sch. of Comput. Sci. & Telecommun. Eng., Jiangsu Univ., Zhenjiang, China","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","195","199","In recent years, blogs have been very popular on the Web as a grassroots publishing platform. Some research has been conducted on them and blog opinion retrieval is one of the key issues. In this paper, we investigate if data fusion can be useful for improvement of effectiveness of blog opinion retrieval. Extensive experimentation with the runs submitted to the blog opinion retrieval task in TREC 2008 is carried out and a few data fusion methods including Comb Sum, Comb MNZ, Borda count, and the linear combination method are investigated. We observe that generally speaking, all data fusion methods involved are very competitive compared with the best component retrieval system. Especially, the linear combination method with proper training is superior to other data fusion methods and it is able to beat the best component retrieval system by a clear margin. This study demonstrates that data fusion can be an effective technique for blog opinion retrieval if proper fusion methods are applied.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.36","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059817","Blog system;Data fusion;Linear combination;Opinion retrieval","Blogs;Feeds;Fitting;Linear regression;Measurement;USA Councils","Internet;Web sites;information retrieval;sensor fusion","Borda count;Comb MNZ;Comb Sum;TREC 2008;World Wide Web;blogs;component retrieval system;data fusion methods;fusing blog opinion retrieval;grassroots publishing platform;linear combination method","","0","","17","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"Improving Bag-of-Words model with spatial information","E. Zhang; M. Mayo","The University of Waikato, Knighton Road, Hamilton, New Zealand","2010 25th International Conference of Image and Vision Computing New Zealand","20120209","2010","","","1","8","Bag-of-Words (BOW) models have recently become popular for the task of object recognition, owing to their good performance and simplicity. Much work has been proposed over the years to improve the BOW model, where the Spatial Pyramid Matching technique is the most notable. In this work, we propose three novel techniques to capture more refined spatial information between image features than that provided by the Spatial Pyramids. Our techniques demonstrate a performance gain over the Spatial Pyramid representation of the BOW model.","2151-2191;21512191","Electronic:978-1-4244-9631-0; POD:978-1-4244-9629-7","10.1109/IVCNZ.2010.6148795","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6148795","Bags-of-words;Object Recognition;SIFT Keypoints;Spatial Pyramid Matching","Histograms;Object recognition;Shape;Training;Vectors;Visualization;Vocabulary","information retrieval;object recognition","BOW model;bag-of-words model;image features;object recognition;spatial information;spatial pyramid matching technique;spatial pyramid representation","","8","","30","","","8-9 Nov. 2010","","IEEE","IEEE Conference Publications"
"Effects of ""Advanced Search"" on User Performance and Search Efforts: A Case Study with Three Digital Libraries","X. Zhang; Y. Li","","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","1605","1614","This study investigated into the effects of the ""Advanced search"" feature of three digital libraries on users' search performance and search efforts. Three operational digital libraries, i.e., the ACM digital library, the IEEE Computer Society digital library, and the IEEE Xplore digital library, were used in this study. Thirty-five students participated in an experiment and completed the assigned search task, using their preferred search feature(s). The results demonstrate that for ACM and IEEE CS, the use of Advanced search did not have a significant effect on improving search performance. Only in Xplore, the use of Advanced search significantly improved search performance. The search efforts increased significantly for the combined-use of Advanced search and Basic search in ACM and IEEE CS. The reasons leading to the results are discussed.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.234","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149080","digital libraries;information search;interaction design;search interface design;user experience","IEEE Computer Society;IEEE Xplore;Indexing;Libraries;Search methods;Usability;User interfaces","digital libraries;information retrieval;user interfaces","ACM digital library;Advanced Search effect;Advanced Search feature;Basic Search feature;IEEE Computer Society digital library;IEEE Xplore digital library;user search effort;user search performance","","0","","16","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"A New Efficient Data Structure for Storage and Retrieval of Multiple Biosequences","S. Steinbiss; S. Kurtz","University of Hamburg, Hamburg","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20120126","2012","9","2","345","357","Today's genome analysis applications require sequence representations allowing for fast access to their contents while also being memory-efficient enough to facilitate analyses of large-scale data. While a wide variety of sequence representations exist, lack of a generic implementation of efficient sequence storage has led to a plethora of poorly reusable or programming language- specific implementations. We present a novel, space-efficient data structure (GtEncseq) for storing multiple biological sequences of variable alphabet size, with customizable character transformations, wildcard support, and an assortment of internal representations optimized for different distributions of wildcards and sequence lengths. For the human genome (3.1 gigabases, including 237 million wildcard characters) our representation requires only 2 + 8 · 10<sup>-6</sup> bits per character. Implemented in C, our portable software implementation provides a variety of methods for random and sequential access to characters and substrings (including different reading directions) using an object-oriented interface. In addition, it includes access to metadata like sequence descriptions or character distributions. The library is extensible to be used from various scripting languages. GtEncseq is much more versatile than previous solutions, adding features that were previously unavailable. Benchmarks show that it is competitive with respect to space and time requirements.","1545-5963;15455963","","10.1109/TCBB.2011.146","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081847","Data storage representations;biology and genetics;reusable libraries.;software engineering","Bioinformatics;Data structures;Encoding;Genomics;Libraries;Particle separators;Software","authoring languages;benchmark testing;bioinformatics;genetics;genomics;information retrieval;object-oriented programming;optimisation;programming languages;software engineering;spatial data structures","benchmarks;character distributions;genome analysis applications;internal representation optimization;large-scale data;metadata like sequence descriptions;multiple biosequence retrieval;multiple biosequence storage;object-oriented interface;portable software implementation;programming language;reading directions;scripting languages;space-efficient data structure;wildcards","Algorithms;Computational Biology;Databases, Genetic;Information Storage and Retrieval;Models, Genetic;Multigene Family;Sequence Analysis","6","","42","","20111115","March-April 2012","","IEEE","IEEE Journals & Magazines"
"A Text Mining Model for Strategic Alliance Discovery","Y. Zhou; Y. Zhang; N. Vonortas; J. Williams","George Washington Univ., Washington, DC, USA","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","3571","3580","Strategic alliances among organizations are one of the central drivers of innovation and economy and have raised strong interest among policymakers, strategists and economists. However, discovery of alliances has relied on pure manual search and has limited scope. This research addresses the limitations by proposing a text mining framework that automatically extracts alliances from news articles. The model not only integrates meta-search, entity extraction and shallow and deep linguistic parsing techniques, but also proposes an innovative ADT-based relation extraction method to deal with the extremely skewed and noisy news articles and AC Rank to further improve the precision using various linguistic features. Evaluation from an IBM alliances case study showed that ADT-based extraction achieved 78.1% in recall, 44.7% in precision and 0.569 in F-measure and eliminated over 99% of the noise in document collections. AC Rank further improved precision to 97% with the top-20% extracted alliance instances. Our case study also showed that the widely cited Thomson SDC database only covered less than 20% of total alliances while our automatic approach can covered 67%.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.86","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149255","information extraction;knowledge discovery;strategic alliance;text mining","Collaboration;Databases;Educational institutions;Feature extraction;Technological innovation;Text mining","business data processing;data mining;information retrieval;innovation management;organisational aspects;strategic planning;text analysis","ACRank;F-measure;IBM alliances;Thomson SDC database;document collections;economists;economy drivers;entity extraction;innovation drivers;innovative ADT based relation extraction method;linguistic parsing techniques;meta search;news articles;organizations;policymakers;pure manual search;strategic alliance discovery;strategists;text mining model","","0","","20","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"Data mining and visualization on legal documents","D. Gaur","Department of Computer Engineering, College of Technology, G.B. Pant University of Agriculture and Technology, Pantnagar, Uttarakhand-263145, India","2011 International Conference on Recent Trends in Information Systems","20120209","2011","","","132","136","LIRFSS (Legal Information Retrieval and Focused Semantic Search) is a system that performs IR(Information Retrieval) and IE(Information Extraction) on Indian Supreme Court decisions, specifically on criminal cases related to murder and provide focused semantic search results, offering a high potential of assistance for judges, lawyers and citizens in getting access to law. In this paper, development process of LIRFSS is discussed. Appropriate information to be extracted for criminal cases such as date, location of occurrence and IPC (Indian Penal code)[1] sections were determined from a sample set of documents. Better instruments than manual browsing and other search methods are required to make the most out of freely available law on internet, therefore a semantic search is carried out using extracted information and most pertinent cases were returned as a result. As analyzing the vast volumes of data becomes increasingly difficult, so information visualization is also carried out in this paper and a new hypotheses is proposed and verified. From 20 training documents, a satisfactory amount of precision was obtained.","","Electronic:978-1-4577-0792-6; POD:978-1-4577-0790-2","10.1109/ReTIS.2011.6146854","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6146854","Annotations;Data Mining;Database Searching;Information Extraction;Statistical Visualization","Data mining;Data visualization;Databases;Law;Logic gates;Semantics","Internet;data mining;data visualisation;document handling;information retrieval;law administration;search problems","IPC section;Indian Penal code;Indian Supreme Court decision;Internet;LIRFSS;criminal case;data mining;data visualization;document set;focused semantic search method;information extraction;legal document;legal information retrieval","","0","","6","","","21-23 Dec. 2011","","IEEE","IEEE Conference Publications"
"The Web Accessibility Crisis of the Korea's Electronic Government: Fatal Consequences of the Digital Signature Law and Public Key Certificate","H. M. Park","Int. Univ. of Japan, Japan","2012 45th Hawaii International Conference on System Sciences","20120209","2012","","","2319","2328","Korea's e-government ranked top in e-government benchmarking for the past five years but showed relatively lower scores in Web accessibility. As a result of the Digital Signature Act, public key certificate was introduced and digital certificate software was developed using de facto technology standards, Microsoft ActiveX. Government, certificate authorities, and certificate consumers all overlooked the implications of using Microsoft standards and ignored criticisms of those who do not use Microsoft products. Government failed to implement digital signature policy successfully. Its consequences include unbelievable Microsoft monopoly with almost 99 percent market shares of Microsoft products, chronic addiction to Microsoft standards, bad computing practices, and fatal Web accessibility problems. ActiveX should be removed immediately to support diverse operating systems and Web browsers. Eventually current client-side certificate should be switched to server-side system. This paper calls for careful evaluation of Korea's e-government.","1530-1605;15301605","Electronic:978-1-5090-5638-5; POD:978-1-4577-1925-7; USB:978-0-7695-4525-7","10.1109/HICSS.2012.591","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149295","Web accesbility;benchmarking;digital signature;e-goverment;public key certificate","Electronic government;Public key;Software;Standards;Web sites","Internet;digital signatures;government data processing;information retrieval;law;marketing;monopoly;online front-ends;operating systems (computers);public key cryptography","Korea e-government benchmarking;Korea electronic government;Microsoft ActiveX;Microsoft product;Microsoft standard;Web accessibility crisis problem;Web browser;client-side certificate authority;de facto technology standard;digital certificate software;digital signature law;digital signature policy;market shares;operating system;public key certificate consumer;server-side system;unbelievable Microsoft monopoly","","3","","33","","","4-7 Jan. 2012","","IEEE","IEEE Conference Publications"
"An answer extraction method based on discourse structure and rank learning","H. Zong; Z. Yu; J. Guo; Y. Xian; J. Li","School of Information Engineering and Automation, Kunming University of Science and Technology, China","2011 7th International Conference on Natural Language Processing and Knowledge Engineering","20120126","2011","","","132","139","For the complex questions of Chinese question answering system such as `why', `how' these non-factoid questions, we proposed an answer extraction method using discourse structures features and ranking algorithm. This method takes the judge problem of answers relevance as learning to rank answers. First, the method analyses questions to generate the query string, and then uses rhetorical structure theory and the natural language processing technology of vocabulary, syntax, semantic analysis to analyze the retrieved documents, so as to determine the inherent relationship between paragraphs or sentences and generate the answer candidate paragraphs or sentences. Thirdly, construct the answer ranking model, extract five group features of similarity features, density and frequency features, translation features, discourse structure features and external knowledge features to train ranking model. Finally, re-ranking the answers with the training model and find the optimal answers. Experiments show that the proposed method can effectively improve the accuracy and quality of non-factoid answers.","","Electronic:978-1-61284-729-0; POD:978-1-4799-1689-4","10.1109/NLPKE.2011.6138181","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138181","answer extracting;complex questions;discourse structure;learning to rank","Analytical models;Biomedical optical imaging;Integrated optics;Optical reflection;Pragmatics","document handling;natural language processing;query processing;question answering (information retrieval);vocabulary","Chinese question answering system;answer candidate paragraph generation;answer extraction method;answer rank learning;answer ranking model;discourse structures features;document;group feature extraction;judge problem;knowledge features;natural language processing technology;query string generation;ranking algorithm;rhetorical structure theory;semantic analysis;sentences;syntax;vocabulary","","0","","18","","","27-29 Nov. 2011","","IEEE","IEEE Conference Publications"
"An automated course feedback system using opinion mining","V. K. Singh; P. Kumari; A. Singh; J. Thapa","Department of Computer Science, South Asian University, New Delhi, India","2011 World Congress on Information and Communication Technologies","20120130","2011","","","72","76","This paper presents our experimental work on designing an automated web-based course feedback system that uses opinion mining to compute the overall feedback score. The system collects user responses through an online form. The online feedback form consists of a set of binary and graded response questions as well as a free-form input text box where a student can write about his overall experiences with the course. Every student is encouraged to write in detail about his experience. The text entered by the student in free form input text box is then used for opinion mining to label the feedback as `positive' or `negative'. We have used an unsupervised SO-PMI-IR based approach for opinion mining. The final feedback of a course is thus a combination of an overall score (computed on various binary and graded questions) and a `positive' or `negative' label to mark the student's overall evaluation. We evaluated the system for more than 20 courses with a total of about 1000 reviews.","","Electronic:978-1-4673-0126-8; POD:978-1-4673-0127-5","10.1109/WICT.2011.6141220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6141220","Course Review Mining;Online Feedback;Opinion Mining","Databases;Educational institutions;Machine learning;Machine learning algorithms;Semantics;Text categorization","data mining;educational administrative data processing;educational courses;information retrieval","automated Web-based course feedback system;binary response questions;free-form input text box;graded response questions;information retrieval;online feedback form;opinion mining;semantic orientation;unsupervised SO-PMI-IR based approach","","1","","12","","","11-14 Dec. 2011","","IEEE","IEEE Conference Publications"
"Goal-oriented Self-management of In-memory Distributed Data Grid Platforms","L. Rosa; L. Rodrigues; A. Lopes","Inst. Super. Tecnico, Univ. Tec. de Lisboa, Lisbon, Portugal","2011 IEEE Third International Conference on Cloud Computing Technology and Science","20120119","2011","","","587","591","This paper addresses the self-management of in-memory distributed data grid platforms. A growing number of applications rely in these platforms to speed up access to large sets of data. However, they are complex to manage due to the diversity of configuration and load profiles. The proposed approach employs an adaptation policy expressed in terms of high-level goals to facilitate the task of the system manager, and address the complexity issues posed by the management of multiple configurations. The approach is validated experimentally using the open-source RedHat's Infinispan platform.","","Electronic:978-0-7695-4622-3; POD:978-1-4673-0090-2","10.1109/CloudCom.2011.90","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6133199","Self-management;autonomic computing;distributed data grids","Distributed databases;Monitoring;Planning;Runtime;Strontium;Web servers","distributed memory systems;grid computing;information retrieval","data access;goal oriented self-management;in memory distributed data grid platform;open source RedHat's infinispan platform","","2","1","17","","","Nov. 29 2011-Dec. 1 2011","","IEEE","IEEE Conference Publications"
"A Semantics-Enabled Web API Registry","D. Bianchini; V. de Antonellis; M. Melchiori","Dip. di Ing. dell'Inf., Univ. di Brescia, Brescia, Italy","2011 22nd International Workshop on Database and Expert Systems Applications","20120116","2011","","","247","251","Nowadays, Web applications can be quickly developed by combining existing APIs, independently provided by third parties. In this paper we present a semantics-enabled registry for Web APIs and we address the problem of supporting the retrieval and exploratory browsing of available APIs. Web APIs are semantically organized in the registry according to similarity and coupling criteria. Preliminary results of effectiveness and efficacy of the registry are presented.","1529-4188;15294188","Electronic:978-0-7695-4486-1; POD:978-1-4577-0982-1","10.1109/DEXA.2011.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6059825","Web API registry;semantic Web API selection","Conferences;Couplings;Graphical user interfaces;Mashups;Ontologies;Publishing;Semantics","Internet;application program interfaces;information retrieval;search engines","Web API registry;exploratory browsing;retrieval;semantics-enabled registry","","1","","10","","","Aug. 29 2011-Sept. 2 2011","","IEEE","IEEE Conference Publications"
"An Information Extraction Method from Different Structural Web Sites by Word Distances between a User Instantiated Label and Similar Entity","D. Nakajima; Y. Mitsui; M. Samejima; M. Akiyoshi","Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan","2011 IEEE 11th International Conference on Data Mining Workshops","20120123","2011","","","1177","1182","This paper addresses an information extraction from different structural web sites by using a user instantiated example. A user instantiated example consists of labels as criteria for decision making on purchasing a target product or service and instances related to the labels. When information extraction method outputs the information in table form, labels are used as column heading of table and instances are used as instances filled in the table. Because there are various labels and information that does not correspond to the target on the web site, it is difficult to extract the target information related to the target. Information of the target tends to be written in a similar string to the instances that is called ""similar entity"". And target information is written close to the labels. So, the proposed method extracts information using the number of words among a user instantiated label and similar entities. Additionally, in order to extract a piece of information described across the web sites, the proposed method extracts information from linked web sites that are similar to the web site used for a user instantiated example. Experimental results show that the proposed method can extract information at recall rate of 65% and precision rate of 91%.","2375-9232;23759232","Electronic:978-0-7695-4409-0; POD:978-1-4673-0005-6","10.1109/ICDMW.2011.29","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6137514","Different Structure;Information Extraction;Link Selection;User Instantiated Label;Web Sites;Word Distances","Adaptive optics;Data mining;Digital cameras;Monitoring;Optical sensors;Pattern matching;Web sites","Web sites;information retrieval","decision making;information extraction method;structural Web sites;target information;user instantiated example;word distances","","0","","9","","","11-11 Dec. 2011","","IEEE","IEEE Conference Publications"
