"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6693181,6693540,6694366,6693113,6699089,6693516,6699024,6694327,6694503,6693551,6693366,6694119,6694252,6693303,6693515,6671814,6690721,6690000,6690082,6690719,6690053,6690023,6689991,6690046,6689990,6690724,6690040,6690001,6690003,6690013,6690939,6689558,6690038,6365185,6678261,6684793,6686055,6684834,6686034,6507319,6686038,6686088,6688932,6686328,6686238,6687676,6685066,6688730,6682873,6683913,6680479,6680486,6680492,6680497,6680386,6680013,6682352,6680197,6682235,6682145,6680471,6682117,6680521,6680374,6682431,6682059,6680373,6680463,6680528,6680381,6680474,6681123,6679986,6682239,6479688,6681238,6681227,6682406,6504513,6680333,6680326,6680376,6680458,6679821,6680377,6681250,6680874,6680531,6679470,6676208,6676592,6676903,6674516,6676002,6676919,6676910,6676909,6676930,6675476,6674958",2017/05/04 22:20:39
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"iQuiz: Integrated assessment environment to improve Moodle Quiz","J. R. A. Rodrigues; L. O. Brandão; M. Nascimento; P. Rodrigues; A. A. F. Brandão; H. Giroire; O. Auzende","Univ. of Sao Paulo, Sao Paulo, Brazil","2013 IEEE Frontiers in Education Conference (FIE)","20131219","2013","","","293","295","Moodle is a well-known open source system to support teaching and learning through the web. It provides Quiz, a tool for learning assessment, which is also adopted by a large community along the world. Another tool that allows automatic assessment within Moodle is the iAssign package. iAssign provides means for integrating interactive Learning Modules (iLM) to Moodle, empowering it with interactive intense activities concerning specific issues implemented in iLM. However, such tools present some limitations that prevent their users to take more benefit of the question types and iLM such as (i) authoring is not a simple task; (ii) iAssign integrates iLM to Moodle without incorporating Moodle questionnaires; (iii) Quiz database for questions and questionnaires is not integrated to a repository with search and retrieving tools; (iv) in the current version of Moodle, Quiz didn't allows the incorporation or exportation of assessment content that follows the IMS-QTI 2.1 (Question and Test Interoperability) specification. In this paper we address such limitations proposing a generic model and its implementation for the Moodle system.","0190-5848;01905848","Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4","10.1109/FIE.2013.6684834","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684834","IMS-QTI 2.1;Moodle;Quiz;assessment;iAssign;repository","Least squares approximations;Multimedia communication","computer aided instruction;database management systems;formal specification;information retrieval;interactive systems;open systems;public domain software;teaching","IMS-QTI 2.1 specification;Moodle Quiz database;automatic assessment;iAssign;iAssign package;iLM;iQuiz;interactive intense activities;interactive learning modules;learning assessment;open source system;question and test interoperability specification;retrieving tool;search tool;teaching","","5","","22","","","23-26 Oct. 2013","","IEEE","IEEE Conference Publications"
"Evaluation of different audio features for musical genre classification","B. K. Baniya; D. Ghimire; J. Lee","Division of Computer Engineering, Chonbuk National University, Republic of Korea","SiPS 2013 Proceedings","20131202","2013","","","260","265","Musical genre classification is an important issue for the music information retrieval system. There are two essential components for music genre classification, which are audio features and classifier. This paper considers various kinds of the features for genre classification related with dynamics, rhythm, spectral, and tonal characteristics of music. In the paper up to the 4th order central moments for different features are considered to evaluate the overall classification accuracy. In addition, Extreme Learning Machine (ELM) with bagging is introduced and compared with well-known Support Vector Machines (SVM) in terms of the overall classification accuracy. Based on the aforementioned features sets and ELM classifier, experiments are performed with well-known datasets: GTZAN with ten different musical genres. Through the experiments we found that some type of features is more important to others and the two classifiers provide comparable results for genre classification.","2162-3562;21623562","Electronic:978-1-4673-6238-2","10.1109/SiPS.2013.6674516","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6674516","ELM with bagging;Musical genre classification;SVM;audio features","Accuracy;Bagging;Equations;Feature extraction;Standards;Support vector machines;Training","audio signal processing;information retrieval;music;signal classification;support vector machines","4th order central moments;ELM;GTZAN;SVM;audio features;extreme learning machine;music information retrieval system;musical genre classification;support vector machines","","2","","14","","","16-18 Oct. 2013","","IEEE","IEEE Conference Publications"
"Research of vertical search engine in news industry","M. Li; X. J. Gu; Z. X. Yang","Lishui Univ., Lishui, China","2012 International Symposium on Management of Technology (ISMOT)","20131209","2012","","","253","256","In summing up the existing network of reptiles, and full-text retrieval based on theoretical knowledge, conducted a Web crawler optimization algorithm so that it can adapt to the needs of vertical search engines, and then sub-word component of Pango and Lucene.Net build an efficient full-text search functions. The innovation of the paper is the analysis of the characteristics of news sites to integrate its features into the traditional vertical search engines. News site on the information requirements for the characteristics of the network by studying the relevant full-text search framework to multithreaded data collection and retrieval of the vertical search engine, performance and user experience goals are to achieve abetter. The entire system by small and medium news site commissioning tests designed to meet the test show that the crawlers can adapt to the new network news industry efficient and timely collection requirements, Lucene.Net segmentation. The integration of Pango built the content for news and information Full-text retrieval system can achieve the accuracy of search engine queries for information and efficient response time demands, thereby increasing the amount of information and user experience.","","Electronic:978-1-4673-4593-4; POD:978-1-4673-4591-0","10.1109/ISMOT.2012.6679470","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679470","Lucene;Pangu word;full-text search;vertical search engine","","information retrieval;search engines","Lucene;Pango;Web crawler optimization algorithm;full-text retrieval;full-text retrieval system;full-text search framework;multithreaded data collection;theoretical knowledge;vertical search engine;vertical search engines","","0","","9","","","8-9 Nov. 2012","","IEEE","IEEE Conference Publications"
"Towards an Agent-Based Approach for Automatic Generation of Researcher Profiles Using Multiple Data Sources","O. Holanda; E. Elias; E. Costa; B. Fonseca; I. I. Bittencourt","Comput. Inst., Fed. Univ. of Alagoas, Maceio, Brazil","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","3","","163","166","In this paper, we study the problem concerning the automatic generation of user profiles as a function of multiple data sources. Hence, we introduce an agent-based approach for collecting and integrating data from multiple sources, such as Linked In, Google Scholar and Lattes, in order to automatically generate researcher profiles. We performed a simple evaluation involving seven users who have data in all the used sources. Preliminary, the results show the feasibility of our approach.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.174","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690719","Information Extraction;Intelligent Agents;Ontologies;Researcher Profile","Books;Data mining;Databases;Education;Google;LinkedIn;Ontologies","data integration;information retrieval;multi-agent systems;ontologies (artificial intelligence);social networking (online)","Google Scholar;Lattes;Linked In;agent-based approach;data collection;data integration;multiple data sources;researcher profiles generation;user profiles generation","","0","","6","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Data architecture for telehealth services research: A case study of home tele-monitoring","S. Nepal; J. Jang-Jaccard; B. Celler; B. Yan; L. Alem","Comput. Inf., CSIRO, Sydney, NSW, Australia","9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20131212","2013","","","458","467","Telehealth services research projects often require to access a variety of data sources under different data access policies and privacy constrains. There is a need to link these clinical and administrative records from different data custodians and produce a research data for analytics. One of the challenges is that the research data must meet the data access policies and privacy constraints of all data custodians participating in the project. These data custodians often operate in different jurisdictions. In this paper, we present our practical experience through the design and implementation of a service-oriented data architecture for extracting research data for telehealth services research in the context of a tele-home monitoring project. This project is being carried out at six locations in five different states in Australia. Each site represents a different model of care for the management of chronic disease in the community ranging from community-based, nurse-led programs to a hospital-focused, chronic-disease management program. The aims of this project are wide ranging and investigate many aspects of deploying at home telehealth services to better manage chronic disease. This paper however focuses on data architecture. We highlight the underlying issues, our experience and explain a practical health data linkage protocol adopted in the project. We also explain the features of the research data service portal in operation.","","Electronic:978-1-936968-92-3; POD:978-1-4799-2754-8; USB:978-1-936968-91-6","10.4108/icst.collaboratecom.2013.254220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680013","data architecture;data privacy;data services;health data linkage;telehealth research data portal","Communities;Data models;Data privacy;Ethics;Hospitals;Protocols","data privacy;diseases;electronic health records;information retrieval;portals;protocols;service-oriented architecture;telemedicine","Australia;administrative records;chronic-disease management program;clinical records;community-based management program;data custodians;data source access policies;home telemonitoring;hospital-focused management program;nurse-led programs;practical health data linkage protocol;privacy constrains;research data extraction;research data service portal;service-oriented data architecture;tele-home monitoring project;telehealth service research project","","1","","34","","","20-23 Oct. 2013","","IEEE","IEEE Conference Publications"
"Secure sharing of financial records with third party application integration in cloud computing","K. Kanagasabapathi; S. Balaji","Akshaya Coll. of Eng. & Technol., Coimbatore, India","2013 International Conference on Current Trends in Engineering and Technology (ICCTET)","20131202","2013","","","418","420","The Cloud has become a new vehicle for delivering resources such as computing and storage to customers on demand. It is to make use of computing resources that are delivered as a service over a network. In this paper, we propose a generalized model of record sharing in cloud computing using attribute based encryption (ABE). To divide the users in the system into multiple security domains that greatly reduces the key management complexity and supports on-demand user revocation. This concept is implemented for financial organization. However, now financial organizations face the issue of security due to the increased number of data leaks unless there is an adoption of an Attribute Based Encryption technique to keep the data protected. Searching is very difficult in encrypted cloud data. Therefore to adopt the hash based searching technique to retrieve the encrypted information in cloud. This work also proposes to integrate the third party application with financial organization. The implementation run and analysis shows that the proposed approach is highly efficient and secure under existing security models.","","Electronic:978-1-4799-2584-1; POD:978-1-4799-2585-8","10.1109/ICCTET.2013.6676002","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676002","Cloud Computing;Data sharing;Financial Organization;access control;attribute-based encryption;on-demand revocation","","cloud computing;cryptography;financial data processing;information retrieval","ABE;attribute based encryption;cloud computing;computing resources;data protection;encrypted information retrieval;financial organization;hash based searching technique;key management complexity;on-demand user revocation;resource delivery;secure financial records sharing;security models;third party application integration","","1","","2","","","3-3 July 2013","","IEEE","IEEE Conference Publications"
"A GPU-Accelerated Large-Scale Music Similarity Retrieval Method","L. Xiao; Y. Zheng; W. Tang; G. Yao; L. Ruan; X. Wang","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","1839","1843","High-quality content-based music similarity retrieval methods are non-vectorial and use non-metric divergence measures, which prevents the expansion of music recommendation systems. We presents a GPU-based method to speed up content-based music similarity search in large-scale collections, in order to improve the response speed without reducing retrieval accuracy. The method also introduce an optimization technique based on memory layout to improve memory access. The efficiency of our method is validated through extensive experiments. Evaluation results show that our single GPU implementation achieves 10x speedup ratio on NVIDIA GTX480, when compared to a typical general purpose CPU's execution time.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.341","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682352","CUDA;GPU;music recommendation;music similarity retrieval","Acceleration;Fluctuations;Graphics processing units;Instruction sets;Music;Optimization;Recommender systems","content-based retrieval;graphics processing units;information retrieval;music;optimisation;recommender systems","GPU-based method;NVIDIA GTX480;high-quality content-based music similarity retrieval;large-scale music similarity retrieval;memory access;memory layout;music recommendation system;nonmetric divergence measures;optimization technique","","1","","16","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"A Novel Private Cloud Document Archival System Architecture Based on ICmetrics","H. Tahir; R. Tahir; K. McDonald-Maier","Dept. of Comput. Eng., Mohammad Ali Jinnah Univ., Islamabad, Pakistan","2013 Fourth International Conference on Emerging Security Technologies","20131212","2013","","","102","106","This paper proposes a novel architecture that offers features for a truly interactive yet private document archival cloud. The proposed secure document archival system has been specifically designed for the elimination of paper based systems in organization. The architecture attempts to shift paper based systems to an electronic archival system where the documents are secure and the system is supportive to user demands and features. Therefore besides document archival the proposed architecture also offers the authentication and encryption for secure keeping of data on the cloud. We propose a novel combination of SSL coupled with ICMetrics for the document archival system, which will prove to be a very valuable tool for encrypting and authenticating in the cloud world.","","Electronic:978-0-7695-5077-0; POD:978-1-4799-0999-5","10.1109/EST.2013.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680197","Cloud computing architecture;ICMetrics;SSL certificates;document archival system","Authentication;Cloud computing;Computer architecture;Cryptography;Feature extraction;Hardware","cloud computing;cryptography;document handling;information retrieval systems","ICmetrics;SSL;Secure Sockets Layer;data security;electronic archival system;encryption;private cloud document archival system architecture","","3","","13","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"An IO Optimized Data Access Method in Distributed Key-Value Storage System","L. Chao; W. Guangjun; W. Shupeng; L. Yixi","Inst. of Comput. Technol., Beijing, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","1276","1281","Distributed KEY-VALUE storage system is a new storage framework for cloud computing. It can enable an application to dynamically adapt to growing workloads by increasing the number of servers. However, current distributed KEY-VALUE storage systems are still inefficient on range query for larger result set. When the result set become large, the file layout, cache hit rate are both key points for IO efficiency. In this paper, we will introduce our experience under the development of China Mobile Big Cloud KEY-VLAUE DB (BC-kvDB). We will discuss how we increase IO efficiency in BC-kvDB. BC-kvDB is based on single-table space data model and provides SQL-LIKE DDL and DML language. BC-kvDB's high throughput is benefit from data locality storage, column-storage structure and multi-layer caches. Data can be accessed in local cache or local blocks through block index. Experimental results show that the random writing performance of BC-kvDB is 2.5 times better than HBase and the random reading performance is 1.8-2 times than HBase.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682235","Cloud Storage;Column Store;Distributed Storage;KEY-VALUE DB","Data handling;Data models;Data storage systems;Indexes;Information management;Servers;Throughput","Big Data;SQL;cache storage;cloud computing;information retrieval;query processing","BC-kvDB;China mobile big cloud KEY-VLAUE DB;DML language;IO efficiency;IO optimized data access method;SQL-LIKE DDL;block index;cache hit rate;cloud computing;column-storage structure;data access;data locality storage;distributed KEY-VALUE storage system;file layout;local blocks;local cache;multilayer caches;random writing performance;range query;single-table space data model;storage framework","","0","","10","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"UAI-IOT Framework: A Method of Uniform Interfaces to Acquire Information from Heterogeneous Enterprise Information Systems","H. Li; Y. Tian; Y. Liu; T. Li; W. Mao","Comput. Network Inf. Center, Beijing, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","724","730","Enterprise information systems store great amount of information, which describes details of the objects in Internet of things. Due to these complex systems lack of uniform interface, users could hardly get the valuable information. We propose the UAI-IOT framework, which provides a method of uniform interfaces to actively acquire information from heterogeneous enterprise systems. To be compatible with these systems, the framework obtains information from log files and transformed the log information into semantic events, which could be acquired by any user through the interfaces of information service. Besides, integrating UAI-IOT framework and RNS would make contribution to share information among organizations. We also introduce a use case and make a comprehensive discussion.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.131","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682145","Internet of things;heterogeneity;information acquiring;log","Business;Generators;Information services;Information systems;Monitoring;Object recognition;Semantics","Internet of Things;business data processing;information retrieval;information systems","Internet of Things;UAI-IOT framework;heterogeneous enterprise information systems;information acquisition;information sharing;method-of-uniform interfaces;semantic events","","1","","20","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Do Health Care Users Think Electronic Health Records are Important for Themselves and Their Providers? Exploring Group Differences in a National Survey","D. L. Anthony; C. Campos-Castillo","Dept. of Sociology, Dartmouth Coll., Hanover, NH, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","141","146","Patient access to electronic health records (EHR) is expected to have a variety of benefits, including enhanced patient involvement in care and access to health information, yet little is known about potential demand. We used the 2007 Health Information and National Trends Survey, a national probability-based survey, to determine which health care users with Internet access are likely to report that electronic access to their health records is important for themselves and their providers. Respondents who represent populations that generally experience health and healthcare disparities (Blacks, Latina/os, and patients with psychological distress) were among the most likely to report that the EHR was very important for them. Women were less likely than men to deem the EHR very important for their providers, but it is unclear if this is due to men's higher technology adoption rates. All findings were consistent, even after controlling for respondents' socio-economic status, health status, health care context, and disposition toward health information. Health policies and the designs of EHRs should consider these patterns, which could help address health and health care disparities.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680471","electronic health records;health disparities;patient involvement;patient knowledge","Context;Educational institutions;Insurance;Internet;Medical services;Psychology;Sociology","Internet;electronic health records;health care;information retrieval","EHR;Health Information and National Trends Survey;Internet access;electronic access;electronic health records;health care users;health information;health status;healthcare disparities;national probability-based survey;patient involvement;psychological distress;socioeconomic status;technology adoption rates","","0","","25","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Integrating Microblogging Into Domain Specific Language Editors","O. Díaz; C. Arellano","Comput. Sci. Dept., ONEKIN Group, Univ. of the Basque Country, San Sebastian, Spain","2013 International Conference on Cloud and Green Computing","20131219","2013","","","219","225","Micro logging is emerging as a suitable means for question-answering in working settings. This leads to different efforts to seamlessly integrate microblogging into the daily-used tools. Specifically, microblogging is being regarded as particularly useful during software development, akin to the tradition of Q&A forums. This paper looks at a particular kind of software: the one being developed by domain experts through the use of Domain Specific Languages (DSLs). We believe this setting is specially amenable to benefit from Q&A microblogging due to inherent limitations of the target audience. This brings the twist of domain specific ness into microblogging, i.e. the Q&A process is now framed by the semantics of the DSL constructs. This permits the introduction of editing assistants that embed domain knowledge about the kind of questions that can be posed, and the way answers can be selected. This opens an opportunity for more focused and assisted microblogging. This paper introduces Crowd Call, an in place microblogging mediator for DSL editors. The aim is to make microblogging a natural gesture during the conception of the DSL expressions, making transparent the interplay between the DSL editor and the Social Networking Services. In addition, Crowd Call can be configured to the constructs and resolution strategies of the DSL at hand so that questions and answers are framed by the semantics of the DSL. The approach is illustrated for three DSLs: the Google Spreadsheets formula language, SQL and Sticklet. We show how Crowd Call-mediated microblogging is tuned for the semantics of each DSL.","","Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6","10.1109/CGC.2013.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686034","Domain Specific Languages;Microblogging;Social Networking Services","Communities;Computer languages;DSL;Google;Semantics;Social network services;Software","SQL;question answering (information retrieval);social networking (online);specification languages;text editing","CrowdCall-mediated microblogging;DSL constructs;DSL editors;DSL expressions;DSL semantics;Google Spreadsheets formula language;Q and A microblogging;Q and A process;SQL;Sticklet;domain experts;domain knowledge;domain specific language editors;inplace microblogging mediator;question answering;resolution strategies;social networking services;software development","","0","","24","","","Sept. 30 2013-Oct. 2 2013","","IEEE","IEEE Conference Publications"
"Resource Annotation, Dissemination and Discovery in the Semantic Web of Things: A CoAP-Based Framework","M. Ruta; F. Scioscia; A. Pinto; E. Di Sciascio; F. Gramegna; S. Ieva; G. Loseto","DEI, Politec. di Bari, Bari, Italy","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","527","534","The Semantic Web of Things (SWoT) vision aims to provide more advanced resource management and discovery w.r.t. standard Internet of Things architectures, by means of the integration of knowledge representation and reasoning techniques originally devised for the Semantic Web. This paper proposes a novel SWoT framework, based on a backward-compatible extension of the Constrained Application Protocol (CoAP), supporting non-standard inference services for semantic matchmaking. It allows retrieval and logic-based ranking of annotated resources. A computationally efficient data mining is also integrated in the framework to process raw data gathered from the environment in order to detect high-level events and characterize them with machine-understandable metadata. In order to test the effectiveness of the proposed approach, a case study about environmental risk prevention for Vehicular Ad-hoc Networks (VANETs) is presented.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682117","CoAP;Data mining;Matchmaking;Resource discovery;Semantic Web of Things","Data mining;Ontologies;Protocols;Semantics;Sensor phenomena and characterization;Standards","Internet of Things;data mining;inference mechanisms;information retrieval;knowledge representation;protocols;semantic Web","CoAP-based framework;Internet of Thing architectures;SWoT framework;VANETs;backward-compatible extension;constrained application protocol;data mining;environmental risk prevention;high-level event detection;knowledge representation;logic-based ranking;machine-understandable metadata;nonstandard inference services;raw data process;reasoning techniques;resource annotation;resource discovery;resource dissemination;resource management;semantic Web of Things;semantic matchmaking;vehicular ad-hoc networks","","5","","28","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Improving Online Clinical Trial Search Efficiency Using Natural Language Processing and Biomedical Ontology Mapping Approach","D. H. Wei; T. Campbell","Comput. Sci. & Inf. Syst. - BUSN, Richard Stockton Coll. of New Jersey, Galloway, NJ, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","485","485","Summary form only given. With the increasing demand for healthcare, computer technology and the internet are playing a more important role for patients, practitioners, and researchers. Oftentimes the process of seeking or providing care does not start in a waiting room or in a doctor's office, but online. Because of this, special attention must be concentrated in increasing the efficiency of online search engines so that the rest of the care process may also run smoothly. This research proposes a solution to improve the search efficiency for patients using natural language processing and SNOMED mapping techniques. In this research, clinical trials are extracted from ClinicalTrials.gov and n-gram method is applied to process the clinical trial contents. The processed terms are then mapped to SNOMED terms and a covariance matrix is formulated with the Jaccard similarity coefficient measuring similarity between a pair of clinical trials. Based on the similarity measures, the most relevant clinical trials are extracted for the searcher's needs. In the end, a comparative study is conducted to prove the enhancement of the search efficiency. In conclusion, the combination of n-gram model and SNOMED terminology mapping to process the clinical trial contents is proved to improve the efficiency for the online search of the clinical trials. Future research with clinical trials will use multiple methods such as ontological and statistical approaches to improve the precision and recall of the search results. Another interesting next step may be to explore clustering by analyzing the correlation structure of the clinical trial contents.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680521","","Biomedical measurement;Clinical trials;Conferences;Educational institutions;Informatics;Natural language processing","Internet;information retrieval;medical information systems;natural language processing;ontologies (artificial intelligence);pattern clustering;search engines;statistical analysis","ClinicalTrials.gov;Internet;Jaccard similarity coefficient;SNOMED mapping techniques;SNOMED terminology mapping;biomedical ontology mapping approach;clinical trial contents;clustering;computer technology;correlation structure analysis;doctors office;healthcare;n-gram method;natural language processing;online clinical trial search efficiency;online search engines;statistical approaches","","0","","","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Latent Co-development Analysis Based Semantic Search for Large Code Repositories","R. Venkataramani; A. Asadullah; V. Bhat; B. Muddu","Int. Inst. of Inf. Technol., Bangalore, India","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","372","375","Distributed and collaborative software development has increased the popularity of source code repositories like GitHub. With the number of projects in such code repositories exceeding millions, it is important to identify the domains to which the projects belong. A domain is a concept or a hierarchy of concepts used to categorize a project. We have proposed a model to cluster projects in a code repository by mining the latent co-development network. These identified clusters are mapped to domains with the help of a taxonomy which we constructed using the metadata from an online Question and Answer (Q&A) website. To demonstrate the validity of the model, we built a prototype for semantic search on source code repositories. In this paper, we outline the proposed model and present the early results.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676910","Human aspects of software evolution;Software repository analysis and mining;semantic search;source code repositories","Communities;Data mining;Electronic publishing;Information services;Semantics;Software;Taxonomy","Web sites;data mining;distributed databases;groupware;meta data;question answering (information retrieval);software engineering;ubiquitous computing","GitHub;collaborative software development;concept hierarchy;distributed software development;latent co-development analysis based semantic search;latent co-development network mining;metadata;online question-and-answer Web site;project categorization;semantic search;source code repositories","","0","","11","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
"Service Discovery Method Based on User Intent","Y. Kataoka; T. Watanabe; K. Tanaka; S. Higashino","NTT Service Evolution Labs., Nippon Telegraph & Telephonecorporation, Yokosuka, Japan","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","473","480","This paper introduces a method that supports a user who has only a vague idea of service use in discovering suitable mobile applications. The proposal allows the user to add application functions after selecting the purpose of service use from a list of novel application functions. The proposal is based on our concept of ISHI (Intent of Service and Human Interface) which works as an interface between service and user. ISHI can automatically determine the importance level and the novelty level of an application from various data. The proposed method is based on not only natural language but also the structured data of mobile applications. User experiments show that the proposed method yields, compared to the conventional method, better performance with respect to the time duration of application search and the frequency of recourse to new service functions.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690053","information recommendation;machine learning;service discovery;service indexing","Androids;Humanoid robots;Indexing;Mobile communication;Natural languages;Silicon;Vectors","human factors;information retrieval;mobile computing;natural languages;recommender systems","ISHI;application functions;application search;importance level;intent of service and human interface;mobile applications;natural language;novelty level;recourse frequency;service discovery method;structured data;time duration;user experiments;user intent","","0","","11","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Towards an Expressive and Scalable Twitter's Users Profiles","J. Subercaze; C. Gravier; F. Laforest","Telecom St.-Etienne, St.-Etienne, France","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","101","108","Microblogging websites such as Twitter produce tremendous amount of data each second. Consequently, real-time recommendation systems require very efficient algorithm to quickly proceed this massive amount of data. In this paper we propose a scalable and extensible way of building content-based user profiles. Scalability refers to the relative complexity of algorithms involved in building the users profiles with respect to state-of-the-art solutions. Extensibility consider avoiding to recompute the model for newcomers. We present a tractable algorithm to build user profiles out of their tweets. Our model is a graph of terms cooccurency, driven by the fact that user sharing similar interests will share similar terms. We then present how this model can be encoded as a binary footprint, hence boosting comparison of users. We provide an empirical study to measure how the distance between users in the hash space differs from distance between users using standard Information Retrieval techniques. This experiment is based on a Twitter dataset we crawled, and represents 25K users and 1 million tweets. Our approach is driven by real-time analysis requirements and is thus oriented on a trade-off between expressivity and efficiency. Experimental results shows that our approach outperforms vector space model by three orders of magnitude, with a precision of 58%.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690000","recommendation;twitter","Buildings;Collaboration;Recommender systems;Sparse matrices;Twitter","data handling;graph theory;information retrieval;recommender systems;social networking (online)","Twitter dataset;binary footprint;content-based user profiles;expressive Twitter user profiles;hash space;microblogging Web sites;real-time analysis requirements;real-time recommendation systems;scalable Twitter user profiles;standard information retrieval techniques;terms cooccurency graph;tractable algorithm","","2","","37","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Extraction of Linked Data Triples from Japanese Wikipedia Text of Ukiyo-e Painters","F. Kimura; K. Mitsui; A. Maeda","Kinugasa Res. Organ., Ritsumeikan Univ., Kinugasa, Japan","2013 International Conference on Culture and Computing","20131212","2013","","","192","193","DBpedia provides Linked Data extracted from info boxes in Wikipedia articles. Extraction is easier from an infobox than from text because an info box has a fixed-format table to represent structured information. To provide more Linked Data, we propose a method for Linked Data triple extraction from Wikipedia text. In this study, we conducted an experiment to extract Linked Data triples from Wikipedia text of ukiyo-e painters and achieved precision of 0.605.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680374","Linked Data;Wikipedia;triple","Data mining;Educational institutions;Electronic publishing;Encyclopedias;Internet;Painting","Web sites;information retrieval;natural language processing;text analysis","DBpedia;Japanese wikipedia text;Linked Data triple extraction;fixed-format table;infobox;structured information representation;ukiyo-e painters;wikipedia articles","","0","","4","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"The GMES Space Component Data Access System: Harmonizing Earth Observation Products Flows and Services for GMES","A. Tassa; G. Vingione; R. Knowelden; J. Martin; G. Ottavianelli; C. Lopes; O. Barois; P. Mougnaud; V. Amans; E. Monjoux","EOP-G, Eur. Space Agency, Frascati, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20131219","2014","7","1","3","16","The Global Monitoring for Environment and Security (GMES) Space Component relies on a constellation of dedicated space missions (called the Sentinels, scheduled for first launch by 2013), as well as on a set of space missions not dedicated to GMES but contributing to it. In charge of optimizing access and usability of the available space-based remote sensing resources for the pre-operations, the European Space Agency has elaborated the GMES Space Component Data Access concept, implementing mechanisms for aligning the data access across the GMES contributing missions, harmonising user information services, ordering and data delivery processes, and monitoring the data and service quality. Overall, this is a large-scale integrated system of systems, involving today more than 10 different ground segments and 20 space missions, serving users according to different modes of operations derived from pre-identified users' needs.","1939-1404;19391404","","10.1109/JSTARS.2013.2252328","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6507319","Data handling systems;earth observing system;interoperability","Earth;Europe;Monitoring;Satellites;Security;Space missions;Standards","data handling;geophysical techniques;information resources;information retrieval;remote sensing","Earth observation product flow;Earth observation service;European Space Agency;GMES Space Component Data Access System;Global Monitoring for Environment and Security;Sentinels;access optimization;data access;data delivery process;data monitoring;ground segments;service quality;space missions;space-based remote sensing resource;usability optimization;user information service","","1","","36","","20130423","Jan. 2014","","IEEE","IEEE Journals & Magazines"
"Rated Tags: Adding Rating Capability to Collaborative Tagging","D. Kailer; P. Mandl; A. Schill","Dept. of Comput. Sci. & Math., Munich Univ. of Appl. Sci., Munich, Germany","2013 International Conference on Cloud and Green Computing","20131219","2013","","","249","255","Collaborative tagging is a popular way to organize content. But sometimes it is also used as a way to express opinions, which is normally done through rating- or text review systems. This paper will demonstrate that there is a gap between rating- and text review systems, that limits the ability of users to find the desired item. A novel concept based on collaborative tagging is presented, which is designed to bridge this gap. This concept, named Rated Tags, uses a hybrid approach to combine tagging-, rating- and review functionality. A key element of Rated Tags is the combination of a traditional user generated tag and a 5-star rating scale. Such a tag is called rated tag. We will discuss the design and the challenges of Rated Tags in detail and demonstrate its application based on a prototype. Furthermore we will show that similar related works suffer from an ambiguity problem, which was avoided in Rated Tags.","","Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6","10.1109/CGC.2013.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686038","collaborative tagging;rated tags;ratings;reviews;social tagging;tags","Bridges;Cameras;Collaboration;Decision making;Headphones;Prototypes;Tagging","collaborative filtering;information retrieval;text analysis","5-star rating scale;collaborative filtering;collaborative tagging;rated tags;rating functionality;rating review systems;review functionality;text review systems","","0","","20","","","Sept. 30 2013-Oct. 2 2013","","IEEE","IEEE Conference Publications"
"Bring best of two worlds in a software engineering class, student outcomes of Accreditation Board of Engineering and Technology (ABET) and information literacy standards of Association of College amp; Research Libraries (ACRL)","A. Naz; M. Casto","Comput. Sci. & Inf. Syst. Dept., West Virginia Univ. Inst. of Technol., Montgomery, WV, USA","2013 IEEE Frontiers in Education Conference (FIE)","20131219","2013","","","80","86","In this collaborative project with librarians, the faculty members of Nelson College of Engineering, West Virginia University Institute of Technology (WVU Tech) identify the importance of information literacy (IL) in accreditation documents and then leverage accreditation guidelines from Accreditation Board of Engineering and Technology (ABET) student outcomes to integrate IL skills from standards of the Science and Technology Section (STS) of the Association of College and Research Libraries (ACRL) into an existing software engineering course. This project was sponsored by West Virginia University Information Literacy Course Enhancement Grant. We included the full range of the university libraries' resources, expertise, and services in course planning and delivery by incorporating different IL units: an introductory talk, special workshops, library sessions and consultations with the librarian throughout the semester. As conducting researches, designing projects, and writing reports are most effective vehicles for students' learning of strategic and rigorous information retrieval and management, the class includes a group software-development project and an individual paper-writing project. We included different data collection and evaluation methods distributed throughout the semester, including Standardized Assessment of Information Literacy Skills (SAILS), a web-based tool to document IL skill levels and to pinpoint areas for improvement.","0190-5848;01905848","Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4","10.1109/FIE.2013.6684793","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684793","ABET outcome;ACRL Standards;Information Literacy;Software Engineering","Accreditation;Educational institutions;Ethics;Libraries;Software engineering;Standards;Teamwork","Internet;academic libraries;accreditation;computer aided instruction;computer science education;educational courses;information management;information retrieval;information science education;research libraries;software engineering","ABET student;ACRL;Accreditation Board of Engineering and Technology student;Assessment of Information Literacy Skills;Association of College and Research Libraries;IL skill levels;IL skills;IL units;Nelson College of Engineering;SAILS;STS;Science and Technology Section;WVU Tech;Web-based tool;West Virginia University Information Literacy Course Enhancement Grant;West Virginia University Institute of Technology;accreditation documents;collaborative project;course planning;faculty members;group software development project;introductory talk;leverage accreditation guidelines;library sessions;paper-writing project;project design;report writing;software engineering class;software engineering course;special workshops;strategic information management;strategic information retrieval;student outcomes;university library resources","","0","","11","","","23-26 Oct. 2013","","IEEE","IEEE Conference Publications"
"Visual Analytics for Big Data Using R","A. Nasridinov; Y. H. Park","Dept. of Multimedia Sci., Sookmyung Women's Univ., Seoul, South Korea","2013 International Conference on Cloud and Green Computing","20131219","2013","","","564","565","The growth in volumes of data has affected today's large organization, where commonly used software tools to capture, manage, and process the data cannot handle big data effectively. The main challenge is that organizations must analyze a large amount of big data and extract useful information or knowledge for future actions in a short time. This type of demands has produced the markets for various innovative big data control mechanisms, such as visual analytics for big data. In this paper, we propose to visually analyze the big data using R statistical software. The proposed method is composed of three steps. In the first step, we extract the data set from the target Web site. In the second step, we parse the extracted raw data according to the types, and store in a database. In the third, we perform visual analysis from the stored data in database using R statistical software.","","Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6","10.1109/CGC.2013.96","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686088","R statistical software;big data;visual analytics","Data handling;Data mining;Data storage systems;Information management;Organizations;Social network services;Software","Big Data;data visualisation;information retrieval;program compilers;statistical analysis","Big Data analysis;Big Data control mechanisms;R statistical software;Web site;data capturing;data management;data processing;data set extraction;extracted raw data parsing;information extraction;knowledge extraction;software tools;visual analytics","","0","","4","","","Sept. 30 2013-Oct. 2 2013","","IEEE","IEEE Conference Publications"
"Using Candidate Exploration and Ranking for Abbreviation Resolution in Clinical Document","J. B. Kim; H. S. Oh; S. S. Nam; S. H. Myaeng","Mobile Commun. Co., LG Electron. Inc., Seoul, South Korea","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","317","326","In biomedical texts, abbreviations are frequently used due to their inclusion of many technical expressions of some length. Accordingly, appropriate recognition of abbreviations and their full form pairs is an essential task in automatic text processing of biomedical documents. However, unlike the biomedical literature, clinical notes have many abbreviations without their full forms available in the text or without standard definitions in dictionaries due to the nature of the documents. This causes difficulties in adapting traditional approaches for abbreviation disambiguation such as classification among fixed candidates or pattern-based definition extraction. Because of this reason, we consider the task as search problem and propose an approach with two steps: a) exploring possible full form candidates from various resources and b) choosing most acceptable one among retrieved candidates by ranking. To discover full form candidates and their features, we exploited external academic resources such as MEDLINE and UMLS as well as the clinical note corpus itself. To rank the candidates properly based on human criteria, we adopted Rank Boost, one of the learning-to-rank models developed from information retrieval and machine learning communities. Experimental results show the suggested two-step approach is promising for this kind of task.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680492","Abbreviation Resolution;Learning to Rank;Medical Text Processing","Communities;Dictionaries;Medical diagnostic imaging;Standards;Text recognition;Unified modeling language;Vocabulary","Unified Modeling Language;dictionaries;document handling;information retrieval;learning (artificial intelligence);medical computing;medical information systems","MEDLINE;Rank Boost;UMLS;abbreviation disambiguation;abbreviation resolution;automatic text processing;biomedical documents;biomedical literature;biomedical texts;candidate exploration;clinical document;clinical note corpus;clinical notes;dictionary;external academic resources;human criteria;information retrieval;learning-to-rank models;machine learning community;pattern-based definition extraction;retrieved candidates;search problem","","1","","30","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Cocktail Approach for Travel Package Recommendation","Q. Liu; E. Chen; H. Xiong; Y. Ge; Z. Li; X. Wu","Sch. of Comput. Sci. & Technol., Univ. of Sci. & Technol. of China, Hefei, China","IEEE Transactions on Knowledge and Data Engineering","20131220","2014","26","2","278","293","Recent years have witnessed an increased interest in recommender systems. Despite significant progress in this field, there still remain numerous avenues to explore. Indeed, this paper provides a study of exploiting online travel information for personalized travel package recommendation. A critical challenge along this line is to address the unique characteristics of travel data, which distinguish travel packages from traditional items for recommendation. To that end, in this paper, we first analyze the characteristics of the existing travel packages and develop a tourist-area-season topic (TAST) model. This TAST model can represent travel packages and tourists by different topic distributions, where the topic extraction is conditioned on both the tourists and the intrinsic features (i.e., locations, travel seasons) of the landscapes. Then, based on this topic model representation, we propose a cocktail approach to generate the lists for personalized travel package recommendation. Furthermore, we extend the TAST model to the tourist-relation-area-season topic (TRAST) model for capturing the latent relationships among the tourists in each travel group. Finally, we evaluate the TAST model, the TRAST model, and the cocktail recommendation approach on the real-world travel package data. Experimental results show that the TAST model can effectively capture the unique characteristics of the travel data and the cocktail approach is, thus, much more effective than traditional recommendation techniques for travel package recommendation. Also, by considering tourist relationships, the TRAST model can be used as an effective assessment for travel group formation.","1041-4347;10414347","","10.1109/TKDE.2012.233","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365185","Travel package;cocktail;collaborative filtering;recommender systems;topic modeling","Collaboration;Companies;Data models;Educational institutions;Feature extraction;Mathematical model;Recommender systems","data mining;information retrieval;recommender systems;travel industry","TAST model;TRAST model;cocktail approach;latent relationship;online travel information;personalized travel package recommendation;topic distribution;topic extraction;topic model representation;tourist-area-season topic;tourist-relation-area-season topic","","20","","46","","20121129","Feb. 2014","","IEEE","IEEE Journals & Magazines"
"Privacy-Preserving Multi-Keyword Ranked Search over Encrypted Cloud Data","N. Cao; C. Wang; M. Li; K. Ren; W. Lou","Walmart Labs., Mountain View, CA, USA","IEEE Transactions on Parallel and Distributed Systems","20131125","2014","25","1","222","233","With the advent of cloud computing, data owners are motivated to outsource their complex data management systems from local sites to the commercial public cloud for great flexibility and economic savings. But for protecting data privacy, sensitive data have to be encrypted before outsourcing, which obsoletes traditional data utilization based on plaintext keyword search. Thus, enabling an encrypted cloud data search service is of paramount importance. Considering the large number of data users and documents in the cloud, it is necessary to allow multiple keywords in the search request and return documents in the order of their relevance to these keywords. Related works on searchable encryption focus on single keyword search or Boolean keyword search, and rarely sort the search results. In this paper, for the first time, we define and solve the challenging problem of privacy-preserving multi-keyword ranked search over encrypted data in cloud computing (MRSE). We establish a set of strict privacy requirements for such a secure cloud data utilization system. Among various multi-keyword semantics, we choose the efficient similarity measure of ""coordinate matching,"" i.e., as many matches as possible, to capture the relevance of data documents to the search query. We further use ""inner product similarity"" to quantitatively evaluate such similarity measure. We first propose a basic idea for the MRSE based on secure inner product computation, and then give two significantly improved MRSE schemes to achieve various stringent privacy requirements in two different threat models. To improve search experience of the data search service, we further extend these two schemes to support more search semantics. Thorough analysis investigating privacy and efficiency guarantees of proposed schemes is given. Experiments on the real-world data set further show proposed schemes indeed introduce low overhead on computation and communication.","1045-9219;10459219","","10.1109/TPDS.2013.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6674958","Cloud computing;keyword search;privacy-preserving;ranked search;searchable encryption","","cloud computing;cryptography;data privacy;information retrieval","Boolean keyword search;cloud computing;cloud data utilization system;commercial public cloud;coordinate matching measure;data management systems;data owners;data privacy;data search service;data utilization;encrypted cloud data;inner product similarity;multikeyword semantics;plaintext keyword search;privacy requirements;privacy-preserving multikeyword ranked search;searchable encryption;similarity measure;single keyword search","","100","","36","","","Jan. 2014","","IEEE","IEEE Journals & Magazines"
"Text analysis based on time series","I. Badea; S. Trausan-Matu","Dept. of Comput. Sci., Univ. Politeh. of Bucharest, Bucharest, Romania","2013 17th International Conference on System Theory, Control and Computing (ICSTCC)","20131219","2013","","","37","41","The paper presents a text mining application for searching and computing the correlations between the rhythmicities of terms with high frequency appearance, using the time series model. Some theoretical basics of the model are presented, including details about preprocessing the observed texts using natural language processing techniques, followed by implementation details and graphical results.","","CD-ROM:978-1-4799-2227-7; Electronic:978-1-4799-2228-4; POD:978-1-4799-2229-1","10.1109/ICSTCC.2013.6688932","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688932","Corpus;Natural Language Processing;Text Mining;Time Series;rhythmicity detection","Correlation;HTML;Natural language processing;Text mining;Time series analysis;Time-frequency analysis","data mining;information retrieval;natural language processing;text analysis;time series","correlation computation;correlation searching;graphical results;high frequency appearance;implementation details;natural language processing techniques;term rhythmicities;text analysis;text mining application;time series model","","0","","16","","","11-13 Oct. 2013","","IEEE","IEEE Conference Publications"
"Identifying Domain Experts in the Blogosphere -- Ranking Blogs Based on Topic Consistency","P. Berger; P. Hennig; C. Meinel","Hasso-Plattner-Inst., Potsdam, Germany","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","252","259","Current ranking algorithms, such as Page Rank, Technorati authority, and BI-Impact, favor blogs that report on a diversity of topics since those attract a large audience and thus more visitors, links, and comments. On the other side, niche blogs with a very specific topic only attract a small audience and thus have only a small reach. This results in a low ranking from today's blog retrieval systems. We argue that the consistency of a blog, i.e. how focused an author reports on a single topic, is a sign for expert knowledge. To find these blogs is particular important for other domain experts to identify blogs that they would like to follow and stay in active contact. To ease the retrieval of expert blogs, i.e. to separate them from the mass of blogs that report on random topics, we introduce a metric for blogs based on topic consistency. We divide the consistency ranking in four different aspects: (1) intra-post, (2) inter-post, (3) intra-blog, and (4) inter-blog consistency. By evaluating the metric with a test data set of 12,000 crawled blogs, we demonstrate the plausibility of our approach.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690023","Blog Analysis;Experts;Ranking;Social Media;Topic Consistency;Web Mining;Weblogs","Blogs;Joining processes;Market research;Measurement;Media;Publishing;Vectors","Web sites;data integrity;information retrieval;software metrics","blog consistency;blogosphere;crawled blogs;domain expert identification;expert blog retrieval systems;interblog consistency ranking;interpost consistency ranking;intrablog consistency ranking;intrapost consistency ranking;metric evaluation;test data;topic consistency","","1","","26","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Towards a Weighted Voting System for Q&A Sites","D. Romano; M. Pinzger","Software Eng. Res. Group, Delft Univ. of Technol., Delft, Netherlands","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","368","371","Q&A sites have become popular to share and look for valuable knowledge. Users can easily and quickly access high quality answers to common questions. The main mechanism to label good answers is to count the votes per answer. This mechanism, however, does not consider whether other answers were present at the time when a vote is given. Consequently, good answers that were given later are likely to receive less votes than they would have received if given earlier. In this paper we present a Weighted Votes (WV) metric that gives different weights to the votes depending on how many answers were present when the vote is performed. The idea behind WV is to emphasize the answer that receives most of the votes when most of the answers were already posted. Mining the Stack Overflow data dump we show that the WV metric is able to highlight between 4.07% and 10.82% answers that differ from the most voted ones.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676909","Metrics;Mining Repositories;Q&amp;A Sites;Social Coding;Social Media;Software Engineering;Stack Overflow","Communities;Data mining;Educational institutions;Knowledge discovery;Measurement;Software engineering","Web sites;data mining;question answering (information retrieval);software metrics","Q&A sites;knowledge repositories;question-answering Web sites;stack overflow data dump mining;weighted votes metric;weighted voting system","","0","","11","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
"Finding Information through Integrated Ad-Hoc Socializing in the Virtual and Physical World","C. v. d. Weth; M. Hauswirth","Digital Enterprise Res. Inst. (DERI), Nat. Univ. of Ireland, Galway, Ireland","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","37","44","Despite the services of sophisticated search engines, there are interesting information sources which are useful but largely inaccessible to current web users. These sources are often ad-hoc, location-specific and only useful for users over short periods of time, or relate to tacit knowledge of users or crowds. The solution presented in this paper introduces an integrated concept of ""location"" and ""presence"" across the physical and virtual worlds enabling ad-hoc socializing of users looking for similar information. While the definition of presence in the physical world is straightforward their definitions in the virtual world are neither obvious nor trivial. We provide an integrated spatial model spanning both worlds which enables us to define presence of users in a unified way. This integrated model allows us to enable ad-hoc socializing of users browsing the Web with users in the physical world specific to their joint information needs and allows us to unlock the untapped information sources mentioned above. We describe our proof-of-concept implementation and provide an empirical analysis based on real-world experiments.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689991","ad-hoc socializing;browser add-on;presence awareness;virtual location","Browsers;Collaboration;Context;Privacy;Search engines;Servers;Web pages","information retrieval;search engines;user interfaces","empirical analysis;information finding;information sources;integrated spatial model;location concept;physical world;presence concept;search engines;user ad-hoc socialization;virtual world","","0","","28","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"An Anomaly Intrusion Detection Method Based on PageRank Algorithm","Q. Qian; J. Li; J. Cai; R. Zhang; M. Xin","Sch. of Comput. Eng. & Sci., Shanghai Univ., Shanghai, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","2226","2230","Page Rank is the Google web page ranking algorithm which is based on web link analysis, and has been widely used in search engines, data mining, medicine analysis and many other fields. In this paper the improved Page Rank algorithm will be introduced into the short system-call sequences anomaly detection. There are four core steps to fulfill. Firstly, use a fixing length sliding window to split the target program system call sequences to create a short sequence pattern library. Then, use a pattern library to create a system call graph. Thirdly, use improved Page Rank algorithm to compute the weights between adjacent two nodes. Finally, the Hamming distance with the Page Rank weight to evaluate anomaly degree of different system calls. From the experiments, it shows that the Page Rank based anomaly detection is more stable than classical STIDE detection method.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.421","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682431","Anomaly detection;PageRank algorithm;STIDE method","Algorithm design and analysis;Computers;Data mining;Hamming distance;Hidden Markov models;Intrusion detection;Libraries","information retrieval;security of data","Google Web page ranking algorithm;STIDE detection method;Web link analysis;anomaly intrusion detection method;data mining;fixing length sliding window;improved Page Rank algorithm;medicine analysis;program system call sequences;search engines;sequence time-delay embedding;short sequence pattern library;short system-call sequences anomaly detection;system call graph","","0","","18","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Energy Consumption and Efficiency in Mobile Applications: A User Feedback Study","C. Wilke; S. Richly; S. Götz; C. Piechnick; U. Aßmann","Software Technol. Group, Tech. Univ. Dresden, Dresden, Germany","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","134","141","The energy efficiency of mobile applications has been a highly tackled research problem within the last years. Many research groups have focused on optimizing the hardware of mobile devices, as well as their middleware and applications, increasing both the devices' uptime and their users' satisfaction. However, only scarce work has analyzed whether users notice and care about energy-efficiency problems in mobile applications. Thus, in this paper, we address these questions by evaluating a large set of user comments extracted from the Google Play market place for Android applications. We analyze more than 9 million user comments and show that more than 18% of all commented applications have comments complaining about energy consumption. Besides, we identify major causes for the inefficiency of many mobile applications.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682059","Android;Energy consumption;Mobile devices;User Feedback;User Satisfaction","Androids;Batteries;Energy consumption;Games;Google;Mobile communication;Mobile handsets","customer satisfaction;energy conservation;energy consumption;information retrieval;middleware;mobile computing;operating systems (computers);optimisation;power aware computing","Android application;Google Play market place;energy consumption;energy efficiency;middleware;mobile application;mobile device hardware optimization;user comment extraction;user feedback;user satisfaction","","12","","11","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Acculturation of the Clothing Life in Japan Seen from Digital Archives of Dress, Fashion and Behavior","H. Takahashi","Fac. of Liberal Arts, Osaka-Shoin Women's Univ., Japan","2013 International Conference on Culture and Computing","20131212","2013","","","190","191","This paper describes the development and structure of the Digital Archives for fashion, dress, and behavior that is a collection of inter-connected databases and its use for the study of acculturation of the clothing life in Japan as an example.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680373","Acculturation;Behavior;Digital Archives;Dress;Fashion;Kimono;Westernization;clothing culture","Art;Clothing;Image databases;Libraries;Market research;Materials","clothing;database management systems;humanities;information retrieval systems","Japan;acculturation;behavior;clothing life;digital archives;dress;fashion;interconnected databases","","0","","2","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Empowering Collaborative Intelligence by the Use of User-Centered Social Network Aggregation","X. T. Vu; P. Morizet-Mahoudeaux; M. H. Abel","Heudiasyc, Univ. de Technol. de Compiegne, Compiegne, France","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","425","430","Online social networks (OSNs) such as Facebook, Twitter, Linked In, Google+, have become extremely popular and ubiquitous today. Users are actively connected to these services for creating and sharing contents and events with others, and, in some cases, this activity takes place in the scope of groups of interests. Therefore, from amongst the morass of data generated every day by users, a part of information may match the interests of certain groups. In practice, members are not all linked to each other via each OSN they are connected to. It is also not realistic to assume that each member can manually explore all others' social profiles to reach the information that may be relevant to their interests. Thus, there is a need for aggregating members' social streams on a single information support to collect relevant information, and, consequently, to promote collaborative knowledge-sharing. However, the disconnected nature of today social websites prevents a straightforward aggregation process. An efficient automated aggregation model is needed. We present, in this paper, the idea of empowering collaborative intelligence by the use of a user-centered approach for OSN aggregation. We illustrate the approach by a first experience to evaluate its impact on users information sharing and enrichment capabilities.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690046","collaborative intelligence;knowledge management;social networks;user profiles aggregation","Collaboration;Data models;Facebook;Information management;Media;Twitter","collaborative filtering;content management;information retrieval;social networking (online)","Facebook;Google+;Linked In;OSN aggregation;Twitter;automated aggregation model;collaborative intelligence;collaborative knowledge-sharing;content creation;content sharing;information sharing;member social stream aggregation;online social networks;relevant information collection;single information support;social Websites;social profiles;user-centered approach;user-centered social network aggregation","","2","","17","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Visual Typo Correction by Collocative Optimization: A Case Study on Merchandize Images","X. Y. Wei; Z. Q. Yang; C. W. Ngo; W. Zhang","College of Computer Science, Sichuan University, Chengdu, China","IEEE Transactions on Image Processing","20131220","2014","23","2","527","540","Near-duplicate retrieval (NDR) in merchandize images is of great importance to a lot of online applications on e-Commerce websites. In those applications where the requirement of response time is critical, however, the conventional techniques developed for a general purpose NDR are limited, because expensive post-processing like spatial verification or hashing is usually employed to compromise the quantization errors among the visual words used for the images. In this paper, we argue that most of the errors are introduced because of the quantization process where the visual words are considered individually, which has ignored the contextual relations among words. We propose a “spelling or phrase correction” like process for NDR, which extends the concept of collocations to visual domain for modeling the contextual relations. Binary quadratic programming is used to enforce the contextual consistency of words selected for an image, so that the errors (typos) are eliminated and the quality of the quantization process is improved. The experimental results show that the proposed method can improve the efficiency of NDR by reducing vocabulary size by 1000% times, and under the scenario of merchandize image NDR, the expensive local interest point feature used in conventional approaches can be replaced by color-moment feature, which reduces the time cost by 9202% while maintaining comparable performance to the state-of-the-art methods.","1057-7149;10577149","","10.1109/TIP.2013.2293427","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678261","Near-duplicate retrieval;binary quadratic programming;visual word quantization","Accuracy;Context modeling;Lips;Optimization;Quantization (signal);Visualization;Vocabulary","Web sites;electronic commerce;image processing;information retrieval;quadratic programming","binary quadratic programming;collocative optimization;color-moment feature;e-commerce Websites;expensive local interest point feature;general purpose NDR;merchandize images;near-duplicate retrieval;online applications;phrase correction;quantization errors;spatial verification;visual typo correction;visual words;vocabulary size","","3","","38","","20131203","Feb. 2014","","IEEE","IEEE Journals & Magazines"
"Using Co-occurrence Analysis to Expand Consumer Health Vocabularies from Social Media Data","L. Jiang; C. C. Yang","Coll. of Inf. Sci. & Technol., Drexel Univ., Philadelphia, PA, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","74","81","As health consumerism in the United States has remarkably risen over the past decade, more and more health consumers are actively seeking health related information on their own. However, health consumers' efforts in seeking healthcare information could be challenging due to the lack of professional knowledge in medicine, and the gap between health professional vocabulary and health consumer vocabulary is one of the biggest issues. It has long been recognized that consumers and health professionals often express health concepts in different ways and consumers usually find it difficult to understand some medical terminologies or express themselves using the professional language for information seeking. To address this problem, many researchers have been working on developing Consumer Health Vocabulary (CHV) to bridge the language gap. One crucial step in developing CHV is the identification of consumer health expressions. Most of the methods used by existing studies in identifying consumer health expressions involve human efforts, which is very time-consuming. In this study, we propose an automatic method to identify consumer health expressions from consumer-contributed content, which represent consumers' language the best. Co-occurrence analysis was used to identify terms that co-occur frequently with a set of seed terms. A corpus containing 120,393 discussion messages was used as a dataset and co-occurrence analysis was used to extract the most related consumer expressions. The experiment results demonstrated that the proposed method achieved a good performance in identifying consumer health expressions.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680463","Co-occurrence Analysis;Consumer Health Vocabulary;Social Media","Diseases;Drugs;Heart;Kidney;Message systems;Vocabulary","information retrieval;medical information systems;social networking (online)","CHV;United States;consumer health expressions;consumer health vocabularies;consumer-contributed content;cooccurrence analysis;health concepts;health consumerism;health professional vocabulary;health related information;information seeking;medicine;professional knowledge;professional language;social media data","","7","","28","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"CRLRM: Category Based Recommendation Using Linear Regression Model","G. Jain; N. Mishra; S. Sharma","Sch. of Inf. Technol., Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal, India","2013 Third International Conference on Advances in Computing and Communications","20131219","2013","","","17","20","A system that suggests list of most popular items to a set of users on the basis of their interest is named as recommendation system. Recommendation system filters the unnecessary information by applying knowledge discovery techniques for online users and has become the most powerful and admired tools in E-Business. ERPM is one of the easiest movie recommendation method, which overcomes the limitations of scalability and sparsity of recommendation system, but it generates predictions on the basis of probability model, which are less accurate and requires more time for calculations. This article presents a novel method named CRLRM (Category based Recommendation using Linear Regression Model) which is based on linear regression model that improves the prediction accuracy and speed up the calculations. Performance of proposed method is evaluated on the basis of MAE (Mean Absolute Error) comparison, and result obtained is far much better than ERPM and shows improvement in 30-40% of user ratings.","","Electronic:978-0-7695-5033-6; POD:978-1-4799-0854-7","10.1109/ICACC.2013.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686328","Collaborative Filtering;ERPM;MAE;Recommendation system;Regression model","Collaboration;Computational modeling;Equations;Filtering;Mathematical model;Motion pictures;Predictive models","data mining;electronic commerce;information retrieval;recommender systems;regression analysis","CRLRM;ERPM;MAE;category based recommendation;e-business;knowledge discovery;linear regression model;mean absolute error;movie recommendation method;probability model","","0","","9","","","29-31 Aug. 2013","","IEEE","IEEE Conference Publications"
"Recommender System Based on Social Trust Relationships","C. Chen; J. Zeng; X. Zheng; D. Chen","Dept. of Comput. Sci., Zhejiang Univ., Hangzhou, China","2013 IEEE 10th International Conference on e-Business Engineering","20131219","2013","","","32","37","The development of social network has increased the importance of social recommendation. However, social recommender systems have only recently been given research attention. Social relationships between users, especially trust relationships, can facilitate the design of social recommender systems. Such systems are based on the idea that users linked by a social network tend to share similar interests. Existing recommender approaches based on social trust relationships do not fully utilize such relationships and thus have low prediction accuracy or slow convergence speed. We propose a factor analysis approach that explicitly and implicitly uses social trust relationships simultaneously to overcome this limitation and fully utilize social trust relationships. Our approach combines the advantages of the existing two approaches, social recommendation using probabilistic matrix factorization and learning to recommend with social trust ensemble. Based on Epinions data sets, our approach has both significantly higher prediction accuracy and convergence speed than traditional collaborative filtering technology and state-of-the-art trust-based recommendation approaches.","","Electronic:978-0-7695-5111-1; POD:978-1-4799-1453-1","10.1109/ICEBE.2013.5","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686238","collaborative filtering;matrix factorization;recommender system;social network","Accuracy;Collaboration;Convergence;Recommender systems;Social network services;Training","information retrieval;matrix decomposition;probability;recommender systems;social networking (online)","factor analysis approach;probabilistic matrix factorization;recommender system;social network;social recommendation;social trust relationship","","6","","27","","","11-13 Sept. 2013","","IEEE","IEEE Conference Publications"
"Automatic Patient Search Using Bernoulli Model","Y. Gu; C. Kallas; J. Zhang; J. Marx; J. Tjoe","Dept. of Electr. Eng. & Comput. Sci., Univ. of Wisconsin-Milwaukee, Milwaukee, WI, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","517","522","Objective: Develop algorithms to automatically identify qualified patients for breast cancer clinical trials from free-text medical reports. Design: The Bernoulli model was trained to search for a qualified patient based on criterion. Measurement: The performance of the Bernoulli model was evaluated by the Precision-Recall curve and F-score. Results: The Single-word Bernoulli model trained in a two-class mode has greater performance than the model trained in a one-class mode. The performance of the model was also compared with some other techniques. Conclusions: The Bernoulli model method is easier to implement and performs better than several competing techniques.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.80","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680528","Automatic Patient Search;Bernoulli Model;Information Search;Sentence Model","Breast cancer;Clinical trials;Manuals;Training;Unified modeling language;Vectors","information retrieval;medical information systems;text analysis","F-score;automatic patient search;breast cancer clinical trials;free-text medical reports;precision-recall curve;qualified patient automatic identification;single-word Bernoulli model","","3","","10","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Building biomedical pipelines for large-scale sequencing analysis based on Galaxy and Cloud","B. Liu; J. Li; C. Liu","NEC Labs. China, Beijing, China","The 6th 2013 Biomedical Engineering International Conference","20131219","2013","","","1","5","With the widespread adoption of increasing and high-throughput sequencing data, the need for easy access to biomedical analysis tools, efficient data sharing and retrieval has presented significant challenges. Galaxy helps to address this problem by providing an open, Web-based platform for performing accessible and reproducible genomic analysis. To meet the needs for variable computing and storage resources, this paper deploys Galaxy on Cloud infrastructure for on-demand resources allocation, auto-scaling and pay-as-you-go pricing. We further extend Galaxy by complementing user-specific analysis functions, providing reliable and high-performance data transfer capabilities, and realizing Cloud-based distributed computing for Galaxy jobs. A biomedical pipeline and performance evaluation are presented to validate the effectiveness of our proposed approach.","","Electronic:978-1-4799-1467-8; POD:978-1-4799-1465-4","10.1109/BMEiCon.2013.6687676","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687676","Bioinformatics;Cloud computing;Galaxy;Sequencing analysis","Bioinformatics;Computer architecture;Genomics;Pipelines;Resource management;Sequential analysis;Software","Web services;bioinformatics;cloud computing;electronic data interchange;genomics;information retrieval;medical computing;pipeline processing;resource allocation;sequences","Cloud infrastructure;Cloud-based distributed computing realization;Galaxy method;Web-based platform;accessible genomic analysis;autoscaling;biomedical analysis tool access;biomedical pipeline building;data retrieval efficiency;data sharing efficiency;high-performance data transfer capability;high-throughput sequencing data;large-scale sequencing analysis;on-demand resource allocation;pay-as-you-go pricing;reliable data transfer capability;reproducible genomic analysis;user-specific analysis function","","0","","15","","","23-25 Oct. 2013","","IEEE","IEEE Conference Publications"
"Information Extraction for Computer Science Academic Rankings System","C. Shi; J. Quan; M. Li","Dept. of Comput. Sci. & Eng., Shanghai Jiao Tong Univ., Shanghai, China","2013 International Conference on Cloud and Service Computing","20140102","2013","","","69","76","Today the academic ranking for computer science is a hot and importmant problem. This paper introduces Computer Science Academic Rankings System (CSAR) which aims at academic information extracting, mining and ranking. In this paper we mainly present approaches for information extraction and normalization in CSAR. For semi-structured and unstructured web pages such as paper-view pages, we propose a method with natural language processing n-gram model and web grammar. We generate an optimal matching bipartite graph to extract authors and organizations information with maximum likelihood. CSAR also uses KM algorithm and Hungarian algorithm to find authors and emails correspondence. For information normalization, we introduce n-gram model, EM algorithm and trigram model with linear interpolation to construct part-of-speech tagger, with which to extract useful information from web source. Then TF-IDF model and string edit distance are applied to finish normalizing organization names. In experiment, our proposed approaches obtain high accuracy rate and great improvements of academic information extraction.","","Electronic:978-0-7695-5157-9; POD:978-1-4799-3937-4","10.1109/CSC.2013.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693181","","Bipartite graph;Data mining;Electronic mail;Grammar;Organizations;Social network services;Web pages","computer science education;expectation-maximisation algorithm;graph theory;information retrieval;natural language processing","CSAR system;Hungarian algorithm;KM algorithm;TF-IDF model;Web grammar;Web pages;computer science academic rankings system;expectation-maximization algorithm;information extraction;information normalization;linear interpolation;maximum likelihood estimation;natural language processing n-gram model;optimal matching bipartite graph;paper-view pages;part-of-speech tagger;term frequency-inverse document frequency model;trigram model","","0","","22","","","4-6 Nov. 2013","","IEEE","IEEE Conference Publications"
"A Distant Supervision Method for Product Aspect Extraction from Customer Reviews","J. Bross","Inst. of Comput. Sci., Freie Univ. Berlin, Berlin, Germany","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","339","346","In this paper, we describe a distant supervision approach for the task of detecting product aspect mentions in customer reviews (e.g., in hotel reviews, we want to associate the aspect ``sleep quality"" to a sentence such as ""We both slept like rocks.""). Detecting such aspects represents an important subtask of aspect-oriented review mining systems, which aim at automatically generating structured summaries of customer opinions. The main advantage of the proposed method is that it allows for the high accuracy of a supervised approach and at the same time avoids the costs of manually labeling a training set. We show how to exploit the inherent structure of customer reviews to automatically gather large amounts of labeled data. Our experimental results show that the method achieves a performance as good as a traditional, fully supervised approach.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.65","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693540","distant supervision;review mining;sentiment analysis","Cameras;Data mining;Internet;Knowledge based systems;Labeling;Training;Training data","data mining;information retrieval;learning (artificial intelligence)","aspect-oriented review mining systems;customer reviews;distant supervision method;labeled data;product aspect detection;product aspect extraction","","1","","43","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Classification of CT Figures in Biomedical Articles Based on Body Segments","Z. Xue; S. Antani; L. R. Long; D. Demner-Fushman; G. R. Thoma","Lister Hill Nat. Center for Biomed. Commun., Nat. Libr. of Med. Bethesda, Bethesda, MD, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","264","268","Figures in biomedical articles provide important information that can be utilized to enrich user experience in biomedical article retrieval. One method to improve retrieval performance is to categorize figures into various modalities. We have previously used a hierarchical classification strategy that significantly improves retrieval performance. In this paper, we extend the hierarchy and add body segment classification, i.e., classifying the figures in CT (computed tomography) modality into different body segments, such as head, abdomen, pelvis, or thorax. To address the large variety of article images, we extracted a wide set of feature types (feature vector length of 2321) and applied a multi-class SVM classifier. Feature selection was applied to reduce the feature vector to length 50. Evaluation of the proposed method on a dataset consisting of 2465 figures from a subset of open access biomedical articles from the National Library of Medicine's (NLM) PubMed Central® repository achieves classification accuracy of over 90%. This demonstrates its effectiveness and potential to become a vital component in biomedical document retrieval systems such as OpenI, a multimodal biomedical literature search system developed at NLM.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680486","CT image classification;biomedical article retrieval;content-based image retrieval;figure classification","Biomedical imaging;Feature extraction;Image color analysis;Image segmentation;Pelvis;Support vector machines;Thorax","computerised tomography;content-based retrieval;feature extraction;image retrieval;information retrieval;medical information systems;pattern classification;special libraries;support vector machines","CT figure classification;CT modality;NLM PubMed Central repository;National Library of Medicine;OpenI;article images;biomedical article retrieval;biomedical document retrieval systems;body segment classification;body segments;classification accuracy;computed tomography modality;feature selection;feature type extraction;feature vector length;hierarchical classification strategy;multiclass SVM classifier;multimodal biomedical literature search system;open access biomedical articles;retrieval performance;user experience","","0","","19","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Robust/fast out-of-vocabulary spoken term detection by N-gram index with exact distance through text/speech input","N. Sakamoto; S. Nakagawa","Toyohashi Univ. of Technol., Toyohashi, Japan","2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","20140102","2013","","","1","4","For spoken term detection, it is very important to consider Out-of-Vocabulary (OOV). Therefore, sub-word unit based recognition and retrieval methods have been proposed. This paper describes a very fast Japanese spoken term detection system that is robust for considering OOV words. We used individual syllables as sub-word unit in continuous speech recognition and an n-gram index of syllables in a recognized syllable-based lattice. We proposed an n-gram indexing/retrieval method in the syllable lattice for attacking OOV and high speed retrieval. Specially, in this paper, we redefineded the distance of the n-gram and used trigram, bigram and unigram that instead of using only trigram to calculate the exact distance. In our experiments, where using text and speech query, we achieved to improve the retrieval performance.","","Electronic:978-986-90006-0-4; POD:978-1-4799-2794-4","10.1109/APSIPA.2013.6694366","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694366","","Arrays;Hidden Markov models;Indexes;Lattices;Robustness;Speech;Speech recognition","information retrieval;natural language processing;speech recognition;text analysis","Japanese spoken term detection system;N-gram index;OOV;indexing-retrieval method;robust-fast out-of-vocabulary spoken term detection;speech query;speech recognition;subword unit;text query;text-speech input","","1","","12","","","Oct. 29 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"User attention oriented augmented reality on documents with document dependent dynamic overlay","T. Toyama; W. Suzuki; A. Dengel; K. Kise","DFKI GmbH, Japan","2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)","20131223","2013","","","299","300","When we read a document (any kind of, scientific papers, novels, etc.), we often encounter a situation that the information from the reading document is too less to comprehend what the author(s) would like to convey. In this paper, we demonstrate how the combination of a wearable eye tracker, a see-through head-mounted display (HMD) and an image based document retrieval engine enhances people's reading experiences. By using our proposed system, the reader can get supportive information in the see-through HMD when he wants. A wearable eye tracker and a document retrieval engine are used to detect which line in the document the reader is reading. We propose a method to detect the reader's attention on a word in a reading document, in order to present information at a preferable moment. Furthermore, we also propose a method to project a point of the document to a point of the HMD screen, by calculating the pose of the reading document in the camera image. This projection enables the system to overlay the information dynamically in an augmented view on the reading line. The results from the user study and the experiments show the potential of the proposed system in a practical use case.","","Electronic:978-1-4799-2869-9; POD:978-1-4799-2870-5","10.1109/ISMAR.2013.6671814","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671814","","Augmented reality;Calibration;Dictionaries;Engines;Feature extraction;Tracking;Visualization","augmented reality;helmet mounted displays;information retrieval","HMD;camera image;document dependent dynamic overlay;image based document retrieval engine;see-through head-mounted display;user attention oriented augmented reality;wearable eye tracker","","0","","8","","","1-4 Oct. 2013","","IEEE","IEEE Conference Publications"
"Local Trust Versus Global Trust Networks in Subjective Logic","C. Haydar; A. Roussanaly; A. Boyer","Lab. Loria, Univ. de Lorraine, Vand&#x0153;uvres-les-Nancy, France","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","29","36","Social web permits users to acquire information from anonymous people around the world. This leads to a serious question about the trustworthiness of information and sources. During the last decade, numerous models were proposed to model social trust in the service of social web. Trust modeling follows two main axes, local trust (trust between pair of users), and global trust (user's reputation within the community). Subjective logic, is an extension of probabilistic logic that deals with the cases of lack of evidences. An elaborated local trust model based on subjective logic already exists. The aim of this work is to apply this model to the first time on a real data set. Then, we propose another global trust model based also on subjective logic. We apply both models on a real data set of a question answering social network that aims to assist people to find solutions to their technical problems in various domains. Our proposed global trust model ensures a better performance thanks to its precise interpretation of the context of trust, and its ability to satisfy new arrived users.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.5","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689990","global trust;local trust;recommender systems;reputation;subjective logic;trust","Communities;Computational modeling;Context;Context modeling;Data models;Mathematical model;Uncertainty","probabilistic logic;question answering (information retrieval);social networking (online);trusted computing","anonymous people;global trust networks;local trust networks;probabilistic logic;question answering social network;social Web;social trust;subjective logic;technical problems;trust modeling","","1","","33","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Research on Semantic Similarity Calculation of Linked Data Based on Multiple Factors","Z. Zhi-Yun; J. Li-Mei; W. Zhen-Fei; G. Yi-Ke","Sch. of Inf. Eng., Zhengzhou Univ., Zhengzhou, China","2013 International Conference on Cloud and Green Computing","20131219","2013","","","358","362","Semantic similarity calculation has been an important role in information retrieval of linked data, so calculation results will directly affect data mining results. To solve the problem of lower computation precision caused by the unity of factors on the research of semantic similarity calculation on linked data and the underutilization on semantic information of concept, this paper proposes a new semantic similarity calculation method based on multiple factors. This method combines the importance of attribute, types of attribute value with correlation. Firstly, it assigns the corresponding weight to the attribute, and uses the matching similarity algorithm of attributes according to the types of attribute value, and then similarity computation based on concept attributes is done. Secondly, it defines the path of correlation and determines the length of path, and then similarity computation based on correlation is done. Finally, through integrating the results of computation, the more accurate similarity can be get The experiment confirms that the proposed method fully utilizes semantic information of the concept and the calculation result can better reveal the similarity relation between concepts in linked data, compared with the calculation results based on the unity of factors.","","Electronic:978-0-7695-5114-2; POD:978-1-4799-1362-6","10.1109/CGC.2013.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686055","Attribute of Concept;Correlation;Linked Data;Semantic Similarity","Correlation;Data mining;Data models;Equations;Films;Ontologies;Semantics","data mining;information retrieval","attribute value;data mining;information retrieval;linked data;semantic similarity calculation;similarity computation","","0","","13","","","Sept. 30 2013-Oct. 2 2013","","IEEE","IEEE Conference Publications"
"A Digital Archiving of Large 3D Woven Cultural Artifacts of the ""Fune-hoko""","W. Wakita; H. T. Tanaka","Dept. of Human & Comput. Intell., Ritsumeikan Univ., Kusatsu, Japan","2013 International Conference on Culture and Computing","20131212","2013","","","206","207","We archived large 3D woven cultural artifacts of the ""Fune-hoko"" based on the image-based technique. After the Gion Festival of 2011, the repair work of the Fune-hoko storage was scheduled. Therefore, we developed a 3D scanning system for the large 3D woven cultural artifacts measurement with the laser range scanner and tried to scan and model the 3D woven cultural artifacts at high definition in 3D. Most woven cultural artifact of the Fune-hoko is very large and is composed of gold thread, glass, cotton, and brilliant embroidery. Therefore, we measured with moving the scanner at regular intervals and changing the laser strength according to the material of the fabric. Then we modeled the scanned data by image-based technique for the real-time reproduction and rendering. This paper describes a digital archiving process of large 3D woven cultural artifacts of the Fune-hoko.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.67","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680381","3D digitizing and scanning;digital archive;image-based modeling","","history;image processing;information retrieval systems;rendering (computer graphics)","3D scanning system;3D woven cultural artifacts measurement;Fune-hoko storage;Gion Festival of 2011;digital archiving process;fabric material;image-based technique;laser range scanner;laser strength;real-time rendering;real-time reproduction;repair work;scanned data","","0","","1","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Supporting Intelligence Analysts with a Trust-Based Question-Answering System","M. d. Boer; P. P. v. Maanen; G. Vreeswijk","Dept. of Cognitive Syst. Eng., TNO Human Factors, Soesterberg, Netherlands","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","3","","183","186","Intelligence analysts have to work in highly demanding circumstances. This causes mistakes with severe consequences, which is the reason that support systems for intelligence analysts have been developed. The support system proposed in this paper assists humans by offering support that improves their performance, without reducing them in their freedom. This is done with a trust-based question answering system (T-QAS). An important part of T-QAS are trust models which keep track of trust in each of the agents gathering information. Using these trust models, the system can support the intelligence analyst by: 1) helping to decide which agents are trusted enough to receive questions, 2) providing information about the reliability of each of the sources used, and 3) advising in making decisions based on information from possibly unreliable sources. An implementation of last two capabilities of T-QAS is evaluated in an experiment in which participants perform a decision making task with information from possibly unreliable sources. Results show that the proposed T-QAS support indeed helps participants to improve their performance. We therefore expect that future intelligence analyst support systems can benefit from the inclusion of T-QAS.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.179","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690724","Decision support systems;Performance evaluation","Analytical models;Artificial intelligence;Electronic mail;Human factors;Organizations;Reliability","decision making;multi-agent systems;question answering (information retrieval);trusted computing","T-QAS;agents;decision making;intelligence analyst support systems;intelligence analysts;trust models;trust-based question-answering system","","0","","16","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"AutoComment: Mining question and answer sites for automatic comment generation","E. Wong; Jinqiu Yang; Lin Tan","Univ. of WaterlooWaterloo, Waterloo, ON, Canada","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","20140102","2013","","","562","567","Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q&A) site. Q&A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings.We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoComment to analyze Java and Android tagged Q&A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate, adequate, concise, and useful in helping them understand the code.","","Electronic:978-1-4799-0215-6; POD:978-1-4799-0216-3","10.1109/ASE.2013.6693113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693113","automated comment generation;documentation;natural language processing for software engineering;program comprehension","Androids;Cloning;Databases;Humanoid robots;Java;Natural language processing;Software","Android (operating system);Java;data mining;public domain software;question answering (information retrieval);software maintenance","Android tagged Q&A post analysis;AutoComment;Java tagged Q&A post analysis;automatic comment generation;code comments;code segments;code-description mapping extraction;comment scarcity issue;natural language processing;open-source projects;program comprehension;programming question-and-answer site mining;software engineering;software maintainability improvement","","10","","23","","","11-15 Nov. 2013","","IEEE","IEEE Conference Publications"
"Delivering diverse BGP data in real-time and through multi-format archiving","C. Olschanowsky; M. L. Weikum; J. Smith; C. Papadopoulos; D. Massey","Colorado State Univ., Fort Collins, CO, USA","2013 IEEE International Conference on Technologies for Homeland Security (HST)","20140102","2013","","","698","703","The Internet relies on BGP for global routing, but there are many open questions related to BGP. Some researchers rely on BGP data to better understand routing behavior and develop new routing algorithms. Other researchers use BGP data to investigate issues that range from IP allocations to regional Internet connectivity in the face of political turmoil. And of course BGP data is used in routing security issue, both to detect issues, evaluate solutions, and even issue warnings or block invalid routes. All of these research challenges require access to a reliable set of BGP data from geographically diverse locations. This papers presents the BGPmon approach to collecting and distributing BGP data at global scale. BGPmon collects data from diverse set of peers and distributes the data in real-time to any interested client. BGPmon fulfills three main design objectives: it provides a scalable data collection solution, maintains data integrity despite traffic surges and slow client processing, and it provides a suite of associated tools in order to ease the overhead of developing BGP data processing tools. We demonstrate the effectiveness of the framework with a brief characterization of the data collected from direct peers.","","CD-ROM:978-1-4799-3963-3; Electronic:978-1-4799-1535-4; POD:978-1-4799-3964-0","10.1109/THS.2013.6699089","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699089","BGP;Monitoring;Networking","Data collection;Educational institutions;IP networks;Internet;Routing;Security;XML","Internet;computer network security;data acquisition;data integrity;information retrieval systems;routing protocols","BGP data collection;BGP data distribution;BGP data processing tools;BGPmon approach;IP allocation;data integrity;global routing security;multiformat archiving;regional Internet connectivity;scalable data collection solution","","0","","10","","","12-14 Nov. 2013","","IEEE","IEEE Conference Publications"
"Keyword Query Expansion on Linked Data Using Linguistic and Semantic Features","S. Shekarpour; K. Höffner; J. Lehmann; S. Auer","Dept. of Comput. Sci., Univ. of Leipzig, Leipzig, Germany","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","191","197","Effective search in structured information based on textual user input is of high importance in thousands of applications. Query expansion methods augment the original query of a user with alternative query elements with similar meaning to increase the chance of retrieving appropriate resources. In this work, we introduce a number of new query expansion features based on semantic and linguistic inferencing over Linked Open Data. We evaluate the effectiveness of each feature individually as well as their combinations employing several machine learning approaches. The evaluation is carried out on a training dataset extracted from the QALD question answering benchmark. Furthermore, we propose an optimized linear combination of linguistic and lightweight semantic features in order to predict the usefulness of each expansion candidate. Our experimental study shows a considerable improvement in precision and recall over baseline approaches.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693516","linguistic fetures;query expansion;semantic features","Benchmark testing;Feature extraction;Pragmatics;Semantics;Support vector machines;Vectors;Vocabulary","inference mechanisms;learning (artificial intelligence);query processing;question answering (information retrieval);semantic Web","QALD question answering benchmark;alternative query elements;keyword query expansion;lightweight semantic features;linguistic feature;linguistic inference;linguistic semantic features;linked data;linked open data;machine learning;query expansion features;query expansion method;semantic inference;structured information;textual user input;training dataset","","4","1","19","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Adaptive Topic Modeling for Detection Objectionable Text","J. Zeng; J. Duan; C. Wu","Sch. of Comput. Sci., Fudan Univ., Shanghai, China","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","381","388","Objectionable text content on the Web is harmful to young children. Although keyword-based methods are superior in achieving faster detection, they fail to detect text content that is semantically objectionable. A novel framework based on adaptive topic modeling is proposed to detect objectionable text content. Firstly, a weighted graph is constructed based on several seed words and a set of training texts. Feature words are then selected from the graph according to the measure which shows how likely a word to be sensitive. Adaptive LDA (Latent Dirichlet Allocation) topic model in which topic number can be automatically estimated is proposed to find the latent objectionable topic structure for the text set. An objectionable topic criterion is devised for the adaptive selection method which takes the objectionable topic characteristic into consideration. Finally, detection for a given text is evaluated based on its probability value with respect to the model. Extensive comparison experiments on real world text sets show that the proposed method can effectively detect objectionable text. The performance is superior to that of keyword-based methods with several different approaches to generate keyword list. Experiments also show that the performance is better than that of detection methods based on traditional topic modeling.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690040","adaptive topic model;detection;feature selection;objectionable text","Adaptation models;Computational modeling;Feature extraction;Filtering;Mathematical model;Semantics;Training","graph theory;information retrieval;text analysis","adaptive LDA topic model;adaptive selection method;adaptive topic modeling;keyword-based methods;latent Dirichlet allocation;latent objectionable topic structure;objectionable text detection;objectionable topic criterion;seed words;topic number;weighted graph","","0","","23","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Analyzing terror attacks using latent semantic indexing","I. Toure; A. Gangopadhyay","Dept. of Inf. Syst., Univ. of Maryland Baltimore County (UMBC) Baltimore, Baltimore, MD, USA","2013 IEEE International Conference on Technologies for Homeland Security (HST)","20140102","2013","","","334","337","Terrorism activities occur in many parts of the world. Such activities seemingly occur randomly in different locations, at different times, and are caused by different perpetrators. Thus, it is a challenging task to find patterns in activities related to terrorism. Clustering of perpetrators into similar groups can provide valuable information such as common characteristics among the various groups, the types of targets typically attacked, and weapons used in such attacks. In this research, we develop a method to classify terrorist groups based on their attack patterns by analyzing textual descriptions of such attacks using latent semantic indexing and clustering. The resulting information can be used for counter-terrorism globally, and that can help develop security measures that can be taken to protect potentially affected entities such as hospitals, schools, and government agencies as well as saving the lives of innocent people.","","CD-ROM:978-1-4799-3963-3; Electronic:978-1-4799-1535-4; POD:978-1-4799-3964-0","10.1109/THS.2013.6699024","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699024","","Data mining;Indexing;Large scale integration;Semantics;Terrorism;Vectors;Weapons","database indexing;information retrieval;pattern classification;pattern clustering;terrorism","counter-terrorism;government agencies;hospitals;latent semantic indexing;perpetrator clustering;schools;security measures;terror attack pattern analysis;terrorism activities;terrorist group classification;textual description analysis","","0","","10","","","12-14 Nov. 2013","","IEEE","IEEE Conference Publications"
"Any Language Early Detection of Epidemic Diseases from Web News Streams","R. Brixtel; G. Lejeune; A. Doucet; N. Lucas","GREYC, Normandy Univ. - Unicaen, Caen, France","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","159","168","In this paper, we introduce a multilingual epidemiological news surveillance system. Its main contribution is its ability to extract epidemic events in any language, hence succeeding where state-of-the-art in surveillance systems usually fails : the objective of reactivity. Most systems indeed focus on a selected list of languages, deemed important. However, evidence shows that events are first described in the local language, and translated to other languages later, if and only if they contained important information. Hence, while systems handling only a sample of human languages may indeed succeed at extracting epidemic events, they will only do so after someone else detected the importance of the news, and made the decision to translate it. Thus, with events first described in other languages, such automated systems, that may only detect events that were already detected by humans, are essentially irrelevant for early detection. To overcome this weakness of the state-of-the-art in terms of reactivity, we designed a system that can detect epidemiological events in any language, without requiring any translation, be it automated or human-written. The solution presented in this paper relies on properties that may be called language universals. First, we observe and exploit properties of the news genre that remain unchanged, whatever the writing language. Second, we handle language variations, such as declensions, by processing text at the character-level, rather than at the word level. This additionally allows to handle various writing systems in a similar fashion. We present experiments with 5 languages, steoreotypical of different language families and writing systems : English, Chinese, Greek, Polish and Russian. Our system, DAnIEL, achieves an average F-measure score around 85%, slightly below top-performing systems for the languages that such systems are able to handle. However, its performance is superior for morphologically-rich languages. And it performs of- course infinitely better for the languages that other systems are not able to handle : The richest system in the state-of-the-art handles around 10 languages, while there exists about 6,000 languages in the world, 300 of which are spoken by more than one million people. The DAnIEL system is able to process each of them.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680474","","Arrays;Diseases;Feeds;Knowledge based systems;Pragmatics;Surveillance;Writing","Internet;diseases;information retrieval;medical computing;natural language processing","Chinese languages;DAnIEL system;English languages;Greek language;Polish language;Russian language;Web news streams;any language early detection;automated systems;declensions;epidemic diseases;epidemic event extraction;human languages;language universals;multilingual epidemiological news surveillance system;writing language","","0","","22","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Extending MOWL for Event Representation (E-MOWL)","N. Pahal; S. Chaudhury; B. Lall","Dept. of Electr. Eng., Indian Inst. of Technol. Delhi, New Delhi, India","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","3","","171","174","Events in multimedia objects represent landmarks of a story encoded in the media. This paper proposes a new specification for representing Events using MOWL (E-MOWL). Using this approach, higher-level complex events are detected based on the hierarchy of events present in the Event Ontology created using MOWL. The MOWL definition of Spatio-Temporal relations is extended to represent the Context of an event. The Context is any kind of information which gets defined with respect to an event instance wherein Context has its own structure and involves parameters like Time, Geographical location, and Actors/ Entities involved in the event. Such an extension would enhance event detection and facilitate complete semantics, besides providing scope for personalized multimedia information retrieval.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690721","BN;Event;MOWL;Observation Model","Context;Google;Media;Multimedia communication;Ontologies;Probabilistic logic;Streaming media","information retrieval;knowledge representation languages;multimedia systems;ontologies (artificial intelligence)","E-MOWL;actors-entities parameter;event detection;event ontology;event representation;geographical location parameter;multimedia objects;ontology Web language;personalized multimedia information retrieval;semantics facilitation;spatio-temporal relations;time parameter","","3","","6","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Advanced Use of Historic-Archival Resources in the Management of Built Heritage: The District of Monti","D. Calisi; M. G. Cianci; F. Geremia","Dipt. di Architettura, Univ. degli studi di Roma Tre, Rome, Italy","2013 13th International Conference on Computational Science and Its Applications","20131212","2013","","","196","199","In recent years there has been considerable activity in the digitisation of Europe's cultural heritage, for creation of network-accessible databases (Europeana, CARARE, 3D-ICONS), in implementing strategies for shared global standards (EPOCH, Minerva EC) and for use in Web GIS programmes (MAPPA, Mapping Gothic France). However such initiatives are insufficient when it comes to detailed knowledge and management at the local level. Our research addresses this shortcoming by proposing an innovative methodology for investigating urban heritage, capable of representing the transformations of the city over time, and experimenting new techniques in knowledge and representation.","","Electronic:978-0-7695-5045-9; POD:978-1-4799-0616-1","10.1109/ICCSA.2013.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681123","3D modelling;Cultural heritage;Web GIS;cartographic research;digital database","Buildings;Cities and towns;Cultural differences;Documentation;Fabrics;Image reconstruction;Three-dimensional displays","database management systems;geographic information systems;history;information retrieval systems","3D-ICONS;CARARE;EPOCH;Europe cultural heritage digitisation;Europeana;MAPPA;Mapping Gothic France;Minerva EC;Monti district;Web GIS programmes;global standards;heritage management;historic-archival resources;network-accessible database;urban heritage","","0","","5","","","24-27 June 2013","","IEEE","IEEE Conference Publications"
"Innovation in graduate projects: Learning to identify critical functions","V. Viswanathan; P. Ngo; C. Turner; J. Linsey","Woodruff Sch. of Mech. Eng., Georgia Inst. of Technol., Atlanta, GA, USA","2013 IEEE Frontiers in Education Conference (FIE)","20131219","2013","","","1419","1425","Design-by-analogy is considered to be a powerful tool for engineering design. The difficulty of finding suitable analogies for solving a given design problem gives rise to the current efforts on computational tools for analogy-based design. In searching for analogies, the critical functions in a design problem are potential search criteria. The study described in this paper investigates whether novice designers identify an expert-derived critical function in a design problem under three scenarios: when they are asked to report the important functions in the problem, when they are directly asked to report the critical function and when they are asked to use design-by-analogy. It is observed that student designers do not identify the expert-derived critical functions when directly asked or when asked to list important functions. However, they inherently use the expert-derived critical functions in their analogical mapping process. This suggests that, during analogical reasoning, designers tend to identify and use the same critical functions regardless of experience, and also that critical functions are valid search criteria for deriving analogies from a computational database. This insight is highly valuable for current efforts to develop computational tools for analogical reasoning.","0190-5848;01905848","Electronic:978-1-4673-5261-1; POD:978-1-4673-5259-8; USB:978-1-4673-5260-4","10.1109/FIE.2013.6685066","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685066","Analogy;Critical functions;Design by Analogy;Graduate Design Projects","Cognition;Data collection;Databases;Educational institutions;Search problems;Solids;Technological innovation","inference mechanisms;information retrieval","analogical mapping process;analogical reasoning;analogy retrieval;computational database;design-by-analogy;expert-derived critical function;graduate projects","","0","","31","","","23-26 Oct. 2013","","IEEE","IEEE Conference Publications"
"Automatic mood classification of Indonesian tweets using linguistic approach","V. Wijaya; A. Erwin; M. Galinium; W. Muliady","Inf. & Commun. Technol. Fac., Swiss German Univ., Tangerang, Indonesia","2013 International Conference on Information Technology and Electrical Engineering (ICITEE)","20131202","2013","","","41","46","Research concerning Twitter mining becomes an interesting research topic recently. It is proven by numerous number of published paper related with this topic. This research is intended to develop a prototype system for classifying Indonesian language tweets. The prototype includes preprocessing step, main information retrieval and classification system. This research proposes a system that uses grammatical rule for retrieving main information from the tweet, and then classifies the information to the suitable mood space. The classification algorithm, which is used, is lexicon based classifier. The proposed classification system has 53.67% accuracy for classifying tweets into 12 mood spaces and 75% accuracy for classifying tweets into 4 mood spaces. As the comparison, the same dataset is also classified using SVM and Naïve Bayes.","","Electronic:978-1-4799-0425-9; POD:978-1-4799-0422-8","10.1109/ICITEED.2013.6676208","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676208","Grammatical Rule;Lexical Approach;Text Mining;Twitter","","classification;computational linguistics;data mining;information retrieval;social networking (online);text analysis","Indonesian language tweets classification;Naive Bayes classification;SVM;Twitter mining;automatic mood classification;classification algorithm;information classification system;information retrieval;lexicon based classifier;linguistic approach","","3","","21","","","7-8 Oct. 2013","","IEEE","IEEE Conference Publications"
"Evaluation on text categorization for mathematics application questions","L. C. Yu; H. L. Hu; W. H. Lin","Dept. of Inf. Manage., Yuan Ze Univ., Chung-Li, Taiwan","2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","20140102","2013","","","1","2","In learning environments, developing intelligent systems that can properly respond learners' emotions is a critial issue for improving learning outcome. For example, systems should consider to replace the current question with an easier one when detecting negative emotions expressed by learners. Conversely, systems can try to retrieve a more challenging question when learners have contempt emotion or feel bored. This paper proposes the use of text categorization to automatically classify mathematics application questions into different difficulty levels. Applications can then benefit from such classification results to develop retrieval systems for proposing questions based on learners' emotion states. Experimental results show that the machine learning algorithm C4.5 achieved the highest accuracy 78.53% in a binary classification task.","","Electronic:978-986-90006-0-4; POD:978-1-4799-2794-4","10.1109/APSIPA.2013.6694327","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694327","","Accuracy;Computers;Equations;Machine learning algorithms;Niobium;Text categorization","classification;emotion recognition;learning (artificial intelligence);question answering (information retrieval);text analysis","C4.5;binary classification task;contempt emotion;difficulty levels;emotion states;intelligent systems;learning environments;machine learning algorithm;mathematics application questions;negative emotions;retrieval systems;text categorization","","0","","16","","","Oct. 29 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"Exploiting phase measurements of EPC Gen2 RFID tags","J. Huiting; H. Flisijn; A. B. J. Kokkeler; G. J. M. Smit","Dept. of Electr. Eng., Math. & Comput. Sciencem, Univ. of Twente, Enschede, Netherlands","2013 IEEE International Conference on RFID-Technologies and Applications (RFID-TA)","20140102","2013","","","1","6","This paper presents a 2d localization system for UHF RFID tags. By measuring the phase between the transmitted continuous wave and received backscatter from the tag at different frequencies, it is possible to estimate the distance between the reader and tag. By determining distance estimates to three antennas it is possible to determine a location of a tag with the help of trilateration. The location of the antennas has to be known and any error in distance measurement influences the location estimate. Mainly to overcome the expected impact of the environment on the distance estimate, we opt for the use of reference tags and the K-Nearest Neighbor (KNN) algorithm to derive a location. An experiment is done and results are compared with an KNN algorithm based on received signal strength. Furthermore some experiments are done to verify the use of phase measurements for the detection of stationary tags in a portal application.","","Electronic:978-1-4799-2114-0; POD:978-1-4799-2116-4","10.1109/RFID-TA.2013.6694503","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694503","","Calibration;RFID tags;UHF measurements","UHF antennas;distance measurement;information retrieval;pattern classification;radiofrequency identification;telecommunication computing","2D localization system;EPC Gen2 RFID tags;KNN algorithm;UHF RFID tags;distance measurement;k-nearest neighbor algorithm;received backscatter;reference tags;stationary tags;transmitted continuous wave","","5","","18","","","4-5 Sept. 2013","","IEEE","IEEE Conference Publications"
"An RDF-Based Framework for Semantic Indexing of Web Pages","F. Amato; V. Moscato; F. Persia; A. Picariello; F. Gargiulo","Dip. di Ing. Elettr. e Tecnol. dell'Inf., Univ. of Naples Federico II, Naples, Italy","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","395","396","Managing efficiently and effectively very large amount of digital documents requires the definition of indexes able to capture and express documents' semantics. In this work, we propose an RDF based framework for semantic indexing of web pages considering the related textual information. In particular, we propose to capture the semantic nature of a given document, commonly expressed in natural language, by retrieving a number of RDF triples and to semantically index the documents on the base of meaning of the triples' elements (i.e. subject, verb, object). Preliminary experiments are reported to evaluate the proposed index strategy.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693551","Information Extraction;RDF;Semantic Indexing","Buildings;Indexing;Natural language processing;Resource description framework;Semantics;Web pages","indexing;information retrieval;natural language processing;semantic Web","RDF-based framework;Web pages;digital document;natural language;semantic indexing","","0","","7","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Building multi-model collaboration in detecting multimedia semantic concepts (invited paper)","H. Y. Ha; F. C. Fleites; S. C. Chen","Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA","9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20131212","2013","","","205","212","The booming multimedia technology is incurring a thriving multi-media data propagation. As multimedia data have become more essential, taking over a major potion of the content processed by many applications, it is important to leverage data mining methods to associate the low-level features extracted from multimedia data to high-level semantic concepts. In order to bridge the semantic gap, researchers have investigated the correlation among multiple modalities involved in multimedia data to effectively detect semantic concepts. It has been shown that multimodal fusion plays an important role in elevating the performance of both multimedia content-based retrieval and semantic concepts detection. In this paper, we propose a novel cluster-based ARC fusion method to thoroughly explore the correlation among multiple modalities and classification models. After combining features from multiple modalities, each classification model is built on one feature cluster, which is generated from our previous work FCC-MMF. The correlation between medoid of a feature cluster and a semantic concept is introduced to identify the capability of a classification model. It is further applied with the logistic regression method to refine ARC fusion method proposed in our previous work for semantic concept detection. Several experiments are conducted to compare the proposed method with other related works and the proposed method has outperform other works with higher Mean Average Precision (MAP).","","Electronic:978-1-936968-92-3; POD:978-1-4799-2754-8; USB:978-1-936968-91-6","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679986","Feature Correlation;Multi-model Fusion;Semantic concept detection","Correlation;Feature extraction;Hidden Markov models;Mathematical model;Multimedia communication;Reliability;Semantics","data mining;information retrieval;multimedia computing;pattern classification;pattern clustering","ARC fusion method;FCC-MMF;MAP;classification models;cluster-based ARC fusion method;data mining methods;high-level semantic concepts;low-level features;mean average precision;multimedia content-based retrieval;multimedia data propagation;multimedia semantic concepts detection;multimedia technology;multimodel collaboration","","0","","26","","","20-23 Oct. 2013","","IEEE","IEEE Conference Publications"
"A Conjunction for Private Stream Searching","M. Oehler; D. S. Phatak","Dept. of Comput. Sci. & Electr. Eng., Univ. of Maryland Baltimore County, Baltimore, MD, USA","2013 International Conference on Social Computing","20140102","2013","","","441","447","Our contribution defines a conjunction operator for private stream searching. Private stream searching is a system of cryptographic methods that preserves the confidentiality of the search criteria and the result. The system uses an encrypted filter to conceal the search terms, processes a search without decrypting these terms, and saves the result to an encrypted buffer. Fundamentally, the system provides a private search capability based on a logical disjunction of search terms. Our conjunction operator broadens the search capability, and achieves this without significantly increasing the complexity of the private search system. The conjunction is processed as a bit wise summation of hashed keyword values to reference an encrypted entry in the filter. The method is best suited for a conjunction of fields from a record, does not impute a calculation of bilinear map, as required in prior research, and offers a practical utility that integrates into private stream searching. We demonstrate the practicality by including the conjunction operator into our domain specific language for private packet filtering.","","Electronic:978-0-7695-5137-1; POD:978-1-4799-1519-4","10.1109/SocialCom.2013.69","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693366","Data Privacy;Oblivious Transfer;Packet Filtering;Private Search;Security Language","Buffer storage;Dictionaries;Encryption;IP networks;Indexes;Public key","cryptography;data privacy;information retrieval;specification languages","bilinear map;bit wise summation;conjunction filter;conjunction operator;cryptographic methods;domain specific language;encrypted buffer;encrypted filter;hashed keyword values;logical disjunction;private packet filtering;private stream searching;search capability;search criteria confidentiality","","0","","11","","","8-14 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Publish/Subscribe Middleware for Body and Ambient Sensor Networks that Mediates between Sensors and Applications","C. Seeger; K. V. Laerhoven; J. Sauer; A. Buchmann","Databases & Distrib. Syst., Tech. Univ. Darmstadt, Darmstadt, Germany","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","199","208","Continuing development of an increasing variety of sensors has led to a vast increase in sensor-based telemedicine solutions. A growing range of modular sensors, and the need of having several applications working with those sensors, has led to an equally extensive increase in efforts for system development. In this paper, we present an event-driven middleware for on-body and ambient sensor networks that allows multiple applications to define information types of their interest in a publish/subscribe manner. Incoming sensor data is hereby transformed into the desired data representation which lifts the burden of adapting the application with respect to the connected sensors off the developer's shoulders. Furthermore, an unsupervised on-the-fly reloading of transformation rules from a remote server allows the system's adaptation to future applications and sensors at run-time. Application-specific event channels provide tailor-made information retrieval as well as control over the dissemination of critical information. The system is evaluated based on an Android implementation, with transformation rules implemented as OSGi bundles that are retrieved from a remote web server. Evaluation shows a low impact of running the transformation rules on a phone and highlights the reduced energy consumption by having fewer sensors serving multiple applications. It also points out the behavior and limits of the application-specific event channels with respect to CPU utilization, delivery ratio, and memory usage.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680479","Android;body sensor network;event-based systems;middleware;publish subscribe systems;wireless sensor network","Communication channels;Electrocardiography;Heart rate;Middleware;Monitoring;Smart phones;Subscriptions","body sensor networks;data structures;information retrieval;message passing;middleware;patient monitoring;telemedicine","Android implementation;CPU utilization;OSGi;ambient sensor networks;body sensor networks;critical information dissemination;data representation;delivery ratio;event-driven middleware;information retrieval;memory usage;publish-subscribe middleware;remote Web server;remote server;sensor-based telemedicine;transformation rules;unsupervised on-the-fly reloading","","2","","20","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Routing Questions in Twitter: An Effective Way to Qualify Peer Helpers","C. Souza; J. Magalhães; E. Costa; J. Fechine","Lab. of Artificial Intell.-LIA, Fed. Univ. of Campina Grande, Campina Grande, Brazil","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","109","114","Essentially social query consists in sharing a question on a social network and waiting for responses. The usual strategy of sharing a public question is inefficient because it does not ensure an answer, either quality or agility. We assume that directing the question to a specific user is the best way to get a right and quick answer. However, choosing this person could be a hard task and by choosing a wrong one we could receive an incorrect answer or the person might just ignore the question. In this paper, we proposed, implemented and evaluated a model based on a multi-criteria approach to find the most suitable user to receive a question on Twitter. We rank expert candidates using the Weight Product Model. One of the main differentials of our model is the use of multi-criteria and that it was built for a social network context. In the evaluation, using as metrics the Normalized Discounted Cumulative Gain and the Spearman Rank Correlation Coefficient, our technique achieved promising results in predicting the usefulness of real responders answer.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690001","expertise finding;query routing;social networks;social query;twitter","Context;Context modeling;Mathematical model;Measurement;Proposals;Twitter","query processing;question answering (information retrieval);social networking (online)","Spearman rank correlation coefficient;Twitter;expert candidate ranking;incorrect answer;multicriteria approach;normalized discounted cumulative gain;peer helper qualification;public question sharing strategy;question routing model;responder answering;social network;social query;weight product model","","0","","18","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Leading Users Detecting Model in Professional Community Question Answering Services","S. Song; Y. Tian; W. Han; X. Que; W. Wang","State Key Lab. of Networking & Switching, Beijing Univ. of Posts & Telecommun., Beijing, China","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","1302","1307","Community question answering (CQA) services provide platforms for increasing number of users to seek for help and exchange ideas in recent years. As a professional community, Quora is quite different from other traditional CQA websites, such as Baidu Zhidao and Yahoo! Answers. As users' social statuses play a more important role, besides just raising questions and finding a proper answer which the asker is satisfied with, people are inclined to have more discussions about the question, corresponding answers and related topics. Due to the spirit of wiki, people, especially some leading users work hard together to make the community a knowledge base of high quality, which in turn attracts more people to take part in this job. Thus, in this study, we focus on detecting leading users within a specific topic area of professional CQA services. A leading capacity model is utilized and three major criteria, which are authority, activity and influence, are taken into consideration. Then, an improved Borda count model is proposed to make a collective decision. Experimental results show effectiveness of the proposed method for finding leading users.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682239","Borda count model;activity measurement;authority measuremen;influence measurement;leading capacity;professional community question answering","Atmospheric measurements;Communities;Decision making;Estimation;Internet;Knowledge discovery;Particle measurements","Web sites;question answering (information retrieval)","Baidu Zhidao;Borda count model;CQA Web sites;Quora;Wiki;Yahoo! Answers;leading capacity model;professional CQA services;professional community question answering services;user detecting model;user social statuses","","2","","9","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"On the Relationship between the Vocabulary of Bug Reports and Source Code","L. Moreno; W. Bandara; S. Haiduc; A. Marcus","Dept. of Comput. Sci., Wayne State Univ., Detroit, MI, USA","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","452","455","Text retrieval (TR) techniques have been widely used to support concept and bug location. When locating bugs, developers often formulate queries based on the bug descriptions. More than that, a large body of research uses bug descriptions to evaluate bug location techniques using TR. The implicit assumption is that the bug descriptions and the relevant source code files share important words. In this paper, we present an empirical study that explores this conjecture. We found that bug reports share more terms with the patched classes than with the other classes in the system. Furthermore, we found that the class names are more likely to share terms with the bug descriptions than other code locations, while more verbose parts of the code (e.g., comments) will share more words. We also found that the shared terms may be better predictors for bug location than some TR techniques.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676930","Bug location;source code vocabulary;text retrieval","Art;Computer bugs;Data collection;Large scale integration;Software systems;Vocabulary","information retrieval;program debugging;text analysis","TR techniques;bug descriptions;bug location techniques;bug reports;source code;text retrieval techniques","","4","","9","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
"On Constructing Seminal Paper Genealogy","D. H. Bae; S. M. Hwang; S. W. Kim; C. Faloutsos","Dept. of Electron. & Comput. Eng., Hanyang Univ., Seoul, South Korea","IEEE Transactions on Cybernetics","20131212","2014","44","1","54","65","Let us consider that someone is starting a research on a topic that is unfamiliar to them. Which seminal papers have influenced the topic the most? What is the genealogy of the seminal papers in this topic? These are the questions that they can raise, which we try to answer in this paper. First, we propose an algorithm that finds a set of seminal papers on a given topic. We also address the performance and scalability issues of this sophisticated algorithm. Next, we discuss the measures to decide how much a paper is influenced by another paper. Then, we propose an algorithm that constructs a genealogy of the seminal papers by using the influence measure and citation information. Finally, through extensive experiments with a large volume of a real-world academic literature data, we show the effectiveness and efficiency of our approach.","2168-2267;21682267","","10.1109/TCYB.2013.2246565","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479688","Paper genealogy construction;seminal paper finding","Cybernetics;Educational institutions;Equations;Market research;Mathematical model;Search engines;Vectors","citation analysis;electronic publishing;information retrieval","academic literature data;citation information;influence measure;seminal paper genealogy","","2","","33","","20130314","Jan. 2014","","IEEE","IEEE Journals & Magazines"
"A Lightweight Algorithm for Automated Forum Information Processing","W. Y. Lim; A. Sachan; V. L. L. Thing","Cybercrime & Security Intell. (CSI) Dept., Inst. for Infocomm Res., Singapore, Singapore","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","121","126","The vast variety of information on Web forums makes them a valuable resource for various purposes such as scam detection, national security protection and sentiment analysis. However, it is challenging to extract useful information from Web forums accurately and efficiently. First, several page types exist in Web forums and content is presented in different formats in these pages. Second, the content on the forum pages is stored in the form of data blocks. For the information to be meaningful, it is necessary to extract the relevant data blocks separately. The main problem with generic content extraction systems is that they cannot distinguish among various pages nor extract information with the required granularity. Although, several content extraction methods exist for Web forums, these methods either do not satisfy the above requirements or use heuristics based approaches (such as assumptions on standard visual appearances, etc., resulting in limited applicability to different varieties of forum). In this paper, we propose a general and efficient content extraction method using the properties of links present in forum pages. The effectiveness of our proposed method is shown through our experimental results.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690003","DOM tree;content extraction;forum;web","Data mining;Feature extraction;HTML;Training;Visualization;Web pages","Web sites;information retrieval","Web forum pages;automated forum information processing;data block extraction;generic content extraction systems;information extraction;lightweight algorithm;link properties","","1","","18","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"Customized SLAs in Cloud Environments","N. Mavrogeorgi; V. Alexandrou; S. Gogouvitis; A. Voulodimos; D. Kiriazis; T. Varvarigou; E. K. Kolodner","Dept. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece","2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20131212","2013","","","262","269","Nowadays, the demand for online storage is increasing. Customers store their data and can retrieve them anytime and from anywhere without having to care about back up and data recovery. SLAs between Cloud providers and customers determine the level of service guaranteed by the former. A customer can be anyone from an individual person to enterprises. Customers have different needs and preferences. Our proposed system permits the creation and enforcement of customized SLAs, where customers choose the requirements that they need fulfilled. The proposed SLAs are associated with content terms pertaining to the nature of the content of the data stored. SLA enforcement is based on rules that are updated in runtime in order to proactively detect possible SLA violations and handle them in an appropriate manner.","","Electronic:978-0-7695-5094-7; POD:978-1-4799-1266-7","10.1109/3PGCIC.2013.45","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681238","Cloud;SLA Management;SLA schema;content centric storage;policies;proactive SLA violation detection","Availability;Communications technology;Containers;Measurement;Media;Medical services;Monitoring","cloud computing;contracts;customer services;information retrieval;storage management","SLA customization;SLA enforcement;SLA violation detection;cloud environment;cloud provider;customer service;data retrieval;online data storage","","0","","18","","","28-30 Oct. 2013","","IEEE","IEEE Conference Publications"
"Augmented Reality on Construction Sites Using a Smartphone-Application","K. Kirchbach","","2013 17th International Conference on Information Visualisation","20131202","2013","","","398","403","Civil Engineering is characterized by the ineffectiveness of production planning and control. The origins of these problems lie in unclear and late information. Augmented Reality (AR) can tackle these problems and help to improve the information flow. This paper describes the benefits of Augmented Reality on construction sites and introduces a smartphone application for an intuitive information retrieval on construction sites. It explains the structure of the application, especially the mathematical calculation of the AR view, and demonstrates its use in a practical case.","1550-6037;15506037","Electronic:978-0-7695-5049-7; POD:978-1-4799-0834-9","10.1109/IV.2013.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676592","augmented reality;construction site;information flow;smartphone;visualisation","","augmented reality;civil engineering computing;construction industry;information retrieval;mobile computing;smart phones","augmented reality;civil engineering;construction sites;information flow;intuitive information retrieval;production planning;smart phone","","0","","34","","","16-18 July 2013","","IEEE","IEEE Conference Publications"
"Enabling a Metric Space for Content Search in Information-Centric Networks","P. Truong; J. F. Peltier","Orange, Lannion, France","2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20131212","2013","","","186","192","The emergence of vivacious bandwidth-consuming services leveraged by the overwhelmingly content-centric usage on Internet has fostered over last years a new foundation for networking architecture design. Several Information-Centric Networking (ICN) architectures have then been proposed to tackle the challenging problems for content dissemination and distribution. Those architectures propose a forwarding plane based on a naming scheme that uniquely identifies content objects in the network regardless of their location. Routing in ICN is thus performed on content names by means of a name resolution service that can be operated independently on the current endpoint-centric networking infrastructure. In order to sketch out the efficient design of such information-centric networks, an important functionality need to be taken into account for resolving information usability which consists in finding appropriate content objects matching users' interest. We propose in this paper a novel forwarding scheme which integrates content search in the network path when resolving content names into locators (where content is actually stored). The content indexing and discovery function relies on distance sensitive hashing based DHT for similarity search between content objects. The proposed routing algorithm can be used for better performance than using classical DHTs for resolving names in ICN. We conceptually embed the high-dimension set of content objects into a low-dimension for reducing processing complexity, which allows us to design an efficient and scalable information-centric forwarding plane with in-network content search.","","Electronic:978-0-7695-5094-7; POD:978-1-4799-1266-7","10.1109/3PGCIC.2013.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681227","Distributed Hash Table (DHT);Information Centric Networking (ICN);distance;in-network content search;indexing;name resolution;routing;similarity","Communities;Convergence;Indexing;Internet;Peer-to-peer computing;Routing;Search problems","file organisation;indexing;information networks;information retrieval","DHT;ICN architectures;content discovery function;content dissemination;content distribution;content indexing;content names;content search;distance sensitive hashing;distributed hash table;endpoint-centric networking infrastructure;forwarding plane;forwarding scheme;in-network content search;information usability;information-centric networks;metric space;name resolution service;naming scheme;similarity search","","0","","8","","","28-30 Oct. 2013","","IEEE","IEEE Conference Publications"
"Improving Performance of E-Government System from the User Perspective","H. Zhang; S. Zhang; M. Xiong; S. Tang","","2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing","20131212","2013","","","2108","2113","For a B/S based e-government system, system performance is one of the metrics for evaluating the system, which determines user experience and even affects the image of the government in public. To improve the performance of the e-government system, people should pay attention not only on the process of system design (like web design) and system deployment (like using powerful web server), but also on the way of accessing data. This paper proposes a method of measuring performance from the user perspective, so as to evaluate the data access performance of the e-government system. It measures the response time of loading a page with different methods of data access, in which data are accessed by using the traditional SQL statements, view, and temporary table methods. Experiment results show that there is no significant difference in terms of performance between the traditional SQL statement method and the view method, and the method of using temporary table with any amount of data achieves better performance than the other two methods.","","Electronic:978-0-7695-5046-6; POD:978-1-4799-0631-4","10.1109/GreenCom-iThings-CPSCom.2013.396","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682406","User Perspective;Web Performance","Databases;Electronic government;Internet;Servers;Time factors;Time measurement","SQL;government data processing;information retrieval","B/S based e-government system;SQL statements;data access performance;e-government system performance;page loading;performance measurement;public government;system deployment;system design;system evaluation metrics;temporary table methods;user experience;user perspective;view method","","0","","19","","","20-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Spoken document retrieval using both word-based and syllable-based document spaces with latent semantic indexing","K. Ichikawa; S. Tsuge; N. Kitaoka; K. Takeda; K. Kita","Nagoya Univ., Nagoya, Japan","2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","20140102","2013","","","1","5","In this paper, we propose a spoken document retrieval method using vector space models in multiple document spaces. First we construct multiple document vector spaces, one of which is based on continuous-word speech recognition results and the other on continuous-syllable speech recognition results. Query expansion is also applied to the word-based document space. We proposed to apply latent semantic indexing (LSI) not only to the word-based space but also to the syllable-based space, to reduce dimensionality of the spaces using implicitly defined semantics. Finally, we combine the distances and compare the distance between the query and the available documents in various spaces to rank the documents. In this procedure, we propose to model the document by hyperplane. To evaluate our proposed method, we conducted spoken document retrieval experiments using the NTCIR-9 SpokenDoc data set. The results showed that using the combination of the distances, and using LSI on the syllable-based document space, improved retrieval performance.","","Electronic:978-986-90006-0-4; POD:978-1-4799-2794-4","10.1109/APSIPA.2013.6694119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694119","","Indexes;Large scale integration;Semantics;Speech;Speech recognition;Vectors;Web pages","document handling;indexing;information retrieval;speech recognition","LSI;NTCIR-9 SpokenDoc data set;continuous syllable speech recognition;continuous word speech recognition;hyperplane;latent semantic indexing;multiple document vector spaces;query expansion;spoken document retrieval performance;syllable based document spaces;vector space models;word based document spaces","","0","","9","","","Oct. 29 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"Integrating an enterprise architecture ontology in a case-based reasoning approach for project knowledge","A. Martin; S. Emmenegger; G. Wilke","Inst. for Inf. Syst., Univ. of Appl. Sci. Northwestern Switzerland FHNW, Olten, Switzerland","Proceedings of the First International Conference on Enterprise Systems: ES 2013","20131223","2013","","","1","12","The retrieval of historical project knowledge is still a challenge for enterprises nowadays. This paper introduces a case-based reasoning (CBR) approach for project knowledge. This approach improves the case-based reasoning by reusing enterprise specific domain knowledge that is defined in an enterprise ontology. Since the retrieval of relevant project knowledge from historical cases is a knowledge intensive task that relies heavily on enterprise specific domain knowledge, we represent both, historical cases as well as the necessary domain knowledge, in an enterprise ontology structure. The contribution of the paper is the introduction of a novel case retrieval mechanism that emphasizes enterprise specific domain knowledge by reusing an enterprise ontology named ArchiMEO. This ontology is a representation of the enterprise architecture ArchiMate® and other integrated standards. This work is based on a real-world scenario elicited from a business partner of the Swiss CTI research project [sic!]. The approach tackles the information need that might occur during a prospective project.","","Electronic:978-1-4673-6412-6; POD:978-1-4673-6413-3","10.1109/ES.2013.6690082","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690082","case-based reasoning;enterprise architecture;enterprise ontology;instance similarity","Adaptation models;Business;Cognition;Computer architecture;Information systems;Ontologies;Standards","business data processing;case-based reasoning;information needs;information retrieval;ontologies (artificial intelligence)","ArchiMEO;ArchiMate;Swiss CTI research project;case retrieval mechanism;case-based reasoning;enterprise architecture ontology;enterprise ontology structure;enterprise specific domain knowledge;information need;knowledge intensive task;project knowledge retrieval;standards","","3","","55","","","7-8 Nov. 2013","","IEEE","IEEE Conference Publications"
"OnPerDis: Ontology-Based Personal Name Disambiguation on the Web","Z. Lu; Z. Yan; L. He","Dept. of Comput. Sci. & Technol., East China Normal Univ., Shanghai, China","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","185","192","With the growth of web documents, the ambiguity of personal name becomes more common and brings poor performance of web search. Identifying a correct personal entity from the a piece of or the whole document is still a very challenging problem, especially for Chinese websites. In this paper, we propose a novel Ontology-based approach for Personal Name Disambiguation (named ""OnPerDis""). This approach has two main steps: first, we construct person ontology (PO) with rich conceptual modeling as well as a large set of supporting instances, second, for a given personal name on the web, we create a temporary instance and extract features from the web documents, calculate the similarity between this temporary instance and the instances in the PO. The one with the highest similarity score is chosen as the appropriate personal name. Our extensive evaluations with two rich real-life datasets (CIPS-SIGHAN 2012 NERD and Chinese web documents) shows OnPerDis' efficacy on personal name disambiguation on the Web.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690013","Conceptual modeling;Instance matching;Ontology population;Personal name disambiguation","Data mining;Educational institutions;Encyclopedias;Feature extraction;Ontologies;Sociology;Statistics","Web sites;document handling;feature extraction;information retrieval;natural language processing;ontologies (artificial intelligence);pattern matching","CIPS-SIGHAN 2012 NERD;Chinese Web documents;Chinese Web sites;OnPerDis;PO;Web search;conceptual modeling;feature extraction;ontology-based personal name disambiguation;person ontology;personal entity Identification;personal name ambiguity;real-life datasets;similarity score;temporary instance","","0","","27","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"An Efficient Recommendation Method for Improving Business Process Modeling","Y. Li; B. Cao; L. Xu; J. Yin; S. Deng; Y. Yin; Z. Wu","Sch. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","IEEE Transactions on Industrial Informatics","20131212","2014","10","1","502","513","In modern commerce, both frequent changes of custom demands and the specialization of the business process require the capacity of modeling business processes for enterprises effectively and efficiently. Traditional methods for improving business process modeling, such as workflow mining and process retrieval, still requires much manual work. To address this, based on the structure of a business process, a method called workflow recommendation technique is proposed in this paper to provide process designers with support for automatically constructing the new business process that is under consideration. In this paper, with the help of the minimum depth-first search (DFS) codes of business process graphs, we propose an efficient method for calculating the distance between process fragments and select candidate node sets for recommendation purpose. In addition, a recommendation system for improving the modeling efficiency and accuracy was implemented and its implementation details are discussed. At last, based on both synthetic and real-world datasets, we have conducted experiments to compare the proposed method with other methods and the experiment results proved its effectiveness for practical applications.","1551-3203;15513203","","10.1109/TII.2013.2258677","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504513","Business process modeling;enterprise systems;industrial informatics;string edit distance;workflow;workflow recommendation","","business data processing;commerce;data mining;graph theory;information retrieval;recommender systems","DFS;business process graphs;business process modeling;candidate node sets;commerce;custom demands;enterprises;minimum depth-first search codes;process retrieval;recommendation method;workflow mining;workflow recommendation technique","","30","","34","","20130419","Feb. 2014","","IEEE","IEEE Journals & Magazines"
"Shape Retrieval for Khon 3D Model","S. Rodkhwan; P. Kanongchaiyos","Dept. of Comput. Eng., Chulalongkorn Univ., Bangkok, Thailand","2013 International Conference on Culture and Computing","20131212","2013","","","68","73","We proposed a method for shape retrieval on collection of Khon 3D model. Khon is Thai traditional mask dance. Khon 3D model is important for not only the archival of dance but also for dance technology or digital content. Furthermore, Khon 3D model are complex and unique especially their structure and surface. Moreover, Khon 3D model compose of numerous components and costume with Thai patterns, demonstrating the aesthetic of Thai culture. Therefore our method proposes using Reeb graph to represent structural properties and decomposition into subparts to represent complex Khon 3D model. Moreover, the surface of each subpart is described by a Pose Oblivious Shape Signature. We measure similarity by comparing structure of Khon 3D model based on the Approximate Maximum Common Sub graph which preserves topology. From average precision, we successfully to retrieval on Khon 3D model. The average precision of our method is 0.61 while previous work is 0.51.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680333","Khon 3D model Retrieval;Partial Shape;Reeb Graph;Retrieval","Approximation methods;Computational modeling;Deformable models;Level measurement;Shape;Solid modeling;Three-dimensional displays","graph theory;humanities;information retrieval;solid modelling","Reeb graph;Thai culture;Thai patterns;Thai traditional mask dance;approximate maximum common subgraph;complex Khon 3D model;dance archival;dance technology;decomposition;digital content;pose oblivious shape signature;shape retrieval;structural property","","0","","25","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Ontology-based model in tourism context-aware systems","S. Alhazbi; L. Lotfi; R. Ali; R. Suwailih","Comput. Sci. & Eng. Dept., Qatar Univ., Doha, Qatar","2013 International Conference on ICT Convergence (ICTC)","20131202","2013","","","775","779","Different approaches have been used to model contextual information in context-aware systems. Comparing to other models, ontology-based modeling offers more expressiveness, semantically sharing and interoperability. It supports reasoning tasks in a better way than other approaches. However, the main concern with ontology implementation is the expensive computational request for the reasoning process which makes this model unsuitable for critical time application. In tourism domain, using ontology model affects response time which is unacceptable for on-the-move tourists. Late response based on current tourist location might recommend him points of his interest that are already behind him especially when he is riding. In this paper, we propose an approach that uses current user location and speed to provide tourist with a recommendation about points of his interest before reaching them. Besides location and user speed, our approach is aware of network speed as an important factor that affects response time, and consequently impacts calculation of predicted user location.","2162-1233;21621233","Electronic:978-1-4799-0698-7; POD:978-1-4799-0696-3; USB:978-1-4799-0697-0","10.1109/ICTC.2013.6675476","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6675476","Context-aware;ontology-based model;points of interests;reasoning;tourism domain","","inference mechanisms;information retrieval;ontologies (artificial intelligence);recommender systems;travel industry;ubiquitous computing","contextual information;critical time application;current user location;expressiveness;interoperability;network speed;on-the-move tourist;ontology-based model;reasoning process;reasoning task;recommendation;response time;semantic sharing;tourism context-aware systems;tourism domain;tourist location","","0","","32","","","14-16 Oct. 2013","","IEEE","IEEE Conference Publications"
"Multi-lingual Analysis of Future-Related Information on the Web","A. Jatowt; H. Kawai; K. Kanazawa; K. Tanaka; K. Kunieda; K. Yamada","Kyoto Univ., Kyoto, Japan","2013 International Conference on Culture and Computing","20131212","2013","","","27","32","Future prediction is one of the crucial activities of humans. In this paper, we report the results of exploratory analysis of future-related information on the Web in three different languages: English, Japanese and Polish. We focus on the future-related information which is grounded in time, that is, the information on events whose expected occurrence dates are already known. Our datasets are constructed by crawling search engine indices. We investigate multiple aspects of future-related information in web pages across different languages such as its amount, time span, topics, associated sentiment levels as well as the relation to the future-related content in news articles.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680326","collective predictions;future-related information;multi-lingual analysis;opinion analysis","Forecasting;Market research;Meteorology;Search engines;Sociology;Statistics;Web pages","Web sites;information retrieval;natural language processing;search engines;text analysis","English;Japanese;Polish;Web page;crawling search engine indices;future related information prediction;multilingual analysis;news article","","0","","19","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Improving User Control and Transparency in the Digital Humanities","C. Hampson; G. Munnelly; E. Bailey; S. Lawless; O. Conlan","Knowledge & Data Eng. Group, Trinity Coll., Dublin, Ireland","2013 International Conference on Culture and Computing","20131212","2013","","","196","197","Digital humanities is a research field focused on the intersection of computer science and the humanities. The development of new search, analysis, personalisation and collaboration tools are some examples of how information technology has hugely benefited humanities practitioners. Despite the obvious benefits computers bring, users often feel a lack of control over, or understanding of, the automatic processing underlying digital humanities systems. This control is important to curators of digital archives who need to maintain the integrity of their collection, as well as to end users who might not understand why they are receiving particular recommendations or search results. The CULTURA project is developing a next-generation digital humanities portal, which provides users with more control over the underlying systems and makes more transparent the processes involved. This paper discusses this with reference to entity extraction and personalisation techniques.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.62","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680376","CULTURA;Digital Humanities;Entity Extraction;Personalisation;User Control","Accuracy;Cultural differences;Data mining;Feature extraction;Portals;Process control;Tag clouds","humanities;information retrieval;portals;recommender systems","CULTURA project;automatic processing;collaboration tools;computer science;digital archive curators;digital humanities systems;entity extraction;information technology;next-generation digital humanities portal;personalisation techniques;personalisation tools;user control","","0","","1","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Managing a Coastal Sensors Network in a Nowcast-Forecast Information System","J. Gomes; M. Rodrigues; A. Azevedo; G. Jesus; J. Rogeiro; A. Oliveira","Hydraulics & Environ. Dept., LNEC, Lisbon, Portugal","2013 Eighth International Conference on Broadband and Wireless Computing, Communication and Applications","20131223","2013","","","518","523","This paper presents a study on the integration of heterogeneous sensor nodes into a wireless sensor network and its use in providing real-time information about water conditions, contributing to water resources management. The work focuses on presenting the methodology used to automatically communicate, parse and store the sensor network data and its application to a real case study. The development of a new platform for efficient management and analysis of the gathered data and its use for automatic assertion of real-time model forecasts quality against stored data is also presented. On-going work on the development of a mobile application for real-time data access in mobile devices is also anticipated.","","Electronic:978-0-7695-5093-0; POD:978-1-4799-1468-5","10.1109/BWCCA.2013.89","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690939","coastal systems;mobile application development;mobile visualisation of data;real time spatial data analysis;real-time forecasting;visualization tools","Data visualization;Monitoring;Predictive models;Real-time systems;Sea measurements;Sensors;Wireless sensor networks","environmental science computing;forecasting theory;information retrieval;mobile computing;mobile handsets;real-time systems;storage management;water supply;wireless sensor networks","coastal sensors network;heterogeneous sensor nodes;mobile devices;nowcast-forecast information system;real-time data access;real-time information;real-time model forecasts quality;sensor network data;stored data;water conditions;water resources management;wireless sensor network","","0","","22","","","28-30 Oct. 2013","","IEEE","IEEE Conference Publications"
"Treatment tuberculosis retrieval using decision tree","S. Benbelkacem; B. Atmani; M. Benamina","Comput. Sci. Lab. of Oran (LIO), Univ. of Oran, Oran, Algeria","2013 International Conference on Control, Decision and Information Technologies (CoDIT)","20131223","2013","","","283","288","Due to the large volume of data generated in healthcare organizations, the use of data mining techniques becomes essential for improving the quality of care, physician practices and disease management. However, expert knowledge is not based only on rules, but also on a mixture of knowledge and experiences. It is in this context that we set the involvement of data mining techniques and CBR to support medical decision making in order to optimize the time and benefit from the experience of experts. We propose a support system for medical decision-making based on CBR and data mining. This system allows, from a database of examples, engaging a method of Symbolic induction and Cellular Inference Engine (MIC) for the construction of a case retrieval model. To evaluate this new approach we have customized the platform jCOLIBRI with a real case base about the treatment of tuberculosis.","","Electronic:978-1-4673-5549-0; POD:978-1-4673-5548-3","10.1109/CoDIT.2013.6689558","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6689558","","Cognition;Data mining;Decision trees;Interrupters;Medical diagnostic imaging;Medical services;Training","case-based reasoning;data mining;decision making;decision support systems;decision trees;diseases;information retrieval;medical diagnostic computing;patient treatment","CBR;MIC;case retrieval model;cellular inference engine;data mining techniques;decision tree;disease management;expert knowledge;healthcare organizations;jCOLIBRI platform;medical decision making;physician practices;quality of care;support system;symbolic induction;treatment tuberculosis retrieval","","0","","18","","","6-8 May 2013","","IEEE","IEEE Conference Publications"
"Geodemlia: Persistent storage and reliable search for peer-to-peer location-based services","C. Gross; B. Richerzhagen; D. Stingl; C. Münker; D. Hausheer; R. Steinmetz","Multimedia Commun. Lab., Tech. Univ. Darmstadt, Darmstadt, Germany","IEEE P2P 2013 Proceedings","20131219","2013","","","1","2","Location-based services have become increasingly popular in the recent years due to the vast deployment of position-aware devices such as smartphones and tablet PCs and the ubiquitous availability of fast Internet connectivity. Existing location-based services are realized as cloud services, which cause considerably high costs. Furthermore, they are not location-aware leading to unnecessary long transmission paths between the users and the cloud infrastructure. The concept of Peer-to-Peer has proven to be a valid alternative for realizing the functionality of location-based services, which resulted in a plethora of approaches for location-based search [1], [4], [5]. Existing concepts, however, suffer from two major drawbacks: (i) they are not robust against high peer churn and (ii) they do not allow for the persistent storage of location-based data. To this end, in this demo we present the prototype of the overlay Geodemlia [3], which allows for both: the persistent storage of location-based information as well as the reliable search even under high churn rates. Location-based information in Geodemlia is stored in a location-aware way, reducing the length of the transmission path for store and search operations.","2161-3559;21613559","Electronic:978-1-4799-0521-8; POD:978-1-4799-0514-0","10.1109/P2P.2013.6688730","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688730","churn;location-based search;overlay;peer-to-peer;replication","Data models;Mobile radio mobility management;Peer-to-peer computing;Prototypes;Reliability;Routing;Search problems","geographic information systems;information retrieval;information storage;mobile computing;overlay networks;peer-to-peer computing","Internet connectivity;location-based data storage;location-based persistent information storage;overlay Geodemlia;peer-to-peer location-based services;position-aware devices;reliable information search;smartphones;tablet PC;transmission path length reduction","","0","","7","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Toward Creating a Gold Standard of Drug Indications from FDA Drug Labels","R. Khare; J. Li; Z. Lu","Nat. Center for Biotechnol. Inf., U.S. Nat. Libr. of Med., Bethesda, MD, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","30","35","Having quick access to trustworthy drug-disease relationships (which drug(s) are approved for treating or preventing which disease(s)) is one of the top information needs of health providers, consumers, and researchers. This paper presents a semi-automatic approach that can lead to the creation of a gold standard of drugs and their indications. As our system input, we use the Daily Med, which houses the most current drug labels submitted to FDA by pharmaceutical companies. Extraction of specific indications from FDA labels is a challenging problem that requires distinguishing indications from other disease mentions. In response, we first identify the candidate indications from drug labels using UMLS resources and BioNLP tools, and then rely on expert judgments to validate those pre-computed indications through an interactive Web interface. For preliminary analysis, we recruited two experts to manually annotate 100 labels of frequently sought human prescription drugs at PubMed Health. We find that the resultant expert-curated gold standard on drugs and their indications is high-quality (precision=97%, recall=94%), and differs from existing resources in that it is factual, structured, and dose-form specific. The study findings suggest the feasibility of the proposed method toward building a comprehensive resource of drug indications.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680458","annotation;drug indication;expert curation;gold standard","Diseases;Drugs;Gold;Guidelines;Joints;Standards;Unified modeling language","diseases;drugs;information retrieval;interactive systems;medical information systems;user interfaces","BioNLP tools;DailyMed;FDA drug labels;PubMed Health;UMLS resources;comprehensive drug indication resource;disease prevention;disease treatment;drug indications;expert-curated gold standard;human prescription drugs;interactive Web interface;manual label annotation;pharmaceutical companies;precision value;recall value;semiautomatic approach;system input;trustworthy drug-disease relationship access","","1","","18","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Context-Aware Baby Monitor for the Automatic Selective Archiving of the Language of Infants","E. Rincon; J. Beltran; M. Tentori; J. Favela; E. Chavez","Comput. Sci. Dept., CICESE, Ensenada, Mexico","2013 Mexican International Conference on Computer Science","20131212","2013","","","60","67","Understanding the language of a newborn is not an easy task, and parents use baby monitors to monitor the language of their baby and infer the baby's needs. Available baby monitors trying to identify the language of babies consider the information after the baby is crying, and according to pediatricians the sounds and movements before crying define what the baby wants. In this paper, we explore the automatic selective archiving of the movements and sounds infants make before crying to give parents information they could later use to reflect on these behaviors and determine what the infant wants. Following a user-centered design methodology we developed a capture and access tool that uses accelerometers to detect when the infant is moving, and an algorithm to detect when the infant is crying. The movement detection algorithm uses a motion shimmer sensor and computes the magnitude exerted over the three axes, and the cry detection algorithm uses a Multi-Band Spectral Entropy Signature (MBSES) and a Support Vector Machine (SVM) to detect sustained crying. To show the feasibility of the performance of our system under realistic conditions, we tested how our sound algorithm performs under noise scenarios. The results show our algorithm is accurate, 98% precision, and performs better than algoritms using the MFCC feature. We close discussing directions for future work.","1550-4069;15504069","Electronic:978-0-7695-5087-9; POD:978-1-4799-1145-5","10.1109/ENC.2013.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679821","capture and access tools;cry detection;selective archiving;spectral entropy","Algorithm design and analysis;Feature extraction;Indexes;Manuals;Monitoring;Pediatrics;Tracking","human computer interaction;information retrieval systems;spectral analysis;support vector machines;ubiquitous computing","MBSES;SVM;automatic selective archiving;context-aware baby monitor;cry detection algorithm;infant language;language monitor;motion shimmer sensor;movement detection algorithm;multiband spectral entropy signature;support vector machine;user-centered design methodology","","1","","27","","","Oct. 30 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"An Ontology of the Worldview of Islam to Annotate Islamic Texts Digital Archives","J. Bourdon; M. S. B. Rosli","Center for Integrated Area Studies (CIAS), Kyoto Univ., Kyoto, Japan","2013 International Conference on Culture and Computing","20131212","2013","","","216","217","Magazines and newspapers written in Jawi, an Arabic script used to write Malay until the late 50's, provided an active arena for discussions. In order to understand what happened in this era, we started to provide these in digital form. However, it is difficult to understand the documents contained in this digital archive without knowing their context. Particularly, as Jawi was used mainly by scholars of Islam, a lot of Islam key terms are left unexplained. In this paper, we created an ontology of the worldview of Islam and propose a semantic approach to let expert users annotate the archive so that the context becomes explicit. We present our architecture and illustrate it with the annotation of an article extracted from the Qalam digital archive.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.72","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680386","Digital Archives;Islam;Jawi;Semantic Web;semantic annotations","Context;Cultural differences;Databases;Electronic mail;Face;Ontologies;Semantics","humanities;information retrieval;information retrieval systems;ontologies (artificial intelligence);text analysis","Arabic script;Islam key terms;Islam worldview;Islamic scholars;Islamic text digital archive annotation;Jawi language;Malay language;Qalam digital archive;article annotation;explicit context;magazines;newspapers;ontology;semantic approach","","0","","9","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"On Mapping the Ontologies of Leishu - A Preliminary Investigation","C. H. Chung; J. Hsiang","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan","2013 International Conference on Culture and Computing","20131212","2013","","","198","199","Leishu is a unique form of ancient Chinese reference books that seem to have no counterpart in the western civilization. The original purpose of leishu was to provide quick reference to ancient texts. Therefore a leishu usually contains two parts, a knowledge structure and a large number of quoted texts from older books. In this paper we outline a preliminary investigation on two important leishu, yiwenleiju and taipingyulan, written around 624AD and 977AD, respectively. We processed the texts of both books by identifying the entries (quoted texts) with their sources, and built a system for full-text search and analysis. Two additional purposes of the system are to provide a way to compare the texts and explore such questions as text inherence and the change of the knowledge structures. We use ""fish"" as an example to illustrate our point.","","Electronic:978-0-7695-5047-3; POD:978-1-4799-0617-8","10.1109/CultureComputing.2013.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680377","leishu;ontology;taipingyulan;yiwenleiju","Algorithm design and analysis;Books;Educational institutions;Information technology;Marine animals;Ontologies;Writing","information retrieval;ontologies (artificial intelligence);text analysis","ancient Chinese reference books;ancient texts;full-text analysis;full-text search;knowledge structure;leishu;quoted texts;taipingyulan;text inherence;text processing;yiwenleiju","","0","","6","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Towards a more efficient sparse coding based audio-word feature extraction system","C. C. M. Yeh; Y. H. Yang","Res. Center for Inf. Technol. Innovation, Acad. Sinica, Taipei, Taiwan","2013 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference","20140102","2013","","","1","7","This paper is concerned with the efficiency of sparse coding based audio-word feature extraction system. In particular, we have defined and added the concept of early and late temporal pooling to the classic sparse coding based audio-word feature extraction pipeline, and we have tested them on the genre tags subset of the CAL10k data set. We define temporal pooling as any functions that are able to transforms the input time series representation into a more temporally compact representation. Under this definition, we have examined the following two temporal pooling functions for improving the feature extraction's efficiency, and they are: Early Texture Window Pooling and Multiple Frame Representation. Early texture window pooling tremendously boost the efficiency by compromising the retrieving accuracy, while multiple frame representation slightly improve both the feature extracting efficiency and retrieving accuracy. Overall, our best feature extraction setup achieves 0.202 in mean average precision on the genre tags subset of the CAL10k data set.","","Electronic:978-986-90006-0-4; POD:978-1-4799-2794-4","10.1109/APSIPA.2013.6694252","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694252","","Accuracy;Computational efficiency;Dictionaries;Encoding;Feature extraction;Pipelines;Vectors","audio coding;feature extraction;information retrieval;time series;word processing","CAL10k data set;early temporal pooling concept;early texture window pooling;feature extracting efficiency;feature retrieving accuracy;genre tag subset;input time series representation;late temporal pooling concept;multiple frame representation;sparse coding based audio-word feature extraction system;temporally compact representation","","3","","31","","","Oct. 29 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"Public-Key Encryption with Keyword Search from Lattice","C. Hou; F. Liu; H. Bai; L. Ren","China Mobile Commun. Corp. Res. Inst., Beijing, China","2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing","20131212","2013","","","336","339","Public-keyword encryption with keyword search (PEKS) is a newly emerged primitive which can provide data search functionality that is absent in traditional public-key encryption (PKE). In this work, we propose the first provably secure PEKS from lattice.","","Electronic:978-0-7695-5094-7; POD:978-1-4799-1266-7","10.1109/3PGCIC.2013.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681250","Lattice;Learning with Errors;PEKS;Provably Secure","Encryption;Games;Lattices;Public key;Zinc","information retrieval;public key cryptography","PEKS;PKE;data search functionality;public-key encryption;public-keyword encryption with keyword search","","1","","26","","","28-30 Oct. 2013","","IEEE","IEEE Conference Publications"
"Which Feature Location Technique is Better?","E. Hill; A. Bacchelli; D. Binkley; B. Dit; D. Lawrie; R. Oliveto","Montclair State Univ., Montclair, NJ, USA","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","408","411","Feature location is a fundamental step in software evolution tasks such as debugging, understanding, and reuse. Numerous automated and semi-automated feature location techniques (FLTs) have been proposed, but the question remains: How do we objectively determine which FLT is most effective? Existing evaluations frequently use bug fix data, which includes the location of the fix, but not what other code needs to be understood to make the fix. Existing evaluation measures such as precision, recall, effectiveness, mean average precision (MAP), and mean reciprocal rank (MRR) will not differentiate between a FLT that ranks higher these related elements over completely irrelevant ones. We propose an alternative measure of relevance based on the likelihood of a developer finding the bug fix locations from a ranked list of results. Our initial evaluation shows that by modeling user behavior, our proposed evaluation methodology can compare and evaluate FLTs fairly.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676919","Concern location;Empirical studies;Feature location;Relevance measures","Computer bugs;Educational institutions;Measurement;Navigation;Software maintenance;Topology","information retrieval;program debugging;reverse engineering","FLTs;IR technique;MAP;MRR;bug fix data;information retrieval technique;mean average precision;mean reciprocal rank;rank topology metric;semiautomated feature location techniques;software evolution tasks","","1","","11","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
"iPEKS: Fast and Secure Cloud Data Retrieval from the Public-Key Encryption with Keyword Search","F. K. Tseng; R. J. Chen; B. S. P. Lin","Dept. of Comput. Sci., Nat. Chiao-Tung Univ., Hsinchu, Taiwan","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications","20131212","2013","","","452","458","In recent years, considerable concern has arisen over the security of the data stored in the cloud. A number of studies have suggested the use of cryptographic primitives to protect the data. As these tools transform the data into an unintelligible form, secure and efficient retrieval of the encrypted data from the cloud becomes a major challenge. The public-key encryption with keyword search (PEKS) scheme and many of its variants have been proposed to respond to this challenge. However, given a large number of data (or searchable keywords) would be tested sequentially in these PEKS schemes, previous search results should be employed to improve the efficiency of future searches. In this paper, we present an interactive construction named iPEKS where the search time is linear to the total number of distinct searched keywords instead of the total number of the searchable keywords. The more the keywords have been searched previously, the better the efficiency can be improved. We provide theoretical analysis to show the security and privacy. In addition, implementation and performance experiments exhibit a great improvement in efficiency compared with the previous schemes.","2324-898X;2324898X","Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9","10.1109/TrustCom.2013.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680874","cloud data;implementation;interactive search;public-key encrith keyword search;search efficiency","Cloud computing;Encryption;Public key;Receivers;Servers","cloud computing;data privacy;information retrieval;public key cryptography","cryptographic primitives;data encryption;data protection;data storage;iPEKS;public key encryption with keyword search;secure cloud data retrieval","","3","","23","","","16-18 July 2013","","IEEE","IEEE Conference Publications"
"Pasokh: A standard corpus for the evaluation of Persian text summarizers","B. Behmadi Moghaddas; M. Kahani; S. A. Toosi; A. Pourmasoumi; A. Estiri","Web Technol. Lab., Ferdowsi Univ. of Mashhad, Mashhad, Iran","ICCKE 2013","20131216","2013","","","471","475","The increasingly vast amount of information, particularly on the Web, has resulted in a profound need for automatic summarization systems. The systems, in turn, need to be evaluated in terms of how desirably they can retrieve information. The evaluation is done by comparing the machine summaries against a standard reference corpus containing a reasonably large number of text sources and the summaries that human beings have made out of them. Due to the lack of such a standard corpus for Persian, the summarizers that were developed used to be evaluated against the small corpora constructed by the developers of the proposed systems. This made the systems non-comparable. Thus, Pasokh was constructed as a standard large enough reference corpus. It took over 2000 man-hours of work.","","Electronic:978-1-4799-2093-8; POD:978-1-4799-2094-5","10.1109/ICCKE.2013.6682873","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682873","computational processing of Persian;evaluation corpus;evaluation of automatic summarization;multi-document automatic summarization;single-document automatic summarization","Calendars;Cultural differences;Databases;Economics;Guidelines;Standards;XML","information retrieval;natural language processing;text analysis","Pasokh;Persian text summarizer evaluation;automatic summarization systems;information retrieval;machine summaries;reference corpus;text sources","","1","","14","","","Oct. 31 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"Improving Computational Efficiency for Personalized Medical Applications in Mobile Cloud Computing Environment","G. Mathew; Z. Obradovic","Center for Data Analytics & Biomed. Inf., Temple Univ., Philadelphia, PA, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","535","540","Mobile computing and cloud services are two technologies that have gained momentum in recent times. The proliferation of mobile computing devices and network connectivity has made it an attractive platform for delivering personalized services in many business domains including healthcare. Personalized health and wellness mobile applications have computational and data requirements that are necessitated by the localized processing needs of the application. The on-demand provisioning capability and elasticity of cloud services combined with the local processing capability of mobile devices can provide an ecosystem for pervasive access to health information. In this study we explore some of the specifics of these health and wellness applications. We introduce promotion algorithm as a mechanism to efficiently process data points locally by mobile devices. This algorithm can take advantage of the local processing power of smart phones and help reduce communication costs between mobile endpoint and cloud-based long-term data services. Experiments were performed using an Android smart phone for real time data acquisition of more than 10 million data points and a Linux server in a private cloud over 4G network simulating a health service. Results showed that the proposed algorithm could help preserve battery life by a factor of 10 and reduce data communication time by a factor of 20 as a result of utilizing local computation on a mobile device.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.83","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680531","medical informatics;mobile cloud computing;personalized wellness","Batteries;Cloud computing;Mobile communication;Mobile computing;Servers;Smart phones","4G mobile communication;Linux;cloud computing;cost reduction;data acquisition;health care;information retrieval;medical information systems;mobile computing;smart phones","4G network;Android smartphone;Linux server;business domain;cloud-based long term data service;communication cost reduction;computational efficiency;computational requirement;data acquisition;data communication time reduction;data process;data requirement;ecosystem;elasticity;health service;healthcare;mobile cloud computing environment;mobile computing device;mobile endpoint;network connectivity;on-demand provisioning capability;personalized medical application;personalized service delivery;pervasive health information access;private cloud;promotion algorithm;wellness mobile application","","4","","23","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"DSN: A Knowledge-Based Scholar Networking Practice Towards Research Community","J. Zhao; K. Dong; J. Yu; W. Hu","Comput. Network Inf. Center, Beijing, China","2013 IEEE 9th International Conference on e-Science","20131216","2013","","","238","244","In this paper, we carry out a knowledge-based scholar network practice towards Research community, named Research Social Networking, shortly DSN, by setting up a large knowledge base of scientists. We discuss key technologies in the paper, including scholar disambiguation and relationship extraction with the better performance evaluation than traditional methods. The DSN system has been implemented and integrated with Duckling cloud service, known as Research Online, with more than 60 thousand scientists and 100 thousand papers.","","Electronic:978-0-7695-5083-1; POD:978-1-4799-0870-7","10.1109/eScience.2013.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683913","collaboration environment;data-intensive;knowledge base;search engine;social network","Collaboration;Communities;Computational modeling;Knowledge based systems;Search engines;Social network services;Support vector machines","information retrieval;knowledge based systems;social networking (online)","DSN system;Duckling cloud service;knowledge base;knowledge-based scholar networking practice;performance evaluation;relationship extraction;research community;research online;research social networking;scholar disambiguation","","0","","19","","","22-25 Oct. 2013","","IEEE","IEEE Conference Publications"
"Sociometric Methods for Relevancy Analysis of Long Tail Science Data","A. Rajasekar; S. Sankaran; H. Lander; T. Carsey; J. Crabtree; H. C. Kum; M. Crosas; G. King; J. Zhan","Univ. of North Carolina Chapel Hill, Chapel Hill, NC, USA","2013 International Conference on Social Computing","20140102","2013","","","1","6","As the push towards electronic storage, publication, curation, and discoverability of research data collected in multiple research domains has grown, so too have the massive numbers of small to medium datasets that are highly distributed and not easily discoverable - a region of data that is sometimes referred to as the long tail of science. The rapidly increasing, sheer volume of these long tail data present one aspect of the Big Data problem: how does one more easily access, discover, use, and reuse long tail data to lead to new multidisciplinary collaborative research and scientific advancement? In this paper, we describe Data Bridge, a new e-science collaboration environment that will realize the potential of long tail data by implementing algorithms and tools to more easily enable data discoverability and reuse. Data Bridge will define different types of semantic bridges that link diverse datasets by applying a set of sociometric network analysis (SNA) and relevance algorithms. We will measure relevancy by examining different ways datasets can be related to each other: data to data, user to data, and method to data connections. Through analysis of metadata and ontology, by pattern analysis and feature extraction, through usage tools and models, and via human connections, Data Bridge will create an environment for long tail data that is greater than the sum of its parts. In the project's initial phase, we will test and validate the new tools with real-world data contained in the Data verse Network, the largest social science data repository. In this short paper, we discuss the background and vision for the Data Bridge project, and present an introduction to the proposed SNA algorithms and analytical tools that are relevant for discoverability of long tail science data.","","Electronic:978-0-7695-5137-1; POD:978-1-4799-1519-4","10.1109/SocialCom.2013.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693303","Long tail data;data discoverability;sociometric network analysis","Algorithm design and analysis;Collaboration;Communities;Data models;Distributed databases;Educational institutions;Electrical resistance measurement","feature extraction;groupware;indexing;information retrieval;meta data;ontologies (artificial intelligence);research and development;storage management","Data Bridge project;SNA;big data problem;data curation;data verse network;discoverability;e-science collaboration environment;electronic storage;feature extraction;long tail science data;meta data;multidisciplinary collaborative research;ontology;pattern analysis;publication;relevancy analysis;research data;scientific advancement;sociometric network analysis","","0","","46","","","8-14 Sept. 2013","","IEEE","IEEE Conference Publications"
"Finding Dominating Set from Verbal Contextual Graph for Personalized Search in Folksonomy","T. Jin; H. Xie; J. Lei; Q. Li; X. Li; X. Mao; Y. Rao","Shanghai Key Lab. of Intell. Inf. Process., Fudan Univ., Shanghai, China","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","1","","367","372","With the development of the Internet, user-generated data has been growing tremendously in Web 2.0 era. Facing such a big volume of resources in folksonomy, people need a method of fast exploration and indexing to find their demanded data. To achieve this goal, contextual information is indispensable and valuable to understand user preference and purpose. In sociolinguistics, context can be mainly categorized as verbal context and social context. Comparing with verbal context, social context not only requires domain knowledge to pre-define contextual attributes but also acquires additional data from users. However, there is no research of addressing irrelevant contextual factors for verbal context model so far. The dominating set from verbal context proposed in this paper is to fill this blank. We present the verbal context in folksonomy to capture the user intention, and propose a dominating set discovering method for this verbal context model to prune the irrelevant contextual factors and keep the major characteristics at the same time. Furthermore, the experiments, which are conducted on a public data set, show that the proposed method gives convincing results.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690038","Dominating set;context;folksonomy;personalized search","Context;Context modeling;Measurement;Mood;Motion pictures;Semantics;Vectors","Internet;graph theory;information retrieval;social sciences computing","Web 2.0;contextual attributes;contextual information;domain knowledge;dominating set discovering method;folksonomy;irrelevant contextual factors;personalized search;public data set;social context;sociolinguistics;user intention;user preference;user purpose;user-generated data;verbal contextual graph","","2","","24","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"An Unsupervised Data-Driven Cross-Lingual Method for Building High Precision Sentiment Lexicons","P. Sangiorgi; A. Augello; G. Pilato","ICAR (Ist. di Calcolo e Reti ad Alte Prestazioni), Palermo, Italy","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","184","190","In this paper we present a completely unsupervised approach for creating a sentiment lexicon. The approach has been realized by designing a pipeline which implements an unsupervised system that covers different aspects: the automatic extraction of user reviews, the pre-processing of text, the use of a scoring measure which combines: entropy, term frequency, inverse document frequency, and finally a cross lingual intersection. We have validated the approach though the analysis of a previews present in the Google Play market. The results show the effectiveness of the approach given by satisfactory values of precision for the obtained lexicon.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693515","Machine Learning;Sentiment Analysis;Sentiment Lexicon","Buildings;Dictionaries;Entropy;Frequency measurement;Google;Pipelines;Pragmatics","computational linguistics;entropy;information retrieval;text analysis;unsupervised learning","Google Play market;cross lingual intersection;entropy;high precision sentiment lexicons;inverse document frequency;scoring measure;term frequency;text preprocessing;unsupervised data-driven cross-lingual method;unsupervised system;user reviews automatic extraction","","0","","22","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"New Horizons for Patient Safety: LIGRA (LIfe Guard for Robotic Surgery Assistance), an Interactive Platform Centralizing Information and Control Robotic Surgery Operating Rooms","J. Vaucher; H. Bleuler; E. Oleari; A. Morandi; M. Verga; A. Sanna","Robot. Syst. Lab. LSRO, EPFL, Lausanne, Switzerland","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","361","366","The present paper describes LIGRA, an innovative technological solution developed in the context of the SAFROS co-funded EU project. Its general goal is to improve patient safety during minimal invasive robotic surgery. Focusing on the risks introduced in the surgical workflow by new robotic technologies, the LIGRA project consists in the design and implementation of a graphical user interface (GUI), on top of a dedicated middleware for message transport. This GUI is intuitive, non-invasive and versatile. Being visible to the whole surgical staff and easily understandable, the interface monitors the status of all robotic components, informs about any potential dangerous situation and displays the needed steps to recover. In addition, LIGRA records the data sent by the components, allowing retrieving technical and procedural information about the surgery and the staff afterwards. Furthermore, it gives access to an interactive version of the World Health Organization Patient Safety Checklist that recalls the essential checks and verifications to be made prior, during, and after the surgery. The approach adopted during this contribution, justified by the lack of a similar tool in OR nowadays, is to establish a common framework, which could have an important positive impact on patient safety.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680497","guard;interface;monitoring;observer;patient safety;robotic surgery;supervisory platform","Context;Monitoring;Robots;Safety;Surgery;Usability","graphical user interfaces;information retrieval;interactive systems;medical administrative data processing;medical robotics;middleware;patient care;surgery","GUI;LIGRA project;OR;SAFROS co-funded EU project;World Health Organization Patient Safety Checklist;graphical user interface;innovative technological solution;interactive platform;life guard for robotic surgery assistance;message transport;middleware;minimal invasive robotic surgery;procedural information retrieval;robotic component status monitoring;robotic surgery operating rooms;surgical staff;technical information retrieval","","0","","18","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Enhancing Software Traceability by Automatically Expanding Corpora with Relevant Documentation","T. Dasgupta; M. Grechanik; E. Moritz; B. Dit; D. Poshyvanyk","Univ. of Illinois at Chicago, Chicago, IL, USA","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","320","329","Software trace ability is the ability to describe and follow the life of a requirement in both a forward and backward direction by defining relationships to related development artifacts. A plethora of different trace ability recovery approaches use information retrieval techniques, which depend on the quality of the textual information in requirements and software artifacts. Not only is it important that stakeholders use meaningful names in these artifacts, but also it is crucial that the same names are used to specify the same concepts in different artifacts. Unfortunately, the latter is difficult to enforce and as a result, software trace ability approaches are not as efficient and effective as they could be - to the point where it is questionable whether the anticipated economic and quality benefits were indeed achieved. We propose a novel and automatic approach for expanding corpora with relevant documentation that is obtained using external function call documentation and sets of relevant words, which we implemented in Trace Lab. We experimented with three Java applications and we show that using our approach the precision of recovering trace ability links was increased by up to 31% in the best case and by approximately 9% on average.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676903","API call;machine learning;software traceability","Documentation;Educational institutions;Java;Semantics;Software;Vectors;Vocabulary","information retrieval;software engineering;system documentation","Java applications;TraceLab;external function call documentation;information retrieval techniques;relevant documentation;software artifacts;software traceability enhancement;textual information quality","","8","","48","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
