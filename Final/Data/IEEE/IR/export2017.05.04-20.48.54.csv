"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6650277,6650271,6255742,6226403,6648594,6646081,6642703,6642201,6644391,6642453,6639331,6640112,6637644,6641304,6639326,6640334,6636755,6620935,6632842,6634003,6633957,6628784,6628593,6628824,6630363,6619489,6626055,6626100,6626045,6625309,6621356,6621346,6616470,6618668,6615289,6614670,6612223,6612200,6606629,6605812,6602667,6597169,6487483,6585303,6497036,6488675,6583583,6578408,6578430,6572694,6571767,6553952,6243127,6544920,6544827,6544916,6543196,6341764,6509867,6407128,6508313,6506150,6353569,6496480,6498458,6493655,6481826,6470211,6467497,6353403,6463486,6460319,6461357,6449411,6454481,6427166,6429039,6424219,6421362,5989804,6060823,6109256,6095558,6095555,6081868,6086540,6414060,6412987,6412986,6414185,6413729,6413355,6411922,6410140,6409154,6405271,6395002,6406532,6405955,6407926",2017/05/04 20:48:54
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Shallow Information Extraction for the knowledge Web","D. Barbosa; H. Wang; C. Yu","University of Alberta, Edmonton, Canada","2013 IEEE 29th International Conference on Data Engineering (ICDE)","20130624","2013","","","1264","1267","A new breed of Information Extraction tools has become popular and shown to be very effective in building massive-scale knowledge bases that fuel applications such as question answering and semantic search. These approaches rely on Web-scale probabilistic models populated through shallow language processing of the text, pre-existing knowledge, and structured data already on the Web. This tutorial provides an introduction to these techniques, starting from the foundations of information extraction, and covering some of its key applications.","1063-6382;10636382","Electronic:978-1-4673-4910-9; POD:978-1-4673-4909-3","10.1109/ICDE.2013.6544920","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544920","","Data mining;Electronic publishing;Encyclopedias;Information retrieval;Knowledge based systems;Semantics","Internet;knowledge based systems;natural language processing;question answering (information retrieval);text analysis","Web-scale probabilistic models;information extraction tools;knowledge Web;massive-scale knowledge bases;pre-existing knowledge;question answering;semantic search;shallow information extraction;shallow language processing;structured data;text processing","","4","","31","","","8-12 April 2013","","IEEE","IEEE Conference Publications"
"On the Presence of Child Sex Abuse in BitTorrent Networks","Y. Shavitt; N. Zilberman","Tel-Aviv University","IEEE Internet Computing","20130429","2013","17","3","60","66","Peer-to-peer networks' widespread use makes multimedia files available to users worldwide. However, such networks are often used to spread illegal material while the data source and acquiring users remain anonymous. This article analyzes activity measurements in the BitTorrent network and examines child sex abuse activity through three major BitTorrent Web portals.","1089-7801;10897801","","10.1109/MIC.2013.2","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407128","BitTorrent;P2P;child sex abuse;file sharing;peer to peer","Information filtering;Information retrieval;Internet;Law enforcement;Peer to peer computing;Portals;Privacy;Web search","gender issues;law;multimedia computing;peer-to-peer computing;portals;social sciences","BitTorrent Web portals;BitTorrent networks;activity measurements;child sex abuse activity;data source;illegal material;multimedia files;peer-to-peer network","","0","","11","","20130108","May-June 2013","","IEEE","IEEE Journals & Magazines"
"Web-Scale Image Retrieval Using Compact Tensor Aggregation of Visual Descriptors","R. Negrel; D. Picard; P. H. Gosselin","University of Cergy-Pontoise, France","IEEE MultiMedia","20130823","2013","20","3","24","33","The main issues for Web-scale image retrieval are achieving good accuracy while retaining low computational time and memory footprint. This article proposes a compact image signature by aggregating tensors of visual descriptors. Efficient aggregation is achieved by preprocessing the descriptors. Compactness is achieved by projection and quantization of the signatures. The authors compare the proposed method to other efficient signatures on a 1 million images dataset and show the soundness of the approach.","1070-986X;1070986X","","10.1109/MMUL.2013.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487483","computer vision;image processing;image/video retrieval;multimedia;near-duplicate search","Computer vision;Image processing;Information retrieval;Multimedia communication;Search methods;Video communication","Internet;image retrieval;tensors","Web-scale image retrieval;compact image signature;compact tensor aggregation;computational time;images dataset;memory footprint;visual descriptors","","10","","18","","20130327","July-Sept. 2013","","IEEE","IEEE Journals & Magazines"
"Automated Bangla essay scoring system: ABESS","M. M. Islam; A. S. M. L. Hoque","Department of CSE, Mymensingh Engineering College, Mymensingh, Bangladesh","2013 International Conference on Informatics, Electronics and Vision (ICIEV)","20130801","2013","","","1","5","Essays are the most useful tool to assess learning outcomes. However, teachers have not enough time to evaluate a student's writing properly because of their other assigned responsibilities. Several Automated Essay Grading (AEG) systems have been developed to evaluate the human written (not hand written) essays easily. But, most of the AEG systems are used for grading English language or essays written in pure European languages. We have developed an Automated Bangla Essay Scoring System (ABESS) using Generalized Latent Semantic Analysis (GLSA). GLSA is an improved information retrieval technique. The ABESS has been evaluated using essay sets for two domains: standard Bangla essays titled “Bangladesher Shadhinota Songram ()”, and “Karigori Shiksha ()”. We have gained a higher level of accuracy as compared to human grader.","","Electronic:978-1-4799-0400-6; POD:978-1-4799-0397-9","10.1109/ICIEV.2013.6572694","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572694","Automated Bangla Essay Scoring System;Automated Essay Grading;Latent Semantic Analysis;N-grams;Singular Value Decomposition","Accuracy;Indexes;Information retrieval;Matrix decomposition;Semantics;Training;Vectors","computer aided instruction;information retrieval;natural language processing;teaching","ABESS;AEG;Bangladesher shadhinota songram;English language;European languages;GLSA;automated Bangla essay scoring system;automated essay grading systems;generalized latent semantic analysis;information retrieval technique;karigori shiksha;learning outcomes;teachers","","0","","20","","","17-18 May 2013","","IEEE","IEEE Conference Publications"
"Knowledge harvesting from text and Web sources","F. Suchanek; G. Weikum","Max Planck Institute for Informatics 66123 Saarbruecken, Germany","2013 IEEE 29th International Conference on Data Engineering (ICDE)","20130624","2013","","","1250","1253","The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources has enabled the automatic construction of very large knowledge bases. Recent endeavors of this kind include academic research projects such as DBpedia, KnowItAll, Probase, ReadTheWeb, and YAGO, as well as industrial ones such as Freebase and Trueknowledge. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, deep question answering, and semantic search for entities and relations in Web and enterprise data. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications.","1063-6382;10636382","Electronic:978-1-4673-4910-9; POD:978-1-4673-4909-3","10.1109/ICDE.2013.6544916","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544916","","Electronic publishing;Encyclopedias;Information retrieval;Internet;Knowledge based systems;Semantics","Internet;knowledge based systems;natural language processing;text analysis","DBpedia;Freebase;Google knowledge graph;IBM Watson question answering system;KnowItAll;Probase;ReadTheWeb;Web data;Web sources;Wikipedia;YAGO;automatic construction;cognitive applications;deep question answering;enterprise data;knowledge bases;knowledge centric services;knowledge harvesting;knowledge sharing communities;mutual relationships;natural-language text;scalable information extraction;semantic classes;semantic search;text sources","","4","","126","","","8-12 April 2013","","IEEE","IEEE Conference Publications"
"Information-Theoretic Outlier Detection for Large-Scale Categorical Data","S. Wu; S. Wang","Chinese Academy of Sciences, Beijing","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","589","602","Outlier detection can usually be considered as a pre-processing step for locating, in a data set, those objects that do not conform to well-defined notions of expected behavior. It is very important in data mining for discovering novel or rare events, anomalies, vicious actions, exceptional phenomena, etc. We are investigating outlier detection for categorical data sets. This problem is especially challenging because of the difficulty of defining a meaningful similarity measure for categorical data. In this paper, we propose a formal definition of outliers and an optimization model of outlier detection, via a new concept of holoentropy that takes both entropy and total correlation into consideration. Based on this model, we define a function for the outlier factor of an object which is solely determined by the object itself and can be updated efficiently. We propose two practical 1-parameter outlier detection methods, named ITB-SS and ITB-SP, which require no user-defined parameters for deciding whether an object is an outlier. Users need only provide the number of outliers they want to detect. Experimental results show that ITB-SS and ITB-SP are more effective and efficient than mainstream methods and can be used to deal with both large and high-dimensional data sets where existing algorithms fail.","1041-4347;10414347","","10.1109/TKDE.2011.261","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6109256","Outlier detection;attribute weighting;greedy algorithms;holoentropy;outlier factor;total correlation","Complexity theory;Greedy algorithms;Holoentropy;Information retrieval;Mutual information;Search methods","data mining;entropy;optimisation","1-parameter outlier detection method;ITB-SP;ITB-SS;categorical data sets;data mining;high-dimensional data sets;holoentropy;information-theoretic outlier detection;large-scale categorical data;optimization model;outlier factor;preprocessing step;similarity measure;total correlation","","12","","51","","20111220","March 2013","","IEEE","IEEE Journals & Magazines"
"Towards a practical semantic search - case study on the TV guide domain","M. Ciglan; L. Hluchý","Institute of Informatics, Slovak Academy of Sciences, Slovakia","2013 IEEE 17th International Conference on Intelligent Engineering Systems (INES)","20131017","2013","","","33","38","Ad-hoc semantic search aims at the retrieval from semantic data using queries formulated in a natural language. In this paper we explore several approaches to the ad-hoc semantic search and study their behaviour and quality of results in the scope of a domain specific application. The application concerns with the search in the program listings of TV stations. The goal is to find relevant program items, given a query formulated in natural language, or by using keywords. The queried data is a semantic knowledge base containing the data on the program listings as well as detailed information on distinct program items. We explore two approaches to the entity modelling and entity search, the first is based on traditional information retrieval methods and is inspired by the recent developments in the field. The second methods exploit deeper query analysis, query mapping to entities of the underlying knowledge base and the graph topology exploration to identify answers to the input query. The evaluation of the two approaches shows that the second method outperforms the existing approach, having the mean average precision almost two times higher than the former one. The results indicate that for a well specified, closed domain, ad-hoc semantic retrieval models should go beyond the traditional information retrieval approaches.","1543-9259;15439259","Electronic:978-1-4799-0830-1; POD:978-1-4799-0827-1; USB:978-1-4799-0829-5","10.1109/INES.2013.6632842","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632842","ad-hoc search;semantic graph;semantic search","Information retrieval;Knowledge based systems;Motion pictures;Natural languages;Semantics;TV;Topology","knowledge based systems;natural language processing;query processing;television;topology","TV guide domain;TV stations program listings;ad-hoc semantic retrieval model;ad-hoc semantic search;closed domain semantic retrieval model;distinct program items;domain specific application;entity modelling;entity search;graph topology exploration;information retrieval methods;keywords;mean average precision;natural language;query analysis;query mapping;semantic knowledge base","","0","","10","","","19-21 June 2013","","IEEE","IEEE Conference Publications"
"A Blocking Framework for Entity Resolution in Highly Heterogeneous Information Spaces","G. Papadakis; E. Ioannou; T. Palpanas; C. Niederée; W. Nejdl","L3S Res. Center, Leibniz Univ. of Hanover, Hanover, Germany","IEEE Transactions on Knowledge and Data Engineering","20131028","2013","25","12","2665","2682","In the context of entity resolution (ER) in highly heterogeneous, noisy, user-generated entity collections, practically all block building methods employ redundancy to achieve high effectiveness. This practice, however, results in a high number of pairwise comparisons, with a negative impact on efficiency. Existing block processing strategies aim at discarding unnecessary comparisons at no cost in effectiveness. In this paper, we systemize blocking methods for clean-clean ER (an inherently quadratic task) over highly heterogeneous information spaces (HHIS) through a novel framework that consists of two orthogonal layers: the effectiveness layer encompasses methods for building overlapping blocks with small likelihood of missed matches; the efficiency layer comprises a rich variety of techniques that significantly restrict the required number of pairwise comparisons, having a controllable impact on the number of detected duplicates. We map to our framework all relevant existing methods for creating and processing blocks in the context of HHIS, and additionally propose two novel techniques: attribute clustering blocking and comparison scheduling. We evaluate the performance of each layer and method on two large-scale, real-world data sets and validate the excellent balance between efficiency and effectiveness that they achieve.","1041-4347;10414347","","10.1109/TKDE.2012.150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6255742","Information integration;blocking methods;entity resolution","Blocking methods;Context awareness;Data mining;Information retrieval;Redundancy","pattern clustering","HHIS;attribute clustering blocking;block creation;block processing;blocking framework;clean-clean ER;comparison scheduling;duplicate detection;effectiveness layer;efficiency layer;entity resolution;highly heterogeneous information spaces;highly heterogeneous-noisy-user-generated entity collections;large-scale real-world data sets;layer performance evaluation;missed match likelihood;orthogonal layers;overlapping blocks;pairwise comparisons;quadratic task","","12","","33","","20120731","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Multi-document summarization using sentence clustering","V. K. Gupta; T. J. Siddiqui","Samsung India Software Oper., Bangalore, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","20130321","2012","","","1","5","This paper presents an approach to query focused multi document summarization by combining single document summary using sentence clustering. Both syntactic and semantic similarity between sentences is used for clustering. Single document summary is generated using document feature, sentence reference index feature, location feature and concept similarity feature. Sentences from single document summaries are clustered and top most sentences from each cluster are used for creating multi-document summary. We observed an average F-measure of 0.33774 on DUC 2002 multi-document dataset, which is comparable to three best performing systems reported on the same dataset.","","Electronic:978-1-4673-4369-5; POD:978-1-4673-4367-1","10.1109/IHCI.2012.6481826","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481826","DUC-2002;Multi document summarization;feature extraction;sentence clustering method","Cancer;Feature extraction;Indexes;Information retrieval;Semantics;Syntactics;Vectors","document handling;grammars;pattern clustering;pattern matching","F-measure;concept similarity feature;document feature;location feature;multidocument dataset;multidocument summarization;semantic similarity;sentence clustering;sentence reference index feature;single document summary;syntactic similarity","","5","","13","","","27-29 Dec. 2012","","IEEE","IEEE Conference Publications"
"Convex non-negative matrix factorization for automatic music structure identification","O. Nieto; T. Jehan","Music and Audio Research Lab, New York University, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","236","240","We propose a novel and fast approach to discover structure in western popular music by using a specific type of matrix factorization that adds a convex constrain to obtain a decomposition that can be interpreted as a set of weighted cluster centroids. We show that these centroids capture the different sections of a musical piece (e.g. verse, chorus) in a more consistent and efficient way than classic non-negative matrix factorization. This technique is capable of identifying the boundaries of the sections and then grouping them into different clusters. Additionally, we evaluate this method on two different datasets and show that it is competitive compared to other music segmentation techniques, outperforming other matrix factorization methods.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637644","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637644","matrix factorization;music structure analysis;segmentation","Clustering algorithms;Conferences;Feature extraction;Matrix decomposition;Music information retrieval;Sparse matrices;Vectors","audio signal processing;matrix decomposition;music","Western popular music;automatic music structure identification;convex constrain;convex nonnegative matrix factorization;music segmentation technique;musical piece;weighted cluster centroids","","6","","23","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"User customized playlist generation based on music similarity","G. Dubey; K. K. Budhraja; A. Singh; A. Khosla","Nat. Inst. of Technol., Jalandhar, India","2012 NATIONAL CONFERENCE ON COMPUTING AND COMMUNICATION SYSTEMS","20130117","2012","","","1","5","Playlist generation may be defined as the generation of a list of musical items or songs based on a pre-existing set of songs selected by the user. A common approach would be to suggest a number of songs based on the currently playing song. An exhaustive algorithm would involve computation of similarity of musical attributes of all of the songs with respect of the reference song. This paper introduces a methodology focused towards decreasing the computational complexity of such an algorithm. The suggested algorithm uses a distance function derived from a measure of similarity. This metric is cumulative and thus replaces complex calculations with operations involving only addition. Values of similarity are modified based on user feedback.","","Electronic:978-1-4673-1953-9; POD:978-1-4673-1952-2","10.1109/NCCCS.2012.6412987","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412987","customization;music similarity;pheromone;playlist","Computational modeling;Databases;Integrated circuit modeling;Measurement;Mel frequency cepstral coefficient;Music information retrieval;Vectors","feedback;information retrieval;music","exhaustive algorithm;music similarity;musical attribute similarity computation;musical item list generation;user customized playlist generation;user feedback","","0","","18","","","21-22 Nov. 2012","","IEEE","IEEE Conference Publications"
"Decision Guidance for Optimizing Web Data Quality - A Recommendation Model for Completing Information Extraction Results","C. Feilmayr","Inst. of Applic.-Oriented Knowledge Process. (FAW), Johannes Kepler Univ. (JKU), Linz, Austria","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","113","117","Incomplete information in web intelligence applications has serious consequences: inaccurate statements predominate, resulting primarily in erroneous annotations and ultimately in inaccurate reasoning on the web. This research work focuses on improving the completeness of extraction results by applying judiciously selected assessment methods to information extraction within the principle of complementarity. On the one hand, this paper discusses several requirements an assessment method must meet in terms of process ability and profitability to guarantee effective operation in a complementarity approach. On the other hand, it proposes a recommendation model to guide an IE system designer in selecting the appropriate methods for optimizing web data quality. The paper concludes with an application scenario that supports the theoretical approach.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621356","Data and Text Mining;Decision Support;Information Extraction;Information Quality","Data mining;Data models;Feature extraction;Information retrieval;Profitability;Semantics;Training","Web sites;data mining;decision support systems;information retrieval;learning (artificial intelligence);optimisation;profitability","IE system design;Web data quality optimization;Web intelligence application;assessment method selection;complementarity approach;decision guidance;information extraction completeness;processability;profitability;recommendation model","","0","","12","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"Discovering essential code elements in informal documentation","P. C. Rigby; M. P. Robillard","Department of Software Engineering Concordia University Montreal, QC, Canada","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","832","841","To access the knowledge contained in developer communication, such as forum posts, it is useful to determine automatically the code elements referred to in the discussions. We propose a novel traceability recovery approach to extract the code elements contained in various documents. As opposed to previous work, our approach does not require an index of code elements to find links, which makes it particularly well-suited for the analysis of informal documentation. When evaluated on 188 StackOverflow answer posts containing 993 code elements, the technique performs with average 0.92 precision and 0.90 recall. As a major refinement on traditional traceability approaches, we also propose to detect which of the code elements in a document are salient, or germane, to the topic of the post. To this end we developed a three-feature decision tree classifier that performs with a precision of 0.65-0.74 and recall of 0.30-0.65, depending on the subject of the document.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606629","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606629","","Benchmark testing;Compounds;Context;Documentation;Indexes;Information retrieval;Java","decision trees;software engineering;system recovery","StackOverflow answer posts;essential code elements;informal documentation;three-feature decision tree classifier;traceability recovery approach","","23","","12","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"Text detection from camera captured images using a novel fuzzy-based technique","A. F. Mollah; S. Basu; M. Nasipuri","Sch. of Mobile Comput. &amp; Commun., Jadavpur Univ., Kolkata, India","2012 Third International Conference on Emerging Applications of Information Technology","20130110","2012","","","291","294","Text information extraction from camera captured text embedded images has a wide variety of applications. In this paper, a fuzzy membership based robust text detection technique is presented. The given image is partitioned into blocks that are assigned two types of fuzyy memberships. The membership values are post-processed for finer classification as foreground block or background block. Adjacent foreground blocks form foreground components. Then, a feature-based Multi Layer Perceptron is used to classify the foreground components as text or non-text. Experiments show that the number of false negative is very small compared to that of the false positives. The technique yields an average of 99.75% recall and 93.75% precision rates.","","Electronic:978-1-4673-1827-3; POD:978-1-4673-1828-0","10.1109/EAIT.2012.6407926","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6407926","","Bismuth;Cameras;Educational institutions;Image color analysis;Image edge detection;Information retrieval;Training","cameras;feature extraction;fuzzy set theory;image classification;multilayer perceptrons;text detection","background block;camera captured text embedded images;feature-based multilayer perceptron;foreground block;foreground component classification;fuzzy membership values;fuzzy-based technique;text detection technique;text information extraction","","0","","10","","","Nov. 30 2012-Dec. 1 2012","","IEEE","IEEE Conference Publications"
"Data Mining Meets the Needs of Disaster Information Management","L. Zheng; C. Shen; L. Tang; C. Zeng; T. Li; S. Luis; S. C. Chen","School of Computing and Information Sciences, Florida International University, Miami, FL, USA","IEEE Transactions on Human-Machine Systems","20131018","2013","43","5","451","464","Techniques to efficiently discover, collect, organize, search, and disseminate real-time disaster information have become national priorities for efficient crisis management and disaster recovery tasks. We have developed techniques to facilitate information sharing and collaboration between both private and public sector participants for major disaster recovery planning and management. We have designed and implemented two parallel systems: a web-based prototype of a Business Continuity Information Network system and an All-Hazard Disaster Situation Browser system that run on mobile devices. Data mining and information retrieval techniques help impacted communities better understand the current disaster situation and how the community is recovering. Specifically, information extraction integrates the input data from different sources; report summarization techniques generate brief reviews from a large collection of reports at different granularities; probabilistic models support dynamically generating query forms and information dashboard based on user feedback; and community generation and user recommendation techniques are adapted to help users identify potential contacts for report sharing and community organization. User studies with more than 200 participants from EOC personnel and companies demonstrate that our systems are very useful to gain insights about the disaster situation and for making decisions.","2168-2291;21682291","","10.1109/THMS.2013.2281762","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620935","Data mining;disaster information management;dynamic query form;hierarchical summarization;user recommendation","Contingency management;Data mining;Disaster management;Information management;Information retrieval;Real time systems","Internet;business continuity;data mining;emergency management;information retrieval;probability;recommender systems","EOC personnel;Web-based prototype;all-hazard disaster situation browser system;business continuity information network system;community generation;crisis management;data mining;disaster information management;disaster recovery planning;disaster recovery tasks;information dashboard;information extraction;information retrieval techniques;mobile devices;private sector participants;probabilistic models;public sector participants;query forms;real-time disaster information;report summarization techniques;user feedback;user recommendation techniques","","14","","43","","20131004","Sept. 2013","","IEEE","IEEE Journals & Magazines"
"Annotating Search Results from Web Databases","Y. Lu; H. He; H. Zhao; W. Meng; C. Yu","Binghamton University, Binghamton","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","514","527","An increasing number of databases have become web accessible through HTML form-based search interfaces. The data units returned from the underlying database are usually encoded into the result pages dynamically for human browsing. For the encoded data units to be machine processable, which is essential for many applications such as deep web data collection and Internet comparison shopping, they need to be extracted out and assigned meaningful labels. In this paper, we present an automatic annotation approach that first aligns the data units on a result page into different groups such that the data in the same group have the same semantic. Then, for each group we annotate it from different aspects and aggregate the different annotations to predict a final annotation label for it. An annotation wrapper for the search site is automatically constructed and can be used to annotate new result pages from the same web database. Our experiments indicate that the proposed approach is highly effective.","1041-4347;10414347","","10.1109/TKDE.2011.175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5989804","Data alignment;data annotation;web database;wrapper generation","Clustering algorithms;Data mining;Database systems;HTML;Information retrieval;Ontologies;Semantics","Internet;Web sites;hypermedia markup languages;information retrieval systems","HTML form-based search interfaces;Internet comparison shopping;SRR;WDB;Web data collection;Web databases;annotation wrapper;automatic annotation approach;database encoding;encoded data units;human browsing;machine processable data units;search result records;search site","","15","","36","","20110818","March 2013","","IEEE","IEEE Journals & Magazines"
"Mining lexico-syntactic patterns to extract chemical entities with their associated properties","S. Eltyeb; N. Salim","Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Malaysia","2013 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRICAL AND ELECTRONIC ENGINEERING (ICCEEE)","20131017","2013","","","332","335","Specific information on newly discovered compound is often difficult to be found in chemical databases. The chemical and drug literature is very rich with the information resulted from new chemical synthesis. This paper presents a survey on the types of approaches that have been used to extract information associated with chemical compounds from chemical and drug text. Thereafter, it gives a description for a novel pattern-based extraction method to be developed in the future taking into account specific types of information associated with chemical compounds not explored before in the automated extraction from a text. The paper focuses on the extraction of the properties that influence the bioavailability of drug candidates' compounds. The result of this study can help the database curators in compiling the drug related chemical databases and the researchers to digest the huge amount of textual information which is growing rapidly.","","Electronic:978-1-4673-6232-0; POD:978-1-4673-6230-6","10.1109/ICCEEE.2013.6633957","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6633957","Information extraction;chemical compounds;chemical databases;pattern-based approach","Chemical compounds;Chemicals;Compounds;Data mining;Databases;Drugs;Information retrieval","chemistry computing;data mining;drugs;information retrieval;text analysis","automated extraction;bioavailability;chemical compounds;chemical entity extraction;chemical text;drug candidates compounds;drug text;lexico-syntactic pattern mining;pattern-based extraction method","","0","","10","","","26-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Text similarity computing based on sememe Vector Space","Ke Zhang; Jun Luo; Xilin Chen","College of Computer, Chongqing University, 400044, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","20130930","2013","","","208","211","Vector Space Model (VSM) is a classic text presentation model in natural language processing. However the assumption that text terms are pairwise orthogonal is not suitable. General Vector Space Model (GVSM) was proposed to improve the VSM by using term similarity to overcome the pairwise orthogonal term assumption. In this paper, based on GVSM a new approach using HowNet sememe similarity to calculate text similarity in sememe space was proposed and verified by experiment.","2327-0586;23270586","Electronic:978-1-4673-5000-6; POD:978-1-4673-4999-4","10.1109/ICSESS.2013.6615289","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615289","GVSM;HowNet;VSM;orthogonal term;sememe similarity;text similarity","Information retrieval","computational linguistics;natural language processing;text analysis;vectors","GVSM;HowNet sememe similarity;general vector space model;natural language processing;pairwise orthogonal term assumption;sememe space;sememe vector space;term similarity;text presentation model;text similarity computing","","0","","12","","","23-25 May 2013","","IEEE","IEEE Conference Publications"
"Natural language understanding for soft information fusion","S. C. Shapiro; D. R. Schlegel","Department of Computer Science and Engineering, Center for Multisource Information Fusion and Center for Cognitive Science, University at Buffalo, The State University of New York, USA","Proceedings of the 16th International Conference on Information Fusion","20131021","2013","","","380","388","Tractor is a system for understanding English messages within the context of hard and soft information fusion for situation assessment. Tractor processes a message through syntactic processors, and represents the result in a formal knowledge representation language. The result is a hybrid syntactic-semantic knowledge base that is mostly syntactic. Tractor then adds relevant ontological and geographic information. Finally, it applies hand-crafted syntax-semantics mapping rules to convert the syntactic information into semantic information, although the final result is still a hybrid syntactic-semantic knowledge base. This paper presents the various stages of Tractor's natural language understanding process, with particular emphasis on discussions of the representation used and of the syntax-semantics mapping rules.","","Electronic:978-1-4799-0284-2; POD:978-1-5090-0006-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641304","","Agricultural machinery;Information retrieval;Logic gates;Natural languages;Semantics;Syntactics;Transforms","knowledge representation languages;natural language processing","English messages;formal knowledge representation language;geographic information;hybrid syntactic semantic knowledge;hybrid syntactic semantic knowledge base;natural language understanding;ontological information;situation assessment;soft information fusion;syntactic processors;syntax semantics mapping rules","","3","","20","","","9-12 July 2013","","IEEE","IEEE Conference Publications"
"Probability based playlist generation based on music similarity and user customization","K. K. Budhraja; A. Singh; G. Dubey; A. Khosla","Nat. Inst. of Technol., Jalandhar, India","2012 NATIONAL CONFERENCE ON COMPUTING AND COMMUNICATION SYSTEMS","20130117","2012","","","1","5","Playlist generation may be defined as the generation of a list of music items or songs based on a preexisting set of songs. A common approach would be to suggest a number of songs based on the currently playing song. An exhaustive algorithm, based on similarity calculation between two songs, would involve computation of similarity of musical attributes of all of the songs with respect of the reference song. This paper introduces a methodology focused towards decreasing the computational complexity of such an algorithm. The suggested algorithm uses a probability based model inspired by Ant Colony Optimization (ACO) to select only a small number of candidate songs for the playlist, instead of the entire database. The calculation of similarity and probability values is modified based on user feedback.","","Electronic:978-1-4673-1953-9; POD:978-1-4673-1952-2","10.1109/NCCCS.2012.6412986","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6412986","music similarity;pheromone;playlist;probability","Computational modeling;Databases;Music information retrieval;Probabilistic logic;Probability;Statistics;Vectors","ant colony optimisation;computational complexity;information filtering;music;probability;relevance feedback","ACO;ant colony optimization;computational complexity;exhaustive algorithm;music database;music item list generation;music similarity;musical attributes;probability-based playlist generation;similarity calculation;similarity computation;songs list generation;user customization;user feedback","","0","","18","","","21-22 Nov. 2012","","IEEE","IEEE Conference Publications"
"Coreference resolution between sources of opinions in Spanish texts","F. Acerenza; M. Rabosto; M. Zubizarreta; A. Rosá; D. Wonsever","Facultad de Ingenier&#x00ED;a, Universidad de la Rep&#x00FA;blica, Montevideo, Uruguay","2012 XXXVIII Conferencia Latinoamericana En Informatica (CLEI)","20130204","2012","","","1","8","This paper presents a system that identifies coreferent elements in Spanish texts, with the purpose of contributing to an opinion extraction project. In particular, the system looks for the actual source when it is not identified in the opinion and solves coreferences between opinion sources in digital media texts in Spanish. The developed system takes as input texts where the opinions have already been identified, and uses syntactic and semantic information to identify the relationships between the different entities, in order to create coreference chains between the sources of the opinions in the text. The algorithm uses a score method to select the correct antecedent between the candidates. It achieved a precision of 82.8% and a recall of 85.6%.","","Electronic:978-1-4673-0793-2; POD:978-1-4673-0794-9","10.1109/CLEI.2012.6427166","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427166","Spanish;coreference resolution;information extraction;opinion analysis","Abstracts;Algorithm design and analysis;Information retrieval;Media;Semantics;Silicon;Syntactics","multimedia computing;natural language processing;text analysis","Spanish texts opinions;coreference resolution;coreferent elements;digital media texts;input texts;opinion extraction;opinion sources;semantic information;syntactic information","","0","","9","","","1-5 Oct. 2012","","IEEE","IEEE Conference Publications"
"System for document clustering from mixed sources based on Fuzzy ART neural network","M. Rojček; I. Mokriš","Dept. of Inf., Catholic Univ. in Ruzomberok, Ruzomberok, Slovakia","2013 International Conference on System Science and Engineering (ICSSE)","20130930","2013","","","259","262","The article presents a model for text document clustering based on Fuzzy ART neural network with two separate network segments. The first segment (Internet) enables clustering for the classification of documents into new categories, and the second segment (intranet) enables the modified Fuzzy ART algorithm to assign documents into existing categories. The article observe behavior of the model based on Fuzzy ART network, into which entering the different strategies in the different segments of synthetic text documents.","2325-0909;23250909","Electronic:978-1-4799-0009-1; POD:978-1-4799-0008-4; USB:978-1-4799-0006-0","10.1109/ICSSE.2013.6614670","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614670","","Clustering algorithms;Information retrieval;Internet;Neural networks;Subspace constraints;Testing;Vectors","ART neural nets;Internet;fuzzy neural nets;intranets;pattern classification;pattern clustering;text analysis","Internet;adaptive resonance theory;document classification;fuzzy ART neural network;intranet;network segments;synthetic text documents;text document clustering","","0","","15","","","4-6 July 2013","","IEEE","IEEE Conference Publications"
"An automatic solution of accessible information extraction from CityGMLLoD4 files","L. Niu; Y. Song","Faculty of Surveying and Spatial Information, Henan University of Urban Construction, 467036Pingdingshan, China","2013 21st International Conference on Geoinformatics","20131010","2013","","","1","6","This paper introduces an automatic method of extracting accessible information from CityGMLLoD4 files. The proposed solution could generate accessible rectangles from supplied files, and then use these rectangles and their spatial relationships to produce their intersection partsand nodes. Thus, we can use these accessible objects to meet the path finding demand of navigation and evacuation simulation. Furthermore, the stored accessible navigation rectangles could be utilized to meet dynamic routing demand, whichcould affect the navigation environment, in the future work.","2161-024X;2161024X","Electronic:978-1-4673-6228-3; POD:978-1-4673-6226-9","10.1109/Geoinformatics.2013.6626055","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626055","LoD4;accessible information extraction;automatic;citygml","Data mining;Floors;Histograms;Information retrieval;Navigation;Routing","building management systems;computerised navigation;emergency services;geographic information systems;information retrieval","City GMLLoD4 files;City Geography Markup Language;GIS;Geographic Information Systems;accessible information extraction;accessible navigation rectangles;dynamic routing demand;evacuation simulation;indoor building information;path finding demand;spatial relationships","","0","","11","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"Ontology-based semantic metadata extraction approach","B. Jebali; R. Farhat","Research Laboratory of Technologies of Information and Communication & Electrical Engineering (LaTICE) University Of Tunis 5 Avenue Taha Hussein, B.P. 56, Bab Menara, Tunis, Tunisia","2013 International Conference on Electrical Engineering and Software Applications","20130815","2013","","","1","5","In this paper we describes our approach for automatic generation of learning objects' semantic metadata. The extraction process is based on the OBIE (Ontology Based Information Extraction) systems' principles. The input of our approach is a set of IEEE LOM metadata elements in conformance with two requirements. First, each data element must describe the educational content of the learning object. Second, it must be one of the data elements frequently filled by the learning objects' authors and required by most of the LOM application profiles. Concerning the outputs, each one is a couple of a domain concept (from domain ontology) and a degree of pertinence. Moreover we present the details concerning the integration of our approach to learning objects' repositories by taking the COLORS repository as example. In fact the ultimate goal behind our approach is the improvement of repositories' services by offering semantic metadata.","","Electronic:978-1-4673-6301-3; POD:978-1-4673-6302-0","10.1109/ICEESA.2013.6578430","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578430","LOM metadata;Semantic metadata;ontology based information extraction system","Context;Data mining;Image color analysis;Information retrieval;Ontologies;Semantics;Standards","","","","1","","22","","","21-23 March 2013","","IEEE","IEEE Conference Publications"
"Social Multimedia Signals: Sense, Process, and Put Them to Work","S. D. Roy; G. Lotan; W. Zeng","University of Missouri","IEEE MultiMedia","20130213","2013","20","1","7","13","Social media gives ordinary people the power to be content creators and information disseminators. This information is embedded in multimedia shared across social networks, containing valuable indications about various facets of human life-about what captures our attention, our sharing biases, and the digital traces we abdicate. Social multimedia signal processing aims to transform the noise-like phenomena in social media into signals useful for building novel, socially aware multimedia applications and targeted advertising techniques as well as exploring new marketing methods. With a fresh way to look at the existence of multimedia in online social networks, we can also explore new marketing methods and targeted advertising techniques.","1070-986X;1070986X","","10.1109/MMUL.2013.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461357","Facebook;Twitter;YouTube;advertising;media networks;multimedia;multimedia applications;real-time event multimedia;social media;social networks;trend drifts","Content management;Information processing;Information retrieval;Marketing and sales;Noise measurement;Social network services","multimedia systems;signal processing;social networking (online)","advertising techniques;content creators;digital traces;human life;information disseminators;marketing methods;noise-like phenomena;online social networks;sharing biases;social multimedia signal processing;socially aware multimedia applications","","1","","9","","","Jan.-March 2013","","IEEE","IEEE Journals & Magazines"
"A prevailing judicial package for clustering and sorting information extraction","V. Annapoorani; A. Vijaya","Department of MCA Paavai Engineering College Pachal, Tamil Nadu, India","2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering","20130415","2013","","","241","244","Spontaneous sorting clock hoop around two significant concepts of sorting and summarizing data. While sorting is the primary concern of this tool, summarization is its secondary concern. As its name itself signifies, the primary concern sorting, the tool sorts the data simultaneously into various groups based on the title. It constructs an index in a clockwise manner, which makes it simpler and easier for the researches in searching for the required data. Since, the sorting is done in clock-based form, where the starting point collides with the ending point; likewise the earliest data meets the rearmost. So, the data search is performed in both forward and backward directions which in turn doubles up the speed of the same process done only in the forward direction.","","Electronic:978-1-4673-5845-3; POD:978-1-4673-5843-9","10.1109/ICPRIME.2013.6496480","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496480","Bootstrapping Algorithm;Spontaneous sorting clock;corpus;legal knowledge base","Buildings;Context;Indexes;Information retrieval;Semantics;Sorting;Terrorism","information retrieval;law administration;pattern clustering","data search;data sorting;data summarization;information extraction clustering;information extraction sorting;legal knowledge base;prevailing judicial package;spontaneous sorting clock","","0","","15","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Effective pseudo-relevance feedback for spoken document retrieval","Y. W. Chen; K. Y. Chen; H. M. Wang; B. Chen","National Taiwan Normal University, Taipei, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","8535","8539","With the exponential proliferation of multimedia associated with spoken documents, research on spoken document retrieval (SDR) has emerged and attracted much attention in the past two decades. Apart from much effort devoted to developing robust indexing and modeling techniques for representing spoken documents, a recent line of thought targets at the improvement of query modeling for better reflecting the user's information need. Pseudo-relevance feedback is by far the most commonly-used paradigm for query reformulation, which assumes that a small amount of top-ranked feedback documents obtained from the initial round of retrieval are relevant and can be utilized for this purpose. Nevertheless, simply taking all of the top-ranked feedback documents obtained from the initial retrieval for query modeling (reformulation) does not always work well, especially when the top-ranked documents contain much redundant or non-relevant information. In the view of this, we explore in this paper an interesting problem of how to effectively glean useful cues from the top-ranked documents so as to achieve more accurate query modeling. To do this, different kinds of information cues are considered and integrated into the process of feedback document selection so as to improve query effectiveness. Experiments conducted on the TDT (Topic Detection and Tracking) task show the advantages of our retrieval methods for SDR.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6639331","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639331","Kullback-Leibler (KL)-divergence;Spoken document retrieval;pseudo-relevance feedback;query modeling","Density measurement;Information retrieval;Mathematical model;Maximum likelihood estimation;Speech;Speech processing;Transmission line measurements","query formulation;query processing;speech processing","effective pseudo relevance feedback;feedback document selection;query modeling;query reformulation;spoken document retrieval;topic detection and tracking","","1","","23","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Multimedia Retrieval Conference Enjoys Texas Hospitality","R. Jain; B. P. Prabhakaran","University of California, Irvine","IEEE MultiMedia","20130823","2013","20","3","8","9","The Third ACM International Conference on Multimedia Retrieval (ICMR) was held in Dallas, Texas, from 16-19 April 2013. The conference aims to promote intellectual exchanges and interactions among scientists, engineers, students, multimedia researchers in academia as well as industry through various programs and events. The year's ACM ICMR included a keynote address by Mor Naaman of Rutgers University on &#x0022;Time for Events, and the Future of (Social) Multimedia,&#x0022; a panel entitled &#x0022;Recommendation Systems Have Taken Control: Is Multimedia Retrieval Still Relevant?&#x0022; and a new doctoral symposium event.","1070-986X;1070986X","","10.1109/MMUL.2013.40","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6585303","ACM;ACM SIGMM;ICMR;multimedia;multimedia applications;multimedia retrieval;multimedia technology","Industries;Information retrieval;Media;Multimedia communication","","","","0","","","","","July-Sept. 2013","","IEEE","IEEE Journals & Magazines"
"Ontology-specific API for a Curricula Management System","A. Tang; J. Hoh","Dept. of Computer Science & Networked Systems, Sunway University, Petaling Jaya, Malaysia","2013 Second International Conference on E-Learning and E-Technologies in Education (ICEEE)","20131024","2013","","","294","297","The ontology approach is used as the basis of a Computing Curricula Management System. Owing to the limitations of the Protégé platform, manipulation of ontological knowledge was found to be limited. A customized Application Programming Interface (API) is proposed to improve the knowledge manipulation and extraction capabilities of the system. However, the API can only operate effectively if the ontology is treated as an XML document. This paper discusses the API design and development. It then concludes with its contribution, limitation and future work.","","Electronic:978-1-4673-5094-5; POD:978-1-4673-5095-2","10.1109/ICeLeTE.2013.6644391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6644391","API;XML;computing ontology;curricula management;ontology manipulation","Data mining;Information retrieval;Ontologies;Semantic Web;Visualization;XML","XML;application program interfaces;computer aided instruction;computer science education;educational courses;ontologies (artificial intelligence)","API design;Protégé platform;XML document;computing curricula management system;customized application programming interface;ontological knowledge;ontology-specific API","","0","","10","","","23-25 Sept. 2013","","IEEE","IEEE Conference Publications"
"On Evaluation of Segmentation-Free Word Spotting Approaches without Hard Decisions","W. Pantke; V. Märgner; T. Fingscheidt","Inst. for Commun. Technol., Tech. Univ. Braunschweig, Braunschweig, Germany","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","1300","1304","Word spotting systems are intended to retrieve occurrences of a given keyword in document images without actually recognizing the full document content. As there is a trend towards segmentation-free word spotting methods, we propose a methodology to evaluate these methods by employing measures that take the quality of the retrieved word locations into account without making hard decisions. We derive a desired evaluation behavior with the help of synthetic examples and show discrepancies of existing evaluation methods. New measures following this behavior are introduced and their differences exemplarily described. The proposed evaluation method is applied to a state-of-the-art word spotting approach.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.263","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628824","Evaluation;Location Quality;Measure;Precision;Recall;Segmentation-Free;Word Spotting","Area measurement;Handwriting recognition;Image segmentation;Information retrieval;Performance evaluation;Text analysis","document image processing;image retrieval","document images;keyword occurrence retrieval;segmentation-free word spotting approach evaluation;word location retrieval quality","","2","","20","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"The research on the method of personalized restricted domain Q&A retrieval based on the linguistic model and the user model","C. l. Mao; Z. t. Yu; T. Shen; L. f. Yang; J. y. Guo; X. Zhao","School of Information Engineering and Automation, Kunming University of Science and Technology, 650500, China","Proceedings of the 32nd Chinese Control Conference","20131021","2013","","","3961","3964","The answer retrieval based on the personalized field Q&A system require system to obtain a text list contains the user's interesting with the user's query. Aimed at the traditional answer retrieval method of field Q&A system can't reflect user's personalization requirement, a personalized restricted domain Q&A retrieval method based on the linguistic model and the user model is proposed. Firstly, the method use MSVDD to construct user model, and deduce the spatial distance between document and user model, on that basis define personalization membership sort function. Secondly, constructing the personalized retrieval model that apply to field Q&A combine the language retrieval model that has smooth attribute. The experimental results show this method can effectively improve user's personalized experience and increase the accuracy of retrieval.","","Electronic:978-9-8815-6383-5; POD:978-1-4799-0030-5","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640112","Linguistic Model;Personalization;Q&A;User Model","Collaboration;Computational modeling;Educational institutions;Electronic mail;Information retrieval;Pragmatics;Support vector machines","computational linguistics;question answering (information retrieval);text analysis","MSVDD;Q&A retrieval method;answer retrieval;document model;language retrieval model;linguistic model;personalization membership sort function;personalized field Q&A system;personalized restricted domain;retrieval accuracy;user model;user personalized experience","","0","","10","","","26-28 July 2013","","IEEE","IEEE Conference Publications"
"A tree-based conceptual matching for plagiarism detection","A. H. Osman; N. Salim; A. A. E. Elhadi","Universiti Teknologi Malaysia, Faculty of Computer Science and Information Systems, Skudai, Johor, Malaysia","2013 INTERNATIONAL CONFERENCE ON COMPUTING, ELECTRICAL AND ELECTRONIC ENGINEERING (ICCEEE)","20131017","2013","","","571","579","This paper discusses a new plagiarism detection method for text documents called Tree-based Conceptual Matching. The proposed method not only represents the content of a text document as a tree, but it also captured the underlying semantic meaning in terms of the relationships among its concepts. The method was adopted to detect plagiarism in text documents. The tree-based played a very important role in this method. It looked at the amount of detecting plagiarized sentences from the original documents. Experiments have been carried out using the CS11 standard plagiarism detection corpus. The results were evaluated using information retrieval measurements, which are Recall, Precision and F-measure. The results were compared with other methods for plagiarism detection and we found our method outperforms the other methods for plagiarism detection.","","Electronic:978-1-4673-6232-0; POD:978-1-4673-6230-6","10.1109/ICCEEE.2013.6634003","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634003","Concept Extraction;Plagiarism Detection;Semantic Similarity;Tree-based representation","Educational institutions;Equations;Information retrieval;Mathematical model;Plagiarism;Semantics;Vectors","pattern matching;security of data;text analysis;trees (mathematics)","CS11 standard plagiarism detection corpus;F-measure;information retrieval measurements;plagiarism detection method;plagiarized sentences detection;precision measurement;recall measurement;semantic meaning;text documents;tree-based conceptual matching","","0","","31","","","26-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"View: Visual Information Extraction Widget for improving chart images accessibility","J. Gao; Y. Zhou; K. E. Barner","University of Delaware, Department of Electrical and Computer Engineering, Newark, 19716, USA","2012 19th IEEE International Conference on Image Processing","20130221","2012","","","2865","2868","Chart images visually represent quantitative information. Most of these visual information are represented by graphical symbols and textual descriptions; without access to the Object Model of a graphic it is difficult for viewers to acquire the accurate underlying data. In response we propose the VIEW (Visual Information Extraction Widget), a system that automatically extracts information from raster-format charts to improve accessibility. Taking a chart image as input, the system first segments the image into connected-components and distinguishes them as graphical and textual components. By analyzing the graphical components, the system then identifies the graphic type and further conducts category-specific methods to infer the underlying data. Using the images drawn from the web, we conduct experiments to demonstrate the effectiveness of the proposed system. Based on the extracted information, VIEW generates a general-purpose descriptive data table, leading the production of multi-modal representations under the task-oriented design principle.","1522-4880;15224880","Electronic:978-1-4673-2533-2; POD:978-1-4673-2534-9; USB:978-1-4673-2532-5","10.1109/ICIP.2012.6467497","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467497","Chart image understanding;accessibility;image processing;machine learning;text detection","Data mining;Feature extraction;Image segmentation;Information retrieval;Support vector machines;Visualization","computer graphics;document image processing;feature extraction;image texture;text analysis","VIEW;category-specific methods;chart image accessibility;connected-components;general-purpose descriptive data table;graphical symbols;multimodal representations;object model;raster-format charts;task-oriented design principle;textual descriptions;visual information extraction widget","","2","1","14","","","Sept. 30 2012-Oct. 3 2012","","IEEE","IEEE Conference Publications"
"λ-diverse nearest neighbors browsing for multidimensional data","O. Kucuktunc; H. Ferhatosmanoglu","The Ohio State University, Columbus","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","481","493","Traditional search methods try to obtain the most relevant information and rank it according to the degree of similarity to the queries. Diversity in query results is also preferred by a variety of applications since results very similar to each other cannot capture all aspects of the queried topic. In this paper, we focus on the lambda-diverse k-nearest neighbor search problem on spatial and multidimensional data. Unlike the approach of diversifying query results in a postprocessing step, we naturally obtain diverse results with the proposed geometric and index-based methods. We first make an analogy with the concept of Natural Neighbors (NatN) and propose a natural neighbor-based method for 2D and 3D data and an incremental browsing algorithm based on Gabriel graphs for higher dimensional spaces. We then introduce a diverse browsing method based on the distance browsing feature of spatial index structures, such as R-trees. The algorithm maintains a Priority Queue with mindivdist of the objects depending on both relevancy and angular diversity and efficiently prunes nondiverse items and nodes. We experiment with a number of spatial and high-dimensional data sets, including Factual's (http://www.factual.com/) US points-of-interest data set of 13M entries. On the experimental setup, the diverse browsing method is shown to be more efficient (regarding disk accesses) than k-NN search on R-trees, and more effective (regarding Maximal Marginal Relevance (MMR)) than the diverse nearest neighbor search techniques found in the literature.","1041-4347;10414347","","10.1109/TKDE.2011.251","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095558","Diversity;Gabriel graph;angular similarity;diverse nearest neighbor search;natural neighbors","Diversity methods;Information retrieval;Nearest neighbor searches;Query processing;Search methods;Search problems;Spatial databases","computational geometry;query processing;queueing theory;relevance feedback;search problems;set theory;tree data structures;trees (mathematics)","λ-diverse k-nearest neighbor search problem;λ-diverse nearest neighbors browsing;2D data;3D data;Factual US points-of-interest data set;Gabriel graph;MMR;R-trees;angular diversity;distance browsing feature;diverse browsing method;geometric methods;high-dimensional data sets;higher dimensional spaces;incremental browsing algorithm;index-based methods;information ranking;k-NN search;maximal marginal relevance;multidimensional data;natural neighbor-based method;object mindivdist;priority queue;spatial data sets;spatial index structures;topic query","","3","","33","","20111206","March 2013","","IEEE","IEEE Journals & Magazines"
"A Learning Approach to SQL Query Results Ranking Using Skyline and Users' Current Navigational Behavior","Z. Chen; T. Li; Y. Sun","Dept. of Inf. Syst., Univ. of Maryland Baltimore County, Baltimore, MD, USA","IEEE Transactions on Knowledge and Data Engineering","20131028","2013","25","12","2683","2693","Users often find that their queries against a database return too many answers, many of them irrelevant. A common solution is to rank the query results. The effectiveness of a ranking function depends on how well it captures users' preferences. However, database systems often do not have the complete information about users' preferences and users' preferences are often heterogeneous (i.e., some preferences are static and common to all users while some are dynamic and diverse). Existing solutions do not address these two issues. In this paper, we propose a novel approach to address these shortcomings: 1) it addresses the heterogeneous issue by using skyline to capture users' static and common preferences and using users' current navigational behavior to capture users' dynamic and diverse preferences; 2) it addresses the incompleteness issue by using a machine learning technique to learn a ranking function based on training examples constructed from the above two types of information. Experimental results demonstrate the benefits of our approach.","1041-4347;10414347","","10.1109/TKDE.2012.128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6226403","Data and knowledge visualization;interactive data exploration and discovery","Databases;Information retrieval;Query processing;Search problems;Structured query language;Support vector machines","SQL;database management systems;human computer interaction;learning (artificial intelligence);query processing","SQL query results ranking;database systems;machine learning technique;ranking function;skyline;training;user diverse preferences;users common preferences;users current navigational behavior;users dynamic preferences;users static preferences","","0","","32","","20120626","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Ginix: Generalized inverted index for keyword search","H. Wu; G. Li; L. Zhou","Department of Computer Science and Technology, Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, Beijing 100084, China","Tsinghua Science and Technology","20130207","2013","18","1","77","87","Keyword search has become a ubiquitous method for users to access text data in the face of information explosion. Inverted lists are usually used to index underlying documents to retrieve documents according to a set of keywords efficiently. Since inverted lists are usually large, many compression techniques have been proposed to reduce the storage space and disk I/O time. However, these techniques usually perform decompression operations on the fly, which increases the CPU time. This paper presents a more efficient index structure, the Generalized INverted IndeX (Ginix), which merges consecutive IDs in inverted lists into intervals to save storage space. With this index structure, more efficient algorithms can be devised to perform basic keyword search operations, i.e., the union and the intersection operations, by taking the advantage of intervals. Specifically, these algorithms do not require conversions from interval lists back to ID lists. As a result, keyword search using Ginix can be more efficient than those using traditional inverted indices. The performance of Ginix is also improved by reordering the documents in datasets using two scalable algorithms. Experiments on the performance and scalability of Ginix on real datasets show that Ginix not only requires less storage space, but also improves the keyword search performance, compared with traditional inverted indexes.","","","10.1109/TST.2013.6449411","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449411","document reordering;index compression;keyword search","Algorithm design and analysis;Information retrieval;Keyword search;Query processing;Radiation detectors;Search methods;Spatial databases;Upper bound","","","","4","","","","","Feb. 2013","","TUP","TUP Journals & Magazines"
"Efficient ANDSF-assisted Wi-Fi control for mobile data offloading","D. S. Kim; Y. Noishiki; Y. Kitatsuji; H. Yokota","KDDI R&D Laboratories, Inc., 2-1-15 Ohara Fujimino, Saitama, Japan","2013 9th International Wireless Communications and Mobile Computing Conference (IWCMC)","20130822","2013","","","343","348","This paper presents a strategic solution to mobile data offloading between 3GPP and non-3GPP access networks. As mobile data traffic is increasing rapidly, many 3G operators face massive challenges in managing the huge amount of mobile data traffic. Thus, mobile data offloading to non-3GPP access networks in 3G-access networks is necessary. In this paper, we propose a novel ANDSF-assisted Wi-Fi control method based on user's high-level motion states, such as walking, driving, and recognizing whether the user is stationary or not, to avoid unnecessary Wi-Fi scanning and connections. The proposed method is compared to a client-based Wi-Fi connection manager, which performs periodic Wi-Fi scanning. According to the performance results, the proposed mechanism shows significant performance improvement in terms of efficient Wi-Fi control and connectivity with 3G and WiFi.","2376-6492;23766492","Electronic:978-1-4673-2480-9; POD:978-1-4673-2479-3; USB:978-1-4673-2478-6","10.1109/IWCMC.2013.6583583","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583583","ANDSF;EPC;Handover;Mobile Data offloading;Wi-Fi","IEEE 802.11 Standards;Information retrieval;Legged locomotion;Mobile communication;Mobile computing;Servers;Smart phones","3G mobile communication;telecommunication control;telecommunication network management;wireless LAN","3G operators;3GPP access networks;ANDSF assisted Wi-Fi control;client based Wi-Fi connection manager;connectivity;mobile data offloading;mobile data traffic;periodic Wi-Fi scanning;unnecessary Wi-Fi connections;unnecessary Wi-Fi scanning","","5","","6","","","1-5 July 2013","","IEEE","IEEE Conference Publications"
"Selecting Good Expansion Terms for Improving XML Retrieval Performance","M. Zhong","Sch. of Inf. Technol., Jiangxi Univ. of Finance &amp; Econ., Nanchang, China","2012 International Conference on Control Engineering and Communication Technology","20130117","2012","","","480","483","In this paper, we study how to perform XML query expansion effectively from the high quality pseudo-relevance documents. A solution for selecting good expansion information is presented, in which various features impacting weight, such as term element frequency, term inverse element frequency, semantic weight of tag and level information, are analyzed and those term with high weigh value are selected as expansion term. Experiment results show that proposed expansion method is feasible. Compared to original query and traditional expansion method with no structure features considered, our method achieves better retrieval performance.","","Electronic:978-0-7695-4881-4; POD:978-1-4673-4499-9","10.1109/ICCECT.2012.176","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414060","Pseudo-Relevance Feedback;XML query expansion;node level;tag semantic weight","Analytical models;Computational modeling;Educational institutions;Information retrieval;Semantics;Vectors;XML","XML;query processing;relevance feedback","XML query expansion;XML retrieval performance;expansion term;level information;pseudorelevance document;semantic weight;tag information;term inverse element frequency","","0","","12","","","7-9 Dec. 2012","","IEEE","IEEE Conference Publications"
"Nested-SIFT for Efficient Image Matching and Retrieval","P. Xu; L. Zhang; K. Yang; H. Yao","Harbin Institute of Technology, China","IEEE MultiMedia","20130823","2013","20","3","34","46","To improve the effectiveness of feature representation and the efficiency of feature matching, we propose a new feature representation, named Nested-SIFT, which utilizes the nesting relationship between SIFT features to group local features. A Nested-SIFT group consists of a bounding feature and several member features covered by the bounding feature. To obtain a compact representation, SimHash strategy is used to compress member features in a Nested-SIFT group into a binary code, and the similarity between two Nested-SIFT groups is efficiently computed by using the binary codes. Extensive experimental results demonstrate the effectiveness and efficiency of our proposed Nested-SIFT approach.","1070-986X;1070986X","","10.1109/MMUL.2013.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497036","SimHash;feature representation;image matching;image retrieval;multimedia;multimedia applications;nested-SIFT","Feature recognition;Image matching;Image representation;Information retrieval;Media;Multimedia communication","binary codes;data compression;feature extraction;image coding;image matching;image representation;image retrieval;transforms","SimHash strategy;binary codes;feature matching efficiency;feature representation effectiveness;image matching;image retrieval;member feature compression;nested-SIFT group;scale invariant feature transform","","10","","14","","20130411","July-Sept. 2013","","IEEE","IEEE Journals & Magazines"
"A New Algorithm for Inferring User Search Goals with Feedback Sessions","Z. Lu; H. Zha; X. Yang; W. Lin; Z. Zheng","Shanghai Jiao Tong University, Shanghai","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","502","513","For a broad-topic and ambiguous query, different users may have different search goals when they submit it to a search engine. The inference and analysis of user search goals can be very useful in improving search engine relevance and user experience. In this paper, we propose a novel approach to infer user search goals by analyzing search engine query logs. First, we propose a framework to discover different user search goals for a query by clustering the proposed feedback sessions. Feedback sessions are constructed from user click-through logs and can efficiently reflect the information needs of users. Second, we propose a novel approach to generate pseudo-documents to better represent the feedback sessions for clustering. Finally, we propose a new criterion )“Classified Average Precision (CAP)” to evaluate the performance of inferring user search goals. Experimental results are presented using user click-through logs from a commercial search engine to validate the effectiveness of our proposed methods.","1041-4347;10414347","","10.1109/TKDE.2011.248","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6095555","User search goals;classified average precision;feedback sessions;pseudo-documents;restructuring search results","Feedback;Information retrieval;Optimization methods;Search engines;Search methods;Search problems;Web search","information needs;pattern clustering;performance evaluation;query processing;relevance feedback;search engines","CAP criterion;classified average precision criterion;feedback session clustering;performance evaluation;pseudodocument generation;search engine query logs;search engine relevance improvement;user click-through logs;user experience improvement;user information needs;user search goals analysis;user search goals inference","","10","","20","","20111206","March 2013","","IEEE","IEEE Journals & Magazines"
"Discovering Temporal Change Patterns in the Presence of Taxonomies","L. Cagliero","Politecnico di Torino, Turin","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","541","555","Frequent itemset mining is a widely exploratory technique that focuses on discovering recurrent correlations among data. The steadfast evolution of markets and business environments prompts the need of data mining algorithms to discover significant correlation changes in order to reactively suit product and service provision to customer needs. Change mining, in the context of frequent itemsets, focuses on detecting and reporting significant changes in the set of mined itemsets from one time period to another. The discovery of frequent generalized itemsets, i.e., itemsets that 1) frequently occur in the source data, and 2) provide a high-level abstraction of the mined knowledge, issues new challenges in the analysis of itemsets that become rare, and thus are no longer extracted, from a certain point. This paper proposes a novel kind of dynamic pattern, namely the History GENeralized Pattern (HIGEN), that represents the evolution of an itemset in consecutive time periods, by reporting the information about its frequent generalizations characterized by minimal redundancy (i.e., minimum level of abstraction) in case it becomes infrequent in a certain time period. To address HIGEN mining, it proposes HIGEN MINER, an algorithm that focuses on avoiding itemset mining followed by postprocessing by exploiting a support-driven itemset generalization approach. To focus the attention on the minimally redundant frequent generalizations and thus reduce the amount of the generated patterns, the discovery of a smart subset of HIGENs, namely the NONREDUNDANT HIGENs, is addressed as well. Experiments performed on both real and synthetic datasets show the efficiency and the effectiveness of the proposed approach as well as its usefulness in a real application context.","1041-4347;10414347","","10.1109/TKDE.2011.233","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6081868","Data mining;mining methods and algorithms","Context awareness;Data mining;Information retrieval;Itemsets;Search methods;Taxonomy","business data processing;data mining;marketing data processing;pattern recognition","HIGEN;business environments;data mining algorithms;discovering temporal change patterns;frequent itemset mining;history generalized pattern;market environments;source data;taxonomies presence","","10","","27","","20111115","March 2013","","IEEE","IEEE Journals & Magazines"
"Annotated corpus of named entities for Ukrainian language","O. Dmytrash; A. Romanyuk","CAD Department, Lviv Polytechnic National University, Ukraine, Lviv, S. Bandera Street 12","2013 12th International Conference on the Experience of Designing and Application of CAD Systems in Microelectronics (CADSM)","20130620","2013","","","80","81","This paper describes an attempt of creating an annotated corpus of named entities for Ukrainian language.","","Electronic:978-617-607-406-9; POD:978-1-4673-6461-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6543196","Corpus;Information Extraction;Named Entity Recognition","Business;Information retrieval;Logic gates;Manuals;Natural language processing;Sociology;Software","natural language processing","Ukrainian language;annotated corpus;named entities","","1","","9","","","19-23 Feb. 2013","","IEEE","IEEE Conference Publications"
"Urban road information extraction from high resolution remotely sensed image based on semantic model","J. Wang; J. Qian; R. Ma","College of tourism and Geographical Science Yunnan Normal University, Kunming, China","2013 21st International Conference on Geoinformatics","20131010","2013","","","1","5","The road is an important fundamental geographic information. Getting the road information quickly and accurately has a great significance for GIS data updating, image matching, target detection and automated digital mapping. Automatic/semiautomatic extraction of road information of remote sensing images is the problem of visual interpretation computer research, RS and GIS. The application of high resolution satellite images and the development of semantic model theory provide more possibilities and a higher degree of accuracy for object extraction of remotely sensed image. The OAR model of human cognition has been introduced; experimental study has been carried out on extracting road information from Quick Bird multi-spectral Imaging with the semantic model; and the result shows that the length accuracy of extracted road was 89.19%, the width accuracy is 71.54%, and the intact rate 50.32%. The extracted result is better than that of object-oriented extraction. As a whole, the road information extraction semantic model of highresolution satellite remotely sensed image is efficient.","2161-024X;2161024X","Electronic:978-1-4673-6228-3; POD:978-1-4673-6226-9","10.1109/Geoinformatics.2013.6626100","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626100","high-resolution remotely sensed image;information extraction;semantic model;urban road","Data mining;Feature extraction;Image resolution;Information retrieval;Remote sensing;Roads;Semantics","","","","2","","32","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"Knowledge Management for Coalition Information Sharing at the Network Edge","C. Giammanco; R. McGowan; A. Kao; D. Braines; S. R. Poteet; T. Pham; P. Xue","","IEEE Intelligent Systems","20130221","2013","28","1","26","33","This article describes ongoing research on data integration and query services for knowledge management. For such services, Controlled English (CE), a human-friendly, machine-readable language, can represent information content and context.","1541-1672;15411672","","10.1109/MIS.2012.104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353403","human-centered computing;knowledge management applications;natural language processing;query languages","Data integration;Data models;Distributed databases;Information management;Information retrieval;Knowledge management","data integration;knowledge management;military computing;natural languages;query processing","coalition information sharing;controlled English;data integration services;data query services;human-friendly language;information content representation;knowledge management;machine-readable language;network edge","","0","","26","","20121115","Jan.-Feb. 2013","","IEEE","IEEE Journals & Magazines"
"An NER-based Product Identification and Lucene-based Product Linking Approach to CPROD1 Challenge: Description of Submission System to CPROD1 Challenge","Z. Toh; W. Wang; M. Lan; X. Li","Inst. for Infocomm Res., Singapore, Singapore","2012 IEEE 12th International Conference on Data Mining Workshops","20130110","2012","","","869","871","This paper presents our methodology for CPROD1 Challenge, which is to identify the product mentions from text and then link the product to the entries in the catalog file. Our solution follows 2 steps. First, we use processing pipelines to extract product mentions by incorporating multiple techniques including traditional named entities recognition (NER), regular expression rules and gazetteer-based string matching. Second, we view product linking task into an information retrieval (IR) problem, where the description catalog file is populated into a database. Thus, each product mention acts as a search query and the returned results from catalog entry database serve as the links. The F1 scores of our submission on public and private test data are 24.82% and 16.04%, respectively.","2375-9232;23759232","Electronic:978-1-4673-5164-5; POD:978-1-4799-1707-5","10.1109/ICDMW.2012.66","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6406532","named entity recognition;product disambiguation;product identification;product linking","Catalogs;Data mining;Feature extraction;Indexing;Information retrieval;Joining processes;Training data","cataloguing;file organisation;query processing;string matching;text analysis","CPROD1 Challenge;F1 scores;IR problem;Lucene-based product linking approach;NER;NER-based product identification;catalog entry database;catalog file;description catalog file;gazetteer-based string matching;information retrieval problem;named entity recognition;pipeline processing;private test data;product extraction;public test data;regular expression rules;search query;text analysis","","0","","1","","","10-10 Dec. 2012","","IEEE","IEEE Conference Publications"
"Interactive spoken content retrieval by extended query model and continuous state space Markov Decision Process","T. H. Wen; H. y. Lee; P. h. Su; L. S. Lee","National Taiwan University, Taipei, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","8510","8514","Interactive retrieval is important for spoken content because the retrieved spoken items are not only difficult to be shown on the screen but also scanned and selected by the user, in addition to the speech recognition uncertainty. The user cannot playback and go through all the retrieved items to find out what he is looking for. Markov Decision Process (MDP) was used in a previous work to help the system take different actions to interact with the user based on an estimated retrieval performance, but the MDP state was represented by the less precise quantized retrieval performance metric. In this paper, we consider the retrieval performance metric as a continuous state variable in MDP and optimize the MDP by fitted value iteration (FVI).We also use query expansion with the language modeling retrieval framework to produce the next set of retrieval results. Improved performance was found in the preliminary experiments.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6639326","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639326","Fitted Value Iteration;Interactive Retrieval;Language Model Retrieval;MDP;Markov Decision Process","Acoustics;Information retrieval;Lattices;Markov processes;Measurement;Training;Vectors","Markov processes;content-based retrieval;decision theory;natural language processing;query processing;speech recognition","FVI;MDP optimization;MDP state representation;continuous state space Markov decision process;extended query model;fitted value iteration;interactive spoken content retrieval;language modeling retrieval framework;quantized retrieval performance metric;query expansion;speech recognition uncertainty;spoken item retrieval","","2","","37","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Research analytics for reviewer recommendation","Y. h. Xu; X. t. Guo; L. Xu; Y. Chen; Y. y. Zhuang","Fac. of Manage. &amp; Econ., Kunming Univ. of Sci. &amp; Technol., Kunming, China","2012 International Conference on Management Science & Engineering 19th Annual Conference Proceedings","20130117","2012","","","213","217","Peer review plays an important role in research project selection at funding agencies. Given the practical challenge that even the most experienced researcher may be unable to point out the whole deficiencies in a complex body of research work, peer review addresses this problem by introducing independent experts to critically analyze and assess the quality of research proposals. Recommending appropriate reviewers for proposals presents a great challenge for funding agency especially when the number of proposals and reviewers are large. Reviewer recommendation involves several issues which need to be considered: avoiding the conflict of interests between authors and reviewers; whether and to what extent the reviewer has expertise in corresponding areas of proposals. This research investigates how research analytics can be used for reviewer recommendation by integrating three dimensions: connectivity, relevance and quality.","2155-1847;21551847","Electronic:978-1-4673-3014-5; POD:978-1-4673-3015-2","10.1109/ICMSE.2012.6414185","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414185","peer review;research analytics;reviewer recommendation","Abstracts;Data mining;Feature extraction;Information retrieval;Optimization;Proposals","information filtering;recommender systems;research and development","funding agency;peer review;research analytics;research project selection;research proposal quality assessment;reviewer recommendation","","0","","11","","","20-22 Sept. 2012","","IEEE","IEEE Conference Publications"
"Cross-Language Opinion Target Extraction in Review Texts","X. Zhou; X. Wan; J. Xiao","MOE Key Lab. of Comput. Linguistics, Peking Univ., Beijing, China","2012 IEEE 12th International Conference on Data Mining","20130117","2012","","","1200","1205","Opinion target extraction is a subtask of opinion mining which is very useful in many applications. In this study, we investigate the problem in a cross-language scenario which leverages the rich labeled data in a source language for opinion target extraction in a different target language. The English labeled corpus is used as training set. We generate two Chinese training datasets with different features. Two labeling models for Chinese opinion target extraction are learned based on Conditional Random Fields (CRF). After that, we use a monolingual co-training algorithm to improve the performance of both models by leveraging the enormous unlabeled Chinese review texts on the web. Experimental results show the effectiveness of our proposed approach.","1550-4786;15504786","Electronic:978-0-7695-4905-7; POD:978-1-4673-4649-8","10.1109/ICDM.2012.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413729","cross-language information extraction;opinion mining;opinion target extraction","Algorithm design and analysis;Corporate acquisitions;Data mining;Feature extraction;Information retrieval;Labeling;Training","data mining;natural language processing;text analysis;training","CRF-based learning;Chinese opinion target extraction;Chinese review texts;English labeled corpus;conditional random fields-based learning;cross-language opinion target extraction;monolingual cotraining algorithm;opinion mining;rich labeled data;source language;target language;training set","","2","","14","","","10-13 Dec. 2012","","IEEE","IEEE Conference Publications"
"Analysis of Technology Trends Based on Big Data","A. Segev; C. Jung; S. Jung","Dept. of Knowledge Service Eng., KAIST, Daejeon, South Korea","2013 IEEE International Congress on Big Data","20130916","2013","","","419","420","The paper suggests a method for analyzing technology trends. The process, which investigates development of technologies over time, identifies main technologies displaying the fastest growth compared to greater influence of new inventions. The method analyzes term frequency and change over time of technological terms in patents to identify the prior technologies that lead to a new technology and detects technologies that have the biggest impact. The analysis was performed on 4,354,054 patents from the US Patent Office dating from 1975 until today. Some correlation is displayed between technology trends and future US stock market performance.","2379-7703;23797703","Electronic:978-0-7695-5006-0; POD:978-1-4799-0182-1","10.1109/BigData.Congress.2013.65","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597169","patent;technology impact;technology trend;trend analysis","Chemicals;Conferences;Electronic mail;Information retrieval;Market research;Patents;Stock markets","data analysis;patents;stock markets","US Patent Office;big data;future US stock market performance;technology trend analysis;term frequency analysis","","3","","5","","","June 27 2013-July 2 2013","","IEEE","IEEE Conference Publications"
"RSL-PL: A linguistic pattern language for documenting software requirements","D. de Almeida Ferreira; A. R. da Silva","INESC-ID, Instituto Superior T&#x00E9;cnico (IST), Lisbon, Portugal","2013 3rd International Workshop on Requirements Patterns (RePa)","20130919","2013","","","17","24","Software requirements are traditionally documented in natural language (NL). However, despite being easy to understand and having high expressivity, this approach often leads to well-known requirements quality problems. In turn, dealing with these problems warrants a significant amount of human effort, causing requirements development activities to be error-prone and time-consuming. This paper introduces RSL-PL, a language that enables the definition of linguistic patterns typically found in well-formed individual NL requirements, according to the field's best practices. The linguistic features encoded within RSL-PL patterns enable the usage of information extraction techniques to automatically perform the linguistic analysis of NL requirements. Thus, in this paper we argue that RSL-PL can improve the quality of requirements specifications, as well as the productivity of requirements engineers, by mitigating the continuous effort that is often required to ensure requirements quality criteria, such as clearness, consistency, and completeness.","","Electronic:978-1-4799-0948-3; POD:978-1-4799-0949-0","10.1109/RePa.2013.6602667","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602667","Information Extraction;Linguistic Analysis;Requirements Engineering;Requirements Linguistic Patterns","Best practices;Information retrieval;Pattern matching;Pragmatics;Semantics;Syntactics","computational linguistics;natural language processing;software engineering","NL requirements;RSL-PL linguistic pattern language;clearness criteria;completeness criteria;consistency criteria;linguistic pattern;natural language requirements;requirements development activity;software requirements documentation","","3","","18","","","15-15 July 2013","","IEEE","IEEE Conference Publications"
"Bayesian information extraction network for Medline abstract","M. Mannai; W. Ben Abdessalem Karaa","Higher Institute of Management of Tunis, Liberty city Bouchoucha, Bardo, Tunisia","2013 World Congress on Computer and Information Technology (WCCIT)","20131003","2013","","","1","3","Biomedical is a huge domain that combines a variety of research areas. MEDELINE is one of the largest biomedical databases. Thereby, the searching of pertinent information through Medline has become a difficult task. That's why; we need to develop information extraction systems in order to facilitate the treatment and the representation of data according to the user's need. This paper applies Bayesian Networks to support information extraction based on ontological annotation from Medline. We present a tool developed that combines between semantic and probabilistic reasoning techniques.","","Electronic:978-1-4799-0462-4; POD:978-1-4799-0461-7","10.1109/WCCIT.2013.6618668","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618668","Bayesian network;Medline;information extraction;semantic annotation;text mining;transducteur","Abstracts;Bayes methods;Data mining;Information retrieval;Ontologies;Probabilistic logic;Semantics","belief networks;database management systems;inference mechanisms;information retrieval;medical information systems;text analysis","Bayesian information extraction network;Bayesian networks;Medline abstract;biomedical databases;information extraction systems;ontological annotation;probabilistic reasoning techniques;semantic reasoning techniques;textual data information extraction","","0","","10","","","22-24 June 2013","","IEEE","IEEE Conference Publications"
"A new traceable software requirements specification based on IEEE 830","A. Chikh; M. Aldayel","Information System Department, King Saud University, Riyadh, Saudi Arabia","2012 International Conference on Computer Systems and Industrial Informatics","20130207","2012","","","1","6","This paper aims to enrich software requirements specification by integrating the concept of traceability. This integration, which will allow a more helpful reading of that specification, will provide a critical support to different requirements stakeholders. Indeed traceability will help for example designers in refining requirements into lower-level design components and analysts in understanding the implications of a proposed change, to ensure that no extraneous code exists and to minimize or eliminate the presence of faked or frivolous requirements that lack any justification. The main contribution of this paper is building a schema for software requirements specification that make it easily traceable by linking each requirement to real-world dimensions such as detail design deliverables, related system requirements and sources such as user requirement, analyst, regulation and standard.","","Electronic:978-1-4673-5157-7; POD:978-1-4673-5155-3","10.1109/ICCSII.2012.6454481","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6454481","IEEE standard 830;Requirement Traceability;Software Requirements Specification;Traceability schema","IEEE standards;Information retrieval;Semantics;Software;Unified modeling language;XML","formal specification;program diagnostics;program verification","IEEE 830;lower-level design components;real-world dimensions;refining requirements;related system requirements;requirement stakeholders;traceability concept;traceable software requirement specification","","0","","21","","","18-20 Dec. 2012","","IEEE","IEEE Conference Publications"
"Constrained by limited information and recommendation opportunities: An exploration and exploitation problem for recommender systems","S. Chan","University College London, United Kingdom","2013 IEEE 14th International Conference on Information Reuse & Integration (IRI)","20131024","2013","","","54","61","In this paper, we investigate a resource allocation problem in recommendation systems where unknown new items keep coming to the system at different time. The task is to recommend (allocate) each user a limited number of new items. The objective is to maximize the overall positive response rate. This problem is non-trivial because, on one hand, we need to allocate these news items to users who are helpful for learning new item feature profiles using the limited recommendation opportunities (resources) in order to improve prediction accuracy; on the other hand, allocate these items to users who would most likely purchase them on the basis of the new item information gathered so far in order to maximize positive response rate. In this paper, we propose a two-stage batch solution to approximately optimize the objective, using group buying as a working example. During the first stage, we estimate the user purchase decisions towards new items by allocating some resources for exploration. During the second stage, we optimally allocate the remaining resources for exploitation according to the prediction and the operational constraints using the binary integer programming technique. Our experiments indicate that the proposed approach significantly improves the positive response rate.","","Electronic:978-1-4799-1050-2; POD:978-1-4799-2669-5; USB:978-1-4799-1049-6","10.1109/IRI.2013.6642453","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642453","","Estimation;Information retrieval;Linear programming;Mathematical model;Motion pictures;Predictive models;Resource management","integer programming;recommender systems;resource allocation","binary integer programming technique;feature profile;group buying;item allocation;overall positive response rate;prediction accuracy;recommendation opportunities;recommendation systems;resource allocation problem;two-stage batch solution;user purchase decision estimation","","0","","27","","","14-16 Aug. 2013","","IEEE","IEEE Conference Publications"
"Managing Genetic Algorithm Parameters to Improve SegGen -- A Thematic Segmentation Algorithm","N. S. Saygili; T. Acarman; T. Amghar; B. Levrat","Inst. of Sci., Galatasaray Univ., Istanbul, Turkey","2013 24th International Workshop on Database and Expert Systems Applications","20131007","2013","","","58","62","SegGen [1] is a linear thematic segmentation algorithm grounded on a variant of the Strength Pareto Evolutionary Algorithm [2] and aims at optimizing the two criteria of the Salton's [3] definition of segments: a segment is a part of text whose internal cohesion and dissimilarity with its adjacent segments are maximal. This paper describes improvements that have been implemented in the approach taken by SegGen by tuning the genetic algorithm parameters according with the evolution of the quality of the generated populations. Two kinds of reasons originate the tuning of the parameters and have been implemented here. First as it could be measured by the values of global criteria of the population quality, the global quality of the generated populations increases as the process goes and it seems reasonable to set values to parameters and define new operators, which favor intensification and diminish diversification factors in the search process. Second since individuals in the populations are plausible segmentations it seems reasonable to weight sentences in the current segmentation depending on their distance to the boundaries of the segment they belong to for the calculus of similarities between sentences implied in the two criteria to be optimized. Although this tuning of the parameters of the algorithm currently rests on estimations based on experiments, first results are promising.","1529-4188;15294188","Electronic:978-1-4799-2138-6; POD:978-1-4799-2139-3","10.1109/DEXA.2013.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621346","genetic algorithm;hematic segmentation;multi-objective optimization problem","Genetic algorithms;Genetics;Information retrieval;Sociology;Statistics;Tuning;Vectors","Pareto optimisation;genetic algorithms;text analysis","SegGen;genetic algorithm parameter tuning;intensification;linear thematic segmentation algorithm;operators;population global quality;population quality global criteria;strength Pareto evolutionary algorithm;text segmentation","","0","","15","","","26-30 Aug. 2013","","IEEE","IEEE Conference Publications"
"Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links","N. Ali; Y. G. Guéhéneuc; G. Antoniol","&#x00C9;cole Polytechnique de Montr&#x00E9;al, Montr&#x00E9;al","IEEE Transactions on Software Engineering","20130429","2013","39","5","725","741","Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.","0098-5589;00985589","","10.1109/TSE.2012.71","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6341764","Traceability;experts;feature;repositories;requirements;source code;trust-based model","Accuracy;Data mining;Information retrieval;Open source software;Principal component analysis;Software maintenance","data mining;data privacy;information retrieval;software maintenance","IR technique;Jensen-Shannon model;Trustrace approach;information retrieval technique;medium-size open-source system;precision accuracy;recall accuracy;requirement traceability link;software evolution;software maintenance;software repository mining;traceability method;trust-based traceability recovery approach;vector space model","","16","","41","","20121110","May 2013","","IEEE","IEEE Journals & Magazines"
"Feature-to-Code Traceability in Legacy Software Variants","H. Eyal-Salman; A. D. Seriai; C. Dony","Univ. of Montpellier, Montpellier, France","2013 39th Euromicro Conference on Software Engineering and Advanced Applications","20131010","2013","","","57","61","Existing similar software variants, developed by ad-hoc reuse technique such as left clone-and-own right, represent a starting point to build a software product line (SPL) core assets. To re-engineer such legacy software variants into an SPL for systematic reuse, it is important to be able to identify a mapping between features and their implementing source code elements in different variants. Information Retrieval (IR) methods have been used widely to support this mapping in a single software product. This paper proposes a new approach to improve the performance of IR methods when they are applied to a collection of software variants. The novelty of our approach is twofold. On the one hand, it exploits what software variants have in common and how they differ to improve the accuracy of IR results. On the other hand, it reduces the abstraction gap between features and source code by introducing an intermediate level called left code-topic right, for increasing the number of retrieved links that are relevant. We have applied our approach to a collection of seven variants of a large-scale system by using the ArgoUML-SPL modeling tool. The experimental results showed that our approach outperforms conventional application of IR methods as well as the most recent and relevant work on the subject.","1089-6503;10896503","Electronic:978-0-7695-5091-6; POD:978-1-4799-1490-6","10.1109/SEAA.2013.65","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619489","FCA;LSI;Traceability;VSM;features;object-oriented;product line;software variants;source code","Accuracy;Context;Information retrieval;Large scale integration;Lattices;Software;Vectors","information retrieval;large-scale systems;product development;program diagnostics;software maintenance;software reusability","ArgoUML-SPL modeling tool;IR methods;abstraction gap reduction;code-topic;feature-to-code traceability;information retrieval;large-scale system;legacy software variants;software product line;source code elements","","1","","10","","","4-6 Sept. 2013","","IEEE","IEEE Conference Publications"
"Empirical Evaluation of Bug Linking","T. F. Bissyandé; F. Thung; S. Wang; D. Lo; L. Jiang; L. Réveillère","LaBRI, Univ. of Bordeaux, Bordeaux, France","2013 17th European Conference on Software Maintenance and Reengineering","20130415","2013","","","89","98","To collect software bugs found by users, development teams often set up bug trackers using systems such as Bugzilla. Developers would then fix some of the bugs and commit corresponding code changes into version control systems such as svn or git. Unfortunately, the links between bug reports and code changes are missing for many software projects as the bug tracking and version control systems are often maintained separately. Yet, linking bug reports to fix commits is important as it could shed light into the nature of bug fixing processes and expose patterns in software management. Bug linking solutions, such as ReLink, have been proposed. The demonstration of their effectiveness however faces a number of issues, including a reliability issue with their ground truth datasets as well as the extent of their measurements. We propose in this study a benchmark for evaluating bug linking solutions. This benchmark includes a dataset of about 12,000 bug links from 10 programs. These true links between bug reports and their fixes have been provided during bug fixing processes. We designed a number of research questions, to assess both quantitatively and qualitatively the effectiveness of a bug linking tool. Finally, we apply this benchmark on ReLink to report the strengths and limitations of this bug linking tool.","1534-5351;15345351","Electronic:978-0-7695-4948-4; POD:978-1-4673-5833-0","10.1109/CSMR.2013.19","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498458","Bug Linking;ReLink;benchmark;empirical evaluation;missing links","Benchmark testing;Computer bugs;Control systems;Information retrieval;Joining processes;Software;Training data","program debugging;software engineering","Bugzilla system;ReLink solution;bug linking empirical evaluation;bug linking tool;bug tracking system;code change;ground truth dataset;software bug collection;software management;software project;version control system","","14","","39","","","5-8 March 2013","","IEEE","IEEE Conference Publications"
"How to Use Search Engine Optimization Techniques to Increase Website Visibility","J. B. Killoran","Department of English, Long Island University, Brooklyn","IEEE Transactions on Professional Communication","20130220","2013","56","1","50","66","Research questions: This tutorial aims to answer two general questions: (1) What contributes to search engine rankings? and (2) What can web content creators and webmasters do to make their content and sites easier to find by audiences using search engines? Key concepts: Search engines' rankings are shaped by three classes of participants: search engine companies and programmers, search engine optimization practitioners, and search engine users. Key lessons: By applying three key lessons, professional communicators can make it easier for audiences to find their web content through search engines: (1) consider their web content's audiences and website's competitors when analyzing keywords; (2) insert keywords into web text that will appear on search engine results pages, and (3) involve their web content and websites with other web content creators. Implications: Because successful search engine optimization requires considerable time, professional communicators should progressively apply these lessons in the sequence presented in this tutorial and should keep up to date with frequently changing ranking algorithms and with the associated changing practices of search optimization professionals.","0361-1434;03611434","","10.1109/TPC.2012.2237255","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6463486","Hyperlinks;keywords;organic search;search engine optimization;search-ranking algorithms;social media;websites","Google;Information retrieval;Keyword search;Optimization;Search engines;Search methods;Search problems;Tutorials;Web search","Web sites;professional communication;search engines","Web content audiences;Web content creators;Web text;Webmaster;Website competitors;Website visibility;keyword analysis;professional communicators;search engine companies;search engine optimization practitioners;search engine optimization techniques;search engine programmers;search engine rankings;search engine users","","10","","75","","20130215","March 2013","","IEEE","IEEE Journals & Magazines"
"A Case-Intelligence Recommendation System on Massive Contents Processing through RS and RBF","J. Li; X. Liu","Sch. of Comput. & Inf., Hefei Univ. of Technol., Hefei, China","2013 Fifth International Conference on Measuring Technology and Mechatronics Automation","20130404","2013","","","1","4","Though many varieties of recommendation systems have been developed to greatly promote the intelligent level of E-commerce websites for recent years, IEEE Internet Computing points out that current system can not meet the real large-scale e-commerce demands"", ""and has some weakness such as low precision and slow reaction. The personalized recommendation system model based on case intelligence have proposed, which is a comprehensive expression with combination representation of human sense, logics and creativity, and can acquire the user's preferences from the former stored cases to satisfy the personalized needs. The paper focuses on how to perform effective demands on massive contents in websites, so rough sets (RS) and radial basis function network (RBF) techniques are selected to conquer problems caused by the large amounts of data. The new recommender firstly drills from the huge data in RS and reducts the main attributes, and then RBF retrieves the most valuable similar case for recommendation, which processes the same similar knowledge reasoning. The subsequent research indicates that the integrated system gives a fine performance as shown in our experiments.","2157-1473;21571473","Electronic:978-0-7695-4932-3; POD:978-1-4673-5652-7","10.1109/ICMTMA.2013.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493655","Case Retrieval;Case-Intelligence Recommender;Radial Basis Function Network;Rough Sets Reduction","Cognition;Databases;Electronic mail;Information retrieval;Internet;Nickel;Radial basis function networks","Internet;Web sites;electronic commerce;radial basis function networks;recommender systems;rough set theory","IEEE Internet computing;RBF;RS;case-intelligence recommendation system;e-commerce websites;intelligent level;knowledge reasoning;massive contents processing;personalized recommendation system model;radial basis function network technique;rough set technique","","0","","10","","","16-17 Jan. 2013","","IEEE","IEEE Conference Publications"
"Study on air-to-ground missile with strap-down imaging infrared seeker against moving target","F. g. Li; Q. l. Xia; l. q. Xiong; Y. x. Yao","School of Aerospace, Beijing Institute of Technology, 100081, China","Proceedings of the 32nd Chinese Control Conference","20131021","2013","","","5149","5152","In order to solve the problem that air-to-ground missile with a strap-down seeker to strike a moving target, a method to extend attacking area was present. With measurement data of carrier aircraft, quasi-body pursuit guidance was used in midcourse to create a favorable condition for terminal guidance. In terminal guidance, proportional navigation guidance law was used to strike the target accurately. The guidance information extraction method was studied, as well as the method to design a required autopilot. Based on six-freedom trajectory model, the mathematic simulation was done. The result shows that the strategy is validate and is robustness enough. It can satisfy the requirement to strike the moving target.","","Electronic:978-9-8815-6383-5; POD:978-1-4799-0030-5","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640334","guidance information extraction;guidance strategy;moving target;strap-down seeker","Aircraft navigation;Educational institutions;Electronic mail;Imaging;Information retrieval;Mathematical model;Missiles","infrared imaging;missile guidance;proportional control;trajectory control","air-to-ground missile;attacking area;autopilot design;carrier aircraft;measurement data;moving target;proportional navigation guidance law;quasibody pursuit guidance;six-freedom trajectory model;strapdown imaging infrared seeker","","0","","3","","","26-28 July 2013","","IEEE","IEEE Conference Publications"
"Performance evaluation of VSM and LSI models to determine bug reports similarity","I. Chawla; S. K. Singh","Department of Computer Science, Jaypee Institute of Information Technology, Noida, India","2013 Sixth International Conference on Contemporary Computing (IC3)","20130926","2013","","","375","380","Bug reports of open source software systems are increasing exponentially. One reason for growing bug reports is that bug reporters do not browse the bug repository before submitting a bug report. There may be some similar bugs already reported: one, which are exactly similar or duplicate and other, which are semantically similar means they may belong to the same software component or files. The information contained in the previously reported similar bugs can be helpful in fixing and resolving the newly reported bugs. In this paper, we applied and compared performance of two information retrieval (IR) models: Vector Space Model (VSM) and Latent Semantic Indexing (LSI), in extracting existing similar bug reports. The performance of these two models have been evaluated based on the Top Ten results retrieved by them for relevant bug reports. Experiments have been conducted on 106 bug reports of three components from Google chrome, browser. Result shows that LSI performs better in most cases in comparison to VSM.","","Electronic:978-1-4799-0192-0; POD:978-1-4799-0191-3","10.1109/IC3.2013.6612223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612223","Bug report;Latent Semantic Indexing;Vector space model","Computer bugs;Indexing;Information retrieval;Large scale integration;Semantics;Software;Vectors","information retrieval;pattern matching;performance evaluation;program debugging;public domain software;software management","Google chrome;IR model;LSI models;VSM;browser;bug reports similarity determination;bug repository;information retrieval;latent semantic indexing;open source software systems;performance evaluation;vector space model","","1","","19","","","8-10 Aug. 2013","","IEEE","IEEE Conference Publications"
"A Protocol Analysis Platform Based on Open Source Software","L. Hai; Y. Yue-Yue; L. Chang; C. Li-Na","Sch. of Inf. & Electron., Beijing Inst. of Technol., Beijing, China","2012 Second International Conference on Instrumentation, Measurement, Computer, Communication and Control","20130204","2012","","","846","849","Protocol analysis is very important for engineers for developing a complicate communication system, or for network administrators for supervision and fault diagnosis. Although several commercial protocol analyzers are available in the markets, they are too expensive or difficult to be altered to fit in with the needs of new protocols. In this paper, we present a protocol analysis platform based on open source software, which provides a general, extensible, and easy-to-use framework. We have implemented and tested fully functional prototype for TETRA trunking system, which can provide bit-level packet inspection, performance statistics, message flow analysis, and application information extraction. We demonstrate the utility and flexibility of our platform by three application examples.","","Electronic:978-0-7695-4935-4; POD:978-1-4673-5034-1","10.1109/IMCCC.2012.204","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6429039","TETRA;network statistics;protocol analysis;protocol engineering","Engines;Graphical user interfaces;Information retrieval;Open source software;Protocols;Prototypes","protocols;public domain software;radio networks;software architecture;telecommunication computing","TETRA trunking system;application information extraction;bit level packet inspection;communication system;fault diagnosis;message flow analysis;network administrators;open source software;performance statistics;protocol analysis platform;protocol analyzers;software architecture;supervision diagnosis;trunked radio system","","0","","11","","","8-10 Dec. 2012","","IEEE","IEEE Conference Publications"
"Image search reranking with multi-latent topical graph","J. Shen; T. Mei; Q. Tian; X. Gao","Xidian University, Xi'an 710071, P.R. China","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","20130801","2013","","","1","4","Image search reranking has attracted extensive attention. However, existing image reranking approaches deal with different features independently while ignoring the latent topics among them. It is important to mine multi-latent topic from the features to solve the image search reranking problem. In this paper, we propose a new image reranking model, named reranking with multi-latent topical graph (RMTG), which not only exploits the explicit information of local and global features, but also mines multi-latent topic from these features. We evaluate RMTG over the MSRA-MM dataset and show that RMTG outperforms several existing reranking methods.","0271-4302;02714302","Electronic:978-1-4673-5762-3; POD:978-1-4673-5760-9","10.1109/ISCAS.2013.6571767","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571767","","Feature extraction;Information retrieval;Multimedia communication;Optimization;Semantics;Vectors;Visualization","data mining;feature extraction;image recognition;image representation","MSRA-MM dataset;RMTG;global feature information;image search reranking problem;local feature information;multilatent topic mining;reranking-multilatent topical graph","","0","","13","","","19-23 May 2013","","IEEE","IEEE Conference Publications"
"Towards an Information Extraction System Based on Ontology to Match Resumes and Jobs","D. Çelik; A. Karakas; G. Bal; C. Gültunca; A. Elçi; B. Buluz; M. C. Alevli","","2013 IEEE 37th Annual Computer Software and Applications Conference Workshops","20130923","2013","","","333","338","While Internet takes up by far the most significant part of our daily lives, finding jobs/employees on the Internet has started to play a crucial role for job seekers and employers. Online recruitment websites and human resources consultancy and recruitment companies enable job seekers to create their résumé, a brief written formal document including job seeker's basic information such as personal information, educational information, work experience and qualifications in order to find and apply for desirable jobs, whereas they enable companies to find qualified employees they are looking for. However résumés may be written in many ways that make it difficult for online recruitment companies to keep these data in their relational databases. In this study, a project that Kariyer.net (largest online recruitment website in Turkey) and TUBITAK (The Scientific and Technological Research Council of Turkey) have been jointly working is proposed. In this mentioned project, a system enables free structured format of résumés to transform into an ontological structure model. The produced system based on ontological structure model and called Ontology based Résumé Parser (ORP) will be tested on a number of Turkish and English résumés. The proposed system will be kept in Semantic Web approach that provides companies to find expert finding in an efficient way.","","Electronic:978-1-4799-2159-1; POD:978-1-4799-2160-7","10.1109/COMPSACW.2013.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605812","Curriculum Vitae;Information Extraction;Ontology;Résumé;Semantic Web","Companies;Educational institutions;Information retrieval;OWL;Ontologies;Recruitment","Web sites;document handling;information retrieval;natural language processing;ontologies (artificial intelligence);recruitment;relational databases;semantic Web","English résumés;Internet;ORP;TUBITAK;The Scientific and Technological Research Council;Turkey;Turkish résumés;educational information;employers;human resources consultancy;information extraction system;job matching;job seekers;online recruitment Web sites;ontological structure model;ontology based résumé parser;personal information;recruitment companies;relational databases;resume matching;semantic Web approach;work experience;written formal document","","0","","16","","","22-26 July 2013","","IEEE","IEEE Conference Publications"
"Information Extraction of Forum Based on Regular Expression","G. He; Y. Zhang; X. Wu","Beijing Key Lab. of Network Syst. Archit. & Convergence, Beijing Univ. of Posts & Telecommun., Beijing, China","2013 5th International Conference on Intelligent Human-Machine Systems and Cybernetics","20131024","2013","2","","118","122","This paper introduces the popular universal forum systems in domestic mainstream forum and analyzes the unique characteristics of these forum systems. Based on these unique characteristics, we propose the concept of system fingerprint which can used to detect the different systems of forum exactly and extract the users' information efficiently. It contributes to the development of network information auditing. Experimental results show that the approach can achieve high extraction accuracy. It has important application value and practical significance.","","Electronic:978-0-7695-5011-4; POD:978-1-4799-0236-1","10.1109/IHMSC.2013.175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642703","forum system fingerprint;information extraction;regular expression matching","Data mining;Digital video broadcasting;Feature extraction;Fingerprint recognition;Information retrieval;Internet;Lead","Web sites;information retrieval","domestic mainstream forum;forum information extraction;network information auditing;popular universal forum systems;regular expression;system fingerprint","","0","","15","","","26-27 Aug. 2013","","IEEE","IEEE Conference Publications"
"An empirical study on requirements traceability using eye-tracking","N. Ali; Z. Sharafl; Y. G. Guéhéneuc; G. Antoniol","DGIGL, Ecole Polytech. de Montreal, Montreal, QC, Canada","2012 28th IEEE International Conference on Software Maintenance (ICSM)","20130110","2012","","","191","200","Requirements traceability (RT) links help developers to understand programs and ensure that their source code is consistent with its documentation. Creating RT links is a laborious and resource-consuming task. Information Retrieval (IR) techniques are useful to automatically recover traceability links. However, IR-based approaches typically have low accuracy (precision and recall) and, thus, creating RT links remains a human intensive process. We conjecture that understanding how developers verify RT links could help improve the accuracy of IR-based approaches to recover RT links. Consequently, we perform an empirical study consisting of two controlled experiments. First, we use an eye-tracking system to capture developers' eye movements while they verify RT links. We analyse the obtained data to identify and rank developers' preferred source code entities (SCEs), e.g., class names, method names. Second, we use the ranked SCEs to propose two new weighting schemes called SE/IDF (source code entity/inverse document frequency) and DOI/IDF (domain or implementation/inverse document frequency) to recover RT links combined with an IR technique. SE/IDF is based on the developers preferred SCEs to verify RT links. DOI/IDF is an extension of SE/IDF distinguishing domain and implementation concepts. We use LSI combined with SE/IDF, DOI/IDF, and TF/IDF to show, using two systems, iTrust and Pooka, that LSI<sub>DOI/IDF</sub> statistically improves the accuracy of the recovered RT links over LSI<sub>TF/IDF</sub>.","1063-6773;10636773","Electronic:978-1-4673-2312-3; POD:978-1-4673-2313-0","10.1109/ICSM.2012.6405271","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405271","LDA;LSI;Requirements traceability;eye tracking;source code","Accuracy;Conferences;Information retrieval;Java;Large scale integration;Software maintenance;Visualization","formal verification;information retrieval;program diagnostics;statistical analysis","DOI/IDF scheme;IR-based approach accuracy improvement;IR-based approaches;LSI<sub>DOI/IDF</sub>;Pooka system;SCE;SE/IDF scheme;TF/IDF;automatic requirements traceability link recovery;class names;data analysis;domain concepts;domain-or-implementation/inverse document frequency;empirical study;eye-tracking system;iTrust system;implementation concepts;information retrieval techniques;method names;precision value;recall value;requirements traceability;source code entity identification;source code entity ranking;source code entity/inverse document frequency scheme;statistical analysis;weighting schemes","","8","","26","","","23-28 Sept. 2012","","IEEE","IEEE Conference Publications"
"Partial-Duplicate Image Retrieval via Saliency-Guided Visual Matching","L. Li; S. Jiang; Z. J. Zha; Z. Wu; Q. Huang","Chinese Academy of Sciences","IEEE MultiMedia","20130823","2013","20","3","13","23","This article proposes a novel partial-duplicate image-retrieval scheme based on saliency-guided visual matching, where the localization of duplicates is done simultaneously. The image is abstracted by visually salient and rich regions (VSRRs), which are of visual saliency and contain rich visual content. Furthermore, to refine the retrieval, a relative saliency ordering constraint is constructed that captures the robust relative saliency layout of the VSRRs. The authors propose an efficient algorithm to embed this constraint into the index system so as to speed up retrieval. Comparison experiments with state-of-the-art methods on five databases show the efficiency and effectiveness of the proposed approach.","1070-986X;1070986X","","10.1109/MMUL.2013.15","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6488675","VSRR;image representation;multimedia;partial-duplicate image retrieval;relative constraint;saliency analysis;visually salient and rich regions","Image representation;Information analysis;Information retrieval;Media;Multimedia communication","image matching;image retrieval","VSRR;duplicate localization;partial-duplicate image retrieval;relative saliency ordering constraint;robust relative saliency layout;saliency-guided visual matching;visually salient and rich region","","8","","16","","20130327","July-Sept. 2013","","IEEE","IEEE Journals & Magazines"
"A text analytics-based approach to compute coverage, readability and comprehensibility of eBooks","S. Khurana; M. Relan; V. K. Singh","Department of Computer Science, South Asian University, New Delhi, India","2013 Sixth International Conference on Contemporary Computing (IC3)","20130926","2013","","","256","261","This paper presents our text analytics-based algorithmic formulation for qualitative evaluation of eBooks. Our algorithmic approach computes qualitative aspects of an eBook, particularly its coverage, readability and comprehensibility. Measurement of these qualitative aspects required parsing the eBook text and extracting different information from it such as the concepts described in the eBook. For computing coverage, we have used some standard reference documents in a novel manner. For readability, we have resorted to standard definition and metrics for readability described in some past works. The comprehensibility aspect computation is based on coherent, cohesive and sequentially progressive discussion of learning concepts described in the eBook. The qualitative aspects are quantified in terms of numerical values. The system is tested on a small but diverse dataset of about 30 eBooks. The results are validated by using external information about eBook popularity collected from the Word Wide Web, such as eBook review, and performing sentiment analysis on them. The results obtained give us a fair measure of qualitative account of an eBook on the parameters of coverage, readability and comprehensibility and demonstrates the usefulness of our algorithmic approach.","","Electronic:978-1-4799-0192-0; POD:978-1-4799-0191-3","10.1109/IC3.2013.6612200","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612200","Comprehensibility;Concept Extraction;Qualitative Evaluation;Readability;Text Analytics;eBook","Artificial intelligence;Data mining;Educational institutions;Electronic publishing;Information retrieval;Sections;Standards","Web sites;computer aided instruction;electronic publishing;information retrieval;reviews;text analysis","Word Wide Web;comprehensibility aspect computation;document analysis;eBook review;eBook text;information extraction;learning concept;readability;sentiment analysis;text analytics-based approach","","0","","25","","","8-10 Aug. 2013","","IEEE","IEEE Conference Publications"
"Improvement in automatic classification of Persian documents by means of Naïve Bayes and Representative Vector","A. Jafari; M. Hosseinejad; A. Amiri","Islamic Azad Univ. of Zanjan, Zanjan, Iran","2011 1st International eConference on Computer and Knowledge Engineering (ICCKE)","20130117","2011","","","226","229","Representative Vector is a kind of Vector which includes related words and the degree of their relationships. In this paper the effect of using this kind of Vector on automatic classification of Persian documents is examined. In this method, preprocessed documents, extra words as well as word stems are at first found. Next, through one of the known ways, some features are extracted for each category. Then, the Representative Vector, which is made based on the elicited features, leads to some more detailed words which are better Representatives for each category. Findings of the experiments show that Precision and Recall can be increased significantly by extra words omission and addition of few words in the Representative Vectors as well as the use of a famous classification model like Naïve Bayes.","","Electronic:978-1-4673-5713-5; POD:978-1-4673-5712-8","10.1109/ICCKE.2011.6413355","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6413355","Documents Classification;Naïve Bayes Classifier;Representative Vector;Stemming","Computers;Educational institutions;Information retrieval;Semantics;Support vector machine classification;Text categorization;Vectors","Bayes methods;classification;document handling","Naive Bayes;Persian documents;automatic classification model;feature extraction;representative vector","","0","","13","","","13-14 Oct. 2011","","IEEE","IEEE Conference Publications"
"An experience with measuring multi-user online task performance","A. Paul; B. Yadamsuren; S. Erdelez","Indian Inst. of Manage. Kozhikode, Kozhikode, India","2012 World Congress on Information and Communication Technologies","20130110","2012","","","639","644","Growing demand for usability testing as a part of iterative design process requires novel approaches and methods to accelerate the process. This paper introduces a new approach of conducting MUST (Multiple-User Simultaneous Testing) using the Autopilot feature of Morae as a potential way for discounted usability testing method. Seventeen participants performed 15 information retrieval tasks on an academic website using MUST. Taking proper steps in research design and study set up when collecting data on online user interaction in a group setting can improve data quality by avoiding unnecessary mistakes. Also, knowledge about advanced features available in the data collection software can help information retrieval and usability evaluation researchers to become well-prepared for the challenges of data collection. Automatic reports generated by Autopilot are provided that include graphs of average time on tasks and participant responses on likert scale. Advantages and disadvantages of the Autopilot feature are also shared.","","Electronic:978-1-4673-4805-8; POD:978-1-4673-4806-5","10.1109/WICT.2012.6409154","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6409154","Autopilot;MORAE;Multiple-User Simultaneous Testing;Quality of Web service;Usability","Atmospheric measurements;Computers;Data collection;Information retrieval;Particle measurements;Testing;Usability","Web sites;information retrieval;iterative methods;program testing","MUST;Morae;academic Website;autopilot feature;data collection software;information retrieval tasks;iterative design process;likert scale;multiple-user simultaneous testing;multiuser online task performance;online user interaction;usability testing","","0","","20","","","Oct. 30 2012-Nov. 2 2012","","IEEE","IEEE Conference Publications"
"Recommendation agents in intelligent query answering systems","Y. Sun; Z. Li; J. Xie","School of Information Science and Technology, Yunnan Normal University, Kunming, China","2013 8th International Conference on Computer Science & Education","20130715","2013","","","441","445","In this paper, after the introduction of ontology-based knowledge bases and user models, a model of recommendation agents that can query over distributed knowledge bases with heterogeneity for web-based intelligent query answering systems is proposed, and the query answering process of these agents is analyzed. Ontologies are introduced to describe the semantic information of knowledge bases in order to express fully the semantics of queries in our web-based intelligent query answering system. This model improves the relevance of the query results. At the same time, it reflects the dynamic change of the information in the knowledge bases and reduces the consistency and efficiency problem caused by using globe ontology.","","Electronic:978-1-4673-4463-0; POD:978-1-4673-4464-7","10.1109/ICCSE.2013.6553952","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553952","intelligent query answering systems;ontology-based knowledge bases;recommendation agents;user models","Adaptation models;Computers;Indexes;Information retrieval;Ontologies;Semantics;Sun","Internet;knowledge based systems;multi-agent systems;ontologies (artificial intelligence);query processing;recommender systems","Web-based intelligent query answering systems;consistency problem;distributed knowledge bases;efficiency problem;globe ontology;ontology-based knowledge bases;recommendation agents","","0","","13","","","26-28 April 2013","","IEEE","IEEE Conference Publications"
"With LSA Size DOES Matter","C. Layfield","Fac. of ICT, Dept. of CIS, Univ. of Malta, Msida, Malta","2012 Sixth UKSim/AMSS European Symposium on Computer Modeling and Simulation","20130114","2012","","","127","131","Latent Semantic Analysis (LSA) is a technique from the field of Natural Language Processing that enables comparison of semantic similarities between documents using vector operations. This technique has been used in areas from Information Retrieval (IR) to the automated assessment of essays. One property used in document comparison is size. The general philosophy is that more text is better although few concrete examples or guidelines exist that demonstrate this. This paper shows, via a novel concrete example taken from real world data, that larger documents do imply more accurate semantic similarity comparisons.","","Electronic:978-0-7695-4926-2; POD:978-1-4673-4977-2","10.1109/EMS.2012.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6410140","LSA;NLP;automated essay assessment;document length;latent semantic analysis;natural language processing","Correlation;Educational institutions;Information retrieval;Learning systems;Matrix decomposition;Semantics;Vectors","document handling;information retrieval;natural language processing","IR;Information Retrieval;LSA size;document comparison;latent semantic analysis;natural language processing;semantic similarities;vector operations","","2","","22","","","14-16 Nov. 2012","","IEEE","IEEE Conference Publications"
"Measuring the wide ranging instant perceptive query","S. Prasanna; S. S. Kumar","Anna University Regional Centre Madurai, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","20130425","2013","","","196","200","Time is one of the important criteria which we have to consider in day-to-day life. Every one's passion is to retrieve the optimum data within a short span of time. In this paper we are observing the general time sensitive queries. Queries are also important along with finding topic similarity and final document ranking process. The proposal of this paper is to construct a general framework which will handles the time sensitive queries and also identifies sensitive time intervals for those queries. we construct BM25 techniques which will provides the overall ranking mechanism. We analyse extended experimental results with various news articles, TREC data and news archives. Finally we provide a conclusion that our techniques are more efficient and it significantly improves the quality results of time sensitive queries compared to the state-of-the-art retrieval techniques.","","Electronic:978-1-4673-5788-3; POD:978-1-4673-5786-9","10.1109/ICICES.2013.6508313","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508313","Data search and retrieval;Document ranking;Generating time sensitive queries","Data models;Databases;Estimation;Histograms;Information retrieval;Probabilistic logic;Research and development","query processing;relevance feedback","BM25 techniques;TREC data;final document ranking process;general time sensitive queries;news archives;optimum data retrieval;overall ranking mechanism;state-of-the-art retrieval techniques;topic similarity;wide ranging instant perceptive query","","0","","6","","","21-22 Feb. 2013","","IEEE","IEEE Conference Publications"
"Modelling on web dynamic incremental crawling and information processing","K. Gao; Wei Wang; Shen Gao","School of Information Science & Engineering, Hebei University of Science and Technology, China","2013 5th International Conference on Modelling, Identification and Control (ICMIC)","20131024","2013","","","293","298","The amount of web information is increasing rapidly, and it is continuously being produced and updated in anywhere and anytime by means of Internet and social networks. As for a search engine, keeping up with the evolving web is necessary. How to model the change and which part should be updated more often? Towards this goal, this paper presents the modeling on dynamic web evolution and incremental crawling strategy, and concerns about the refresh interval with minimum waiting time. As a result, the crawling probability on some sites is higher than others so these sites will be given more opportunities to be updated. Based on the web site priority level adjusted algorithm, the dynamic web information gathering strategy is proposed. Through monitoring the proposed metrics, the web site priority level can be dynamically adjusted. It is essential when the bandwidth is not wide enough or the resource is limited. Further, some strategies on web information extraction and processing are also present. The experimental results validate the feasibility of the approach.","","Electronic:978-0-9567157-3-9; POD:978-1-4673-6025-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6642201","Search engine;crawler;information extraction;refresh","Information retrieval;Monitoring;Search engines;Web services","Internet;information retrieval;search engines;social networking (online)","Internet;Web dynamic incremental crawling;Web site priority level adjusted algorithm;crawling probability;dynamic Web evolution;dynamic Web information gathering strategy;incremental crawling strategy;information processing;search engine;social networks","","0","","17","","","Aug. 31 2013-Sept. 2 2013","","IEEE","IEEE Conference Publications"
"Ontology-based semantic metadata extraction approach","B. Jebali; R. Farhat","Research Laboratory of Technologies of Information and Communication & Electrical Engineering (LaTICE) University Of Tunis 5 Avenue Taha Hussein, B.P. 56, Bab Menara, Tunis, Tunisia","2013 International Conference on Electrical Engineering and Software Applications","20130815","2013","","","1","5","In this paper we describes our approach for automatic generation of learning objects' semantic metadata. The extraction process is based on the OBIE (Ontology Based Information Extraction) systems' principles. The input of our approach is a set of IEEE LOM metadata elements in conformance with two requirements. First, each data element must describe the educational content of the learning object. Second, it must be one of the data elements frequently filled by the learning objects' authors and required by most of the LOM application profiles. Concerning the outputs, each one is a couple of a domain concept (from domain ontology) and a degree of pertinence. Moreover we present the details concerning the integration of our approach to learning objects' repositories by taking the COLORS repository as example. In fact the ultimate goal behind our approach is the improvement of repositories' services by offering semantic metadata.","","Electronic:978-1-4673-6301-3; POD:978-1-4673-6302-0","10.1109/ICEESA.2013.6578408","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578408","LOM metadata;Semantic metadata;ontology based information extraction system","Context;Data mining;Image color analysis;Information retrieval;Ontologies;Semantics;Standards","information retrieval;learning (artificial intelligence);meta data;ontologies (artificial intelligence)","COLORS repository;IEEE LOM metadata elements;LOM application profiles;OBIE;automatic generation;domain ontology;educational content;extraction process;learning objects;ontology based information extraction;ontology-based semantic metadata extraction approach;repositories services;system principles","","0","","22","","","21-23 March 2013","","IEEE","IEEE Conference Publications"
"Web document description based on ontologies","M. Milička; R. Burget","Fac. of Inf. Technol., Brno Univ. of Technol., Brno, Czech Republic","2013 Second International Conference on Informatics & Applications (ICIA)","20131031","2013","","","288","293","The effective document modeling based on the visual features is not always trivial in the case where we want to apply the advance document processing with the visual features reflection. In this paper, we are suggesting a document description based on RDF and ontologies. Such approach allows modeling on several levels of abstraction that is reflecting the document visual features perceived by human. Such approach also advantageously uses the existing tools for the RDF querying and reasoning. The paper presents four levels of the document description where all of them are based on the ontologies but each level represents a different knowledge. The main part of the paper is oriented on the document description based on the Semantic ontology. The ontology is based on SWRL rules where rules assign the elements to a given classes. Finally, we are presenting examples of the class definitions based on the SWRL rules.","","Electronic:978-1-4673-5256-7; POD:978-1-4673-5254-3","10.1109/ICoIA.2013.6650271","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650271","","Image segmentation;Information retrieval;Ontologies;Rendering (computer graphics);Resource description framework;Semantics;Visualization","document handling;ontologies (artificial intelligence);semantic Web","RDF querying;RDF reasoning;SWRL rules;Semantic Web Rule Language;Web document description;abstraction level;advance document processing;document description;document modeling;document visual features reflection;semantic ontology","","1","","15","","","23-25 Sept. 2013","","IEEE","IEEE Conference Publications"
"Urban road information extraction from high resolution remotely sensed image based on semantic model","J. Wang; J. Qian; R. Ma","College of tourism and Geographical Science, Yunnan Normal University, Kunming, China","2013 21st International Conference on Geoinformatics","20131010","2013","","","1","5","The road is an important fundamental geographic information. Acquiring the road information quickly and accurately has a great significance for GIS data updating, image matching, target detection, and automated digital mapping. Automatic/semi-automatic extraction of road information of remote sensing images is the problem of visual interpretation computer research, RS, and GIS. Application of high resolution satellite images and development of semantic model theory provides more possibilities and a higher degree of accuracy for object extraction of remotely sensed image. The OAR model of human cognition has been introduced, experimental study has been carried out on extracting road information from Quick Bird multi-spectral Imaging with semantic model, the result shows that the length accuracy of extracted road was 89.19%, the width accuracy is 71.54%, and the intact rate 50.32%. The extracted result is better than that of object-oriented extracted. As a whole, that the road information extraction semantic model of highresolution satellite remotely sensed image is efficiency.","2161-024X;2161024X","Electronic:978-1-4673-6228-3; POD:978-1-4673-6226-9","10.1109/Geoinformatics.2013.6626045","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626045","high-resolution remotely sensed image;information extraction;semantic model;urban road","Data mining;Feature extraction;Image resolution;Information retrieval;Remote sensing;Roads;Semantics","feature extraction;geographic information systems;geophysical image processing;image matching;image resolution;object detection;remote sensing;road traffic","GIS data;automated digital mapping;geographic information;high resolution remotely sensed image;human cognition;image matching;object extraction;quick bird multispectral imaging;road information extraction semantic model;satellite image resolution;semantic model;semantic model theory;target detection;urban road information extraction","","2","","32","","","20-22 June 2013","","IEEE","IEEE Conference Publications"
"Term relevance dependency model for text classification","M. S. Wu; H. M. Wang","Information and Communications Research Laboratories, Industrial Technology Research Institute, Hsinchu, Taiwan","Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)","20130214","2012","","","1064","1067","Text classification (TC) has long been an important research topic in information retrieval (IR) related areas. Conventional language model (LM)-based TC is solely based on matching the words in the documents and classes by using a naïve Bayes classifier (NBC). In the literature, both the term association model (TA), which further considers word-to-word information, and the relevance model (RM), which further considers word-to-document information, have been shown to outperform a simple LM for IR. In this paper, we study a novel integration of TA with RM for LM-NBC-based TC. The new model is called the term relevance dependency model. In the model, the probability of a word given a class is represented by a term association LM probability learned by a RM framework. The results of TC experiments on the 20newsgroups and Reuters-21578 corpora demonstrate that the new model outperforms the standard NBC and several other LM-NBC-based methods.","1051-4651;10514651","Electronic:978-4-9906441-0-9; POD:978-1-4673-2216-4","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460319","","Adaptation models;Computational modeling;Data models;Information retrieval;Smoothing methods;Support vector machines;Vectors","classification;information retrieval;probability;text analysis;word processing","LM-NBC-based TC;NBC;RM framework;Reuters-21578 corpora;TA model;information retrieval;language model-based TC;naive Bayes classifier;term association LM probability;term association model;term relevance dependency model;text classification;word matching;word-to-document information;word-to-word information","","0","","13","","","11-15 Nov. 2012","","IEEE","IEEE Conference Publications"
"Latent Dirichlet Allocation for Spatial Analysis of Satellite Images","C. Văduva; I. Gavăt; M. Datcu","Department of Applied Electronics and Information Technology, Faculty of Electronics, Telecommunication and Information Technology, University Politehnica of Bucharest , Bucharest, Romania","IEEE Transactions on Geoscience and Remote Sensing","20130418","2013","51","5","2770","2786","This paper describes research that seeks to supersede human inductive learning and reasoning in high-level scene understanding and content extraction. Searching for relevant knowledge with a semantic meaning consists mostly in visual human inspection of the data, regardless of the application. The method presented in this paper is an innovation in the field of information retrieval. It aims to discover latent semantic classes containing pairs of objects characterized by a certain spatial positioning. A hierarchical structure is recommended for the image content. This approach is based on a method initially developed for topics discovery in text, applied this time to invariant descriptors of image region or objects configurations. First, invariant spatial signatures are computed for pairs of objects, based on a measure of their interaction, as attributes for describing spatial arrangements inside the scene. Spatial visual words are then defined through a simple classification, extracting new patterns of similar object configurations. Further, the scene is modeled according to these new patterns (spatial visual words) using the latent Dirichlet allocation model into a finite mixture over an underlying set of topics. In the end, some statistics are done to achieve a better understanding of the spatial distributions inside the discovered semantic classes.","0196-2892;01962892","","10.1109/TGRS.2012.2219314","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6353569","High-level image understanding;invariant signatures;latent Dirichlet allocation (LDA);spatial relationships","Feature extraction;Geospatial analysis;Histograms;Information retrieval;Semantics;Visualization","","","","22","","21","","20121115","May 2013","","IEEE","IEEE Journals & Magazines"
"Bayesian network based classification of mammography structured reports","A. Farruggia; R. Magro; S. Vitabile","Dipartimento di Biopatologia e Biotecnologie Mediche e Forensi Universit&#x00E1; degli Studi di Palermo Viale del Vespro, 90127, Palermo, Italy","2013 International Conference on Computer Medical Applications (ICCMA)","20130422","2013","","","1","5","In modern medical domain, documents are created directly in electronic form and stored on huge databases containing documents, text in integral form and images. Retrieving right informations from these servers is challenging and, sometimes, this is very time consuming. Current medical technology do not provide a smart methodology classification of such documents based on their content. In this work the radiological structured reports are analysed classified and assigning an appropriate label. The text classifier is used to label a mammographic structured report. The experimental data are real clinical report coming from a hospital server. Analysing the structured report content, the classifier labels the patient structured report as healthy or pathological. The present work uses Information Retrieval techniques to improve the classification process. These technique provide a light semantic analysis to remove negative terms, a removing stop-word step and, finally, a thesaurus is used to uniform used words. The structured reports are classified using a Bayes Naive Classifier. The experimental results provide interesting performance in terms of specificity and sensibility. Others two indexes are computed in order to assess system's robustness: these are the A<sub>z</sub> (Area under Curve ROC) and σ A<sub>z</sub>(A<sub>z</sub> standard error).","","Electronic:978-1-4673-5214-7; POD:978-1-4673-5213-0","10.1109/ICCMA.2013.6506150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6506150","","Bayes methods;Equations;Information retrieval;Pathology;Semantics;Training","belief networks;information retrieval;mammography;medical computing;pattern classification;text analysis","A<sub>z</sub> standard error index;Bayes naive classifier;Bayesian network based classification;area under receiver operating characteristic curve index;document classification;hospital server;information retrieval technique;mammography structured report classification;medical domain;medical technology;radiological structured report;semantic analysis;stop-word step;text classifier","","2","","13","","","20-22 Jan. 2013","","IEEE","IEEE Conference Publications"
"Building a Test Collection for Sorani Kurdish","K. S. Esmaili; D. Eliassi; S. Salavati; P. Aliabadi; A. Mohammadi; S. Yosefi; S. Hakimi","Nanyang Technological University, Singapore","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","7","Despite having a large number of speakers, Sorani - one of the two principle branches of the Kurdish language - is among the less-resourced languages. This paper reports on the outcomes of a project aimed at providing the essential resources for processing Sorani texts. The primary output of this project is Pewan, the first standard Test Collection to evaluate Sorani Information Retrieval systems. The other language resources that we have constructed in this project are: (i) a light-stemmer, (ii) a list of affixes, and (iii) a list of stopwords. We also used these newly-built resources to study the effectiveness of basic IR strategies on Sorani documents. Our experimental results show that normalization and, to a lesser extent, stemming can greatly improve the performance of Sorani IR systems.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616470","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616470","","Buildings;Educational institutions;Information retrieval;Morphology;Reliability;Standards;Writing","information retrieval systems;natural language processing;project management;text analysis","Pewan;Sorani Kurdish language;Sorani information retrieval system evaluation;Sorani text processing;affixes;less-resourced languages;light-stemmer;standard test collection;stopwords","","1","","29","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"Plagiarism detection in text using Vector Space Model","A. Ekbal; S. Saha; G. Choudhary","Department of Computer Science and Engineering, Indian Institute of Technology Patna, India-800 013","2012 12th International Conference on Hybrid Intelligent Systems (HIS)","20130128","2012","","","366","371","Plagiarism denotes the act of copying someone else's idea (or, works) and claiming it as his/her own. Plagiarism detection is the procedure to detect the texts of a given document which are plagiarized, i.e. copied from from some other documents. Potential challenges are due to the facts that plagiarists often obfuscate the copied texts; might shuffle, remove, insert, or replace words or short phrases; might also restructure the sentences replacing words with synonyms; and changing the order of appearances of words in a sentence. In this paper we propose a technique based on textual similarity for external plagiarism detection. For a given suspicious document we have to identify the set of source documents from which the suspicious document is copied. The method we propose comprises of four phases. In the first phase, we process all the documents to generate tokens, lemmas, finding Part-of-Speech (PoS) classes, character-offsets, sentence numbers and named-entity (NE) classes. In the second phase we select a subset of documents that may possibly be the sources of plagiarism. We use an approach based on the traditional Vector Space Model (VSM) for this candidate selection. In the third phase we use a graph-based approach to find out the similar passages in suspicious document and selected source documents. Finally we filter out the false detections<sup>1</sup>.","","Electronic:978-1-4673-5116-4; POD:978-1-4673-5114-0","10.1109/HIS.2012.6421362","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6421362","N-gram language model;Plagiarism detection;Vector Space Model","Computational modeling;Hybrid intelligent systems;Information retrieval;Measurement;Plagiarism;Training;Vectors","graph theory;law;text analysis","NE class;POS class;VSM;candidate selection;character-offsets;graph-based approach;lemma generation;named-entity class;part-of-speech class;plagiarism detection;sentence numbers;source documents;suspicious document;textual similarity;token generation;vector space model","","3","","12","","","4-7 Dec. 2012","","IEEE","IEEE Conference Publications"
"Chord classification of multi-instrumental music using exemplar-based sparse representation","L. W. Kong; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China","2013 IEEE China Summit and International Conference on Signal and Information Processing","20131010","2013","","","113","117","This paper describes a chord classification system based on sparse signal representation. We aim at determining the key and type of a multi-instrumental chord from a short frame of music audio. An exemplar set consisting of magnitude spectra of single notes is constructed. Magnitude spectrum of a chord frame is decomposed into a weighted sum of note spectra by solving an ℓ<sub>1</sub>-norm minimization problem. The weights representing the same pitch class is summed to form a pitch class vector. The chord label can be found by calculating the product index of the legitimate chords and selecting the one with the largest index. In this research, we have evaluated our system using synthesized flute and piano chords. The experimental result shows that our approach is more effective when compared with the chroma-based baseline systems.","","Electronic:978-1-4799-1043-4; POD:978-1-4799-1041-0; USB:978-1-4799-1042-7","10.1109/ChinaSIP.2013.6625309","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625309","Acoustic signal analysis;chord classification;exemplar;multi-instrumental music;sparse signal representation","Harmonic analysis;Hidden Markov models;Indexes;Instruments;Music information retrieval;Support vector machine classification;Vectors","acoustic signal processing;minimisation;music;signal representation","ℓ<sub>1</sub>-norm minimization problem;chord classification system;chord frame;chroma-based baseline systems;exemplar-based sparse representation;instrumental music;legitimate chords;magnitude spectra;music audio;note spectra;piano chords;pitch class vector;product index;sparse signal representation;synthesized flute;weighted sum","","1","","18","","","6-10 July 2013","","IEEE","IEEE Conference Publications"
"Building a Scalable Database-Driven Reverse Dictionary","R. Shaw; A. Datta; D. VanderMeer; K. Dutta","Google Inc., San Jose","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","528","540","In this paper, we describe the design and implementation of a reverse dictionary. Unlike a traditional forward dictionary, which maps from words to their definitions, a reverse dictionary takes a user input phrase describing the desired concept, and returns a set of candidate words that satisfy the input phrase. This work has significant application not only for the general public, particularly those who work closely with words, but also in the general field of conceptual search. We present a set of algorithms and the results of a set of experiments showing the retrieval accuracy of our methods and the runtime response time performance of our implementation. Our experimental results show that our approach can provide significant improvements in performance scale without sacrificing the quality of the result. Our experiments comparing the quality of our approach to that of currently available reverse dictionaries show that of our approach can provide significantly higher quality over either of the other currently available implementations.","1041-4347;10414347","","10.1109/TKDE.2011.225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6060823","Dictionaries;search process;thesauruses;web-based services","Dictionaries;Information processing;Information retrieval;Search methods;Semantics;Web and internet services","Internet;dictionaries;information retrieval;set theory","Web-based services;conceptual search;forward dictionary;performance scale;retrieval accuracy;reverse mapping;runtime response time performance;scalable database-driven reverse dictionary;user input phrase","","1","","41","","20111025","March 2013","","IEEE","IEEE Journals & Magazines"
"Key-Phrase Extraction Based on a Combination of CRF Model with Document Structure","F. Yu; H. W. Xuan; D. Q. Zheng","Sch. of Comput. &amp; Inf. Eng., Harbin Univ. of Commerce, Harbin, China","2012 Eighth International Conference on Computational Intelligence and Security","20130110","2012","","","406","410","Key-Phrase should not only reflect the main content of a document, but also reflect the specialty of this document. Key-Phrase extraction is an important technique in the field of text information processing. With the advent of the Internet age, on-line file shows an astonishing increase in geometry and information explosion has became the main character of this age. Searching and making use of network information becomes more difficult. Therefore, automatically extraction on keyword is required. This paper uses the idea of classification to complete the task of Key-Phrase extraction, which uses SVM to build classification model and uses CRF to extract Key-Phrases. The testing result shows that, the mentioned extraction approach has improved dramatically compared with previous methods in precision and recall rate.","","Electronic:978-0-7695-4896-8; POD:978-1-4673-4725-9","10.1109/CIS.2012.97","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405955","Feature Selection;Information Extraction;Inverse Document Frequency;Key-phrase;Term Frequency","Data models;Educational institutions;Feature extraction;Information retrieval;Text analysis;Training","Internet;information retrieval;pattern classification;random processes;support vector machines;text analysis","CRF model;Internet;SVM;automatic keyword extraction;classification model;conditional random fields;document structure;geometry;information explosion;key-phrase extraction;network information search;online file;support vector machines;text information processing","","3","","19","","","17-18 Nov. 2012","","IEEE","IEEE Conference Publications"
"RE@21 spotlight: Most influential papers from the requirements engineering conference","M. Glinz; R. Wieringa","Department of Informatics University of Zurich Switzerland","2013 21st IEEE International Requirements Engineering Conference (RE)","20131021","2013","","","368","370","Since 2003, an award has been presented annually at the IEEE International Requirements Engineering Conference for the Most Influential Paper presented at the conference 10 years previously. In 2013, we celebrate 21 years of the Requirements Engineering Conference, and we use this as an opportunity to reflect on the Most Influential Papers to date. Two sessions of the 2013 conference highlight the work of previous award winners and provide the authors with the opportunity to describe the trajectory of their work over the ten years that led to the award, and to discuss its impact since.","1090-705X;1090705X","Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0","10.1109/RE.2013.6636755","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636755","Most Influential Paper;RE@21;Requirements Engineering Conference","Awards activities;Conferences;Educational institutions;Embedded systems;IEEE Computer Society Press;Information retrieval","","","","0","","7","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Efficient Keyword Search on Uncertain Graph Data","Y. Yuan; G. Wang; L. Chen; H. Wang","Northeastern Univ., Shenyang, China","IEEE Transactions on Knowledge and Data Engineering","20131028","2013","25","12","2767","2779","As a popular search mechanism, keyword search has been applied to retrieve useful data in documents, texts, graphs, and even relational databases. However, so far, there is no work on keyword search over uncertain graph data even though the uncertain graphs have been widely used in many real applications, such as modeling road networks, influential detection in social networks, and data analysis on PPI networks. Therefore, in this paper, we study the problem of top-k keyword search over uncertain graph data. Following the similar answer definition for keyword search over deterministic graphs, we consider a subtree in the uncertain graph as an answer to a keyword query if 1) it contains all the keywords; 2) it has a high score (defined by users or applications) based on keyword matching; and 3) it has low uncertainty. Keyword search over deterministic graphs is already a hard problem as stated in [1], [2], [3]. Due to the existence of uncertainty, keyword search over uncertain graphs is much harder. Therefore, to improve the search efficiency, we employ a filtering-and-verification strategy based on a probabilistic keyword index, PKIndex. For each keyword, we offline compute path-based top-k probabilities, and attach these values to PKIndex in an optimal, compressed way. In the filtering phase, we perform existence, path-based and tree-based probabilistic pruning phases, which filter out most false subtrees. In the verification, we propose a sampling algorithm to verify the candidates. Extensive experimental results demonstrate the effectiveness of the proposed algorithms.","1041-4347;10414347","","10.1109/TKDE.2012.222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648594","Database;algorithm;graph data;uncertain data","Data mining;Graphs;Information retrieval;Ontologies;Search methods;Text mining;Uncertainty","data analysis;indexing;information filtering;probability;query processing;trees (mathematics)","PKIndex;PPI network;data analysis;data retrieval;deterministic graph;documents;efficient keyword search;false subtree filtering;filtering-and-verification strategy;influential detection;keyword matching;keyword query;path-based probabilistic pruning;path-based top-k probability;probabilistic keyword index;relational database;road network modeling;sampling algorithm;search efficiency;search mechanism;social network;texts;top-k keyword search;tree-based probabilistic pruning;uncertain graph data;uncertain graph subtree","","5","","38","","","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"DNS Configuration in IPv6: Approaches, Analysis, and Deployment Scenarios","S. Park; J. Jeong; C. S. Hong","Samsung Electronics","IEEE Internet Computing","20130627","2013","17","4","48","56","IPv6 provides abundant address space and automatic network-parameter configuration. The IETF has proposed three approaches for DNS configuration in IPv6 hosts for recursive DNS server addresses and the DNS search list. The authors analyze these approaches and describe four deployment scenarios in IPv6 wired and wireless networks. They provide guidelines for DNS configuration that IPv6 network administrators and users can apply in their target networks.","1089-7801;10897801","","10.1109/MIC.2012.96","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6243127","DHCPv6;DNS search list;DNS server;IPv6;anycast;router advertisement","Information retrieval;Internet;Logic gates;Network architecture;Routing protocols;Search methods;Servers","IP networks;Internet;network servers","DNS configuration;DNS search list;IETF;IPv6 wired network;IPv6 wireless network;address space;automatic network-parameter configuration;deployment scenario;recursive DNS server address","","1","2","14","","20120718","July-Aug. 2013","","IEEE","IEEE Journals & Magazines"
"Improved semantic retrieval of spoken content by language models enhanced with acoustic similarity graph","H. y. Lee; T. H. Wen; L. S. Lee","Graduate Institute of Communication Engineering, National Taiwan University","2012 IEEE Spoken Language Technology Workshop (SLT)","20130131","2012","","","182","187","Retrieving objects semantically related to the query has been widely studied in text information retrieval. However, when applying the text-based techniques on spoken content, the inevitable recognition errors may seriously degrade the performance. In this paper, we propose to enhance the expected term frequencies estimated from spoken content by acoustic similarity graphs. For each word in the lexicon, a graph is constructed describing acoustic similarity among spoken segments in the archive. Score propagation over the graph helps in estimating the expected term frequencies. The enhanced expected term frequencies can be used in the language modeling retrieval approach, as well as semantic retrieval techniques such as the document expansion based on latent semantic analysis, and query expansion considering both words and latent topic information. Preliminary experiments performed on Mandarin broadcast news indicated that improved performance were achievable under different conditions.","","Electronic:978-1-4673-5126-3; POD:978-1-4673-5125-6; USB:978-1-4673-5124-9","10.1109/SLT.2012.6424219","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424219","Document Expansion;Latent Semantic Analysis;Query Expansion;Random Walk","Acoustics;Computational modeling;Information retrieval;Interpolation;Lattices;Manuals;Semantics","graph theory;information retrieval;natural language processing;text analysis","Mandarin broadcast news;acoustic similarity graph;enhanced expected term frequencies;expected term frequencies;improved semantic retrieval;inevitable recognition errors;language modeling retrieval approach;language models;latent semantic analysis;lexicon;query expansion;score propagation;semantic retrieval techniques;spoken content;text information retrieval;text-based techniques","","5","","15","","","2-5 Dec. 2012","","IEEE","IEEE Conference Publications"
"Duplication Detection for Software Bug Reports Based on BM25 Term Weighting","C. Z. Yang; H. H. Du; S. S. Wu; I. X. Chen","Dept. of Comput. Sci. &amp; Eng., Yuan Ze Univ., Chungli, Taiwan","2012 Conference on Technologies and Applications of Artificial Intelligence","20130110","2012","","","33","38","Handling bug reports is an important issue in software maintenance. Recently, detection on duplicate bug reports has received much attention. There are two main reasons. First, duplicate bug reports may waste human resource to process these redundant reports. Second, duplicate bug reports may provide abundant information for further software maintenance. In the past studies, many schemes have been proposed using the information retrieval and natural language processing techniques. In this thesis, we propose a novel detection scheme based on a BM25 term weighting scheme. We have conducted empirical experiments on three open source projects, Apache, ArgoUML, and SVN. The experimental results show that the BM25-based scheme can effectively improve the detection performance in nearly all cases.","2376-6816;23766816","Electronic:978-0-7695-4919-4; POD:978-1-4673-4976-5","10.1109/TAAI.2012.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395002","BM25;bug reports;duplication detection;term weighting","Atmospheric measurements;Information retrieval;Natural language processing;Particle measurements;Software;Unified modeling language;Vectors","Unified Modeling Language;program debugging;program testing;public domain software;software maintenance;software quality","Apache;ArgoUML;BM25 term weighting scheme;SVN;detection performance;duplicate bug report detection;duplication detection;human resource wastage;information retrieval;natural language processing techniques;open source projects;software bug report handling;software maintenance","","5","","16","","","16-18 Nov. 2012","","IEEE","IEEE Conference Publications"
"Field Extraction from Administrative Documents by Incremental Structural Templates","M. Rusiñol; T. Benkhelfallah; V. P. dAndecy","ITESOFT, Aimargues, France","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","1100","1104","In this paper we present an incremental framework aimed at extracting field information from administrative document images in the context of a Digital Mail-room scenario. Given a single training sample in which the user has marked which fields have to be extracted from a particular document class, a document model representing structural relationships among words is built. This model is incrementally refined as the system processes more and more documents from the same class. A reformulation of the tf-idf statistic scheme allows to adjust the importance weights of the structural relationships among words. We report in the experimental section our results obtained with a large dataset of real invoices.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628784","Field extraction;administrative document images","Accuracy;Context;Information retrieval;Layout;Optical character recognition software;Text analysis","document image processing;feature extraction;information retrieval;statistics","administrative document images;digital mail-room scenario;document model;field information extraction;importance weights;incremental structural templates;tf-idf statistic scheme;words structural relationships","","5","","12","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"A rule-based, domain independent approach for opinion and holder identification","I. M. Sima; M. Vunvulea","Department of Computer Science Technical University of Cluj-Napoca Cluj-Napoca, Cluj","2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP)","20131024","2013","","","55","62","Mining sentiments from text is currently an important problem in information retrieval systems. In this paper we propose a solution for extracting opinions and opinion holders from large texts. Our goal is to achieve a high level of domain independence by implementing a rule-based approach. The results of our system have proven an accuracy which is comparable to that of systems that use a supervised learning approach, which is domain dependent.","","Electronic:978-1-4799-1494-4; POD:978-1-4799-1492-0","10.1109/ICCP.2013.6646081","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646081","domain independence;opinion;opinion holder;opinion target;sentiment polarity","Computer science;Context;Educational institutions;Information retrieval;Semantics;Speech;Supervised learning","data mining;information retrieval;learning (artificial intelligence);text analysis","holder identification;information retrieval systems;large texts;opinion identification;rule-based domain independent approach;supervised learning approach;text mining","","0","","18","","","5-7 Sept. 2013","","IEEE","IEEE Conference Publications"
"Facilitating Effective User Navigation through Website Structure Improvement","M. Chen; Y. U. Ryu","George Mason University, Fairfax, VA. 22030","IEEE Transactions on Knowledge and Data Engineering","20130124","2013","25","3","571","588","Designing well-structured websites to facilitate effective user navigation has long been a challenge. A primary reason is that the web developers' understanding of how a website should be structured can be considerably different from that of the users. While various methods have been proposed to relink webpages to improve navigability using user navigation data, the completely reorganized new structure can be highly unpredictable, and the cost of disorienting users after the changes remains unanalyzed. This paper addresses how to improve a website without introducing substantial changes. Specifically, we propose a mathematical programming model to improve the user navigation on a website while minimizing alterations to its current structure. Results from extensive tests conducted on a publicly available real data set indicate that our model not only significantly improves the user navigation with very few changes, but also can be effectively solved. We have also tested the model on large synthetic data sets to demonstrate that it scales up very well. In addition, we define two evaluation metrics and use them to assess the performance of the improved website using the real data set. Evaluation results confirm that the user navigation on the improved structure is indeed greatly enhanced. More interestingly, we find that heavily disoriented users are more likely to benefit from the improved structure than the less disoriented users.","1041-4347;10414347","","10.1109/TKDE.2011.238","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6086540","Website design;mathematical programming;user navigation;web mining","Data mining;Data models;Information retrieval;Mathematical model;Mathematical programming;Navigation;Search methods;Web pages","Internet;Web sites;data handling;mathematical programming","Web developers;Website structure improvement;facilitating effective user navigation;mathematical programming model;relink webpages;synthetic data sets","","5","","65","","20111122","March 2013","","IEEE","IEEE Journals & Magazines"
"Path Expression-based Smoothing of Query Likelihood Model for XML Element Retrieval","A. Keyaki; J. Miyazaki; K. Hatano; G. Yamamoto; T. Taketomi; H. Kato","Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol., Nara, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","296","300","In this paper we propose a path expression-based smoothing method of query likelihood model for XML element retrieval (QLMER). Though the query likelihood model, one of the statistical language models, is regarded as an accurate term weighting scheme in document retrieval, it has not been surveyed enough in XML element retrieval. Some term Weighting schemes for XML element retrieval utilize the idea of a path expression and it is effective for accurate retrieval. Therefore, we propose the path expression-based smoothing method. We are also interested in the potential power of QLMER compared with a commonly used term weighting scheme, BM25E, which is a classic probabilistic model. Our experimental evaluations showed that the proposed smoothing method is more effective than the existing one. In addition, BM25E is more effective than QLMER even though the effectiveness improved with the proposed method.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630363","XML element retrieval;query likelihood model;smoothing technique","Accuracy;Educational institutions;Information retrieval;Interpolation;Probabilistic logic;Smoothing methods;XML","XML;probability;query processing;smoothing methods","BM25E;QLMER;XML element retrieval;classic probabilistic model;document retrieval;path expression-based smoothing method;query likelihood model;statistical language models;term weighting scheme","","0","","16","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Language modeling for spoken dialogue system based on sentence transformation and filtering using predicate-argument structures","K. Yoshino; S. Mori; T. Kawahara","Sch. of Inf., Kyoto Univ., Kyoto, Japan","Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference","20130117","2012","","","1","4","We present a novel scheme of language modeling for a spoken dialogue system by effectively exploiting the back-end documents the system uses for information navigation. The proposed method first converts sentences in the document, which are written and plain style, into spoken question-style queries, which are expected in spoken dialogue. In this process, we conduct dependency analysis to extract verbs and relevant phrases to generate natural sentences by applying transformation rules. Then, we select sentences which have useful information relevant to the target domain and thus are more likely to be queried. For this purpose, we define predicate-argument (P-A) templates based on a statistical measure in the target document. An experimental evaluation shows that the proposed method outperforms the conventional method in ASR performance, and the sentence selection based on the P-A templates is effective.","","Electronic:978-0-6157-0050-2; POD:978-1-4673-4863-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6411922","","Filtering;Information retrieval;Navigation;Organizations;Semantics;Sports equipment;Training","filtering theory;natural languages;speech recognition","ASR performance;P-A templates;filtering;information navigation;language modeling;natural sentences;plain style;predicate-argument structures;predicate-argument templates;sentence transformation;spoken dialogue system;statistical measure","","1","","13","","","3-6 Dec. 2012","","IEEE","IEEE Conference Publications"
"SUSIE: Search using services and information extraction","N. Preda; F. Suchanek; W. Yuan; G. Weikum","University of Versailles, 45 Avenue des &#x00C9;tats Unis, 78035 Versailles, France","2013 IEEE 29th International Conference on Data Engineering (ICDE)","20130624","2013","","","218","229","The API of a Web service restricts the types of queries that the service can answer. For example, a Web service might provide a method that returns the songs of a given singer, but it might not provide a method that returns the singers of a given song. If the user asks for the singer of some specific song, then the Web service cannot be called - even though the underlying database might have the desired piece of information. This asymmetry is particularly problematic if the service is used in a Web service orchestration system. In this paper, we propose to use on-the-fly information extraction to collect values that can be used as parameter bindings for the Web service. We show how this idea can be integrated into a Web service orchestration system. Our approach is fully implemented in a prototype called SUSIE. We present experiments with real-life data and services to demonstrate the practical viability and good performance of our approach.","1063-6382;10636382","Electronic:978-1-4673-4910-9; POD:978-1-4673-4909-3","10.1109/ICDE.2013.6544827","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544827","","Context;Databases;Educational institutions;Information retrieval;Knowledge based systems;Standards;Web services","Web services;application program interfaces;information retrieval","API;SUSIE;Web service orchestration system;on-the-fly information extraction;parameter bindings;real life services;real-life data;search using services","","0","","39","","","8-12 April 2013","","IEEE","IEEE Conference Publications"
"A syntactic rule-based method for automatic pathway information extraction from biomédical literature","Y. Wang; Z. Yang; H. Lin; Y. Li","College of Computer Science and Technology, Dalian University of Technology, Dalian, Liaoning 116024, China","2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops","20130225","2012","","","626","633","Automatically extracting protein pathway information from the vast amount of published literature helps us understand diseases and discover new drug targets. In this paper, we propose a syntactic rule-based method for extracting pathway relation information from Medline abstracts. This method takes into account comprehensive dependency types and negative connectives when extracting relations from syntactic analysis results. The benchmarking results demonstrate the effectiveness of our method.","","Electronic:978-1-4673-2747-3; POD:978-1-4673-2746-6","10.1109/BIBMW.2012.6470211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6470211","information extraction;pathway information extraction;syntactic analysis;text minging","Data mining;Information retrieval;Protein engineering;Proteins;Syntactics;Visualization","biology computing;information retrieval;knowledge based systems;proteins","Medline abstracts;automatic pathway information extraction;biomédical literature;comprehensive dependency types;diseases;drug target discovery;negative connectives;protein pathway information extraction;syntactic rule-based method","","0","","19","","","4-7 Oct. 2012","","IEEE","IEEE Conference Publications"
"User-Centric Organization of Search Results","B. J. Gao; D. Buttler; D. C. Anastasiu; S. Wang; P. Zhang; J. Jan","Texas State University","IEEE Internet Computing","20130429","2013","17","3","52","59","The authors investigated the use of microblogs - or weibos - and related censorship practices using 111 million microblogs collected between 1 January and 30 June 2012. Using a matched case-control study design helped researchers determine a list of Chinese terms that discriminate censored and uncensored posts written by the same microbloggers. This list includes homophones and puns created by Chinese microbloggers to circumvent the censors successfully. The study's design also allowed for evaluating the new real-name registration system's impact on microbloggers' posting activities. Findings suggest that this policy might have stopped some microbloggers from writing about social and political subjects.","1089-7801;10897801","","10.1109/MIC.2013.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509867","Mass collaboration;Personalization;Search result organization;User interface","Blogs;Collaboration;Government policies;Information filtering;Information retrieval;Interfaces;Law;Search methods;Web search","information retrieval;natural language processing;organisational aspects;politics;social networking (online)","Chinese microbloggers;Weibos;censored posts;homophone;matched case-control study design;microblogger posting activities;political subjects;puns;real-name registration system;social subjects;uncensored posts;user-centric organization","","0","","5","","","May-June 2013","","IEEE","IEEE Journals & Magazines"
"Intellix -- End-User Trained Information Extraction for Document Archiving","D. Schuster; K. Muthmann; D. Esser; A. Schill; M. Berger; C. Weidling; K. Aliyev; A. Hofmeier","Comput. Networks Group, Tech. Univ. Dresden, Dresden, Germany","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","101","105","Automatic information extraction from scanned business documents is especially valuable in the application domain of document archiving. But current systems for automated document processing still require a lot of configuration work that can only be done by experienced users or administrators. We present an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise. Our evaluation on a large corpus of business documents shows competitive results of above 85% F1-measure on 10 commonly used fields like document type, sender, receiver and date. The system is deployed and used inside the commercial document management system DocuWare.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.28","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628593","","Business;Data mining;Feature extraction;Information retrieval;Layout;Optical character recognition software;Training","business data processing;document handling;information retrieval","F1-measure;Intellix;automated document processing;commercial document management system DocuWare;document archiving;end-user trained information extraction;scanned business documents","","1","","21","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Language-independent information extraction based on formal concept analysis","M. Mirończuk; D. Czerski; M. Sydow; M. A. Kłopotek","Inst. of Comput. Sci., Warsaw, Poland","2013 Second International Conference on Informatics & Applications (ICIA)","20131031","2013","","","323","329","This paper proposes application of Formal Concept Analysis (FCA) in creating character-level information extraction patterns and presents BigGrams: a prototype of a language-independent information extraction system. The main goal of the system is to recognise and to extract of named entities belonging to some semantic classes (e.g. cars, actors, pop-stars, etc.) from semi structured text (web page documents).","","Electronic:978-1-4673-5256-7; POD:978-1-4673-5254-3","10.1109/ICoIA.2013.6650277","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650277","","Context;Data mining;Formal concept analysis;HTML;Information retrieval;Lattices;Seals","formal concept analysis;information retrieval;text analysis","BigGrams;FCA;Web page documents;character-level information extraction patterns;formal concept analysis;language-independent information extraction;named entity extraction;named entity recognition;semistructured text","","0","","19","","","23-25 Sept. 2013","","IEEE","IEEE Conference Publications"
