"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7526907,7515887,7518277,7518446,7514617,7508039,7478822,7501924,7499024,7499010,7496546,7495994,7495539,7498452,7495744,7498366,7498464,7498359,7493985,7490711,7489333,7489041,7486594,7488040,7486208,7485873,7429775,7476101,7474209,7474215,7473374,7394191,7469048,7467726,7455912,7452494,7451550,7446143,7437732,7437741,7435754,7436204,7436979,7434431,7433261,7427225,7429171,7429163,7429392,7428328,7425948,7425971,7425970,7425978,7422450,7422282,7422279,7422995,7424757,7412596,7416573,7415184,7418770,7415390,7406843,7406286,7403660,7403619,7395815,7396278,7360926,7370024,7392490,7387613,7379893,7380681,7379593,7311861,7382156,7383052,7379178,7374929,7378086,7372140,7372153,7371622,7373340,7373883,7372130,7371656,7368462,7368804,7364127,7363924,7359739,7349750,7344253,7344832,7342223,7337990",2017/05/04 20:47:00
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Entropy-Based Term Weighting Schemes for Text Categorization in VSM","T. Wang; Y. Cai; H. f. Leung; Z. Cai; H. Min","Sch. of Software Eng., South China Univ. of Technol., Guangzhou, China","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","20160107","2015","","","325","332","Term weighting schemes have been widely used in information retrieval and text categorization models. In this paper, we first investigate into the limitations of several state-of-the-art term weighting schemes in the context of text categorization tasks. Considering that category-specific terms are more useful to discriminate different categories, and these terms tend to have smaller entropy with respect to these categories, we then explore the relationship between a term's discriminating power and its entropy with respect to a set of categories. To this end, we propose two entropy-based term weighting schemes (i.e., tf.dc and tf.bdc) which measure the discriminating power of a term based on its global distributional concentration in the categories of a corpus. To demonstrate the effectiveness of the proposed term weighting schemes, we compare them with seven state-of-the-art schemes on a long-text corpus and a short-text corpus respectively. Our experimental results show that the proposed schemes outperform the state-of-the-art schemes in text categorization tasks with KNN and SVM.","1082-3409;10823409","Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0","10.1109/ICTAI.2015.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372153","Entropy;Term Weighting;Text Categorization","Entropy;Information retrieval;Power measurement;Radio frequency;Text categorization;Training;Weight measurement","information retrieval;support vector machines;text analysis","KNN;SVM;VSM;category-specific term;entropy-based term weighting scheme;global distributional concentration;information retrieval;long-text corpus;short-text corpus;text categorization model;text categorization task","","","","","","","9-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Climbing Plan Sharing System with a Document Converter for Machine-Readable Climbing Plans","A. Nohara; S. Shiramatsu; T. Ozono; T. Shintani","Dept. of Comput. Sci. & Eng., Nagoya Inst. of Technol., Nagoya, Japan","2015 IIAI 4th International Congress on Advanced Applied Informatics","20160107","2015","","","97","102","In this paper, we developed a climbing plan sharing system. While climbing plan documents created in Microsoft Word or PDF formats contain necessary climbing information, a computer cannot understand such information because the information is written in a natural language. Such climbing plan documents are tedious to write, but are typically required before setting out to climb. Therefore we developed a system that converts climbing plan documents into a machine-readable format, enabling much reuse and sharing of information. We implemented functionality for sharing climbing plans among many users by displaying such plans on a Web page. Our system can therefore help climbers share machine-readable climbing plan documents, progressing the accumulation of climbing information that is difficult to effectively write and share in a natural language.","","Electronic:978-1-4799-9958-3; POD:978-1-4799-9959-0","10.1109/IIAI-AAI.2015.236","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373883","","Computers;Data mining;Databases;Information retrieval;Law enforcement;Natural languages;Safety","Internet;client-server systems;document handling;graphical user interfaces","Microsoft Word;PDF formats;Web page;climbing information accumulation;climbing plan sharing system;document converter;information reuse;information sharing;machine-readable climbing plan documents;machine-readable format;natural language","","","","7","","","12-16 July 2015","","IEEE","IEEE Conference Publications"
"Method to evaluate similarity of music by music features","S. Tamura; S. i. Ito; M. Ito; M. Fukumi","Tokushima University","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","20160128","2015","","","002574","002577","This paper proposes a method to evaluate similarity of music by music features. A music feature extraction method consists of three phases; chord progression pattern detection, rhythm pattern detection and musical instrument information extraction. The music feature extraction is carried out by using frequency analysis. In the chord progression pattern, we employ three evaluation criteria. In the rhythm pattern detection, we evaluate beat per minute(BPM) values. In the musical instrument information, we confirm results of musical instrument information extraction visually. In order to show the effectiveness of the proposed method, we conduct computer simulations of music features extraction.","","Electronic:978-1-4799-1762-4; POD:978-1-4799-1763-1","10.1109/IECON.2015.7392490","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392490","","Continuous wavelet transforms;Data mining;Feature extraction;Information retrieval;Instruments;Rhythm","feature extraction;music;musical instruments","BPM values;beat-per minute values;chord progression pattern detection;computer simulations;evaluation criteria;frequency analysis;music feature extraction method;music similarity evaluation;musical instrument information extraction;rhythm pattern detection","","","","4","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"OntoSeg: A Novel Approach to Text Segmentation Using Ontological Similarity","M. Bayomi; K. Levacher; M. R. Ghorab; S. Lawless","Sch. of Comput. Sci. & Stat., Trinity Coll. Dublin, Dublin, Ireland","2015 IEEE International Conference on Data Mining Workshop (ICDMW)","20160204","2015","","","1274","1283","Text segmentation (TS) aims at dividing long text into coherent segments which reflect the subtopic structure of the text. It is beneficial to many natural language processing tasks, such as Information Retrieval (IR) and document summarisation. Current approaches to text segmentation are similar in that they all use word-frequency metrics to measure the similarity between two regions of text, so that a document is segmented based on the lexical cohesion between its words. Various NLP tasks are now moving towards the semantic web and ontologies, such as ontology-based IR systems, to capture the conceptualizations associated with user needs and contents. Text segmentation based on lexical cohesion between words is hence not sufficient anymore for such tasks. This paper proposes OntoSeg, a novel approach to text segmentation based on the ontological similarity between text blocks. The proposed method uses ontological similarity to explore conceptual relations between text segments and a Hierarchical Agglomerative Clustering (HAC) algorithm to represent the text as a tree-like hierarchy that is conceptually structured. The rich structure of the created tree further allows the segmentation of text in a linear fashion at various levels of granularity. The proposed method was evaluated on a wellknown dataset, and the results show that using ontological similarity in text segmentation is very promising. Also we enhance the proposed method by combining ontological similarity with lexical similarity and the results show an enhancement of the segmentation quality.","","Electronic:978-1-4673-8493-3; POD:978-1-4673-8494-0","10.1109/ICDMW.2015.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395815","Lexical Cohesion;Ontological similarity;Text Segmentation;Vector Space Model","Clustering algorithms;Hidden Markov models;Information retrieval;Measurement;Natural language processing;Ontologies;Semantics","ontologies (artificial intelligence);pattern clustering;text analysis","HAC algorithm;NLP tasks;OntoSeg;document segmentation;document summarisation;hierarchical agglomerative clustering;information retrieval;lexical cohesion;lexical similarity;natural language processing tasks;ontological similarity;ontology-based IR systems;segmentation quality;semantic Web;similarity measure;text blocks;text representation;text segmentation;text subtopic structure;tree-like hierarchy;word-frequency metrics","","2","","30","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Issues about combination of temporal information and spatial information","Y. S. Jeong; B. Kim; H. J. Choi; J. S. Lee","Department of Computer Science, KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon 305-701, South Korea","2016 International Conference on Big Data and Smart Computing (BigComp)","20160307","2016","","","462","465","Documents usually contain temporal expressions such as `morning' and `yesterday', and they also often contain spatial expressions such as `house' and `the South Pole'. Extracting temporal information or spatial information from the documents is important because such information can be useful for various applications. Although there have been many studies aiming at extracting temporal information or spatial information, only few studies attempted to combine the two information. In this paper, we report several issues about the combination of temporal information and spatial information.","","Electronic:978-1-4673-8796-5; POD:978-1-4673-8797-2; USB:978-1-4673-8795-8","10.1109/BIGCOMP.2016.7425971","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425971","Combination of temporal information and spatial information;Spatial information;Temporal information","Computers;Data mining;Electronic mail;Information retrieval;Markup languages;Motion measurement","document handling;information retrieval","spatial information extraction;temporal information extraction","","","","14","","","18-20 Jan. 2016","","IEEE","IEEE Conference Publications"
"Approach for mapping data to client projects","G. Beriwal; A. Y. Bohn; A. Josey; J. Kohn; C. Sherrick; D. Brown; R. Bailey","Department of Systems and Information Engineering, University of Virginia","2016 IEEE Systems and Information Engineering Design Symposium (SIEDS)","20160613","2016","","","89","94","In today's technological era, the volume of data being processed on a daily basis is growing exponentially. The influence of data is affecting consulting firms and the expectations client companies have of consultants. Consulting firms are expected to not only use their client's data but also to leverage open source data and data from other firms that can help address the clients' needs. Manually reviewing the relevance of a dataset to a given problem is time intensive and prone to error. Additionally, it is near impossible to detect non-intuitive correlations in the data through manual review. Hence, this project focuses on building an application that enables users to automatically find and assess the applicability of datasets to a given problem. Given that the project objectives are to increase the applicability of individual datasets, to increase the applicability of clusters of correlated datasets, and to increase the usability of individual datasets, three metrics were derived respectively. They are relevance, coverage, and quality of data. The application, known as UVa Open Miner, measures the relevance of the dataset to the client problem, the degree to which a group of datasets covers different dimensions of a client problem, and the quality of the dataset. The application can be refactored into a reusable solution that consulting firms can use for their client work.","","Electronic:978-1-5090-0970-1; POD:978-1-5090-0971-8","10.1109/SIEDS.2016.7489333","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489333","Data Coverage;Data Quality;Data Relevance;Datasets;Metadata;Ontology Mapping","Business;Conferences;Information retrieval;Measurement;Merging;Metadata;Ontologies","data mining;meta data;pattern clustering;public domain software","UVa Open Miner;client companies;client needs;client problem;client projects;client work;consulting firms;data coverage;data mapping;data quality;data relevance;nonintuitive correlation detection;open source data","","","","11","","","29-29 April 2016","","IEEE","IEEE Conference Publications"
"Indonesian medical sentence transformation for question generation","W. Suwarningsih; I. Supriana; A. Purwarianti","School of Electronic Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2015 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob)","20160111","2015","","","68","71","We present a novel scheme of sentence transformation for Indonesian medical question generation (ImeQG) system by utilizing effectively documents for information navigation. Through the ImeQG proposed method, we conducted a general procedure of dependency analysis for extract verbs and relevant phrases to generate natural sentences by applying transformation rules. For this purpose, we defined some P-A templates based on a statistical measure. An experimental evaluation in this proposed method showed 79.00% for precision, 87.80% for recall and 81.50% for F1.","","CD-ROM:978-1-4799-8289-9; Electronic:978-1-4799-8290-5; POD:978-1-4799-8291-2","10.1109/APWiMob.2015.7374929","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374929","Indonesian medical sentence;QA-pairs;predicate argument structure;sentence transformation","Conferences;Feature extraction;Filtering;Information retrieval;Knowledge discovery;Pattern matching;Semantics","medical administrative data processing;medical computing;natural language processing;text analysis;word processing","Indonesian medical sentence transformation;P-A templates;information navigation;medical question generation system;natural sentence generation;statistical measure;verb extraction","","","","28","","","27-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"From text to XML by structural information extraction","Y. Piao; T. Wang; H. Jiang","School of Software, Dalian University of Technology, Dalian 116620, China","2015 IEEE International Conference on Computer and Communications (ICCC)","20160121","2015","","","448","452","Facing tremendous volume of semi-structured XML and non-structured free text, network information retrieval is one of the most research hotspots in dealing with these data more efficiently, precisely and uniformly. Many traditional IR methods ignore text semantics and their labeling result has usually only one level, lacking of context expression as well, therefore structure extraction from free text and its conversion to XML format are studied, with a CRF based algorithm SIECRF provided. Experiment results are analyzed, showing its efficiency to extracting text structure and has a good application future.","","CD-ROM:978-1-4673-8124-6; Electronic:978-1-4673-8126-0; POD:978-1-4673-8127-7","10.1109/CompComm.2015.7387613","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387613","XML expression;conditional random fields;information extraction;text structural information","Data mining;Entropy;Hidden Markov models;Information retrieval;Labeling;Semantics;XML","XML;information retrieval;text analysis","SIECRF;XML format conversion;conditional random field model;free text structure extraction;structure information extraction model based on CRF","","","","22","","","10-11 Oct. 2015","","IEEE","IEEE Conference Publications"
"Challenges in finding a connection between IL education and research problem formulation","T. Marja; V. Hanna; E. Harri; R. Heli","Lappeenranta Academic Library, Lappeenranta University of Technology, Lappeenranta, Finland","2015 IEEE Frontiers in Education Conference (FIE)","20151207","2015","","","1","5","The current stage of study covers past 5 years of IL education research focusing on undergraduate engineering students. The first observations have risen a question: Could effective IL education improve students' skills to formulate their research problems? This paper is part of a study which aims to find ways to improve students' research skills based on the development in their information retrieval mindset. Tentative analyses have been made of 111 questionnaires and 36 seminar papers. At the current stage, observations deal with two different types of student groups. One group consists mostly of freshmen and the other group is close to their graduation.","","Paper:978-1-4799-8454-1","10.1109/FIE.2015.7344253","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344253","information literacy;research skills;undergraduate students","Bibliographies;Information retrieval;Libraries;Seminars;Standards;Table lookup","computer science education;further education","IL education and research problem formulation;information literacy;undergraduate engineering students","","","","17","","","21-24 Oct. 2015","","IEEE","IEEE Conference Publications"
"Issues in spatial information annotation in Korean texts","Bogyum Kim; Myung Yun Kang; Jae Sung Lee","Dept. of Digital Informatics and Convergence, Chungbuk National University, Cheongju, South Korea","2016 International Conference on Big Data and Smart Computing (BigComp)","20160307","2016","","","458","461","Extraction of spatial information from texts is useful in many applications, such as in navigation systems, robot systems, and question answering systems. As many information extraction systems use annotated corpus for training the systems, annotation methods affect the system's performance. In this paper, we apply the ISO-Space annotation scheme to the Korean texts and present the practical concerns for convenient annotation and effective machine learning.","","Electronic:978-1-4673-8796-5; POD:978-1-4673-8797-2; USB:978-1-4673-8795-8","10.1109/BIGCOMP.2016.7425970","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425970","ISO-Space Information Annotation;Information Extraction;Korean Spatial Information;Spatial Information Annotation","Data mining;Electronic mail;Information retrieval;Labeling;Learning systems;Pragmatics;Semantics","classification;information retrieval;text analysis","ISO-Space annotation;Korean texts;information extraction systems;spatial information annotation","","","","14","","","18-20 Jan. 2016","","IEEE","IEEE Conference Publications"
"A hybrid model for Arabic document indexing","S. Ben Guirat; I. Bounhas; Y. Slimani","LISI: Laboratory of computer science for industrial systems, Carthage University, Tunis, Tunisia","2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","20160721","2016","","","109","114","Using either stems or roots as index terms offered considerable performance to Arabic Information Retrieval (IR) systems compared to the use of surface words for indexing. Many comparative works tried to find out the best from these two indexing approaches but until then, no of the two methods widely overtook the other. Each of the two index types performed better under different test circumstances in terms of recall and precision. In this paper, we propose a hybrid approach combining the two indexing units in a way we take the advantages from both of them and try to overcome their shortcomings. Then, based on some combining techniques, we assign a weight for each indexing unit and try to find out the best weighting values.","","Electronic:978-1-5090-2239-7; POD:978-1-5090-0804-9","10.1109/SNPD.2016.7515887","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515887","Arabic IR;combining;indexing;stemming","Computer architecture;Indexing;Information retrieval;Semantics;Standards;Surface treatment","document handling;indexing;information retrieval;natural languages","Arabic document indexing;IR systems;information retrieval systems","","","","","","","May 30 2016-June 1 2016","","IEEE","IEEE Conference Publications"
"Phrase-based clause extraction for open information extraction system","A. Romadhony; D. H. Widyantoro; A. Purwarianti","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia","2015 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20160225","2015","","","155","162","Recent development of variety and volume of information circulating in the Internet has prompted the emergence of a new paradigm in information extraction, namely the Open Information Extraction (Open IE). An evaluation of several existing Open IE systems shows a good performance on precision. However, improvement is still needed to boost the recall. A relation between entity pair in simple sentence is detected easier by the Open IE system rather than in complex sentence. In this paper, we propose a clause extraction approach employing phrase feature and requiring no learning, focusing on the entity pair. The proposed approach needs less computational cost than the previous work that employing deep parse feature or requiring learning. The experimental result shows that by extracting simpler clause, the performance of Open IE system increases. The average of best F-measure achieved in the evaluation on three benchmark datasets is 0.62, outperforms the previous work.","","Electronic:978-1-5090-0363-1; POD:978-1-5090-0364-8; USB:978-1-5090-0362-4","10.1109/ICACSIS.2015.7415184","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415184","Open IE;clause extraction;entity-focused;text simplification","Data mining;Feature extraction;Grammar;Information retrieval;Internet;Semantics;Syntactics","Internet;text analysis","Internet;Open IE systems;deep parse feature;open information extraction system;phrase feature;phrase-based clause extraction;text simplication","","","","23","","","10-11 Oct. 2015","","IEEE","IEEE Conference Publications"
"Listwise Learning to Rank by Exploring Structure of Objects","O. Wu; Q. You; X. Mao; F. Xia; F. Yuan; W. Hu","Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Knowledge and Data Engineering","20160602","2016","28","7","1934","1939","Listwise learning to rank (LTR) is aimed at constructing a ranking model from listwise training data to order objects. In most existing studies, each training instance consists of a set of objects described by preference features. In a preference feature space for the objects in training, the structure of the objects is associated with the absolute preference degrees for the objects. The degrees significantly influence the ordering of the objects. Nevertheless, the structure of the training objects in their preference feature space has rarely been studied. In addition, most listwise LTR algorithms yield a single linear ranking model for all objects, but this ranking model may be insufficient to capture the underlying nonlinear ranking mechanism among all objects. This study proposes a divide-and-train method to learn a nonlinear ranking model from listwise training data. First, a rank-preserving clustering approach is used to infer the structure of objects in their preference feature space and all the objects in training data are divided into several clusters. Each cluster is assumed to correspond to a preference degree and an ordinal regression function is then learned. Second, considering that relations exist among the clusters, a multi-task listwise ranking approach is then employed to train linear ranking functions for all the clusters (or preference degrees) simultaneously. Our proposed method utilizes both the (relative) preferences among objects and the intrinsic structure of objects. Experimental results on benchmark data sets suggest that the proposed method outperforms state-oft-the-art listwise LTR algorithms.","1041-4347;10414347","","10.1109/TKDE.2016.2535214","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429775","Listwise learning to rank;clustering;multi-task learning;structure","Clustering algorithms;Indexes;Information retrieval;Measurement;Support vector machines;Training;Training data","learning (artificial intelligence)","divide-and-train method;listwise learning to rank;listwise training data;multitask listwise ranking approach;nonlinear ranking mechanism;ordinal regression function;preference degree;preference feature space;single linear ranking model","","","","26","","20160309","July 1 2016","","IEEE","IEEE Journals & Magazines"
"Towards automatic document classification by exploiting only knowledge resources","G. A. Cardoso da Silva; C. F. Dorneles","Federal University of Santa Catarina, Florianopolis, Brasil","2015 34th International Conference of the Chilean Computer Science Society (SCCC)","20160225","2015","","","1","6","Document classification is critical to optimize information retrieval tasks, especially over the web. In this environment, the open domain nature and growing volume of available data remain a challenge for the classification task. In this paper, we deal with these problems by only using knowledge resources. Our approach relies on concepts instances derived from the document and an open domain knowledge base for concept generalization. The set of broader concepts is ranked according to a disparity value, and then the best-placed concept is considered as the document class label. Experimental results on real-world datasets show that this approach can achieve document classification without the need to build an ontology or train and keep a classification model.","","Electronic:978-1-4673-9817-6; POD:978-1-4673-9818-3","10.1109/SCCC.2015.7416573","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416573","concept generalization;document classification;knowledge base","Informatics;Information retrieval;Knowledge based systems;Ontologies;Proposals;Semantics;Training","Internet;document handling;information retrieval;knowledge based systems;optimisation;pattern classification","automatic document classification;best-placed concept;concept generalization;disparity value;document class label;information retrieval task optimization;knowledge resources;open domain knowledge base","","","","22","","","9-13 Nov. 2015","","IEEE","IEEE Conference Publications"
"Latent semantic analysis via truncated ULV decomposition","F. Varçın; H. Erbay; F. Horasan","Bilgisayar M&#252;hendisli&#287;i B&#246;l&#252;m&#252;, K&#305;r&#305;kkale &#220;niversitesi, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","1333","1336","Latent semantic analysis (LSA) usually uses the singular value decomposition (SVD) of the term-document matrix for discovering the latent relationships within the document collection. With the SVD, by disregarding the smaller singular values of the term-document matrix a vector space cleaned from noises that distort the meaning is obtained. The latent semantic structure of the terms and documents is obtained by examining the relationship of representative vectors in the vector space. However, the computational time of re-computing or updating the SVD of the term-document is high when adding new terms and/or documents to pre-existing document collection. Thus, the need a method not only has low computational complexity but also creates the correct semantic structure when updating the latent semantic structure is arisen. This study shows that the truncated ULV decomposition is a good alternative to the SVD in LSA modelling about cost and producing the correct semantic structure.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7495994","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495994","Latent semantic analysis;Singular value decomposition;Truncated ULV decomposition","Computational modeling;Discharges (electric);Entropy;Information retrieval;Matrix decomposition;Semantics","computational complexity;document handling;natural language processing;singular value decomposition","LSA modelling;SVD;computational complexity;document collection;latent semantic analysis;latent semantic structure;singular value decomposition;term-document matrix;truncated ULV decomposition;vector space","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Design of rule based lemmatizer for Kannada inflectional words","R. J. Prathibha; M. C. Padma","Department of Information Science and Engineering, Sri Jayachamarajendra College of Engineering, Mysuru, India","2015 International Conference on Emerging Research in Electronics, Computer Science and Technology (ICERECT)","20160627","2015","","","264","269","Lemmatizer and stemmer are the two basic modules in most of the natural language processing applications. Stemming is the process of stripping off the affixes that are present in the inflectional word to obtain stem. The extracted stem by the stemmer need not be a valid root or linguistically meaningful word. Lemmatizer removes the affixes that are present in the inflectional word by applying linguistic rules and returns the base-form or dictionary-form of the word, which is known as lemma. The split lemma is a valid root and linguistically meaningful word, hence the lemmatizer requires more linguistic knowledge than the stemmer. In linguistics, the objective of lemmatizer is to group together the different inflected forms of a word, such that these inflected words are analyzed as a common term. In this context, it is necessary to design a lemmatizer for Kannada inflectional words. In this paper we have proposed the design of rule based lemmatizer by adding set of linguistic rules to extract proper and meaningful root from Kannada inflectional word. The proposed module is tested on different types of data sets that are specifically created for this work and the accuracy obtained on these data is above 85%.","","Electronic:978-1-4673-9563-2; POD:978-1-4673-9564-9","10.1109/ERECT.2015.7499024","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7499024","Lemmatization;Natural language processing;inflectional words;linguistic rules;stemming","Algorithm design and analysis;Computer science;Computers;Information retrieval;Natural language processing;Pragmatics","knowledge based systems;natural language processing","Kannada inflectional words;linguistic rules;natural language processing;rule based lemmatizer","","","","17","","","17-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Error Analysis in an Automated Narrative Information Extraction Pipeline","J. Valls-Vargas; J. Zhu; S. Ontanon","Drexel University, Philadelphia, PA 19104 (e-mail: josep.vallsvargas@drexel.edu)","IEEE Transactions on Computational Intelligence and AI in Games","","2016","PP","99","1","1","In this article, we present our method for automatically extracting narrative information of characters and their narrative roles from natural language stories. In our corpus of 15 unannotated folk tales, our Voz system identifies 87% of the characters in the stories and correctly assigns 68% of the character roles. To better understand the sources of error in our system, we present an analytical methodology to study how the error is introduced by different modules and how it propagates through the pipeline. This methodology allows us to identify the bottleneck with the largest impact on the final error, which might be different from the module with the largest individual error in isolation. Our methodology can be applied to a wide variety of similar information extraction pipelines.","1943-068X;1943068X","","10.1109/TCIAIG.2016.2575823","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489041","Computational Narrative;Evaluation;Instance-Based Learning;Narrative Role Identification;Natural Language Processing","Computer architecture;Feature extraction;Games;Information retrieval;Natural language processing;Pipelines","","","","","","","","20160610","","","IEEE","IEEE Early Access Articles"
"Improving Strategic Scanning Information Analysis: An Alternative Measure for Information Proximity Evaluation","A. Casagrande; E. Loza Aguirre; L. Vuillon","CERAG, Univ. Grenoble Alpes, Grenoble, France","2015 International Conference on Enterprise Systems (ES)","20160215","2015","","","1","8","Strategic Scanning activities become less effective when faced with managing information overload. This paper introduces ""nearness measure"" (NM), a measure for information proximity evaluation. We also present and analyze the principles and properties of a graphical representation of the measure. We compare the measure to the usual Cosine similarity (CS). Even when the algorithm that calculates NM between texts is larger than CS in terms of complexity, NM with a minimization phase expresses an analysis of the documents from the general to the specific that is the opposite to what CS does. Thus, the manager using NM would require less time to explore collected information.","","Electronic:978-1-4673-8005-8; POD:978-1-4673-8006-5","10.1109/ES.2015.8","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406843","Decision support systems;Information overload;Nearness Measure;Strategic scanning;Texts comparison","Complexity theory;Decision making;Electronic mail;Information retrieval;Probabilistic logic;Semantics;Visualization","document handling;information analysis","CS;NM;cosine similarity;document analysis;information overload;information proximity evaluation;nearness measure;strategic scanning information analysis","","","","34","","","14-15 Oct. 2015","","IEEE","IEEE Conference Publications"
"Document Copy Detection Using the Improved Fuzzy Hashing","G. Wu; E. Fu; L. Wang; M. Zhao","Sch. of Comput. Sci. & Technol., Hang Zhou Dian Zi Univ., Hangzhou, China","2015 International Conference on Computer Science and Mechanical Automation (CSMA)","20160107","2015","","","55","60","Document copy detection is an effective method that can protect intellectual property rights as well as improve the efficiency of information retrieval. To our knowledge, it is a common method that using the fingerprints of one document in the process of detecting. Therefore, selecting the appropriate document fingerprints plays a key role. This paper firstly describes several mature methods of selecting document fingerprints, and analyzes their merit and demerit. Then we review the principle of Fuzzy Hashing, which suffers from the instability and inefficiency of fragmenting. To resolve the critical problems, we finally propose a novel algorithm based on the Fuzzy Hashing. Compared to original method, the proposed document copy detection algorithm can not only ensure the proper size of fragment but also enhance the speed of fragmenting. And in terms of efficiency and accuracy, the algorithm achieves high performance.","","Electronic:978-1-4673-9166-5; POD:978-1-4673-9167-2","10.1109/CSMA.2015.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371622","document copy detection;fingerprint;fuzzyhashing;word encoding","Algorithm design and analysis;Computer science;Encoding;Fingerprint recognition;Information retrieval;Intellectual property;Interference","copy protection;document handling;file organisation;fuzzy set theory;industrial property;information retrieval","document copy detection algorithm;document fingerprint;fuzzy hashing;information retrieval;intellectual property right","","","","11","","","23-25 Oct. 2015","","IEEE","IEEE Conference Publications"
"Text retrieval based on the feature conversion of vector space","M. Zhang; J. Zhu; L. Hua; F. Yuan","School of Computer, Central China Normal University, Wuhan, China","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","2933","2935","This paper presents a text retrieval method based on vector space feature conversion. Even though knowledge-based semantic fingerprint information and semantic information obtained by the Tag-LDA topic model are two different representations of semantic features of the same text, they are incompatible with semantic information. Here we introduce the vector space as a bridge to transfer knowledge-based semantic fingerprint information space into the Tag-LDA model space and prove the rationality of the conversion process with correlation theory. The compatible semantic fingerprint information into the Tag-LDA model generate a new topic model STag-LDA. The Stag-LDA model has certain disambiguation effect for the semantic information of the tag. So, it can mine text semantic information more accurately to improve the retrieval efficiency.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7364127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7364127","STag-LDA;Tag-LDA;retrieval efficiency;semantic fingerprint;vector space","Big data;Computers;Fingerprint recognition;Information retrieval;Matrix converters;Semantics;Transforms","data mining;information retrieval;text analysis","STag-LDA;Tag-LDA topic model;correlation theory;knowledge-based semantic fingerprint information space;retrieval efficiency;semantic feature representations;text retrieval;text semantic information mining;vector space feature conversion","","","","6","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Document clustering using sequential pattern (SP): Maximal frequent sequences (MFS) as SP representation","D. Rahmawati; G. A. P. Saptawati; Y. Widyani","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Bandung, Indonesia","2015 International Conference on Data and Software Engineering (ICoDSE)","20160321","2015","","","98","102","This research proposes an idea to apply Feature Based Clustering (FBC) in document clustering. A huge number of existing documents will be easier to be used if they are clustered into several topics. FBC uses K-Means algorithm to cluster sequential data of features. Features of text document can be presented as sequence of word. In order to be processed as sequential data, features must be extracted from collection of unstructured text documents. Therefore, we need preprocessing tasks to deliver appropriate form of document features. There are two types of sequential pattern using simple form: Frequent Word Sequence (FWS) and Maximal Frequent Sequence (MFS). Both types are appropriate for text data. The difference is in applying the maximum principle in MFS. Therefore, MFS amount from a text document would be less than the amount of its FWS. In this research, we choose maximal frequent sequences (MFS) as feature representation. We proposes framework to conduct FBC using MFS as features. The framework is tested to cluster dataset that is subset of the Twenty News Group Text Data. The result shows that the accuracy of clustering result is affected by the parameter's value, dataset, and the number of target cluster.","","CD-ROM:978-1-4673-8427-8; Electronic:978-1-4673-8430-8; POD:978-1-4673-8431-5","10.1109/ICODSE.2015.7436979","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436979","document clustering;feature-based clustering;maximal frequent sequences;sequential pattern","Clustering algorithms;Clustering methods;Feature extraction;Information retrieval;Software engineering;Text mining","data structures;feature extraction;maximum principle;pattern clustering;text analysis","FBC;FWS;K-means algorithm;MFS;SP representation;Twenty News group text data;cluster sequential data;document clustering;feature representation;feature-based clustering;frequent word sequence;maximal frequent sequence;unstructured text document feature extraction","","","","13","","","25-26 Nov. 2015","","IEEE","IEEE Conference Publications"
"An Approach to Named Entity Extraction from Mongolian Historical Documents","B. Batjargal; G. Khaltarkhuu; A. Maeda","Res. Organ. of Sci. & Technol, Ritsumeikan Univ., Kusatsu, Japan","2015 International Conference on Culture and Computing (Culture Computing)","20160314","2015","","","205","206","In this poster, we demonstrate a named entity extraction method for digitized ancient Mongolian documents by using the features of characters' appearance in a word. Named entities such as personal names and place names will be extracted by employing Support Vector Machine that aims to reduce the labor-intensive analysis on historical text. The preliminary results of our experiment show that the proposed method has gained 0.6993, 0.5679 and 0.6268 of precision, recall and F-measure, respectively.","","CD-ROM:978-1-4673-8231-1; Electronic:978-1-4673-8232-8; POD:978-1-4673-8233-5","10.1109/Culture.and.Computing.2015.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433261","digital humanities;historical documents;machine learning;support vector machine;traditional Mongolian script","Feature extraction;Information retrieval;Libraries;Natural language processing;Support vector machines;Text analysis","document handling;history;natural language processing;support vector machines","Mongolian historical documents;character appearance;digitized ancient Mongolian documents;labor intensive analysis;named entity extraction;personal names;support vector machine","","","","7","","","17-19 Oct. 2015","","IEEE","IEEE Conference Publications"
"A System for Extracting Sentiment from Large-Scale Arabic Social Data","H. Wang; A. Hanafy; M. Bahgat; S. Noeman; O. S. Emam; V. R. Bommireddipalli","Silicon Valley Lab., IBM, San Jose, CA, USA","2015 First International Conference on Arabic Computational Linguistics (ACLing)","20160303","2015","","","71","77","Social media data in Arabic language is becoming more and more abundant. It is a consensus that valuable information lies in social media data. Mining this data and making the process easier are gaining momentum in the industries. This paper describes an enterprise system we developed for extracting sentiment from large volumes of social data in Arabic dialects. First, we give an overview of the Big Data system for information extraction from multilingual social data from a variety of sources. Then, we focus on the Arabic sentiment analysis capability that was built on top of the system including normalizing written Arabic dialects, building sentiment lexicons, sentiment classification, and performance evaluation. Lastly, we demonstrate the value of enriching sentiment results with user profiles in understanding sentiments of a specific user group.","","Electronic:978-1-4673-9155-9; POD:978-1-4673-9156-6","10.1109/ACLing.2015.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422282","Arabic;Big Data;Sentiment Analysis;Social Data","Buildings;Data mining;Information retrieval;Media;Sentiment analysis;Standards","Big Data;data mining;information retrieval;natural language processing;pattern classification;social networking (online)","Arabic language;Arabic sentiment analysis capability;Big Data system;data mining;enterprise system;information extraction;large-scale Arabic social data;multilingual social media data;performance evaluation;sentiment classification;sentiment extraction;sentiment lexicons;written Arabic dialect normalization","","","","24","","","17-20 April 2015","","IEEE","IEEE Conference Publications"
"A Text Clustering Algorithm Based on Find of Density Peaks","P. Liu; Y. Liu; X. Hou; Q. Li; Z. Zhu","Shandong Yingcai Univ., Jinan, China","2015 7th International Conference on Information Technology in Medicine and Education (ITME)","20160310","2015","","","348","352","The text clustering is one of core problems in text mining and information retrieval field, clustering algorithm is divided into four categories: the partitioned clustering algorithm, the hierarchical clustering algorithm, density-based clustering algorithm, as well as intelligence clustering algorithm. However, most clustering algorithms cannot meet the demand of speed and self-adapting about text clustering. This paper proposed a text clustering algorithm based on find of density peaks. The algorithm was implemented by the calculation of text distance and density, which was in accordance with calculation of the text vector similarity. SVM was used to express text to obtain the vector mapping for the similarity calculation. The next work was the finding of the local density and the distance from points of higher density of each text, removing the noise points, selecting the cluster center. The remaining points were assigned into the cluster which its nearest cluster center represented. According to several sets of contrast experiment, the density-based text clustering has an advantage of reliability and robustness.","","CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5","10.1109/ITME.2015.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429163","Density;Feature term;Similarity;Text clustering;Vector distance","Algorithm design and analysis;Clustering algorithms;Clustering methods;Information retrieval;Partitioning algorithms;Robustness;Text mining","data mining;information retrieval;pattern clustering;support vector machines;text analysis;vectors","SVM;cluster center selection;density peaks;density-based clustering algorithm;hierarchical clustering algorithm;information retrieval;intelligence clustering algorithm;noise point removal;partitioned clustering algorithm;similarity calculation;text clustering algorithm;text mining;text vector similarity;vector mapping","","","","17","","","13-15 Nov. 2015","","IEEE","IEEE Conference Publications"
"Information extraction of regulatory enforcement actions: From anti-money laundering compliance to countering terrorism finance","V. Plachouras; J. L. Leidner","Thomson Reuters, Corporate Research and Development, 1 Mark Square, London, EC2A 4EG, United Kingdom","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20160211","2015","","","950","953","Financial fines imposed by regulatory bodies to penalize illegal activities and violations against regulations (cases of non-compliance) have recently become more common, and the sizes of fines have increased. This development coincides with the ongoing increase of complexity of regulatory rules. Huge fines have been imposed on banks for financial fraud and regulations have been made more stringent after 9/11 to curb funding of terrorist groups. Market players would also like to have available a database of fine events for a range of applications, such as to benchmark their competitors performance, or to use it as an early warning system for detecting shifts in regulators' enforcement behavior. To this end, we introduce the task of extracting fines from regulatory enforcement actions and we present a method to extract such fine event instances from timeline-like descriptions of regulatory investigation activities authored by legal professionals for a commercial product. We evaluate how well a rule-based method can extract information about fine events and we compare its performance to a machine-learning baseline. To the best of our knowledge, this work is the first one addressing this task.","","Electronic:978-1-4503-3854-7; POD:978-1-5090-2094-2","10.1145/2808797.2809368","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403660","","Companies;Data mining;Databases;Information retrieval;Law;Regulators","financial data processing;information retrieval;learning (artificial intelligence);stock markets","anti-money laundering compliance;based method;early warning system;financial fines;financial fraud;financial markets;information extraction;machine-learning baseline;regulatory enforcement actions;terrorism finance","","","","15","","","25-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"A novel model of selecting high quality pseudo-relevance feedback documents using classification approach for query expansion","J. Singh; A. Sharan","Jawaharlal Nehru University, New Delhi, India","2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)","20160623","2015","","","1","6","In this paper, we propose a new high quality pseudo-relevance feedback documents selection approach that uses machine learning based classifier for selecting a set of good feedback documents for boosting the effectiveness of Query Expansion (QE). Our proposed classification technique utilizes very small amount of labelled data set for training purpose that is very appropriate to select a set of good documents as feedback in our case. Support vector machine classifier is applied for implementing a classifier. Our experimental analysis confirmed that proposed approach improved the effectiveness of QE's on standard TREC-3 ad-hoc data collection.","","Electronic:978-1-4673-8215-1; POD:978-1-4673-8219-9","10.1109/WCI.2015.7495539","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495539","","Classification algorithms;Information retrieval;Mathematical model;Support vector machines;Testing;Training;Training data","document handling;learning (artificial intelligence);pattern classification;query processing;relevance feedback;support vector machines","high quality pseudorelevance feedback document selection approach;labelled data set;machine learning based classifier;query expansion;standard TREC-3 adhoc data collection;support vector machine classifier;training purpose","","","","25","","","14-17 Dec. 2015","","IEEE","IEEE Conference Publications"
"Painting Semantic Retrieval Algorithm Research Based on Ontology","L. Qiang","Yancheng Teachers Coll. in Jiangsu Province, Yancheng, China","2015 8th International Conference on Intelligent Computation Technology and Automation (ICICTA)","20160519","2015","","","621","624","Semantic retrieve as one of the development direction of the intelligent information retrieve technology occupies extremely important position in intelligent information retrieve field. Aiming at the disadvantage of the traditional retrieve system based on the key matching, a new model of the semantic retrieve based on the Ontology is proposed in this paper. In order to test the performance of the semantic retrieval model, this paper does experiments by comparing this model with traditional model. The result of experiment shows that the semantic model is better than traditional model. It has important reference value in the process of implementing semantic retrieve system in other fields.","","Electronic:978-1-4673-7644-0; POD:978-1-4673-7645-7","10.1109/ICICTA.2015.158","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7473374","Jena reasoning;ontology;painting semantic retrieval","Computational modeling;Information retrieval;OWL;Ontologies;Resource description framework;Semantics","information retrieval;knowledge based systems;ontologies (artificial intelligence)","intelligent information retrieve technology;ontology;painting semantic retrieval algorithm","","","","8","","","14-15 June 2015","","IEEE","IEEE Conference Publications"
"Graph-based retrieval model for semi-structured data","Juneyoung Park; M. Y. Yi","Department of Knowledge Service Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2016 International Conference on Big Data and Smart Computing (BigComp)","20160307","2016","","","361","364","The continuous need to process semi-structured data in the more connected and semantic web requires a retrieval model that can truly reflect the user's intention and capture a user's understanding. As a semantic network shows great potential in representing the inherent structure of information in a document, recent studies have attempted to apply semantic networks into information retrieval. While many of the recent works on semi-structured data retrieval focused on the use of field structure within the data. Solely relying on the field structure is insufficient to portray the user's understanding, which is represented through the use of specific query terms. In this study, we seek to overcome this limitation by utilizing a semantic network to model semi-structured data and apply a graph-based semi-structured data retrieval model. Using both a popular testing environment and a real-life query data, we compare the performance of the suggested model with various competitive state-of-the-art retrieval models. The study's findings demonstrate the strength of the proposed model while providing intriguing opportunities for further application of the model.","","Electronic:978-1-4673-8796-5; POD:978-1-4673-8797-2; USB:978-1-4673-8795-8","10.1109/BIGCOMP.2016.7425948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425948","graph-based retrieval model;semantic networks;semi-structured retrieval","Data models;Encyclopedias;Information retrieval;Mathematical model;Measurement;Motion pictures;Semantics","graph theory;information retrieval;semantic networks","graph-based retrieval model;semantic network;semistructured data retrieval model","","","","15","","","18-20 Jan. 2016","","IEEE","IEEE Conference Publications"
"Adaptive animation generation using web content mining","K. Hassani; W. S. Lee","School of Electrical Engineering and Computer Science University of Ottawa, Canada","2015 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS)","20160104","2015","","","1","8","Creating 3D animation is a labor-intensive and time-consuming process requiring designers to learn and utilize a complex combination of menus, dialog boxes, buttons and manipulation interfaces for a given stand-alone animation design software. On the other hand, conceptual simplicity and naturalness of visualizing imaginations from lingual descriptions motivates researchers for developing automatic animation generation systems using natural language interfaces. In this research, we introduce an interactive and adaptive animation generation system that utilizes data-driven techniques to extract the required common-sense and domain-specific knowledge from web. This system is capable of creating 3D animation based on user's lingual commands. It uses the user interactions as a relevance feedback to learn the implicit design knowledge, correct the extracted knowledge, and manipulate the dynamics of the virtual world in an active and incremental manner. Moreover, system is designed based on a multi-agent methodology which provides it with distributed processing capabilities and cross-platform characteristics. In this paper, we will focus on information retrieval agent which is responsible for extracting numeric data utilized in object attributes, spatiotemporal relations, and environment dynamics using web mining techniques.","","Electronic:978-1-4673-6698-4; POD:978-1-4673-6699-1; USB:978-1-4673-6697-7","10.1109/EAIS.2015.7368804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368804","computer animation;knowledge representation;natural language understanding;web mining","Animation;Data mining;Information retrieval;Natural languages;Semantics;Three-dimensional displays;Visualization","Internet;computer animation;data mining;distributed processing;interactive systems;knowledge acquisition;multi-agent systems;natural language interfaces;relevance feedback","Web content mining techniques;adaptive 3D animation generation;automatic animation generation systems;common-sense knowledge extraction;conceptual simplicity;cross-platform characteristics;data-driven techniques;distributed processing capabilities;domain-specific knowledge extraction;environment dynamics;imagination visualization;implicit design knowledge;information retrieval agent;interactive animation generation system;lingual descriptions;multiagent methodology;natural language interfaces;numeric data extraction;object attributes;relevance feedback;spatiotemporal relations;user interactions","","1","","15","","","1-3 Dec. 2015","","IEEE","IEEE Conference Publications"
"Query-based graph cuboid outlier detection","A. Dalmia; M. Gupta; V. Varma","International Institute of Information Technology, Hyderabad","2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)","20160211","2015","","","705","712","Various projections or views of a heterogeneous information network can be modeled using the graph OLAP (On-line Analytical Processing) framework for effective decision making. Detecting anomalous projections of the network can help the analysts identify regions of interest from the graph specific to the projection attribute. While most previous studies on outlier detection in graphs deal with outlier nodes, edges or subgraphs, we are the first to propose detection of graph cuboid outliers. Further we perform this detection in a query sensitive way. Given a general subgraph query on a heterogeneous network, we study the problem of finding outlier cuboids from the graph OLAP lattice. A Graph Cuboid Outlier (GCOutlier) is a cuboid with exceptionally high density of matches for the query. The GCOutlier detection task is clearly challenging because: (1) finding matches for the query (subgraph isomorphism) is NP-hard; (2) number of matches for the query can be very high; and (3) number of cuboids can be large. We provide an approximate solution to the problem by computing only a fraction of the total matches originating from a select set of candidate nodes and including a select set of edges, chosen smartly. We perform extensive experiments on synthetic datasets to showcase the execution time versus accuracy trade-off. Experiments on real datasets like Four Area and Delicious containing thousands of nodes reveal interesting GCOutliers.","","Electronic:978-1-4503-3854-7; POD:978-1-5090-2094-2","10.1145/2808797.2810061","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7403619","Graph Cuboid Outliers;Graph OLAP;Graph Projection Outliers;Information Networks;Outlier Detection","Data mining;Heterogeneous networks;Image edge detection;Information retrieval;Lattices;Organizations;Yttrium","data mining;graph theory;query processing","Delicious dataset;Four Area dataset;GCOutlier detection task;NP-hard problem;anomalous projection detection;decision making;general subgraph query;graph OLAP framework;graph online analytical processing framework;heterogeneous information network;network edges;network nodes;projection attribute;query matching;query sensitive detection;query-based graph cuboid outlier detection;region-of-interest identification;subgraph isomorphism;synthetic datasets","","","","22","","","25-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Neutron-Gamma Classification by Evolutionary Fuzzy Rules and Support Vector Machines","P. Krömer; Z. Matej; P. Musilek; V. Prenosil; F. Cvachovec","IT4Innovations, VSB - Tech. Univ. of Ostrava, Ostrava, Czech Republic","2015 IEEE International Conference on Systems, Man, and Cybernetics","20160114","2015","","","2638","2642","Accurate and fast methods for neutron-gamma discrimination play an essential role in the development of digital scintillation detectors. Digital detectors allow the use of state-of-the-art data analysis, mining, and classification methods in place of traditional approaches based on analog technology such as the pulse rise-time and charge-comparison methods. This work compares the ability of evolutionary fuzzy rules and support vector machines to perform accurate neutron-gamma classification. The accuracy and performance of both investigated methods are evaluated on two real-world data sets.","","Electronic:978-1-4799-8697-2; POD:978-1-4799-8698-9","10.1109/SMC.2015.461","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379593","Neutron-gamma classification;evolutionary fuzzy rules;support vector machines","Gamma-rays;Genetic programming;Information retrieval;Neutrons;Photonics;Support vector machines;Training","fuzzy systems;gamma-ray detection;neutron detection;support vector machines","accuracy;evolutionary fuzzy rules;neutron-gamma classification;performance;real-world data sets;support vector machines","","","","33","","","9-12 Oct. 2015","","IEEE","IEEE Conference Publications"
"Precision improvement in healthcare information extraction","H. Balaji; A. Govardhan","Dept of CSE, JNTUA, Anantapuramu","2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","20160321","2015","","","1","4","Presently a day, for any sort of data, individuals rely on upon web. They utilize web indexes like Google to pursuit data over web. The inquiries must be precise that will give the data identified with client's Health Care. Be that as it may, there is tremendous measure of data on the web as it's hard to get the significant data effortlessly. If there should arise an occurrence of inquiries on right prescription, individuals for the most part have their own particular inclinations. Wellbeing experts require the adaptability of unstructured content to express their conclusions and treatment systems. In this manner, the vicinity of unstructured content is inescapable, be that as it may, for information mining, data frameworks are obliged to change over the unstructured content to an organized representation for examination. Additionally the human advancement where individuals live will have sway on the quantity of decisions of prescription. So the proposed strategy will be utilized to prescribe best medication and nourishment to the clients or patients. These proposals are in view of the positioning of things and nourishments. The trial results give better exactness contrasted with existing strategies.","","CD-ROM:978-1-4799-7847-2; Electronic:978-1-4799-7849-6; POD:978-1-4799-7850-2","10.1109/ICCIC.2015.7435754","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435754","Information Extraction;Precision;Recommender System;Webmd;health care","Conferences;Fitting;Information retrieval;Insurance;Internet;Medical services;Proposals","Internet;health care;information retrieval;medical information systems","Google;Web index;client health care;data frameworks;healthcare information extraction;medication;patient treatment systems;precision improvement;prescription decisions","","","","11","","","10-12 Dec. 2015","","IEEE","IEEE Conference Publications"
"Implicit Information Extraction from Clinical Notes","S. Perera; A. Sheth","Kno.e.sis Center, Wright State Univ., Dayton, OH, USA","2015 International Conference on Healthcare Informatics","20151210","2015","","","498","498","We address the problem of extracting implicit information from the unstructured clinical notes. Here we introduce the problem of 'implicit entity recognition in clinical notes', propose a knowledge driven approach to address this problem and demonstrate the results of our initial experiments.","","Electronic:978-1-4673-9548-9; POD:978-1-4673-9549-6","10.1109/ICHI.2015.88","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349750","","Data mining;Electronic mail;Extremities;Informatics;Information retrieval;Medical services;Unified modeling language","document handling;medical administrative data processing;natural language processing","implicit entity recognition;implicit information extraction;knowledge driven approach;unstructured clinical notes","","","","1","","","21-23 Oct. 2015","","IEEE","IEEE Conference Publications"
"Compound Concept Semantic Similarity Calculation Based on Ontology and Concept Constitution Features","M. Li; B. Lang; J. Wang","State Key Lab. of Software Dev. Environ., Beihang Univ., Beijing, China","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","20160107","2015","","","226","233","The computation of semantic similarity between words is important in information retrieval, knowledge acquisition and many other fields. The existing studies are mainly aiming at single concepts composed of single terms. For the compound concepts composed of multiple terms, they usually neglect the special constitution features of compounds and only process them as single concepts, which may affect the ultimate accuracy. In this paper, we propose a novel ontology-based Compound Concept Semantic Similarity calculation approach called CCSS which exploits concept constitution features. In CCSS, the compound is decomposed into Subject headings and Auxiliary words (SaA), and the relationships between these two sets are used to measure the similarity. Besides, the errors that may be caused by SaA recognition are corrected. Moreover, several information sources of ontology such as taxonomical features, local density and depth are considered. Extensive experimental evaluations demonstrate that our approach significantly outperforms existing approaches.","1082-3409;10823409","Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0","10.1109/ICTAI.2015.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372140","Semantic similarity;compound concept;constitution feature;ontology;subject heading","Arrays;Compounds;Constitution;Information retrieval;Integrated circuits;Ontologies;Semantics","information retrieval;knowledge acquisition;ontologies (artificial intelligence);text analysis","CCSS;SaA recognition;auxiliary words;concept constitution features;information retrieval;information sources;knowledge acquisition;local density;local depth;ontology-based compound concept semantic similarity calculation based;semantic similarity computation;subject headings;taxonomical features","","","","26","","","9-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"Application of Problem-Based Learning Mode in Nursing Practice Student Teaching","Z. Yue-Ping; G. Yu-Jie; L. Xiao-Yan","Neurosurg. ICU, Hosp. of Nantong Univ., Nantong, China","2015 7th International Conference on Information Technology in Medicine and Education (ITME)","20160310","2015","","","385","389","Objective To explore the effects of problem-based practice teaching mode in nursing practice student teaching. Methods The nursing students participating in practice in neurosurgery ICU of our hospital between the year 2013-2014 were divided into experimental group and control group. The experimental group applied problem-based mode of practice teaching, while the control group applied traditional mode of practice teaching. The two groups were mainly compared in terms of critical thinking and professional knowledge, information retrieval ability, problem solving ability, learning interest, self-directed learning, teamwork, satisfaction of practice teaching and other differences. Results Nursing practice students in the experimental group obtained significant higher scores in critical thinking ability than students in the control group, and so did the other aspects including their professional knowledge, information retrieval, problem solving, learning interest, self-directed learning, cooperation ability and satisfaction scores. Conclusions The problem-based practice teaching mode is effective and feasible for the nursing practice students.","","CD-ROM:978-1-4673-8301-1; Electronic:978-1-4673-8302-8; POD:978-1-4673-8303-5","10.1109/ITME.2015.163","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429171","Problem-based learning (PBL);nursing practice students","Hospitals;Information retrieval;Neurosurgery;Training","biomedical education;information retrieval;neurophysiology;surgery;teaching","applied problem-based mode;control group;critical thinking ability;experimental group;information retrieval ability;learning interest;neurosurgery ICU;nursing practice student teaching;nursing practice students;problem solving ability;problem-based practice learning mode;problem-based practice teaching mode;self-directed learning","","","","9","","","13-15 Nov. 2015","","IEEE","IEEE Conference Publications"
"Benchmarking Blocking Algorithms for Web Entities","V. Efthymiou; K. Stefanidis; V. Christophides","Vasilis Efthymiou is with the University of Crete and ICS-FORTH, Greece. (email: vefthym@ics.forth.gr)","IEEE Transactions on Big Data","","2016","PP","99","1","1","An increasing number of entities are described by interlinked data rather than documents on the Web. Entity Resolution (ER) aims to identify descriptions of the same real-world entity within one or across knowledge bases in the Web of data. To reduce the required number of pairwise comparisons among descriptions, ER methods typically perform a pre-processing step, called blocking, which places similar entity descriptions into blocks and thus only compare descriptions within the same block. We experimentally evaluate several blocking methods proposed for the Web of data using real datasets, whose characteristics significantly impact their effectiveness and efficiency. The proposed experimental evaluation framework allows us to better understand the characteristics of the missed matching entity descriptions and contrast them with ground truth obtained from different kinds of relatedness links.","","","10.1109/TBDATA.2016.2576463","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485873","Blocking;Entity Resolution;Web of Data","Big data;Erbium;Information retrieval;Knowledge based systems;Knowledge engineering;Poles and towers;Semantics","","","","","","","","20160606","","","IEEE","IEEE Early Access Articles"
"Summarizing Events from Massive News Reports on the Web","W. Ge; C. Liu; S. Zhang; X. Xu","Key Lab. of Inf. Syst. Eng., Nanjing, China","2015 International Conference on Network and Information Systems for Computers","20160114","2015","","","170","174","In the big data era, massive news reports about the latest events are being published on the Web. To thoroughly understand an event, we have to read massive reports and keep clues in mind, which is very difficult and usually results in a one-sided interpretation. In this paper, we propose a multi-document summarization approach which summarizes reports of a particular social or political event automatically and comprehensively. To speed up summarization, a pre-summarization approach is introduced to condense each report to a sub-summary, which can reduce the scale of subsequent processing. As an event should be told in chronological order, a timeline is introduced to organize and aggregate event-relevant sub-summaries. With each day's sub-summaries, a key phrase extraction algorithm is used to cluster them into topics and generate a meaningful label for each topic. Finally, a selection criterion is introduced to select relevant and novel sentences for each topic. We perform experiments on a large-scale news dataset, with about 10 million reports collected from news sites. An empirical study shows that our system is feasible under large scale environment. An evaluation on effectiveness shows that it is favoured by users.","","Electronic:978-1-4799-1843-0; POD:978-1-4799-1844-7","10.1109/ICNISC.2015.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311861","multi-document summarization;pre-summarization;topic distillation","Clustering algorithms;Computational linguistics;Conferences;Information retrieval;Information systems;Redundancy;Research and development","Big Data;Internet;electronic publishing;text analysis","Web news reports;aggregate event-relevant subsummaries;big data;chronological order;event summarization;key phrase extraction algorithm;large-scale news dataset;multidocument summarization approach;one-sided interpretation","","","","22","","","23-25 Jan. 2015","","IEEE","IEEE Conference Publications"
"Evaluation of Semantic Similarity Using Vector Space Model Based on Textual Corpus","B. Hssina; B. Bouikhalene; A. Merbouha","Comput. Sci. Dept., Sultan Moulay Slimane Univ., Beni-Mellal, Morocco","2016 13th International Conference on Computer Graphics, Imaging and Visualization (CGiV)","20160512","2016","","","295","300","In this work, we have created a semantic similarity calculation system between text documents to contribute to their semantic clustering. Indeed, semantic clustering of documents is a promising field of research, since it guarantees a quick and targeted access to information. The aim of document clustering is to put together similar documents. We used the algebraic model VSM (Vector Space Model) [2] to represent text documents and the WordNet [1] lexical database, in that it groups words together based on their meanings. In this paper, we will present an overview of the static and semantic methods for calculating the similarity measure and the appropriateness of these methods. As our research is focusing on the treatment of text documents on e-learning systems. We worked on a corpus of a set of text documents from the computer science textbook for high school students in Morocco. To evaluate our system, an experiment has been conducted among students who produced text documents. Experimental evaluations using WordNet prove that the system presented in this work improves the accuracy of semantic similarity between the text documents.","","Electronic:978-1-5090-0811-7; POD:978-1-5090-0812-4","10.1109/CGiV.2016.64","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467726","Clustering;Corpus;Semantic similarity;WordNet;text document","Clustering methods;Computers;Euclidean distance;Indexing;Information retrieval;Semantics;Weight measurement","computer aided instruction;computer science education;educational institutions;pattern clustering;text analysis","Morocco;WordNet lexical database;algebraic model VSM;computer science textbook;document clustering;e-learning systems;high school students;semantic clustering;semantic method;semantic similarity calculation system;static method;text documents;vector space model","","","","26","","","March 29 2016-April 1 2016","","IEEE","IEEE Conference Publications"
"A multi-strategy approach for information extraction of financial report documents","S. Mariyah; D. H. Widyantoro","Department of Statistical Computation, Sekolah Tinggi Ilmu Statistik, Jakarta","2015 International Conference on Information & Communication Technology and Systems (ICTS)","20160114","2015","","","169","174","Information extraction studies have been conducted to improve the efficiency ansd accuracy of information retrieval. We developed information extraction techniques to extract name of company, period of document, currency, revenue, and number of employee information from financial report documents automatically. Different with other works, we applied a multi-strategy approach for developing extraction techniques. We separated information based on its similar characteristics before designing extraction techniques. We assumed that the difference of characteristics owned by each information induces the difference of strategy applied. First strategy is constructing extraction techniques using rule-based extraction method on information, which has good regularity on orthographic and layout features such as name of company, period of document and currency. Second strategy is applying machine learning-based extraction method on information, which has rich contextual and list look-up features such as revenue and number of employee. On the first strategy, rule patterns are defined by combining orthographic, layout, and limited contextual features. Defined rule patterns succeed to extract information and gain precision, recall, and F1-measure more than 0.98. On the second strategy, we conducted extraction task as classification task. First, we built classification models using Naive Bayes and Support Vector Machines algorithms. Then, we extracted the most informative features to train the classification models. The best classification model is used for extraction task. Contextual and list look-up features play important role in improving extraction performance. Second strategy succeed to extract revenue and number of employee information and gains precision, recall, and F-1 measure more than 0.93.","","Electronic:978-1-5090-0096-8; POD:978-1-5090-0097-5","10.1109/ICTS.2015.7379893","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379893","Naїve Bayes;Support Vector Machines;contextual features;information extraction;list lookup features;orthographic features","Companies;Data mining;Feature extraction;Information retrieval;Internet;Layout","Bayes methods;company reports;financial data processing;information retrieval;knowledge based systems;learning (artificial intelligence);pattern classification;support vector machines","F1-measure;classification models;company name;currency;document period;employee information;financial report documents;information extraction techniques;information retrieval accuracy;information retrieval efficiency;layout features;list look-up features;machine learning-based extraction method;multistrategy approach;naive Bayes algorithm;orthographic features;precision;recall;revenue;rule-based extraction method;support vector machine algorithm","","","","13","","","16-16 Sept. 2015","","IEEE","IEEE Conference Publications"
"MXML: Implementation of a web-based application for merging XML documents using XML-SIM","W. Viyanon","Department Mathematics (Computer Science Program), Faculty of Science, Srinakharinwirot University, Bangkok, Thailand","2015 13th International Conference on ICT and Knowledge Engineering (ICT & Knowledge Engineering 2015)","20160104","2015","","","5","10","This paper presents a design, implementation and evaluation of a web-based application called MergeXML (MXML). MXML was developed to integrate XML documents that are similar in terms of structure and content to complete information which can be used for information retrieval. XML documents are clustered into subtrees representing as instances using leaf-node parents as clustering points. The system finds subtree keys from unique values at leaf-node levels. Subtree keys play an important role for mapping subtrees between two documents. Matched subtrees are merged to complete the information on the base XML document. The result shows that MXML is able to cluster subtrees as proper instances. It can merge additional (different) information to the base XML document. MXML is recommended to run with not too large size of XML documents due to time-out settings on the web server.","2157-0981;21570981","Electronic:978-1-4673-9190-0; POD:978-1-4673-9191-7","10.1109/ICTKE.2015.7368462","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368462","Clustering;Keys;Matching;Similarity Detection;XML Similarity Detection","Information retrieval;Integrated circuits;Internet;Lead;Semantics;Servers;XML","Internet;XML;information retrieval","MXML;MergeXML;Web based application;Web server;XML document;XML documents;XML-SIM;clustering points;information retrieval;leaf node parents;subtree keys;time out settings","","","","9","","","18-20 Nov. 2015","","IEEE","IEEE Conference Publications"
"Clinical Decision Support Systems: A Survey of NLP-Based Approaches from Unstructured Data","J. A. Reyes-Ortiz; B. A. González-Beltrán; L. Gallardo-López","Syst. Dept., Metropolitan Autonomous Univ., Mexico City, Mexico","2015 26th International Workshop on Database and Expert Systems Applications (DEXA)","20160215","2015","","","163","167","Clinical Decision Support on patients health outcomes can be performed from free text with Natural Language Processing techniques. However, it becomes a computational challenge due to the complexity of natural language. In recent years, several NLP-based approaches have been proposed to consider clinical decisions support. This paper presents a survey of Natural Language Processing approaches to support clinical decisions on patient health outcomes. The presented approaches are emphasized on the use of free text as input for diverse languages. An analysis of clinical decision support systems based on natural language processing in terms of their performance results is presented.","1529-4188;15294188","Electronic:978-1-4673-7582-5; POD:978-1-4673-7583-2","10.1109/DEXA.2015.47","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406286","Clinical Decision Support;NLP-based Clinical Decision Support Systems.;Natural Language Processing","Decision support systems;Hospitals;Information retrieval;Monitoring;Natural language processing","computational complexity;data structures;decision support systems;natural language processing;text analysis","NLP-based approach;clinical decision support system analysis;computational complexity;natural language processing technique;patient health outcome;unstructured data","","1","","23","","","1-4 Sept. 2015","","IEEE","IEEE Conference Publications"
"A survey on web news retrieval and mining","R. Hassanian-esfahani; M. j. Kargar","Research Institute for ICT, ACECR, Tehran, Iran","2016 Second International Conference on Web Research (ICWR)","20160623","2016","","","90","101","This article is a review on news retrieval and mining research areas in recent years based on a qualitative approach. It addresses news retrieval and mining in four main categories of News Retrieval and Extraction, News Content Analysis, News Propagation Analysis, and News Visualization. Each indicated category entails various research areas that have been investigated through several studies. This study depicts the immense extent of news retrieval and mining, the interconnected methods, tools, and theoretical foundations as well as the evaluation methods and the results. The study helps to gain a better understanding of news mining research areas.","","Electronic:978-1-5090-2166-6; POD:978-1-5090-2167-3","10.1109/ICWR.2016.7498452","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498452","data mining;knowledge discovery;news retrieval;online news;web mining;web news mining","Databases;Feeds;Filtering;Information retrieval;Semantics;Text mining","Internet;data mining;query processing;text analysis","Web news mining;Web news retrieval;news content analysis category;news propagation analysis category;news retrieval-and-extraction category;news visualization category;qualitative approach","","","","91","","","27-28 April 2016","","IEEE","IEEE Conference Publications"
"Probabilistic and grouping methods for morphological root identification for Afaan Oromo","G. M. Wegari; M. Melucci; S. Teferra","IT Doctoral Program, Addis Ababa University, Ethiopia","2016 6th International Conference - Cloud System and Big Data Engineering (Confluence)","20160709","2016","","","12","15","Morphological models are used in many natural language processing tasks including machine translation and speech recognition. We investigated probabilistic and grouping methods to develop a morphological root identification model for Afaan Oromo. In this paper, we have experimentally shown that the proposed methods can improve the morphological root identification performance of some state-of-the-art methods.","","CD-ROM:978-1-4673-8202-1; Electronic:978-1-4673-8203-8; POD:978-1-4673-8204-5","10.1109/CONFLUENCE.2016.7508039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508039","Afaan Oromo;grouping;probabilisic;root identification","Europe;Information retrieval;Natural language processing;Probabilistic logic;Speech recognition;Testing;Writing","group theory;language translation;mathematical morphology;natural language processing;probability","Afaan Oromo;grouping methods;machine translation;morphological root identification model;natural language processing tasks;probabilistic methods;speech recognition","","","","","","","14-15 Jan. 2016","","IEEE","IEEE Conference Publications"
"Hardware-accelerated text analytics","R. Polig; K. Atasu; C. Hagleitner; L. Chiticariu; F. Reiss; H. Zhu; P. Hofstee","IBM Research Zurich, Switzerland","2014 IEEE Hot Chips 26 Symposium (HCS)","20160704","2014","","","1","24","Presents a collection of slides covering the following topics: SystemT text analytics software; hardware-accelerated SystemT; Big text Data; and field programmable gate array.","","Electronic:978-1-4673-8883-2; POD:978-1-4673-8884-9","10.1109/HOTCHIPS.2014.7478822","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478822","","Information retrieval;Logic gates;Text analysis;Text mining;Text processing","Big Data;field programmable gate arrays;text analysis","Big text Data;FPGA;SystemT text analytics software;field programmable gate array;hardware-accelerated SystemT;hardware-accelerated text analytics","","","","","","","10-12 Aug. 2014","","IEEE","IEEE Conference Publications"
"Automated ontology framework for service robots","S. Kanjaruek; D. Li; R. Qiu; N. Boonsim","Institute for Research in Applicable Computing, University of Bedfordshire, UK","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","20160225","2015","","","219","224","This paper presents an automated ontology framework for service robots. The framework is designed to automatically create an ontology and an instance of concept in dynamic environment. Ontology learning from text is applied to build a concept hierarchy using WordNet which provides a rich semantic processing for physical objects. The Automated Ontology is composed of four modules: Concept Creation, Property Creation, Relationship Creation and Instance of Concept Creation. The automated ontology algorithm was implemented in order to create the concept hierarchy in the Robot Ontology. The Semantic Knowledge Acquisition represents knowledge of physical objects in dynamic environments. In simulation experiments, the list of object names and property names was identified. The result shows the concept hierarchy which represents explicit terms and the semantic knowledge of physical objects for performing everyday manipulation tasks.","","Electronic:978-1-4673-9675-2; USB:978-1-4673-9674-5","10.1109/ROBIO.2015.7418770","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418770","","Information retrieval;Knowledge acquisition;OWL;Object recognition;Ontologies;Robots;Semantics","control engineering computing;knowledge acquisition;learning (artificial intelligence);ontologies (artificial intelligence);service robots","WordNet;automated ontology framework;instance of concept creation;ontology learning;property creation;relationship creation;robot ontology;semantic knowledge acquisition;service robots","","","","12","","","6-9 Dec. 2015","","IEEE","IEEE Conference Publications"
"A smartphone app framework for segmented cancer care coordination","T. He; R. Ogunti; M. Puppala; S. Chen; X. Yu; J. J. Mancuso; S. T. C. Wong","Systems Medicine and Bioengineering Department of Houston Methodist Research Institute and Informatics Development Department of Houston Methodist Hospital, Houston, TX 77030 USA","2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","20160421","2016","","","372","375","Complex cancer care requires careful coordination, but resource limitations result in lack of effective coordinated follow-up services. Recent advances in smartphones offer great opportunities for better segmentation of patient populations for cost-effective, targeted care coordination and monitoring or surveillance of cancer patients. This paper presents a framework of a smartphone app that provides such risk assessment and follow-up care monitoring services. This mHealth app framework includes three functional modules: a natural language processing module based on Bayesian model to extract relevant information from free text or medical reports; a cancer risk calculator that uses support vector machine classification to assess the medical risks of cancer patients based on the information extracted; and a health care monitor that provides timely care coordination to high risk cancer patients. The experimental results validate mHealth as a reliable medical risk assessment for post-surgical cancer patients and an effective health care monitoring service for cancer care coordination.","","Electronic:978-1-5090-2455-1; POD:978-1-5090-2456-8","10.1109/BHI.2016.7455912","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455912","","Biomedical monitoring;Cancer;Data mining;Information retrieval;Monitoring;Risk management","Bayes methods;cancer;health care;image classification;image segmentation;medical diagnostic computing;patient care;patient monitoring;risk management;smart phones;support vector machines","Bayesian model;cancer care coordination;cancer patient monitoring;cancer patient surveillance;cancer risk calculator;cost-effective targeted care coordination;follow-up care monitoring services;free text;functional modules;health care monitor;information extraction;mHealth app framework;medical reports;natural language processing module;patient population segmentation;post-surgical cancer patients;reliable medical risk assessment;resource limitations;segmented cancer care coordination;smartphone app framework;support vector machine classification","","","","18","","","24-27 Feb. 2016","","IEEE","IEEE Conference Publications"
"LinkedPolitics: Incremental Semantic Lifting of Political Facts","J. Tscherrig; E. Mugellini; O. A. Khaled; P. Cudré-Mauroux","Comput. Sci., Univ. of Appl. Sci., Fribourg, Switzerland","2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)","20160523","2016","","","1072","1079","Exploiting facts that are published online using semi-structured or unstructured formats is a highly complex task, pieces of data are typically published in isolation and periodically updated in a bulk fashion without any coordination. In this paper, we propose a new software pipeline tackling this issue and semantically lifting online facts as Linked Data in a continuous manner. Our solution, LinkedPolitics, extracts, converts, interlinks, and finally makes available as Linked Data online facts that could not be transparently exposed and exploited otherwise. Our system is online, in the sense that it automatically makes available new facts as they are uploaded online. We also present a deployment of LinkedPolitics centered around the Swiss Parliament. Our deployment extracts and semantically lifts a flurry of political facts that are periodically published in unstructured form by the Library Service of the Swiss Federal Assembly. It then automatically materializes a number of views to support different visualizations of the data in order to capture the actions, announcements or votes made by the politicians, as well as their relationships to each other and to external entities such as companies.","1550-445X;1550445X","Electronic:978-1-5090-1858-1; POD:978-1-5090-1859-8","10.1109/AINA.2016.93","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474209","Data Vizualisation;Linked Data;Linked Politics;Open Data;Semantically Lift;Swiss Parliament","Data mining;Data visualization;Databases;Information retrieval;Resource description framework;Semantics;XML","Internet;data visualisation;government data processing;pipeline processing;politics","Library Service of the Swiss Federal Assembly;Linked Data;LinkedPolitics deployment;Swiss Parliament;data visualization;online facts;political facts;semistructured formats;software pipeline;unstructured formats","","","","13","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"Unified parsing and information extraction language","P. Bednár","Technical University of Ko&#353;ice /Department of Cybernetics and Artificial Intelligence, Ko&#353;ice, Slovakia","2016 IEEE 14th International Symposium on Applied Machine Intelligence and Informatics (SAMI)","20160303","2016","","","131","135","The paper describes declarative language for specification of parsing and information extraction rules used in the common natural language processing tasks. The language unify concepts of tokenization using the regular expressions over the input text, information extraction patterns specified using the regular expressions over the annotations and graph matching patterns used for syntactic or co-reference relations between words or phrases.","","Electronic:978-1-4673-8740-8; POD:978-1-4673-8741-5; USB:978-1-4673-8739-2","10.1109/SAMI.2016.7422995","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422995","","Data mining;Database languages;Information retrieval;Metadata;Natural language processing;Pattern matching;Syntactics","formal specification;grammars;graph theory;natural language processing;pattern matching","common natural language processing task;coreference relation;declarative language;graph matching pattern;information extraction language;information extraction pattern;information extraction rule;parsing;regular expression;syntactic relation;tokenization","","","","11","","","21-23 Jan. 2016","","IEEE","IEEE Conference Publications"
"Efficient object identification and multiple regions of interest using CBIR based on relative locations and matching regions","M. H. Memon; J. P. Li; I. Memon; R. A. Shaikh; F. A. Mangi","School of Computer Science and Engineering, UESTC, Chengdu, 611731 China","2015 12th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","20160620","2015","","","247","250","Nowadays information retrieval systems get more attention due to the increasing use of multimedia technologies. The information may be in the form of video, image, sound and/or text. Application of surveillance, digital libraries, web applications and various other applications that handle enormous volume of data essentially have information retrieval components. This paper demonstrates an image retrieval system based on multiple regions that give a client interface for helping to identify the watershed regions-of-interest inside of an input image. The relationship between semantic ideas and visual elements is established by supervised Bayesian learning from positive bags. For comparison, feature vectors of regions which have similar regions code to the regions of query image can be used during retrieval. On standard datasets the proposed algorithm has been applied and accomplishes great annotation performance.","","Electronic:978-1-4673-8266-3; POD:978-1-4673-8267-0","10.1109/ICCWAMTIP.2015.7493985","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493985","Color-size feature;Content-based image retrieval;Image annotation;Region of Interest (ROI);Region-based image retrieval;Visual feature","Histograms;Image color analysis;Image retrieval;Image segmentation;Information retrieval;Measurement;Shape","Bayes methods;feature extraction;image matching;image retrieval;information retrieval systems;learning (artificial intelligence)","Web applications;digital libraries;efficient object identification;feature vectors;image retrieval system;information retrieval components;information retrieval systems;matching regions;multimedia technologies;relative locations;supervised Bayesian learning;visual elements;watershed regions-of-interest","","1","","15","","","18-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Holistic entity matching across knowledge graphs","M. Pershina; M. Yakout; K. Chakrabarti","New York University","2015 IEEE International Conference on Big Data (Big Data)","20151228","2015","","","1585","1590","Entity matching is the problem of determining if two entities in a data set refer to the same real-world object. In the last decade a growing number of large-scale knowledge bases have been created online. Tools for automatically aligning these sources would make it possible to unify them in a structured knowledge and to answer complex queries. Here we present Holistic Entity Matching (HolisticEM), an algorithm based on Personalized Page Rank for aligning instances in large knowledge bases. It consists of two steps. First, a graph of potential matching pairs is constructed; second, local and global information from the relationship graph is propagated via Personalized Page Rank. We demonstrate that HolisticEM performs competitively and can efficiently handle databases with 110M and 203M entities accurately resolving 1.6M of matching entity pairs.","","Electronic:978-1-4799-9926-2; POD:978-1-4799-9927-9","10.1109/BigData.2015.7363924","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363924","","Big data;Data models;Databases;Information retrieval;Knowledge based systems;Motion pictures;Probabilistic logic","data mining;pattern matching","HolisticEM algorithm;entity pair matching;holistic entity matching algorithm;knowledge graph;large-scale knowledge bases;personalized PageRank;query answering","","1","","21","","","Oct. 29 2015-Nov. 1 2015","","IEEE","IEEE Conference Publications"
"Kannada Named Entity Recognition and Classification using conditional Random Fields","S. Amarappa; S. V. Sathyanarayana","Department of TCE, Jawaharlal Nehru National College of Engineering, Shimoga - 577 204, India","2015 International Conference on Emerging Research in Electronics, Computer Science and Technology (ICERECT)","20160627","2015","","","186","191","Named Entity Recognition and Classification is a process of identification of proper nouns in the text and classification of those nouns into certain predefined categories like person name, location, organization, date, time etc. Named Entity Recognition and Classification in Kannada is an essential and challenging task. The aim of this work is to develop a novel model for Kannada Named Entity Recognition and Classification, based on conditional Random Fields. The paper discusses the various issues in developing the proposed model. The details of implementation and performance evaluation are discussed. The experiments are conducted on a training corpus of size 95,127 tokens and test corpus of 5,000 tokens. It is observed that the model works with Precision, Recall and F1-measure of 85%, 85% and 82% respectively.","","Electronic:978-1-4673-9563-2; POD:978-1-4673-9564-9","10.1109/ERECT.2015.7499010","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7499010","Conditional Random Fields;Development-set;Information Extraction;Named Entity Recognition;Natural Language Processing;Test-set;Training-set","Computer science;Feature extraction;Hidden Markov models;Information retrieval;Natural language processing;Organizations;Text recognition","natural language processing;pattern classification;random processes;text analysis","F1-measure value;NERC;conditional random fields;named entity recognition-and-classification;precision value;proper noun identification;recall value;test corpus;token size;training corpus","","","","20","","","17-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Towards improving the performance of Vector Space Model for Chinese Frequently Asked Question Answering","Ridong Jiang; S. Kim; R. E. Banchs; Haizhou Li","Human Language Technology Department, Institute for Infocomm Research, Singapore 138632","2015 International Conference on Asian Language Processing (IALP)","20160414","2015","","","136","139","This paper presents a method which improves the performance of Vector Space Model (VSM) when applying it to Chinese Frequently Asked Questions (FAQ). This method combines unigram and bigram models in determining the similarity of document vectors. The performance is further improved by applying shallow lexical semantics and the document length information. Experiments showed that the proposed methods outperform baselines (segmentation and bigram) across different datasets which include FAQs from restricted domains and open domains.","","CD-ROM:978-1-4673-9594-6; Electronic:978-1-4673-9596-0; POD:978-1-4673-9597-7","10.1109/IALP.2015.7451550","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451550","bag of words;chinese question answering;vector space model","Computational modeling;Information retrieval;Silicon;TV","document handling;natural language processing;question answering (information retrieval);vectors","Chinese frequently asked question answering;VSM;bigram model;document length information;document vector similarity determination;performance improvement;shallow lexical semantics;unigram model;vector space model","","","","","","","24-25 Oct. 2015","","IEEE","IEEE Conference Publications"
"Domain-specific Relation Extraction: using distant supervision Machine Learning","A. Aljamel; T. Osman; G. Acampora","School of Science and Technology, Nottingham Trent University, NG11 8NS, U.K.","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","20160801","2015","01","","92","103","The increasing accessibility and availability of online data provides a valuable knowledge source for information analysis and decision-making processes. In this paper we argue that extracting information from this data is better guided by domain knowledge of the targeted use-case and investigate the integration of a knowledge-driven approach with Machine Learning techniques in order to improve the quality of the Relation Extraction process. Targeting the financial domain, we use Semantic Web Technologies to build the domain Knowledgebase, which is in turn exploited to collect distant supervision training data from semantic linked datasets such as DBPedia and Freebase. We conducted a serious of experiments that utilise the number of Machine Learning algorithms to report on the favourable implementations/configuration for successful Information Extraction for our targeted domain.","","Electronic:978-9-8975-8164-9; POD:978-1-5090-1967-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526907","Information Extraction;Knowledge-Base;Natural Language Processing;Relation Extraction;Semantic Web;Supervised Machine Learning","Classification algorithms;Data mining;Feature extraction;Information retrieval;Natural language processing;Semantic Web;Training","data handling;decision making;financial data processing;knowledge based systems;learning (artificial intelligence);semantic Web","DBPedia;Freebase;decision-making processes;distant supervision machine learning;domain knowledge base;domain-specific relation extraction;financial domain;information analysis;information extraction;knowledge-driven approach;online data;semantic Web Technologies;semantic linked datasets","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"A rule-based information extraction system for human-readable semi-structured scientific documents","Gang Chen; Baoran An; Sifeng Zeng","Institute of Computer Application, China Academy of Engineering Physics, Mianyang, China","2015 4th International Conference on Computer Science and Network Technology (ICCSNT)","20160616","2015","01","","75","84","The explosion of data over the past twenty years has fostered a huge amount of research in processing semi-structured documents like HTML and XML documents on Web. Nevertheless, the explosion of semi-structured documents that originate from outside the Web domain is more challenging. The data of semi-structured documents are everywhere: in scientific research reports, official journals, electronic health records and any records from other domains. Different from HTML and XML, semi-structured record is usually human-readable and has its own internal schema. On account of the incredible increase of data volumes, the traditional methods of studying on these data cannot meet the needs of high performance and flexible access. Relational database technique is a good alternative technique for managing and organizing data. In this paper, we aim at (1) revealing the semantic structure in human-readable scientific records (HSR), and (2) presenting a transformation system that converts HSR into structured relational database. Our approach identifies the relevant logical sub-structures and objects in HSR according to specific templates. We don't target “the best” methods but provide an available framework for facilitating the semi-structured HSR analysis tasks. The evaluation to the system shows its ability to enable the efficient transformation of HSR and therefore contributes to the increasing corpus of semi-structured HSR documents.","","CD-ROM:978-1-4673-8172-7; Electronic:978-1-4673-8173-4; POD:978-1-4673-8174-1","10.1109/ICCSNT.2015.7490711","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490711","human-readable;information extraction;rule;semi-structured scientific document","Data mining;HTML;Information retrieval;Relational databases;Semantics;Visualization;XML","Internet;XML;hypermedia markup languages;relational databases","HTML;Web;XML documents;human-readable semistructured scientific documents;relational database technique;rule-based information extraction system;semistructured documents;semistructured record","","","","20","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"A Dynamic Display System of Scientist Thought","F. Wang; B. Xu","Nanjing Univ. of Posts & Telecommun., Nanjing, China","2015 11th International Conference on Semantics, Knowledge and Grids (SKG)","20160310","2015","","","264","267","Most academic paper management systems take papers as the basic units instead of the thoughts in papers. And papers are organised in terms of research areas, not scientists, which may fall to satisfy the need of learning a scientist's thoughts. This paper proposes a dynamic system to display a scientist's thoughts. A novel way, which is subway map, is applied to display the change of the concepts in a scientist's thoughts. Users can effectively and intuitively understand the thoughts of a scientist by interacting with the dynamic display system.","","Electronic:978-1-4673-9808-4; POD:978-1-4673-9809-1","10.1109/SKG.2015.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429392","Concept model;semantic link network;subway map","Artificial intelligence;Data models;Extraterrestrial measurements;Information retrieval;Libraries;Public transportation;Semantics","computer aided instruction;data visualisation;scientific information systems","academic paper management systems;dynamic display system;learning;scientific research;scientist thought display;subway map;system interaction;visualization research","","","","","","","19-21 Aug. 2015","","IEEE","IEEE Conference Publications"
"Collaborative Information Service: The Security Question","A. K. Pal; S. Bose","Indian Inst. of Manage. Calcutta, Kolkata, India","2016 49th Hawaii International Conference on System Sciences (HICSS)","20160310","2016","","","348","357","Growth of computing and analytics, internet and distributed computing, and increasing tendency to use crowd sourcing for creative tasks, the use of collaborative idea in information intensive activities is a growing research concern. In CIS companies would cooperate to do their interlinked business together. It responds to customer requests for information retrieval from or writing on data contents. We study the role of a flexible privacy model that can simplify the job of maintaining security in a collaborative environment. There has not been sufficient effort put in this direction though vast amount of literature exists on security in databases, distributed databases and cloud. Not many attempts for the security of collaborative information retrieval systems either exist. IRaaS - Information Retrieval as a Service - introduced 'privacy template', whereas the explicit expression of privacy constraints adopted by CIS for security implementation. The rational approach to security for such problems is a novel idea.","1530-1605;15301605","Electronic:978-0-7695-5670-3; POD:978-1-5090-1981-6","10.1109/HICSS.2016.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7427225","Privacy Issues;Privacy Types;Privacy template;Rational approach;Security framework","Cloud computing;Collaboration;Databases;Information retrieval;Organizations;Privacy;Security","data privacy;groupware;information retrieval systems;information services;security of data","CIS;IRaaS;collaborative environment;collaborative information retrieval systems;collaborative information service;customer requests;data contents;information retrieval as a service;interlinked business;privacy constraints;privacy model;security","","","","34","","","5-8 Jan. 2016","","IEEE","IEEE Conference Publications"
"Document similarity analysis via involving both explicit and implicit semantic couplings","Q. Chen; L. Hu; J. Xu; W. Liu; L. Cao","Advanced Analytics Institute, University of Technology Sydney, Australia","2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)","20151207","2015","","","1","10","Document similarity analysis is increasingly critical since roughly 80% of big data is unstructured. Accordingly, semantic couplings (relatedness) have been recognized valuable for capturing the relationships between terms (words or phrases). Existing work focuses more on explicit relatedness, with respective models built. In this paper, we propose a comprehensive semantic similarity measure: Semantic Coupling Similarity (SCS), which (1) captures intra-term pair couplings within term pairs represented by patterns of explicit term co-occurrences in a document set, (2) extracts inter-term pair couplings between term pairs indicated by implicit couplings between term pairs through indirectly linked terms and paths between terms after term connections are converted to a graph presentation; and (3) semantic coupling similarity, integrating intra- and inter-term pair couplings towards a comprehensive capturing of explicit and implicit couplings between terms across documents. SCS caters for both synonymy and polysemy, and outperforms baseline methods consistently on all real data sets.","","Electronic:978-1-4673-8273-1; POD:978-1-4673-8274-8","10.1109/DSAA.2015.7344832","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344832","","Context;Context modeling;Couplings;Information retrieval;Probabilistic logic;Semantics;Text analysis","Big Data;document handling;graph theory","SCS;big data;document similarity analysis;graph presentation;semantic coupling similarity","","","","36","","","19-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Comments on “Control Cloud Data Access Privilege and Anonymity With Fully Anonymous Attribute-Based Encryption”","H. Ma; R. Zhang; W. Yuan","State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","20160203","2016","11","4","866","867","Most of the known attribute-based encryption (ABE) schemes focused on the data contents privacy and the access control, but less attention was paid to the privilege control and the identity privacy problem. Recently in IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY (TIFS) (DOI:10.1109/TIFS.2014.2368352), Jung et al. proposed an anonymous attribute-based encryption scheme for access privilege and anonymity, which exhibited a lot of interesting ideas and gave the proof in the standard model. However, after carefully revisiting the scheme, we found that any valid user can compute the system-wide master key and their proof has some mistakes, hence, it fails to meet their security definitions.","1556-6013;15566013","","10.1109/TIFS.2015.2509865","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360926","Cryptanalysis;access privilege;anonymity;attribute-based encryption","Cloud computing;Cryptography;Data privacy;Information retrieval","cloud computing;data privacy;information retrieval;public key cryptography","ABE scheme;IEEE;TIFS;Transaction On Information Forensics And Security;anonymous attribute-based encryption;control cloud data access anonymity;control cloud data access privilege;data content privacy;identity privacy problem;system-wide master key","","","","2","","20151218","April 2016","","IEEE","IEEE Journals & Magazines"
"Cuts or thresholds, what is the best reduction method in fuzzy formal concept analysis?","M. E. Cornejo; J. Medina; E. Ramírez-Poussa","Department of Statistic and O.R., University of C&#x00E1;diz, Spain","2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","20151130","2015","","","1","8","Recently α-cut irreducible and δ<sub>1</sub>δ<sub>2</sub>-multi-adjoint concept lattices have been introduced as two different methodologies focus on reducing the size of a given fuzzy concept lattice. The philosophy of both methodologies is completely different and so, the obtained lattices too. This paper analyzes the differences and proposes that the best is to combine both methodologies in order to obtain new procedures to reduce the information retrieval, only considering the important information for the user and with the advantages of both philosophies.","","Electronic:978-1-4673-7428-6; POD:978-1-4673-7429-3","10.1109/FUZZ-IEEE.2015.7337990","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337990","Formal concept analysis;concept lattice reduction;fuzzy sets","Context;Data mining;Databases;Electronic mail;Formal concept analysis;Information retrieval;Lattices","data reduction;formal concept analysis;fuzzy set theory;information retrieval","α-cut irreducible concept lattice;δ1δ2-multiadjoint concept lattice;fuzzy concept lattice;fuzzy formal concept analysis;information retrieval;reduction method","","1","","23","","","2-5 Aug. 2015","","IEEE","IEEE Conference Publications"
"Fully-Automatic XML Clustering by Structure-Constrained Phrases","G. Costa; R. Ortale","ICAR Inst., Rende, Italy","2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)","20160107","2015","","","146","153","Conventional approaches to XML clustering by content and structure are generally affected by a limitation due to the adoption of the bag-of-word model for the representation of their textual contents. This choice may lead to consider structure-constrained textual items of separate XML documents as related, even though the actual meaning of such items in their respective contexts is different. To overcome such a limitation, we propose XML clustering by structure-constrained phrases. The latter is a previously unexplored method relying on the more accurate bag-of-phrase model of the XML textual content, with which to better preserve the meaning of the structure-constrained content items for improved clustering effectiveness. In order to conduct an in-depth and systematic study of the effectiveness of the proposed method, we develop a parameter-free prototypical approach to XML partitioning, which projects the XML documents into a space of XML features representing fixed-length sequences of adjacent textual items in the context of root-to-leaf paths. Feature selection without any tunable threshold is used to choose a subset of the XML features on the basis of their relevance to clustering, which is assessed through a new scoring scheme. A comparative experimentation on real-world benchmark XML corpora reveals a higher effectiveness than several state-of-the-art competitors.","1082-3409;10823409","Electronic:978-1-5090-0163-7; USB:978-1-5090-0162-0","10.1109/ICTAI.2015.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372130","XML Analysis;XML Clustering;XML Topic Models","Benchmark testing;Context;Data models;Electronic mail;Information retrieval;Vegetation;XML","XML;feature selection;pattern clustering;text analysis","XML documents;XML features;XML partitioning;XML textual content;bag-of-phrase model;eXtensible Markup Language;feature selection;fixed-length sequences;fully-automatic XML clustering;parameter-free prototypical approach;real-world benchmark XML corpora;root-to-leaf paths;scoring scheme;structure-constrained content items;structure-constrained phrases;structure-constrained textual items;textual items","","1","","39","","","9-11 Nov. 2015","","IEEE","IEEE Conference Publications"
"A named entity recognition dataset for Turkish","D. Küçük; D. Küçük; N. Arıcı","Elektriksel G&#252;&#231; Teknolojileri B&#246;l&#252;m&#252;, T&#220;B&#304;TAK Enerji Enstit&#252;s&#252;, Ankara, T&#252;rkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","20160623","2016","","","329","332","Named entity recognition is one of the important topics in the research area of natural language processing. Named entity recognition studies conducted on Turkish texts are quite limited, compared to the studies on other languages. Besides, the lack of common data sets makes the comparison of different approaches harder. In this study, a dataset comprising news articles in Turkish annotated with named entities is presented. The annotations comprise the basic named entity types of person, location, and organization names. Additionally, to be used as reference in future studies, a rule-based named entity recognition system is evaluated on the final form of this data set and the corresponding evaluation results are presented. It is envisioned that our study will contribute to the advancement of named entity recognition studies on Turkish texts.","","Electronic:978-1-5090-1679-2; POD:978-1-5090-1680-8; USB:978-1-5090-1678-5","10.1109/SIU.2016.7495744","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495744","Turkish;dataset;information extraction;named entity recognition","Computers;Information retrieval;Natural language processing;Organizations;Standards;Text recognition","natural language processing;text analysis","Turkish;Turkish text;named entity recognition dataset;natural language processing;rule-based named entity recognition system","","","","","","","16-19 May 2016","","IEEE","IEEE Conference Publications"
"Comparator Mining Lead to Better Alternatives for Making Better Decision","P. R. Shirsath; U. M. Patil","Dept. of Comput. Eng., SES's R. C. Patel Inst. of Technol., Shirpur, India","2015 IEEE International Symposium on Nanoelectronic and Information Systems","20160317","2015","","","237","242","Today internet has become easily accessible which allows the user to perform multiple tasks such as access information, do study, make friends, online shopping, search for anything they want and many more. Similarly people do use internet to know the better options and find out relevant alternatives of product, services, places like wise. But searching for better choices may become frustrating and time eating process as search engines shows many links for single thing. Therefore, this paper precisely gives the better result and better alternatives. For this, a weakly supervised bootstrapping approach is used, by which we can identify comparative questions and at the same time evaluate comparable items. It also shows better options for the general term entities and for ambiguous entities in different classes if the specific class is not specified. It also gives the better resultant alternatives for the single entity by categories. The weakly supervised bootstrapping approach is very potential to know better alternatives of any product, place or any services. The experiment result shows the better performance of this method by measuring the Recall, Precision and F-Score. The method works better, effective and efficient in showing the better result and alternative for any single entity, comparable entities and entities that are ambiguous.","","Electronic:978-1-4673-9692-9; POD:978-1-4673-9693-6","10.1109/iNIS.2015.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434431","Bootstrapping;Comparable Entity Mining;Entity Extraction;Entity Raking;Information Extraction;Pattern Evaluation;Pattern Generation;Sequential Pattern Mining","Browsers;Computers;Data mining;Information retrieval;Internet;Reliability;Semantics","Internet;data mining;decision making;information retrieval;search engines;statistical analysis","F-Score measure;Internet;Precision measure;Recall measure;ambiguous entities;comparator mining;decision making;search engines;weakly supervised bootstrapping approach","","","","10","","","21-23 Dec. 2015","","IEEE","IEEE Conference Publications"
"Dark Data: Are we solving the right problems?","M. Cafarella; I. F. Ilyas; M. Kornacker; T. Kraska; C. Ré","University of Michigan, USA","2016 IEEE 32nd International Conference on Data Engineering (ICDE)","20160623","2016","","","1444","1445","With the increasing urge of the enterprises to ingest as much data as they can in what's commonly referred to as “Data Lakes”, the new environment presents serious challenges to traditional ETL models and to building analytic layers on top of well-understood global schema. With the recent development of multiple technologies to support this “load-first” paradigm, even traditional enterprises have fairly large HDFS-based data lakes now. They have even had them long enough that their first generation IT projects delivered on some, but not all, of the promise of integrating their enterprise's data assets. In short, we moved from no data to Dark data. Dark data is what enterprises might have in their possession, without the ability to access it or with limited awareness of what this data represents. In particular, business-critical information might still remain out of reach. This panel is about Dark Data and whether we have been focusing on the right data management challenges in dealing with it.","","Electronic:978-1-5090-2020-1","10.1109/ICDE.2016.7498366","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498366","","Big data;Cleaning;Computer science;Data mining;Databases;Information retrieval;Lakes","","","","","","","","","16-20 May 2016","","IEEE","IEEE Conference Publications"
"ConceptRDF: An RDF presentation of ConceptNet knowledge base","E. Najmi; Z. Malik; K. Hashmi; A. Rezgui","Department of Computer Science, Wayne State University, Michigan, USA","2016 7th International Conference on Information and Communication Systems (ICICS)","20160523","2016","","","145","150","The exponential growth of information on the Web has prompted the introduction of new technologies such as Semantic Web and Common Sense knowledge bases. While there are different models available to present information, RDF, as the cornerstone of Semantic Web technologies, has a dominant place for formats suitable both for human interactions and machine understanding. In this paper we present ConceptRDF, a conversion of ConceptNet, as one of the largest common sense knowledge bases available for public use, to RDF/XML format, suitable for use in different fields of information retrieval.","","Electronic:978-1-4673-8614-2; POD:978-1-4673-8615-9","10.1109/IACS.2016.7476101","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476101","","Context;Data mining;Information retrieval;Knowledge based systems;Ontologies;Resource description framework","information retrieval;knowledge based systems;ontologies (artificial intelligence)","ConceptNet knowledge base;ConceptRDF;common sense knowledge base;information retrieval;resource description framework;semantic Web","","","","24","","","5-7 April 2016","","IEEE","IEEE Conference Publications"
"Evaluation of Matching Accuracy of Template Matching Using a Steganography Algorithm","H. Ishizuka; K. Sakurai; I. Echizen; K. Iwamura","Fac. of Inf. Sci. & Electr. Engneering, Kyushu Univ., Fukuoka, Japan","2015 Third International Symposium on Computing and Networking (CANDAR)","20160303","2015","","","457","462","The steganography that we suggested uses the data that ties strongly to the characteristics of the original image as watermark information without changing the original image. Although similarity with zero-watermarking is pointed out, the major difference is that our method does not need pre-processing for a feature extraction at decoding. Since the decoding can do at high speed, besides using as the original steganography, it is possible to apply it to similar image retrieving, similar image order sorting or template matching. For example, when applying it to medical images, a doctor is possible to pick some similar images from a medical image database and refers the treatments through the images with concealing the information of the patient. This time, we have investigated the identification accuracy of our method with respect to the template matching of similar image searching.","","Electronic:978-1-4673-9797-1; POD:978-1-4673-9798-8","10.1109/CANDAR.2015.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7424757","Digital Watermarking;Low-Frequency Extraction;Steganography;Template Matching","Biomedical imaging;Brightness;Data mining;Decoding;Feature extraction;Information retrieval;Watermarking","decoding;feature extraction;image coding;image matching;image watermarking;steganography","decoding;feature extraction;steganography algorithm;template-matching;watermark information;zero-watermarking","","","","15","","","8-11 Dec. 2015","","IEEE","IEEE Conference Publications"
"An effective approach for relevant paragraph retrieval in Question Answering systems","M. M. Hoque; P. Quaresma","Department of Informatics, University of Evora, Evora, Portugal","2015 18th International Conference on Computer and Information Technology (ICCIT)","20160609","2015","","","44","49","Paragraph retrieval is a substantial task in Question Answering (QA) systems. It represents the extraction of data from a huge data collection, which have the probability to contain an answer to a question and is a significantly important intermediary step between a user of a QA system and the answers. It is barely impossible to analyse a large collection of data extensively in quick time and because of the vast nature of the background data, it is very important to narrow down the search space from where an answer can be looked for. In this paper, we address the information extraction step and present an effectively designed model for the relevant paragraph retrieval, which is efficient in terms of execution time as well. The model deals with the structure and the organization of information, performs an in-depth analysis of user's question, and presents a priority based searching capability for retrieving paragraphs according to the necessity which will be both effective and time efficient. Experiments were carried out and compared against similar systems based on the data of the document retrieval task of Text REtrieval Conference (TREC) 2005. We also tested our methodology against the data set from TREC 2007. The system performance was measured in terms of various parameters such as R-precision, Recall, and Mean Average Precision. A satisfactory result achieved by our approach establishes its competency for getting integrated into any realtime QA systems.","","Electronic:978-1-4673-9930-2; POD:978-1-4673-9931-9","10.1109/ICCITechn.2015.7488040","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488040","","Context;Indexes;Information retrieval;Knowledge discovery;Organizations;Semantics;Syntactics","natural language processing;question answering (information retrieval);text analysis","QA system;data extraction;information extraction;natural language processing;paragraph retrieval;question answering system","","","","22","","","21-23 Dec. 2015","","IEEE","IEEE Conference Publications"
"Application of Image Classification Technology in Mangrove Information Extraction","S. Wu; L. Dong; H. Chen; J. Zhan","Coll. of Inf. Sci. & Technol., Hainan Normal Univ., Haikou, China","2015 11th International Conference on Computational Intelligence and Security (CIS)","20160204","2015","","","167","170","As the particularity of the mangrove ecosystem, it is difficult to do field investigation, which consumes much time, and the obtained data is not real-time. The technology of satellite remote sensing plays an important role in the detection of the ecological system. Some methods, such as band combination, expert classification and fuzzy classification, have been widely used in recent years. This paper explores the application of the image classification technology in mangrove information extraction based on RGB color only, and discusses the method of parallelepiped and Mean shift clustering respectively. According to the different image characteristics, the image classification technology based on RGB color only can distinguish the different research contents in remote sensing image, which uses a computer to analyze the target object quantitatively and classifies each image pixel or region into several categories.","","Electronic:978-1-4673-8660-9; POD:978-1-4673-8661-6","10.1109/CIS.2015.48","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396278","Meanshift clustering method;image classification technology;parallelepiped method;satellite remote sensing","Classification algorithms;Image classification;Image color analysis;Information retrieval;Remote sensing;Rivers;Satellites","geophysical image processing;image classification;image colour analysis;remote sensing","RGB color;band combination;expert classification;fuzzy classification;image classification technology;mangrove ecosystem;mangrove information extraction;mean shift clustering;satellite remote sensing","","","","13","","","19-20 Dec. 2015","","IEEE","IEEE Conference Publications"
"Benchmarking of Scientific Research Clusters by Use of Text Mining Algorithms on Textual Artefacts","S. Schröeder; C. Tummel; I. Isenhardt; S. Jeschke; A. Richert","Center for Learning & Knowledge Manage., RWTH Aachen Univ., Aachen, Germany","2016 International Conference on Information Systems Engineering (ICISE)","20160609","2016","","","22","28","The development of text mining algorithms is far reaching and thus, new application areas arise, inter alia in the framework of applying modern information technologies (e.g. big data analytics) in order to answer social and economic (research) questions (digital humanities). In this paper existing algorithms to reveal results in the context of benchmarking scientific research clusters are combined. For conducting the analysis, two data corpora are gathered. The first data corpus contains abstracts of publications according to the used application case of the Cluster of Excellence Tailor-Made Fuels from Biomass 'TMFB' at RWTH Aachen University. The second data corpus -- based on the analysis of the first data corpus -- contains content-related abstracts of research belonging to the relevant scientific community. In the framework of processing the gathered data, information extraction, information retrieval and named entity recognition among others are utilized for analyzing the publications in order to derive useful findings. Furthermore association mining findings are introduced. As an excerpt of the results, the identification of thematic alignments and developments over time are presented for both corpora and are subsequently compared.","","Electronic:978-1-5090-2288-5; POD:978-1-5090-2289-2","10.1109/ICISE.2016.9","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486208","application;benchmarking;information extraction;named entity recognition;text mining","Benchmark testing;Biomass;Clustering algorithms;Fuels;Information retrieval;Text mining","data analysis;data mining;information retrieval;pattern clustering;text analysis","RWTH Aachen University;association mining;biomass TMFB;cluster-of-excellence tailor-made fuels;content-related abstracts;data analysis;data corpora;information extraction;information retrieval;named entity recognition;scientific research cluster benchmarking;text mining algorithms;textual artefacts","","","","37","","","20-22 April 2016","","IEEE","IEEE Conference Publications"
"Pattern-based rule disambiguation","Jie Zheng; Gang Cheng; Shoushan Li; Fang Kong; C. R. Huang; Guodong Zhou","Natural Language Processing Lab, Soochow University, Suzhou, China","2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)","20160114","2015","","","1444","1449","The biggest challenges to rules-based approaches to Natural Language Processing (NLP) are the resources required to do an exhaustive search for rule-matching, and the decision to select the optimal rule when there are multiple possible matches. In this paper, we propose a novel approach named pattern-based rule disambiguation (PRD) to face these challenges. PRD helps to determine which rule is activated by a pattern when the pattern activates more than one rule. To tackle this task, we first collect and annotate the samples following the same pattern, but activating different rules; Then, we leverage the corpus to train a statistic classifier to disambiguate the pattern. This new approach is applied to the task of emotion cause detection, adopting a linguistic rule-drive paradigm which was the only one available for this task. The experimental results demonstrated the effectiveness of our PRD approach and offered a promising solution of the resolution of multiple-matched rules challenge for future NLP tasks.","","CD-ROM:978-1-4673-7681-5; Electronic:978-1-4673-7682-2; POD:978-1-4673-7683-9","10.1109/FSKD.2015.7382156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382156","Natural Language Processing;Pattern-based approach;Rules-based approaches","Context;Data collection;Feature extraction;Information retrieval;Natural language processing;Pattern matching;Pragmatics","natural language processing;pattern classification","NLP tasks;exhaustive search;linguistic rule-drive paradigm;multiple-matched rules challenge;natural language processing;optimal rule;pattern-based rule disambiguation;rule-matching;statistic classifier","","","","19","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Retrieval of scene image and video frames using date field spotting","P. P. Roy; A. Das; D. Majhi; U. Pal","Dept. of CSE, IIT Roorkee, India","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","20160609","2015","","","705","709","In this paper, we present a date spotting based information retrieval system for natural scene image and video frames where text appears with complex backgrounds. Text retrieval in such scene/video frames is difficult because of blur, low resolution, background noise, etc. In our proposed framework, a line based date spotting approach using Hidden Markov Model is used to detect the date information in text. Given a text line image, we apply an efficient Bayesian classifier based binarization approach to extract the text components. Next, Pyramid Histogram of Oriented Gradient (PHOG) feature is computed from the binarized image for date-spotting framework. For our experiment, three different date models have been constructed to search similar date information in scene/video text. When tested in a custom dataset of 1104 text lines, our date spotting approach provided encouraging results.","","Electronic:978-1-4799-6100-9; POD:978-1-4799-6101-6; USB:978-1-4799-6099-6","10.1109/ACPR.2015.7486594","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486594","","Bayes methods;Decision support systems;Hidden Markov models;Image resolution;Information retrieval;Noise measurement;Pattern recognition","Bayes methods;hidden Markov models;image classification;information retrieval;text detection;video retrieval;video signal processing","Bayesian classifier based binarization approach;PHOG feature;complex text background;date field spotting;date information detection;date spotting based information retrieval system;hidden Markov model;line based date spotting approach;natural scene image retrieval;pyramid histogram of oriented gradient feature;text component extraction;text line image;text retrieval;video frame retrieval","","","","15","","","3-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Multimodal Ensemble Fusion for Disambiguation and Retrieval","Y. Peng; X. Zhou; D. Z. Wang; I. Patwa; D. Gong; C. Fang","University of Florida, Gainesville","IEEE MultiMedia","","2016","PP","99","1","1","In this paper, we first identify the correlative and complementary relations among multiple modalities. Then we propose a multimodal ensemble fusion model to capture the complementary relation and correlative relation between two modalities (images and text) and explain why this ensemble fusion model works. Experimental results on the UIUC-ISD dataset and the Google-MM dataset show our ensemble fusion model outperforms approaches using only single modality for disambiguation and retrieval. Word sense disambiguation and information retrieval are the use cases we studied to demonstrate the effectiveness of our ensemble fusion model.","1070-986X;1070986X","","10.1109/MMUL.2016.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452494","","Fuses;Indexing;Information retrieval;Multimedia communication;Semantics;Streaming media;Visualization","","","","","","","","20160414","","","IEEE","IEEE Early Access Articles"
"Programming to Achieve the Reception Extraction and Translation of GPS Positioning Information","M. Shao; X. Yang","Sch. of Maritime, Shandong Jiaotong Univ., Weihai, China","2015 International Conference on Computer Science and Mechanical Automation (CSMA)","20160107","2015","","","226","230","There is a large amount of information within every group of the received GPS positioning information, Users only need to extract and translate the useful information for them, this can be achieved by computer programs. The GPS information extraction follows the NMEA0183 standard, this paper writes a simple MFC dialog-based program based on the serial communications and GPS communication protocols. The program can be used to extract and translate GPS positioning information, store the positioning information and calculate the differential correction and then present the positioning results to users. This paper describes the function of each part and implementation methods of the program.","","Electronic:978-1-4673-9166-5; POD:978-1-4673-9167-2","10.1109/CSMA.2015.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371656","Programming;extraction and translation of GPS positioning information;serial communication","Data mining;Global Positioning System;Information retrieval;Ports (Computers);Standards;User interfaces;Yttrium","Global Positioning System;protocols","GPS positioning information;MFC dialog-based program;NMEA0183 standard;computer program;information extraction;reception extraction;reception translation;serial communication","","","","4","","","23-25 Oct. 2015","","IEEE","IEEE Conference Publications"
"High Spatial Resolution Remote Sensing Data Computing Pattern Based on Feature Primitives","M. Dongping; Z. Wen; T. Tian; S. Zhanfeng; L. Jiancheng","Sch. of Inf. Eng., China Univ. of Geosci. (Beijing), Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","20160721","2015","","","468","475","With the developments in satellite sensor technology, data acquisition technology developed rapidly and with the start of a series of space-based observation network for Earth science, such as EOS, GTOS, ECOS, GOOS and etc., high performance processing and analysis of tremendous data becomes the bottleneck faced by earth observation. According to the differences of the computational behavior and the computing emphasis, this paper divides high spatial resolution remote sensing data computation into two classes: deep-computation and active-computation. Deep-computation (from data to features) is to extract the feature primitives through certain methods, so deep-computation emphasizes particularly on computing amount. Active-computation (from features to knowledge) is based on the feature primitives obtained by deep-computation. Firstly the spatial relationships between the feature primitives are computed, then the decisions can be made effectively and efficiently with domain knowledge and domain models through web services, so deep-computation emphasizes particularly on intelligence of computation. Finally, by using the distributed computing technique, a case study of information extraction and target recognition from remote sensing image based on feature primitives was given to illustrate and testify the ideas mentioned above. Experimental results shows that the high spatial resolution remote sensing data computation pattern based on feature primitives is feasible and it is practically meaningful to resolve the problem of huge geo-spatial data computation.","","Electronic:978-1-4673-7211-4; POD:978-1-4673-7212-1","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.98","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518277","computing pattern;feature primitive;high spatial resolution;remote sensing","Atmospheric modeling;Data models;Feature extraction;Information retrieval;Remote sensing;Spatial databases;Spatial resolution","Web services;data acquisition;data analysis;feature extraction;geophysics computing;remote sensing","Earth observation;Earth science;Web services;active-computation;computation intelligence;computational behavior;data acquisition technology;data analysis;deep-computation;distributed computing technique;domain knowledge;domain models;feature primitive extraction;geo-spatial data computation;high performance data processing;high spatial resolution remote sensing data computing pattern;information extraction;satellite sensor technology;space-based observation network;target recognition","","","","","","","10-14 Aug. 2015","","IEEE","IEEE Conference Publications"
"Text-Based Event Temporal Resolution and Reasoning for Information Analytics in Big Data","J. Zhang; C. Yao; P. Qu; Y. Sun","Inst. for Sci. & Tech. Inf. of China, Beijing, China","2015 International Conference on Identification, Information, and Knowledge in the Internet of Things (IIKI)","20160310","2015","","","78","81","Events formulate the world of human being and could be regarded as the semantic units in different granularities for information organization. Extracting events and temporal information from texts plays an important role for information analytics in big data. This paper surveys research work on text-based event temporal resolution and reasoning including the identification of events, temporal information resolutions of events, the rule-based temporal relation reasoning between events and the temporal representations. We point out the shortcomings of existing research work and the future trends for advancing the identification of events and the establishing/reasoning of temporal relations in the future.","","Electronic:978-1-4673-8637-1; POD:978-1-4673-8638-8","10.1109/IIKI.2015.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428328","Big Data;Event;Information Analytics;Reasoning;Resolution;Temporal Information","Big data;Cognition;Data mining;Information retrieval;Natural languages;Organizations;Semantics","Big Data;inference mechanisms;knowledge based systems","big data;information analytics;rule-based temporal relation reasoning;semantic units;temporal representation;text-based event temporal resolution","","1","","46","","","22-23 Oct. 2015","","IEEE","IEEE Conference Publications"
"Identifying categories of zones in scientific papers based on lexical and syntactical features","N. Asadi; K. Badie; M. T. Mahmoudi","Knowledge Management & E-Organization Group, IT Research Faculty, ICT Research Institute, Tehran, Iran","2016 Second International Conference on Web Research (ICWR)","20160623","2016","","","177","182","Scientific papers are continually increasing on the web and it is mandatory for the researchers to grasp on some powerful tools which are helpful in an efficient process of large amounts of data. Zone identification is a Natural Language Processing application which is to classify the sentences of scientific papers into a fixed set of zone categories. In this paper, we will propose an algorithm to identify some categories of zones in scientific papers. Regarding this, we make use of some significant lexical and syntactical features of the sentences standing for these categories in a particular way. In this respect, a sequence of sentences has been used. Experimental results show that these features are capable enough to identify the desired categories in a reasonable manner.","","Electronic:978-1-5090-2166-6; POD:978-1-5090-2167-3","10.1109/ICWR.2016.7498464","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498464","Lexical Features;Scientific paper;Sentence Classification;Support Vector Machines;Syntactical Features;Zone Identification;Zone category","Algorithm design and analysis;Classification algorithms;Data mining;Information retrieval;Supervised learning;Support vector machines;Training data","Internet;feature extraction;natural language processing;pattern classification;scientific information systems;text analysis","lexical features;natural language processing;scientific paper classification;syntactical features;zone category identification","","","","15","","","27-28 April 2016","","IEEE","IEEE Conference Publications"
"Interactive Access to LP DAAC Satellite Data Archives Through a Combination of Open-Source and Custom Middleware Web Services","B. N. Davis; J. Werpy; A. Friesz; K. Impecoven; R. L. Quenzer; T. Maiersperger; D. J. Meyer","Science Division, USGS EROS Center, Sioux Falls, South Dakota 57198-0001 United States","IEEE Geoscience and Remote Sensing Magazine","20160202","2015","3","4","8","20","Current methods of searching for and retrieving data from satellite land remote sensing archives do not allow for interactive information extraction. Instead, Earth science data users are required to download files over low-bandwidth networks to local workstations and process data before science questions can be addressed. New methods of extracting information from data archives need to become more interactive to meet user demands for deriving increasingly complex information from rapidly expanding archives. Moving the tools required for processing data to computer systems of data providers, and away from systems of the data consumer, can improve turnaround times for data processing workflows. The implementation of middleware services was used to provide interactive access to archive data. The goal of this middleware services development is to enable Earth science data users to access remote sensing archives for immediate answers to science questions instead of links to large volumes of data to download and process. Exposing data and metadata to web-based services enables machine-driven queries and data interaction. Also, product quality information can be integrated to enable additional filtering and sub-setting. Only the reduced content required to complete an analysis is then transferred to the user.","2473-2397;24732397","","10.1109/MGRS.2015.2505999","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7370024","","Data archiving;Data mining;Data models;Geospatial analysis;Global Positioning System;Information retrieval;Interactive systems;MODIS;Middleware;Open source software;Remote sensing;Satellite communication;Web services","Web services;aerospace computing;artificial satellites;middleware;remote sensing","Earth science data user;LP DAAC satellite data;custom middleware Web service;data consumer;data processing;data provider;data retrieval;interactive information extraction;machine-driven query;metadata;open-source Web service;satellite land remote sensing","","","","11","","","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"Measurement of learning motivation in electronic learning","C. Juliane; A. A. Arman; H. S. Sastramihardja; I. Supriana","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2015 International Conference on Information Technology Systems and Innovation (ICITSI)","20160324","2015","","","1","6","The success of electronic learning (e-learning) process is highly determined by the extent of learning motivation that could be identified and measured through a number of methods. This study was designed to identify several attributes used to measure learning motivation in the context of e-learning as a chance of novelty in the domain of information extraction. The method by distributing questionnaires to 214 potential respondents with an interest in the field of study such as the e-learning facilitators including teachers, lecturers, instructors, laboratory assistants accustomed to assess or measure the participant's learning motivation. The demography of respondents represents eight major islands in Indonesia, including Java, Sumatra, Kalimantan, Sulawesi, Papua, Bali, Nusa Tenggara, and Maluku with an expectation that the results of this study can represent a facilitator assessment towards e-learning Indonesia. A total of 1419 data has been collected and processed using Chi-Square method to test the hypotheses regarding the homogeneity of the respondents in choosing the candidates measurement of attribute in learning motivation. From the data processing, it can be concluded that the attributes of velocity, quantity, and relevancy can be further used as the variables in both the measurement of learning motivation in e-learning context and the field of information extraction.","","Electronic:978-1-4673-6664-9; POD:978-1-4673-6665-6","10.1109/ICITSI.2015.7437732","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437732","chi square;motivation measurement;quantity;relevancy;velocity","Context;Current measurement;Data processing;Electronic learning;Frequency measurement;Information retrieval;Velocity measurement","computer aided instruction","Bali;Java;Kalimantan;Maluku;Nusa Tenggara;Papua;Sulawesi;Sumatra;candidates measurement;chi-square method;data processing;demography;e-learning context;e-learning facilitators;e-learning process;electronic learning process;facilitator assessment;information extraction;instructors;laboratory assistants;lecturers;participant learning motivation;teachers","","","","12","","","16-19 Nov. 2015","","IEEE","IEEE Conference Publications"
"Fast Parallel Mining of Maximally Informative k-Itemsets in Big Data","S. Salah; R. Akbarinia; F. Masseglia","LIRMM, Univ. of Montpellier, Montpellier, France","2015 IEEE International Conference on Data Mining","20160107","2015","","","359","368","The discovery of informative itemsets is a fundamental building block in data analytics and information retrieval. While the problem has been widely studied, only few solutions scale. This is particularly the case when i) the data set is massive, calling for large-scale distribution, and/or ii) the length k of the informative itemset to be discovered is high. In this paper, we address the problem of parallel mining of maximally informative k-itemsets (miki) based on joint entropy. We propose PHIKS (Parallel Highly Informative K-ItemSet) a highly scalable, parallel miki mining algorithm. PHIKS renders the mining process of large scale databases (up to terabytes of data) succinct and effective. Its mining process is made up of only two efficient parallel jobs. With PHIKS, we provide a set of significant optimizations for calculating the joint entropies of miki having different sizes, which drastically reduces the execution time of the mining process. PHIKS has been extensively evaluated using massive real-world data sets. Our experimental results confirm the effectiveness of our proposal by the significant scale-up obtained with high itemsets length and over very large databases.","1550-4786;15504786","Electronic:978-1-4673-9504-5; POD:978-1-4673-9505-2","10.1109/ICDM.2015.86","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373340","informative itemsets;joint entropy;mapreduce;massive distribution;spark","Data mining;Entropy;Information retrieval;Itemsets;Optimization","Big Data;data analysis;data mining;entropy;parallel processing","PHIKS;big data;data analytics;information retrieval;large-scale distribution;maximally informative-itemset discovery;parallel maximally informative k-itemset mining;parallel miki mining algorithm;parallel mining process;real-world data sets","","","","28","","","14-17 Nov. 2015","","IEEE","IEEE Conference Publications"
"Boxcan: A platform realizing fast retrieval of parent-child tree of containers and inner objects over EPCIS events","Y. Sato; T. Sato; J. Mitsugi","Auto-ID Laboratory Japan at Keio University, 5322 Endo, Fujisawa, Kanagawa 252-0882 Japan","2015 21st Asia-Pacific Conference on Communications (APCC)","20160225","2015","","","692","696","This paper introduces ""Boxcan"", an information retrieval platform for aggregated objects in ID-based object management system based on GS1 EPCglobal architecture framework. Boxcan platform receives an EPC of a container, then provides a tree structure of current parent-child relationship between the EPCs of the container and its inner objects. This paper proposes the system design of Boxcan platform and its applications. The proposed system design is evaluated with a field test of a practical object management system including Boxcan platform. A technical challenge to realize this function of Boxcan platform, fast retrieval of current parent-child relationship of EPCs from EPCIS, is also discussed in this paper. The effectiveness of the caching mechanism of EPC's current parent-child tree, which is a solution to this technical problem, is also evaluated by an experiment in a comparison with direct querying to EPCIS.","","Electronic:978-4-8855-2301-4; POD:978-1-4673-6653-3","10.1109/APCC.2015.7412596","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412596","EPCIS;GS1 EPCglobal architecture framework;Internet of Things;Object management;Supply chain management","Containers;Databases;Information retrieval;Information systems;Radiofrequency identification;Real-time systems;System analysis and design","cache storage;information retrieval systems;product codes;trees (mathematics)","Boxcan platform;EPCIS;GS1 EPCglobal architecture framework;ID-based object management system;aggregated objects;caching mechanism;current parent-child relationship;direct querying;information retrieval platform;tree structure","","","","8","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"Semantic-Based Technology Trend Analysis","C. Yang; D. Zhu; G. Zhang","Sch. of Manage. & Econ., Beijing Inst. of Technol., Beijing, China","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","20160114","2015","","","222","228","Technology trend analysis offers a flexible instrument to understand both opportunity and competition for emerging technologies. Semantic information is used in Science, Technology & Innovation (ST&I) records which makes the technology trend analysis more challenging. This paper proposes a semantic-based approach for technology trend analysis through emphasizing Subject-Action-Object (SAO) structure, It also applies the trend analysis approach to extract technology information and identify and predict the trend of technology development more effectively. An empirical study on Graphene is completed to demonstrate the proposed trend analysis approach.","","Electronic:978-1-4673-9323-2; POD:978-1-4673-9324-9","10.1109/ISKE.2015.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383052","Semantic Analysis;Technology roadmapping;Text Mining;technology trend analysis","Data mining;Graphene;Information retrieval;Market research;Patents;Semantics;Transmission line measurements","data mining;graphene;information analysis","SAO structure;ST&I records;graphene;science technology & innovation records;semantic information;semantic-based technology trend analysis;subject-action-object structure","","","","21","","","24-27 Nov. 2015","","IEEE","IEEE Conference Publications"
"Identification and classification of relations for Indian languages using machine learning approaches for developing a domain specific ontology","B. Sinha; M. Garg; S. Chandra","Department of Electronics and Information Technology, New Delhi, India","2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)","20160718","2016","","","415","420","Information extraction and classification using Natural Language Processing techniques of layered architecture such as pre-processing task, processing of semantic analysis etc., helps in implementing further deeper evaluation techniques for the accuracy of natural language based electronic database. This paper explores relational information extraction of multilingual IndoWordNet database matching with domain specific terms. Further, extracted information are processed through conventional statistical methods, Normalized Web Distance (NWD) similarity method and two other machine learning evaluation techniques such as Support Vector Machine (SVM), Neural Network (NN) to compare with their accuracy. Results of machine leaning based techniques outperform with significant improved accuracy over conventional methods. The objective of using these techniques along with semantic web technology is to initiate a proof of concept for ontology generation by identification and classification of extracted relational information from IndoWordNet. This paper also highlights domain specific challenges and issues in developing relational model of ontology.","","Electronic:978-1-5090-0082-1; POD:978-1-5090-0083-8","10.1109/ICCTICT.2016.7514617","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514617","Deep neural network;IndoWordNet electronic database;Normalized web distance;Ontology;Relation classification;Semantic Web;Support vector machine","Databases;Information retrieval;Neural networks;Ontologies;Semantic Web;Semantics;Support vector machines","information retrieval;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence);semantic Web","Indian languages;domain specific ontology;information classification;machine learning evaluation techniques;multilingual IndoWordNet database matching;natural language based electronic database;natural language processing techniques;normalized Web distance similarity method;ontology generation;relational information extraction;semantic Web technology","","","","","","","11-13 March 2016","","IEEE","IEEE Conference Publications"
"A Named Entities Recognition System for Modern Standard Arabic using Rule-Based Approach","H. Elsayed; T. Elghazaly","Comput. & Inf. Sci. Dept., Cairo Univ., Cairo, Egypt","2015 First International Conference on Arabic Computational Linguistics (ACLing)","20160303","2015","","","51","54","Named Entity Recognition (NER) is a task in Information Extraction (IE). The Named Entity Recognition has become very important for Natural Language Processing (NLP). In this paper, we designed a system which enhanced the named entities recognition for Arabic language where the system was developed for Arabic nouns and entities extractions. The nouns extraction system is based on Arabic morphological, the Arabic grammar rules a lot of them are not used before. The noun extraction in the system uses no gazetteers and the system is combined with entities extraction system depending on gazetteers. The system extracts noun according to morphological Arabic and classify them into proper nouns entities, title entities, currency entities, percentage entities, countries entities, cities entities, nationality entities, number entities, places entities, date entities and time entities. The system applied algorithms for generate nationality entities from countries entities, and the system applied Regular Expression (RE) for extract numbers in digit format. The system is not needed to normalization into the text before extraction process. The system tested text that is in the Modern Standard Arabic (MSA), the corpus is in open text. The system achieves results in an average recall of 85%.","","Electronic:978-1-4673-9155-9; POD:978-1-4673-9156-6","10.1109/ACLing.2015.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422279","Corpus;Gazetteers;Information Extraction;Message Understanding Conference (MUC);Named Entities Recognition (NER)","Algorithm design and analysis;Cities and towns;Grammar;Information retrieval;Logic gates;Morphology;Standards","information retrieval;knowledge based systems;natural language processing;pattern classification","Arabic grammar rules;Arabic language;Arabic morphology;IE;MSA;RE;cities entities;classification;countries entities;currency entities;date entities;entities extraction system;gazetteers;information extraction;modern standard Arabic;named entities recognition system;nationality entities;nouns extraction system;number entities;percentage entities;places entities;proper nouns entities;regular expression;rule-based approach;time entities;title entities","","","","11","","","17-20 April 2015","","IEEE","IEEE Conference Publications"
"Configuration of dialogue agent with multiple knowledge sources","R. Jiang; R. E. Banchs; S. Kim; L. F. D'Haro; A. I. Niculescu; K. H. Yeo","Department of Human Language Technology, Institute for Infocomm Research, Singapore 138632","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","20160225","2015","","","840","849","Knowledge base is a key component of a dialogue agent which determines its usability, performance as well as intelligence. Usually one type of knowledge source is tailored towards specific application or task. One challenge for developing multiple-domain dialogue agents is how to represent the domain knowledge in their respective ways for the best efficiency and easy construction of queries from natural language. There are a variety of knowledge sources available for building up dialogue agents such as relational database, ontology, knowledge base (KB), search index, artificial intelligence markup language (AIML) and the World Wide Web, etc. In this paper, we present a systematic way of configuring dialogue agents supported by multiple knowledge sources. With the proposed dialogue framework, we provide a complete solution for the development of various dialogue systems with the support of different dialogue management techniques, multiple protocols for cloud-enabled dialogue services, pluggable infrastructure for component reuse and service enhancement, strong scripting functions in both XML and Python. We have constructed various dialogue agents with this framework. Results show that these agents are robust and the development cycle can be considerably reduced.","","Electronic:978-9-8814-7680-7; POD:978-1-4673-9593-9","10.1109/APSIPA.2015.7415390","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415390","","Information retrieval;Knowledge based systems;Natural languages;Ontologies;Personal digital assistants;Protocols;Resource description framework","XML;cloud computing;interactive systems;knowledge based systems;multi-agent systems;natural language interfaces;query processing;speech processing","KB;Python;XML;cloud-enabled dialogue service;dialogue management technique;knowledge base;knowledge source;multiple-domain dialogue agent;natural language;query construction;speech interface","","","","36","","","16-19 Dec. 2015","","IEEE","IEEE Conference Publications"
"Personalized Hotel Recommendation Using Text Mining and Mobile Browsing Tracking","K. P. Lin; C. Y. Lai; P. C. Chen; S. Y. Hwang","Dept. of Inf. Manage., Nat. Sun Yat-sen Univ., Kaohsiung, Taiwan","2015 IEEE International Conference on Systems, Man, and Cybernetics","20160114","2015","","","191","196","With the prevalence of mobile devices such as smartphones and tablets, the ways people access to the Internet have changed enormously. In addition to the information that can be recorded by traditional Web-based e-commerce like frequent online shopping stores and browsing histories, mobile devices are capable of tracking sophisticated browsing behavior. The aim of this study is to utilize users' browsing behavior of reading hotel reviews on mobile devices and subsequently apply text-mining techniques to construct user interest profiles to make personalized hotel recommendations. Specifically, we design and implement an app where the user can search hotels and browse hotel reviews, and every gesture the user has performed on the touch screen when reading the hotel reviews is recorded. We then identify the paragraphs of hotel reviews that a user has shown interests based on the gestures the user has performed. Text mining techniques are applied to construct the interest profile of the user according to the review content the user has seriously read. We collect more than 5,000 reviews of hotels in Taipei, the largest metropolitan area of Taiwan, and recruit 18 users to participate in the experiment. Experimental results demonstrate that the recommendations made by our system better match the user's hotel selections than previous approaches.","","Electronic:978-1-4799-8697-2; POD:978-1-4799-8698-9","10.1109/SMC.2015.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379178","recommendation;sentiment analysis;text mining","Context;Feature extraction;Filtering;Information retrieval;Mobile communication;Mobile handsets;Sentiment analysis","behavioural sciences;data mining;hotel industry;mobile computing;mobile radio;recommender systems;text analysis","Internet access;Taipei;Web-based e-commerce;browsing histories;hotel reviews;mobile browsing tracking;mobile devices;online shopping stores;personalized hotel recommendation;smartphones;tablets;text mining techniques;touch screen;user browsing behavior;user gestures;user interest profiles","","1","","35","","","9-12 Oct. 2015","","IEEE","IEEE Conference Publications"
"A review study of server log formats for efficient web mining","P. Sharma; S. Yadav; B. Bohra","Computer Science, Maharishi Arvind College of Engg. and Research Center, Jaipur, India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","20160114","2015","","","1373","1377","Now-a-days WWW is the most basic need in the globalized world. It consist the vast data of various fields around the globe. The web has incredible amount of information, which supplies a large, explosive, diverse, dynamic and mostly unstructured data stores. The combination of these web stores are known as the data warehouse and the process of retrieving the data from these warehouses is data mining. In this paper, we will discuss the classification of web data and its categorization. Here the awareness about the log files of web server is also provided along with its types. Server log files are basically the ASCII text files which contain the log record of users.","","Electronic:978-1-4673-7910-6; POD:978-1-4673-7911-3; USB:978-1-4673-7909-0","10.1109/ICGCIoT.2015.7380681","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380681","log files;server log formats;web data;web mining","Algorithm design and analysis;Information retrieval;Insects;Videos;Web mining","Internet;data mining;data warehouses;information retrieval;pattern classification;text analysis","ASCII text files;WWW;Web data categorization;Web data classification;Web server log files;data mining;data retrieval;data warehouse;efficient Web mining;log record;server log formats;unstructured data stores","","","","12","","","8-10 Oct. 2015","","IEEE","IEEE Conference Publications"
"Understanding news stories through SVO triplets","Zae Myung Kim; Y. S. Jeong; Ho-Jin Choi","School of Computing, KAIST, Korea","2016 International Conference on Big Data and Smart Computing (BigComp)","20160307","2016","","","498","501","With the exponential growth in data, we often find ourselves struggling to deal with information overload. Techniques such as timeline summarization tackle this problem by generating short summaries for each time stamp on a timeline. However, we argue that rather than reading a set of blocks of texts, it is easier and quicker for a reader to observe dynamically changing relations between important entities that are illustrated through graphs. It enables her/him to grasp the major events of the whole story quickly. We develop an experimental system to evaluate on the 2014 FIFA World Cup Brazil news articles, and show that the result covers major events of the World Cup and can be understood in a short span of time.","","Electronic:978-1-4673-8796-5; POD:978-1-4673-8797-2; USB:978-1-4673-8795-8","10.1109/BIGCOMP.2016.7425978","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425978","dynamic relation visualization;entitiy relation extraction;timeline summarization","Computational linguistics;Data mining;Feature extraction;Information retrieval;Optimization;Sparse matrices;Visualization","electronic publishing;natural language processing;text analysis","2014 FIFA World Cup Brazil news articles;SVO triplets;news stories understanding;subject-verb-object relations","","","","10","","","18-20 Jan. 2016","","IEEE","IEEE Conference Publications"
"The Symptoms and Pathogenesis Entity Recognition of TCM Medical Records Based on CRF","L. Honglan; Q. Xiaona; F. Bin","Beijing Key Lab. of Knowledge Eng. for Mater. Sci., Beijing, China","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","20160721","2015","","","1479","1484","TCM (Traditional Chinese Medicine) medical records are the great medical wealth of the Chinese nation. Since 1980s, China has begun to attach importance to the heritage of TCM. That how to effectively and maximize use these valuable resources is a problem for the TCM informationization. However, the dialectical information that includes the core idea of the famous doctors is still stored in the form of natural language. Obtaining structural information must rely on information extraction technology. With the development of science and technology information, the symptoms and pathogenesis entity recognition of TCM medical records is the key to build the TCM information extraction system. Conditional Random Fields (CRF) proposed by Lafferty et al in 2001, which combines the features of the Maximum Entropy Model and Hidden Markov Model. In recent years, it achieved good results in word segmentation, part of speech tagging and named entity recognition sequence labeling tasks [1]. This paper use the latest CRFidea, formulate appropriate feature template, using 500 marked TCM medical records from""11th five-year plan"" medical records database to train the CRF model that suit for the symptoms and pathogenesis entity recognition. For verifying the model accuracy, we use this CRF model to label the symptoms and pathogenesis entity. After ten-fold cross validation, its symptoms entity F1 measure reached 81.53%, the pathogenesis entity F1 measure was 83.98%. Experimental results show that its performance is very high, it is suitable for the identification of TCM medical records information extraction.","","Electronic:978-1-4673-7211-4; POD:978-1-4673-7212-1","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.267","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518446","CRF;TCM;entity recognition;information extraction;pathogenesis;symptoms","Entropy;Hidden Markov models;Information retrieval;Medical diagnostic imaging;Parameter estimation;Speech;Training","database management systems;electronic health records;graph theory","CRF;TCM information extraction system;TCM informationization;TCM medical records;Traditional Chinese Medicine;conditional random fields;dialectical information;feature template;hidden Markov model;maximum entropy model;medical record database;named entity recognition sequence labeling tasks;natural language;part-of-speech tagging;pathogenesis entity F1 measure;pathogenesis entity recognition;structural information;symptom recognition;ten-fold cross validation;word segmentation","","","","","","","10-14 Aug. 2015","","IEEE","IEEE Conference Publications"
"Scaling up truth discovery","L. Berti-Équille","Qatar Computing Research Institute, Hamad bin Khalifa University, Doha, Qatar","2016 IEEE 32nd International Conference on Data Engineering (ICDE)","20160623","2016","","","1418","1419","The evolution of the Web from a technology platform to a social ecosystem has resulted in unprecedented data volumes being continuously generated, exchanged, and consumed. User-generated content on the Web is massive, highly dynamic, and characterized by a combination of factual data and opinion data. False information, rumors, and fake contents can be easily spread across multiple sources, making it hard to distinguish between what is true and what is not. Truth discovery (also known as fact-checking) has recently gained lot of interest from Data Science communities. This tutorial will attempt to cover recent work on truth-finding and how it can scale Big Data. We will provide a broad overview with new insights, highlighting the progress made on truth discovery from information extraction, data and knowledge fusion, as well as modeling of misinformation dynamics in social networks. We will review in details current models, algorithms, and techniques proposed by various research communities whose contributions converge towards the same goal of estimating the veracity of data in a dynamic world. Our aim is to bridge theory and practice and introduce recent work from diverse disciplines to database people to be better equipped for addressing the challenges of truth discovery in Big Data.","","Electronic:978-1-5090-2020-1","10.1109/ICDE.2016.7498359","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498359","","Big data;Computational modeling;Data models;Heuristic algorithms;Information retrieval;Knowledge engineering;Tutorials","","","","","","7","","","16-20 May 2016","","IEEE","IEEE Conference Publications"
"Research on Feature Weights of Liheci Word Sense Disambiguation","Z. Zhang; X. Li; X. Tian","Coll. of Comput. Sci. & Technol., Hebei Univ., Baoding, China","2015 8th International Symposium on Computational Intelligence and Design (ISCID)","20160512","2015","2","","7","10","Concerning the problems of the comparative translation is not accurate in machine translation and the useful information is unable to match in information retrieval, a liheci word sense disambiguation method is adopted and a classifier model is established using Support Vector Machines(SVM). To improve the accuracy of the liheci word sense disambiguation, it extracts not only local words(LW), local part-of-speeches(POS), local words and part-of-speeches (LWP) but also the middle insert part of the separated form as disambiguation features according to the characteristics of liheci, When the text features are converted to feature vectors for improving on boolean weight method, we can fix feature weights of some type in turn and change the other two types' to verify the disambiguation effect of three kinds of features, respectively. The results show that the effect of LW, LWP is higher than POS. Setting higher feature weights to LW and LWP, the disambiguation accuracy can effectively improve.","","Electronic:978-1-4673-9587-8; POD:978-1-4673-9588-5","10.1109/ISCID.2015.221","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469048","Liheci;SVM;calssifier;feature weights;word sense disambiguation","Context;Feature extraction;Information retrieval;Kernel;Knowledge based systems;Support vector machines","Boolean algebra;information retrieval;language translation;natural language processing;support vector machines;text analysis","LW;LWP;Liheci word sense disambiguation;POS;SVM;boolean weight method;classifier model;feature vectors;information retrieval;local word-and-part-of-speeches;machine translation;support vector machines;text features","","","","14","","","12-13 Dec. 2015","","IEEE","IEEE Conference Publications"
"Ontology-Based Information Extraction for Knowledge Enrichment and Validation","D. H. Fudholi; W. Rahayu; E. Pardede","Dept. of Comput. Sci. & IT, La Trobe Univ., Melbourne, VIC, Australia","2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)","20160523","2016","","","1116","1123","Ontology is widely used as a mean to represent and share common concepts and knowledge from a particular domain or specialisation. As a knowledge representation, the knowledge within an ontology must be able to evolve along with the recent changes and updates within the community practice. In this paper, we propose a new Ontology-based Information Extraction (OBIE) system that extends existing systems in order to enrich and validate an ontology. Our model enables the ontology to find related recent knowledge in the domain from communities, by exploiting their underlying knowledge as keywords. The knowledge extraction process uses ontology-based and pattern-based information extraction technique. Not only the extracted knowledge enriches the ontology, it also validates contradictory instance-related statements within the ontology that is no longer relevant to recent practices. We determine a confidence value during the enrichment and validation process to ensure the stability of the enriched ontology. We implement the model and present a case study in herbal medicine domain. The result of the enrichment and validation process shows promising results. Moreover, we analyse how our proposed model contributes to the achievement of a richer and stable ontology.","1550-445X;1550445X","Electronic:978-1-5090-1858-1; POD:978-1-5090-1859-8","10.1109/AINA.2016.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7474215","information extraction;knowledge enrichment;knowledge validation;ontology","Feature extraction;Information retrieval;Microwave integrated circuits;Ontologies;Semantics;Stability analysis","knowledge acquisition;ontologies (artificial intelligence)","OBIE system;confidence value;contradictory instance-related statements;herbal medicine domain;knowledge enrichment;knowledge extraction process;knowledge representation;knowledge validation;ontology-based information extraction;pattern-based information extraction technique","","","","24","","","23-25 March 2016","","IEEE","IEEE Conference Publications"
"Analysis of standard clustering algorithms for grouping MEDLINE abstracts into evidence-based medicine intervention categories","V. Dobrynin; Y. Balykina; M. Kamalov","St. Petersburg State University, 7/9 Universitetskaya nab., 199034, Russia","2015 International Conference "Stability and Control Processes" in Memory of V.I. Zubov (SCP)","20151203","2015","","","555","557","The paper describes a process of clustering of article abstracts, taken from the largest bibliographic life sciences and biomedical information MEDLINE database into categories that correspond to types of medical interventions - types of patient treatments. Experiments were carried out to evaluate the quality of clustering for the following algorithms: K-means; K-means++; Hierarchical clustering, SIB (Sequential information bottleneck) together with the LSA (Latent Semantic Analysis) methods and MI (Mutual Information) which allow selecting feature vectors. Best results of clustering were achieved by K-means++ together with LSA then 210-dimensional space was chosen: Purity = 0.5719, Entropy = 1.3841, Normalized Entropy = 0.6299.","","Electronic:978-1-4673-7698-3; POD:978-1-4673-7699-0","10.1109/SCP.2015.7342223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342223","","Algorithm design and analysis;Clustering algorithms;Entropy;Information retrieval;Libraries;Mutual information;Semantics","database management systems;information retrieval;medical computing;patient treatment;pattern clustering;vectors","LSA;MEDLINE abstract grouping;MEDLINE database;MI;evidence-based medicine intervention category;feature vector;k-means++ clustering algorithm;latent semantic analysis;mutual information;patient treatment","","","","11","","","5-9 Oct. 2015","","IEEE","IEEE Conference Publications"
"Near-infrared spectrum discriminant analysis based on information extraction by using the elastic net","Xuhua Liu; Wanhui Chen; Yi Zheng; L. Zhang; Xiongkui He; Shungeng Min","Department of Mathematics, China Agricultural University, Beijing 100193, China","2015 11th International Conference on Natural Computation (ICNC)","20160111","2015","","","759","763","Elastic net method combines the merits of ridge regression and Lasso method. It reduces model prediction error by variable selection while not over-shrinking regression coefficients. In this paper, we take advantages of the elastic net's good properties of variable selection and simultaneous parameter estimation to select the important principal components, then establish discriminant model and apply it to near-infrared spectroscopy quantitative analysis. In the real data set analysis, 103 rhubarb samples were randomly split into two groups, one is viewed as training set which contains 35 samples, another group is considered as testing set which contains 68 samples. All of the samples' protein contents are measured by the national standard Kjeldahl method and the data were called chemical values. In order to testify feasibility and stability of the method, the training set and testing set were conducted random split and analyzed for ten times, respectively. According to these predicting results, the maximum number of false positives was 10, the minimum number of false positives is 5, and average false positive rate is 11.76%. These results showed a significant improvement compared to the results which derived by using ordinary principal component method directly.","","CD-ROM:978-1-4673-7678-5; Electronic:978-1-4673-7679-2; POD:978-1-4673-7680-8","10.1109/ICNC.2015.7378086","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378086","Chemometrics;Discriminate Analysis;Elastic net;Near-Infrared spectroscopy","Analytical models;Information retrieval;Input variables;Predictive models;Principal component analysis;Spectroscopy;Testing","feature extraction;infrared spectroscopy;parameter estimation;principal component analysis;regression analysis","Kjeldahl method;Lasso method;elastic net method;information extraction;model prediction error;near-infrared spectroscopy quantitative analysis;near-infrared spectrum discriminant analysis;parameter estimation;principal component method;regression coefficients;ridge regression","","","","10","","","15-17 Aug. 2015","","IEEE","IEEE Conference Publications"
"Survey on research paper's relations","Y. Sibaroni; D. H. Widyantoro; M. L. Khodra","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2015 International Conference on Information Technology Systems and Innovation (ICITSI)","20160324","2015","","","1","6","This paper discusses research studies related to formation of relations among research papers. From the analysis that has been conducted to some references, it can be concluded that there are two types of research paper's relation i.e. a direct relation and indirect relation. The direct relation has the good potential to be further developed than indirect relation. There are three approaches in this relation type i.e. content-based approach, citation analysis approach and citation context approach. The content-based approach cannot accommodate the special characteristics of research papers like citation and author. This approach only suitable implemented in regular documents and only able to identify similarity relations. Specific characteristics such as the citation and the authors can be handled by using citation analysis approach. Focus of the citation analysis approached is increasing the accuracy of the paper relation, where the results obtained so far is good. This approach can identify more diverse relations types such as relations based on citation or author. However, this approach failed to identify relationships between paper based on the meaning contained in text. The relations based on the meaning contained in a text can only be conducted by using citation context approach. Research related to the classification of citation sentences has the big potential to construct the relation based on meaning contained in a text. The construction process of the relation network based on citation context is also proposed in this paper. This process is important in the utilization of the relations in various fields of research. Finally, we also present research opportunity and state of the art research in three main areas, i.e. the recommender system, retrieval system and research map visualization.","","Electronic:978-1-4673-6664-9; POD:978-1-4673-6665-6","10.1109/ICITSI.2015.7437741","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7437741","Citation Context;Relation;Research Paper","Citation analysis;Context;Couplings;Information retrieval;Media","citation analysis;classification;text analysis","citation analysis;citation context;citation sentences;classification;content-based approach;direct relation;recommender system;regular documents;relation construction process;research map visualization;research paper relations formation;retrieval system;similarity relations;text meaning","","","","34","","","16-19 Nov. 2015","","IEEE","IEEE Conference Publications"
"Learning in Variable-Dimensional Spaces","M. Diligenti; M. Gori; C. Saccà","Department of Information Engineering and Mathematics, University of Siena, Siena, Italy","IEEE Transactions on Neural Networks and Learning Systems","20160516","2016","27","6","1322","1332","This paper proposes a unified approach to learning in environments in which patterns can be represented in variable-dimension domains, which nicely includes the case in which there are missing features. The proposal is based on the representation of the environment by pointwise constraints that are shown to model naturally pattern relationships that come out in problems of information retrieval, computer vision, and related fields. The given interpretation of learning leads to capturing the truly different aspects of similarity coming from the content at different dimensions and the pattern links. It turns out that functions that process real-valued features and functions that operate on symbolic entities are learned within a unified framework of regularization that can also be expressed using the kernel machines mathematical and algorithmic apparatus. Interestingly, in the extreme cases in which only the content or only the links are available, our theory returns classic kernel machines or graph regularization, respectively. We show experimental results that provide clear evidence of the remarkable improvements that are obtained when both types of similarities are exploited on artificial and real-world benchmarks.","2162-237X;2162237X","","10.1109/TNNLS.2015.2497275","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7394191","Content-based similarity;kernel methods;learning in dimensional-variable spaces;link-based similarity;regularization operators;regularization operators.","Coherence;Computer vision;Indexes;Information retrieval;Intelligent agents;Kernel;Learning systems","feature extraction;graph theory;learning (artificial intelligence)","algorithmic apparatus;artificial benchmarks;environment representation;graph regularization;kernel machines mathematical apparatus;learning;pattern links;pattern relationships;pointwise constraints;real-valued feature processing;real-world benchmarks;regularization unified framework;symbolic entities;unified approach;variable-dimension domains;variable-dimensional spaces","","","","27","","20160128","June 2016","","IEEE","IEEE Journals & Magazines"
"An Improved Ontology-Based Web Information Extraction","J. Zhang; W. Z. Ding","Modern Educ. Technol. Center, Nan-Tong Univ., Nan-Tong, China","2015 International Conference of Educational Innovation through Technology (EITT)","20160404","2015","","","37","41","Since most of the Web information is extracted according to content, this research introduce Webpage segmentation into the stage of Webpage pretreatment, by analyzing the ontology-based Web information extraction technology. First, get the extraction region out of the Webpage by Webpage segmentation, second process it according to ontology extraction rules, finally get the information needed. The extraction algorithm promoted by this research have higher precision and recall rate, which will lead to good prospects in practical applications.","","Electronic:978-1-4673-8038-6; POD:978-1-4673-8039-3","10.1109/EITT.2015.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446143","Information extraction;Ontology Extraction rules;Precision;Webpage Segmentation;Webpage segmentation","Algorithm design and analysis;Data mining;Databases;Generators;HTML;Information retrieval;Ontologies","Web sites;ontologies (artificial intelligence)","Webpage pretreatment;Webpage segmentation;ontology extraction rules;ontology-based Web information extraction","","","","31","","","16-18 Oct. 2015","","IEEE","IEEE Conference Publications"
"Integrating association mining into relevance feedback for biomedical literature search","Yanqing Ji; Hao Ying; J. Tran; P. Dews; R. M. Massanari","Department of Electrical and Computer Engineering, Gonzaga University, Spokane, Washington, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20151217","2015","","","531","536","Finding highly relevant articles from biomedical databases is challenging because it is often very difficult to accurately express a user's underlying intention through keywords, and a keyword-based query normally returns a long list of hits. This paper proposes a novel biomedical literature search system, called BiomedSearch, which supports complex queries and relevance feedback. In this system, we developed a weighted interest measure and an association mining algorithm to find the strength of association between the query and each concept in the article(s) selected by the user as feedback. The top ?? concepts were utilized to form a k-profile used for the next-round search. BiomedSearch relies on Unified Medical Language System (UMLS) knowledge sources to map text files to standard biomedical concepts. It was designed to support queries with any levels of complexity. The prototype system was preliminarily evaluated using three topics and related Genomics data from TREC (Text Retrieval Conference) 2006 Genomics Track. Initial results indicated that BiomedSearch could effectively utilize users' relevance feedback to improve search performance.","","Electronic:978-1-4673-6799-8; POD:978-1-4673-6800-1","10.1109/BIBM.2015.7359739","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359739","UMLS;association mining;biomedical literature search;relevance feedback","Databases;Information retrieval;Irrigation;Unified modeling language;Yttrium","biomedical engineering;data mining;feedback;genomics;medical computing","AD 2006;BiomedSearch;TREC;Text Retrieval Conference;UMLS knowledge source;association mining algorithm;biomedical database;biomedical literature search system;genomics data;genomics track;k-profile;unified medical language system;user relevance feedback","","","","17","","","9-12 Nov. 2015","","IEEE","IEEE Conference Publications"
"A Method for Microblog Search by Adjusting the Language Model with Time","S. Li; H. Ning; Z. Han; H. Qi","Sch. of Comput. Sci. & Technol., Harbin Eng. Univ., Harbin, China","2015 Eighth International Conference on Internet Computing for Science and Engineering (ICICSE)","20160303","2015","","","25","28","Time is a good scale for a microblog search. The traditional retrieval model only uses content to query microblog information. Many studies thought that the relevant microblogs most likely appear when it is close to the instant time of query. Through an analysis of microblog data, people find that the relevant information in microblogs possibly appears when it is far from the query time. These two opinions are conflict each other. Therefore, in this paper we report a modified language model which can be used to adjust time difference with appropriate weights. This way the order of the microblogs can be readjusted. The experimental results on microblog show the model works well.","","Electronic:978-1-5090-0454-6; POD:978-1-5090-0455-3","10.1109/ICICSE.2015.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7422450","Informathion Retrieval;Language model;Microblog search;Re-ranking;Temporal information","Computational modeling;Computer science;Estimation;Information retrieval;Mathematical model;Probability;Time factors","Web sites;data analysis;query processing","information retrieval;microblog data analysis;microblog search method;modified language model;query microblog information","","","","9","","","6-8 Nov. 2015","","IEEE","IEEE Conference Publications"
"Estimating the Reliability of the Retrieval Systems Rankings","S. D. Ravana; Z. Shuxiang","Dept. of Inf. Syst., Univ. of Malaya, Kuala Lumpur, Malaysia","2016 International Conference on Software Networking (ICSN)","20160630","2016","","","1","5","Information retrieval evaluation based on the pooling method inherently poses biasness towards systems that contributed to the pool of judged documents. This may distort the results about the relative effectiveness of different retrieval strategies, or rather, the retrieval systems and thus result in unreliable system rankings. The purpose of this study is to suggest a technique to estimate the reliability of the retrieval system effectiveness rank in a list of ranked systems based on its performance in previous experiments. This can be also defined as the strength of rank for the individual retrieval system. By doing so, we will be able to predict the performance of each system in future experiments. To validate the proposed rank strength estimation method, an alternative systems ranking method is proposed to generate a new list of systems rankings which is used together with the proposed rank strength estimation method. The experimentation shows that the correlation coefficients remain above 0.8 across different number of experiments which means the new systems ranking is highly correlated with the gold standard. It suggests that the rank reliability estimation methods have effectively predicted the strength of the system ranks. And also, the results from both TREC 2004 and TREC 8 show the similar outcome which further confirms the effectiveness of the proposed rank reliability estimation method.","","Electronic:978-1-5090-1676-1; POD:978-1-5090-1677-8","10.1109/ICSN.2016.7501924","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501924","","Correlation coefficient;Estimation;Gold;Information retrieval;Reliability;Standards;System performance","information retrieval systems","alternative systems ranking method;information retrieval evaluation;pooling method;rank strength estimation method;retrieval system rankings","","","","25","","","23-26 May 2016","","IEEE","IEEE Conference Publications"
"Web data extraction using textual anchors","A. Pouramini; S. Nasiri","Department of Computer Engineering, Sirjan University of Technology, Sirjan, Iran","2015 2nd International Conference on Knowledge-Based Engineering and Innovation (KBEI)","20160321","2015","","","1124","1129","In this paper, we present an approach and a visual tool, called ABDES, for creating web wrappers to extract data records from web pages. In our approach, we rely mainly on the visible page content, simulating the way a human user scans a web page for specific data. To create a wrapper, we use text features such as textual delimiters, keywords, constants or text patterns, which we call anchors, to create patterns for the target data regions and data records. We offer a polynomial data extraction algorithm, in which these patterns are checked against the page elements in a mixed bottom-up and top-down traverse of the DOM tree. The extracted data is directly mapped onto a hierarchical XML structure as the output of the algorithm. The wrappers generated by the system are robust and independent of the HTML structure. Therefore, they can be adapted to multiple websites to gather and integrate information.","","Electronic:978-1-4673-6506-2; POD:978-1-4673-6507-9","10.1109/KBEI.2015.7436204","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436204","Web Data Record Extraction;Web Information Extraction;Web Wrapper Generation","Data mining;Decision support systems;Information retrieval;Time complexity","Internet;Web sites;XML;information retrieval;text analysis","ABDES;DOM tree;HTML structure;Web data extraction;Web page;Web wrapper;Website;data record extraction;data records;hierarchical XML structure;mixed bottom-up traverse;page elements;polynomial data extraction algorithm;target data regions;text features;textual anchor;top-down traverse;visible page content;visual tool","","","","11","","","5-6 Nov. 2015","","IEEE","IEEE Conference Publications"
"Complex event processing for content-based text, image, and video retrieval","E. K. Bowman; B. D. Broome; V. M. Holland; D. Summers-Stay; R. M. Rao; J. Duselis; J. Howe; B. K. Madahar; A. C. Boury-Brisset; B. Forrester; P. Kwantes; G. Burghouts; J. van Huis; A. Y. Mülayim","Army Research Laboratory, Computational and Information Sciences Directorate","2016 International Conference on Military Communications and Information Systems (ICMCIS)","20160623","2016","","","1","6","This report summarizes the findings of an exploratory team of the North Atlantic Treaty Organization (NATO) Information Systems Technology panel into Content-Based Analytics (CBA). The team carried out a technical review into the current status of theoretical and practical developments of methods, tools and techniques supporting joint exploitation of multimedia data sources. In particular, content-based information retrieval and analytics was considered as a means to allow military experts to exploit multiple data sources in a rapid fashion for sensemaking and knowledge generation. Elements included contextual understanding of complex events through computational/human processing techniques, event prediction through the automated extraction of network features, temporal trends, hidden clusters and resource flows, and the use of machine processing for automated translation, parsing, information extraction, and summarization of unstructured and semistructured data. The main conclusions of the study are that important research gaps exist in all the technical areas covered in this report. Though the research areas and developments are being advanced in the military sector and the civil sector, in particular, they remain at low levels of technical maturity for defense and security system applications. It is recommended that NATO collaborative research effort be expanded to advance those approaches that are most pertinent to our overall aim of enhancing the contextual understanding of complex events through CBA of heterogeneous multimedia streams.","","Electronic:978-1-5090-1777-5; POD:978-1-5090-0472-0; USB:978-1-5090-1778-2","10.1109/ICMCIS.2016.7496546","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496546","content-based analytics;event prediction;knowledge generation;multimedia data sources;sensemaking","Cameras;Data mining;Feature extraction;Information retrieval;Multimedia communication;Personnel;Streaming media","","","","","","9","","","23-24 May 2016","","IEEE","IEEE Conference Publications"
