"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=5347835,5345956,5344304,5345571,5346863,5346098,5344160,5346315,5346119,5342606,5340936,5340933,5342006,5340908,5341539,5340948,5340939,5342925,5340925,5342055,5340910,5341783,5341793,4796195,5338791,5328261,5328884,5328133,5337265,5329112,5337361,5337162,5337304,5337462,5328455,5337161,5335990,5337546,5328045,5337567,5328052,5337584,5334831,5256240,5332595,5332107,5332082,5331413,5333436,5333784,5331702,5332076,5319717,5319165,5319697,5319713,5322155,5319134,5319264,5319079,5325310,5319410,5313774,5313852,5317530,5312904,5313758,5313766,5313789,5313762,5314238,5152946,5300301,5298644,5302320,5305856,5302444,5298596,5306293,5298616,5304437,5306037,5304374,5298647,5298935,5302770,5303433,5305988,5305784,5298635,5305708,5298630,5305982,5297631,5301778,5298658,5291534,5291845,5291528,5291863",2017/05/04 22:38:26
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Constructing Multilingual Preterminological Graphs using various online-community resources","M. Daoud; C. Boitet; K. Kageura; A. Kitamoto; D. Daoud; M. Mangeot","Grenoble Informatics Laboratory, GETALP, Universit&#233; Joseph Fourier, 385, rue de la Biblioth&#232;que, 38041 Grenoble, France","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","116","121","We are describe the concept of dedicated multilingual preterminological graphs MPGs, and some automatic approaches for constructing them by analyzing the behavior of online community users. A multilingual preterminological graph is a special lexical resource that contains massive amount of terms related to a special domain, and can be used as raw material to later build a standardized terminological repository. Building such a graph is difficult using traditional approaches, as it needs huge efforts by domain specialists and terminologists. In our approach, we build such a graph by analyzing the access log files of the Web site of the community, and by finding the important terms that have been used to search in that Web site, and their association with each other. We aim at making this graph as a seed repository so multilingual volunteers can contribute. We are experimenting this approach with the Digital Silk Road Project. We have used its access log files since its beginning in 2003, and obtained an initial graph of around 116000 terms. As an application, we used this graph to obtain a preterminological multilingual database that is serving a CLIR system for the DSR project.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340936","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340936","","Artificial neural networks;Biological neural networks;Decision making;Machine learning;Natural language processing;Neural networks;Radial basis function networks;Support vector machine classification;Support vector machines;Weather forecasting","Web sites;directed graphs;information retrieval;information retrieval systems;natural language processing","CLIR system;DSR project;Digital Silk Road Project;MPG;access log file;automatic construction approach;community Web site;cross-language information retrieval;dedicated multilingual preterminological graph construction;directed graph;lexical resource;natural language processing;online community user behavior analysis;online-community resources;preterminological multilingual database;standardized terminological repository;term search","","1","","21","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Original content extraction oriented to anti-plagiarism","Y. Shen; M. Cheng; X. Yao; W. Wei","School of Information Management, Wuhan University, China, 430072","2009 International Conference on Management Science and Engineering","20091106","2009","","","17","22","In order to reduce the impact of inclusion of citations and references during the detection of plagiarism in academic theses, and extract the original content, the author created three ways to extract original content and remove the citation: 1) Removal of normative citations by symbol features; 2) removal tacit citations by Bayesian method based on the minimum risk and thesis structure; 3) removal common knowledge base on domain public knowledge base. The research results show that during the extraction of original content, the precision decreases as the risk coefficient increases, while the recall rate increases with the risk coefficient. When the risk coefficient is 60, the whole performance achieves the optimum. Plagiarism detection after extracting the original content presents a fault rate decrease from 9.09% to 4.52%.","2155-1847;21551847","POD:978-1-4244-3970-6","10.1109/ICMSE.2009.5317530","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317530","Beyes;citation removal;content extraction;plagiarism;thesis structure","Conference management;Content management;Data mining;Engineering management;Knowledge management;Plagiarism;Prototypes;Risk management;Software libraries;Web pages","belief networks;citation analysis;information retrieval","Bayesian method;content extraction;normative citations removal;plagiarism detection;removal tacit citations","","0","","25","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Finding comparable temporal categorical records: A similarity measure with an interactive visualization","K. Wongsuphasawat; B. Shneiderman","Department of Computer Science & Human-Computer Interaction Lab, University of Maryland, College Park, 20742, USA","2009 IEEE Symposium on Visual Analytics Science and Technology","20091113","2009","","","27","34","An increasing number of temporal categorical databases are being collected: Electronic Health Records in healthcare organizations, traffic incident logs in transportation systems, or student records in universities. Finding similar records within these large databases requires effective similarity measures that capture the searcher's intent. Many similarity measures exist for numerical time series, but temporal categorical records are different. We propose a temporal categorical similarity measure, the M&M (Match & Mismatch) measure, which is based on the concept of aligning records by sentinel events, then matching events between the target and the compared records. The M&M measure combines the time differences between pairs of events and the number of mismatches. To accom-modate customization of parameters in the M&M measure and results interpretation, we implemented Similan, an interactive search and visualization tool for temporal categorical records. A usability study with 8 participants demonstrated that Similan was easy to learn and enabled them to find similar records, but users had difficulty understanding the M&M measure. The usability study feedback, led to an improved version with a continuous timeline, which was tested in a pilot study with 5 participants.","","POD:978-1-4244-5283-5","10.1109/VAST.2009.5332595","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332595","M&M Measure;Similan;Similarity Search;Temporal Categorical Records","Educational institutions;Feedback;Medical services;Particle measurements;Testing;Time measurement;Transportation;Usability;Visual databases;Visualization","data visualisation;information retrieval;interactive systems;temporal databases;time series;very large databases","M&M measure;Match & Mismatch measure;Similan;interactive search tool;interactive visualization tool;large databases;numerical time series;parameters customization;similarity measure;temporal categorical databases;temporal categorical records","","19","","27","","","12-13 Oct. 2009","","IEEE","IEEE Conference Publications"
"Chinese sentence similarity based on word context and semantic","T. Gu; F. Ren","Faculty of Engineering, The University of Tokushima Minami-Josanjima 2-1, Tokushima-shi, Tokushima, 770-8502, Japan, School of Computer, Beijing University of Posts and Telecommunications, Beijing 100876, China","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","5","Sentence similarity computing plays an important role in the nature language processing. Many different methods are proposed to calculate sentence similarity including word, semantic, syntax and so on. In this paper, we proposed a sentence similarity method for travel question answering system by combining the word context information and semantic similarity together. We searched a series of context structures for keywords in a sentence. Experiment has been carried out to show the effectiveness of our method.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313774","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313774","context;semantic;sentence similarity","Dictionaries;Knowledge engineering;Libraries;Natural language processing;Natural languages;Pattern matching;Shape;Space technology;Statistical analysis;Telecommunication computing","information retrieval;information retrieval systems;natural language processing;travel industry","Chinese sentence similarity computing;keyword context structure search;natural language processing;travel question answering system;word context information;word semantic similarity;word syntax","","1","","12","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"CardioGrid: a framework for the analysis of cardiological signals in Grid computing","M. Risk; F. P. Castrillo; J. F. G. Eijo; C. S. Ortega; M. B. Fernandez; A. P. Diaz; M. R. del Solar; R. R. Pollan","Laboratory of Complex Systems, Department of Computer Science, FCEyN, University of Buenos Aires, Argentina","2009 Latin American Network Operations and Management Symposium","20091120","2009","","","1","4","The present paper describes the development of the CardioGrid framework into the Grid infrastructure. The core Grid services; workload management system (WMS), data management system and grid authentication have been implemented. Additionally, a Web-based tool -the CardioGrid portal-has been developed to facilitate the user interaction with the Grid. As a result, the user is able to process the electrocardiogram (ECG) signals obtained form a portable data acquisition device and to process them on the Grid. Once the CardioGrid portal is prompted and the user identity is verified through a digital X.509 certificate, the operator may either upload new raw ECG data to the grid storage elements or use already stored data. Then, subsequent analytics from these data are performed as Grid jobs and relevant medical quantities are derived through a middleware job retrieval mechanism. In summary, this paper describes the development of a medical Grid based system, and its integration to an existing platform for digital repositories infrastructure.","","POD:978-1-4244-4551-6","10.1109/LANOMS.2009.5338791","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5338791","DRI;data grid;medical signal","Authentication;Cardiology;Data acquisition;Data analysis;Electrocardiography;Grid computing;Performance analysis;Portals;Signal analysis;Signal processing","certification;electrocardiography;grid computing;information retrieval;medical information systems;medical signal processing;middleware","CardioGrid;Grid computing;cardiological signals;data management system;digital X.509 certificate;digital repositories infrastructure;electrocardiogram;grid authentication;grid storage elements;middleware job retrieval;portable data acquisition device;user interaction;workload management system","","0","","14","","","19-21 Oct. 2009","","IEEE","IEEE Conference Publications"
"Gene prioritization using a probabilistic knowledge model","S. Wang; M. Hauskrecht; S. Visweswaran","Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA 15260","2009 IEEE International Conference on Bioinformatics and Biomedicine Workshop","20091113","2009","","","272","278","We are interested in exploiting domain knowledge for the task of candidate gene prioritization. In this paper, we present a new gene prioritization method that learns a probabilistic knowledge model and exploits it to prioritize candidate genes. The knowledge model is represented by a network of associations among domain concepts (e.g., genes) and is extracted from a domain database (e.g., protein-protein interaction database). This knowledge model is then used to perform probabilistic inferences and applied to the task of gene prioritization. We evaluate our new method on five diseases and show that it outperforms a recently described network-based method for candidate gene prioritization.","","POD:978-1-4244-5121-0","10.1109/BIBMW.2009.5332107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332107","Gene Prioritization;Probabilistic Knowledge Model","Alzheimer's disease;Bioinformatics;Biological system modeling;Biomedical informatics;Citation analysis;Computer science;Databases;Genomics;Intelligent systems;Proteins","biology computing;database management systems;information retrieval;learning (artificial intelligence)","candidate gene prioritization method;domain database extraction;network-based method;probabilistic knowledge model;protein-protein interaction database","","1","","16","","","1-4 Nov. 2009","","IEEE","IEEE Conference Publications"
"Study on similarity of simple questions based on the catering field","Q. Dong; S. Shi; H. Wang; X. Lv","Chinese Information, Process Research Center, BISTU, Beijing, China","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","5","The similarity between sentences is a theoretical basis and key technology to the question answering system. The method presented in this paper is as follows. Firstly, the dependency question sets are obtained and the key words are extracted from the major components of the question sentences and the target question form the related libraries, and then the candidate question sets are obtained through comparing their field words and key words. Secondly, take the HowNet as the semantic knowledge resource to compute the semantic similarity; and lastly the sentences with the largest similarity are returned. This accuracy of the method to calculate the similarity between the simple sentences is up to 85% in catering field. The experimental results show that the approach presented in this paper achieves good results in specific areas.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313852","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313852","HowNet;interdependence;the catering field;the similarity between sentences","Data mining;Information analysis;Information processing;Libraries;Marine animals;Natural language processing;Shape;Skeleton;Surface treatment;Tagging","catering industry;computational linguistics;information retrieval;information retrieval systems;natural language processing","HowNet semantic knowledge resource;candidate question set;catering field word;dependency question set;keyword extraction;library;natural language processing;question answering system;question sentence semantic similarity","","0","","9","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"An Automatic Semantic Term-Network","S. C. Park; L. C. Choi","Div. of Electron. & Inf. Eng., Chonbuk Nat. Univ., Jeonb uk, South Korea","2009 International Conference on Computing, Engineering and Information","20091117","2009","","","217","220","An automatic semantic term-network construction system using the singular value decomposition (SVD) is implemented in this research. The term-network construction is to compute the similarities between rows of the large term-by-document matrix generated from a document corpus to get the relationships between terms. A reduced matrix, U of SVD, is decomposed from the term-by-document matrix to improve the speed and provide the latent semantic structure. The SSTRESS criterion is used for the numerical measure of closeness between original term by document corpus matrix and the decomposition matrix with different ranks. In order to measure the performance of our system, a standard data collection set, Reuters-21578, is used and about 2000 terms are extracted from the set automatically. This term-network construction could be expected to easily apply to constructing the ontology system and to supporting the semantic retrieval system in the near future.","","POD:978-0-7695-3538-8","10.1109/ICC.2009.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328133","automatic construct;ontology;semantic;term-network;term-weight","Data mining;Equations;Frequency;Intelligent systems;Internet;Matrix decomposition;Measurement standards;Ontologies;Singular value decomposition;Thesauri","document handling;information retrieval;semantic networks;singular value decomposition","Reuters-21578;SSTRESS criterion;automatic semantic term-network construction system;document corpus matrix;latent semantic structure;semantic retrieval system;singular value decomposition;term-by-document matrix","","0","","14","","","2-4 April 2009","","IEEE","IEEE Conference Publications"
"Using Open Web APIs in Teaching Web Mining","H. Chen; X. Li; M. Chau; Y. J. Ho; C. Tseng","Dept. of Manage. Inf. Syst., Univ. of Arizona, Tucson, AZ, USA","IEEE Transactions on Education","20091103","2009","52","4","482","490","With the advent of the World Wide Web, many business applications that utilize data mining and text mining techniques to extract useful business information on the Web have evolved from Web searching to Web mining. It is important for students to acquire knowledge and hands-on experience in Web mining during their education in information systems curricula. This paper reports on an experience using open Web application programming interfaces (APIs) that have been made available by major Internet companies (e.g., Google, Amazon, and eBay) in a class project to teach Web mining applications. The instructor's observations of the students' performance and a survey of the students' opinions show that the class project achieved its objectives and students acquired valuable experience in leveraging the APIs to build interesting Web mining applications.","0018-9359;00189359","","10.1109/TE.2008.930509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5152946","Data mining;Web computing;Web mining;information System;open Web APIs;visualization","Data mining;Education;Educational institutions;Educational programs;Information systems;Internet;Programming profession;Text mining;Web mining;Web pages;Web sites","Internet;application program interfaces;computer aided instruction;computer science education;data mining;information retrieval;open systems;teaching","Internet companies;Web mining teaching;World Wide Web;data mining;information system curricula;open Web API;open Web application programming interfaces;text mining techniques","","10","","39","","20090630","Nov. 2009","","IEEE","IEEE Journals & Magazines"
"Analyzing the technologies of search algorithm based on P2P","S. Zhiwei; M. Shaowu; W. Jianan; T. Xiongyan","National Engineering Laboratory of China Unicom, Beijing","2009 2nd IEEE International Conference on Broadband Network & Multimedia Technology","20091204","2009","","","711","714","The search engine based P2P give a new solution for Internet information search. The best advantage of P2P search engine at using the advanced peer search idea that completely search the Internet without through center server and don't limit for information document format and host equipment. The paper introduces the current technology development of P2P search, analyzes the mainstream P2P search algorithm in detail, and give some suggest for the future P2P search work.","","POD:978-1-4244-4590-5","10.1109/ICBNMT.2009.5347835","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5347835","Internet;P2P;Search technology;summary","Algorithm design and analysis;Databases;File servers;Indexes;Information analysis;Internet;Paper technology;Partitioning algorithms;Peer to peer computing;Search engines","Internet;information retrieval;peer-to-peer computing;search engines","Internet information search;P2P;information document format;search algorithm;search engine","","0","","4","","","18-20 Oct. 2009","","IEEE","IEEE Conference Publications"
"Correlation between human assessment of essays and ROUGE evaluation of essays' summaries","S. Latif; M. M. Wood; G. Nenadic","School of Computer Science, University of Manchester, United Kingdom","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","122","127","In this paper we have addressed the qualitative (human evaluation) and quantitative (ROUGE) evaluation of computer generated summaries of the students' essays. The experimental results show that there is a positive high correlation between ROUGE scores and human assessment of the essays (human assigned marks). We have also found out that human evaluation of the automatic summaries positively correlates with the human assessment of the essays. These correlations can be used to classify students' essays into broad bands of quality.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340933","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340933","","Computer science;Current measurement;Gold;Humans;Measurement standards;Natural language processing;System testing","classification;educational administrative data processing;information retrieval;text analysis","ROUGE evaluation;automatic classification;automatic summarization;classify student essay;computer generated summary;experimental result;human assessment;human assigned mark;human evaluation;information retrieval;positive correlation;qualitative evaluation;quantitative evaluation;source document;text length","","0","","15","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Extraction of User Profile Based on the Hadoop Framework","L. Huang; X. w. Wang; Y. d. Zhai; B. Yang","Coll. of Comput. Sci. & Technol., Jilin Univ., Changchun, China","2009 5th International Conference on Wireless Communications, Networking and Mobile Computing","20091030","2009","","","1","6","With the rapid development of Internet, the Web information dramatically increases, the users are often involved in voluminous information to feel lose, Distributed processing of mass data through a cluster composed by many machines and personalized search services based on the user profile have been the hotspots of research and development. This paper firstly studies the operation mechanism of Hadoop, which is a typical distributed processing framework of Apache, then realizes extraction of user profile from a large number of Web log data and through comparison experiment with single machine to verify its efficiency.","2161-9646;21619646","POD:978-1-4244-3691-0","10.1109/WICOM.2009.5305856","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305856","","Data mining;Data processing;Distributed processing;Fault tolerance;File systems;Java;Logic;Parallel processing;Programming profession;Research and development","Internet;data mining;information retrieval","Apache distributed processing framework;Hadoop framework;Internet;Web information;Web log data;mass data distributed processing;personalized search service;user profile extraction","","2","","14","","","24-26 Sept. 2009","","IEEE","IEEE Conference Publications"
"Knowledge Capitalization in the Automotive Industry Using an Ontology Based on the ISO/TS 16949 Standard","B. D. Rodriguez-Rocha; F. E. Castillo-Barrera; H. Lopez-Padilla","Eng. & Comput. Sci. Postgrad., Autonomous Univ. of San Luis Potosi, San Luis Potosi, Mexico","2009 Electronics, Robotics and Automotive Mechanics Conference (CERMA)","20091201","2009","","","100","106","In most manufacturing industries, the human resources department has the responsibility for training new employees for their new roles and duties. Special instructors, time and several other related resources considered for this training translate into a high cost for any company. Quality, a factor that distinguish the efficiency and competitiveness among enterprises has become a very important to consider. Personnel rotation as well as hiring and training of new one, usually has a big impact on this quality during the time required by this new personnel to acquire experience. The main objective of this work is to present the use of an ontology for knowledge capitalization in an automotive industry, allowing information representation, manipulation and specific document retrieval for regular human resources activities like introducing new employees to their respective roles and duties in a faster way. Additionally, it can become very a powerful tool for auditing internal processes because permits faster location of documents, processes, persons and his duties. Using proper auditors and consultants (in lean manufacturing for example) can eventually translating in lower costs for a company.","","POD:978-0-7695-3799-3","10.1109/CERMA.2009.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342006","ISO/TS 16949 standard;automotive industry;knowledge capitalization;ontology","Automotive engineering;Companies;Costs;Humans;ISO standards;Industrial training;Information representation;Manufacturing industries;Ontologies;Personnel","ISO standards;Q-factor;SQL;auditing;automobile industry;human resource management;information retrieval;ontologies (artificial intelligence);personnel;quality management","ISO/TS 16949 standard;SQL query;auditing internal process;automotive industry;automotive quality system;document retrieval;human resource department;information representation;knowledge capitalization;manufacturing industry;ontology;personnel experience;quality factor;training new employee","","1","","50","","","22-25 Sept. 2009","","IEEE","IEEE Conference Publications"
"Extracting Chinese question-answer pairs from online forums","B. Wang; B. Liu; C. Sun; X. Wang; L. Sun","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China","2009 IEEE International Conference on Systems, Man and Cybernetics","20091204","2009","","","1159","1164","Extracting question-answer pairs from online forums is a meaningful work due to the huge amount of valuable user generated resource contained in forums. In this paper we consider the problem of extracting Chinese question-answer pairs for the first time. We present a strategy to detect Chinese questions and their answers. We propose a sequential rule based method to find questions in a forum thread, then we adopt non-textual features based on forum structure to improve the performance of answer detecting in the same thread. Experimental results show that our techniques are very effective.","1062-922X;1062922X","POD:978-1-4244-2793-2","10.1109/ICSMC.2009.5345956","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345956","classification;information extraction;labeled sequential rules;nontextual features;question answering","Computer science;Cybernetics;Data mining;Feature extraction;Humans;Natural languages;Sun;Testing;USA Councils;Yarn","Internet;data mining;information retrieval;knowledge based systems","Chinese question-answer pairs extraction;nontextual feature;online forum thread;sequential rule based method;user generated resource","","3","","15","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Research on Direction Search Algorithm Based on LAN Agent in P2P Network","L. Wang; Z. Fang; X. Liu; Z. Liu; H. Tang","Internet Center, Jilin Univ., Changchun, China","2009 5th International Conference on Wireless Communications, Networking and Mobile Computing","20091030","2009","","","1","4","In view of the problem of slow search speed conduced by failure topology and limited retransmission nodes quantity in MDBS search algorithm; we proposed the LAN agent-based direction search algorithm-ABDS algorithm. We expounded the basic thought and design principle of ABDS algorithm and carried on the theoretical analysis to such as aspects of its accuracy and parameter values. The analysis results show that ABDS algorithm greatly improves the search speed while doesn't reduce the network coverage.","2161-9646;21619646","POD:978-1-4244-3691-0","10.1109/WICOM.2009.5302444","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5302444","","Algorithm design and analysis;Bandwidth;Classification algorithms;Computer science;Floods;IP networks;Local area networks;Network topology;Physical layer;System performance","information retrieval;local area networks;multi-agent systems;peer-to-peer computing;telecommunication network topology","ABDS algorithm;LAN agent;MDBS search algorithm;P2P network;direction search algorithm;failure topology;limited retransmission nodes quantity;unstructured topology","","0","","8","","","24-26 Sept. 2009","","IEEE","IEEE Conference Publications"
"Extracting templates from radiology reports using sequence alignment","S. Wu; C. P. Langlotz; P. Lakhani; L. H. Ungar","","2009 IEEE International Conference on Bioinformatics and Biomedicine Workshop","20091113","2009","","","320","324","Health care providers often dictate their reports by filling in slots in templates. These slots can be filled with a variety of different procedures, measurements or findings. Many radiologists currently create their own personalized templates, costing time and leading to inconsistencies across physicians. We present a sequence alignment method, RADICAL (Radiology Content Alignment), that uses dynamic programming to efficiently extract templates that are common across sets of reports, and give examples of the extracted templates and the contents of their slots.","","POD:978-1-4244-5121-0","10.1109/BIBMW.2009.5332082","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332082","medical informatics;radiology;sequence alignment;text mining","Bones;Costing;Data mining;Dynamic programming;Filling;Medical services;Radiology;Speech recognition;Text mining;Vocabulary","dynamic programming;health care;information retrieval;medical information systems;radiology","RADICAL;dynamic programming;health care providers;medical informatics;radiology content alignment;radiology reports;sequence alignment method;templates extraction","","0","","15","","","1-4 Nov. 2009","","IEEE","IEEE Conference Publications"
"Learning with Positive and Unlabeled Examples Using Topic-Sensitive PLSA","K. Zhou; X. Gui-Rong; Q. Yang; Y. Yu","Shanghai Jiao-Tong University, Shanghai","IEEE Transactions on Knowledge and Data Engineering","20091124","2010","22","1","46","58","It is often difficult and time-consuming to provide a large amount of positive and negative examples for training a classification system in many applications such as information retrieval. Instead, users often find it easier to indicate just a few positive examples of what he or she likes, and thus, these are the only labeled examples available for the learning system. A large amount of unlabeled data are easier to obtain. How to make use of the positive and unlabeled data for learning is a critical problem in machine learning and information retrieval. Several approaches for solving this problem have been proposed in the past, but most of these methods do not work well when only a small amount of labeled positive data are available. In this paper, we propose a novel algorithm called Topic-Sensitive pLSA to solve this problem. This algorithm extends the original probabilistic latent semantic analysis (pLSA), which is a purely unsupervised framework, by injecting a small amount of supervision information from the user. The supervision from users is in the form of indicating which documents fit the users' interests. The supervision is encoded into a set of constraints. By introducing the penalty terms for these constraints, we propose an objective function that trades off the likelihood of the observed data and the enforcement of the constraints. We develop an iterative algorithm that can obtain the local optimum of the objective function. Experimental evaluation on three data corpora shows that the proposed method can improve the performance especially only with a small amount of labeled positive data.","1041-4347;10414347","","10.1109/TKDE.2009.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4796195","Semisupervised learning;document classification.;topic-sensitive probabilistic latent semantic analysis","","classification;data handling;information retrieval;iterative methods;learning systems;probability;unsupervised learning","classification system;data corpora;information retrieval;iterative algorithm;learning system;machine learning;probabilistic latent semantic analysis;supervision information;topic-sensitive PLSA;topic-sensitive pLSA;unsupervised framework","","12","","33","","20090227","Jan. 2010","","IEEE","IEEE Journals & Magazines"
"Know-who/know-how Navigation Using Development Project-related Taxonomies","T. Madokoro; M. Nakatsuji; K. Okamoto; S. Miyazaki; T. Harada","R&D Center, Nippon Telegraph & Telephone West Corp., Osaka, Japan","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","559","560","In this demonstration, we introduce a corporate knowledge management system. Our system uses development project-specific taxonomies to extract know-who/know-how information from e-mails automatically. After that, it visualizes knowledge on taxonomies in an understandable way. Accordingly, the system lets us easily learn about business keywords and knowledge that each developer has.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298596","Know-who/know-how extraction;Semantics-based knowledge management;development-project-related ontology;e-mail analysis","Data mining;Graphical user interfaces;Knowledge management;Laboratories;Navigation;Research and development;Taxonomy;Telegraphy;Telephony;Visualization","electronic mail;information retrieval;knowledge management;text analysis","business keywords;corporate knowledge management system;development project-specific taxonomies;e-mails;know-how navigation;know-who navigation;knowledge visualization","","0","","3","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Keynote speakers","R. Baeza-Yates","Yahoo! Res., Spain","2009 Latin American Web Congress","20091201","2009","","","xvi","xvii","There are several semantic sources that can be found in the Web that are either explicit, e.g. Wikipedia, or implicit, e.g. derived from Web usage data. Most of them are related to user generated content (UGC) or what is called today the Web 2.0. In this talk we show several applications of mining the wisdom of crowds behind UGC to improve search. These results not only impact the search performance but also the user interface, suggesting new ways of interaction. We will show live demos that find relations in the Wikipedia or improve image search, already available at sandbox.yahoo.com, the demo site of Yahoo! Research. Our final goal is to produce a virtuous data feedback circuit to leverage the Web itself.","","Paper:978-0-7695-3856-3","10.1109/LA-WEB.2009.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341539","","","Internet;data mining;information retrieval;search engines;user interfaces","Web 2.0 mining;Web usage data;Wikipedia;user generated content;user interface;virtuous data feedback circuit","","0","","","","","9-11 Nov. 2009","","IEEE","IEEE Conference Publications"
"Ellipsoidal Harmonics for 3-D Shape Description and Retrieval","A. Mademlis; P. Daras; D. Tzovaras; M. G. Strintzis","Dept. of Electr. & Comput. Eng., Aristotle Univ. of Thessaloniki, Thessaloniki, Greece","IEEE Transactions on Multimedia","20091113","2009","11","8","1422","1433","In this paper, a novel approach for 3-D Shape description and retrieval based on the theory of ellipsoidal harmonics is presented. Four novel descriptors are introduced: the surface ellipsoidal harmonics descriptor, which concerns 3-D objects that are described as polygonal surfaces; the volumetric ellipsoidal harmonics descriptor, which is applicable to volumetric 3-D objects; the generalized ellipsoidal harmonics descriptor that is applied to any local 3-D object descriptors; and, finally, the combined ellipsoidal-spherical harmonics descriptor, which leads to a compact and powerful descriptor that inherits the advantages of both approaches: the rotation invariance properties of the spherical harmonics and the directional information enclosed in ellipsoidal harmonics. Experimental results performed using well-known 3-D object databases prove the retrieval efficiency of the proposed approach.","1520-9210;15209210","","10.1109/TMM.2009.2032690","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5256240","3-D object retrieval;ellipsoidal harmonics;shape description","","computational geometry;information retrieval;visual databases","3D objects database;3D shape description;3D shape retrieval;combined ellipsoidal-spherical harmonics descriptor;directional information;polygonal surface;rotation invariance properties;surface ellipsoidal harmonics descriptor;volumetric ellipsoidal harmonics descriptor","","3","","36","","20090922","Dec. 2009","","IEEE","IEEE Journals & Magazines"
"Fast Subsequence Matching in Plasma Waveform Databases","T. Hochin; Y. Yamauchi; H. Nomiya; H. Nakanishi; M. Kojima","Kyoto Inst. of Technol., Kyoto, Japan","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20091117","2009","","","759","762","This paper proposes a method of subsequence matching of plasma waveforms. The proposed method divides a waveform into fine-grained segments. The similar segments are grouped into a segment group. A multi-dimensional index is used for quick retrieval. Grouping segments could save the amount of the index. In the retrieval, a sequence of segments, which is called a section, is used as a unit in matching subsequences. Overlapping sections could overcome the shift errors of subsequences, and results in good retrieval correctness.","","POD:978-1-4244-4717-6","10.1109/IIH-MSP.2009.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337265","Index;Plasma Waveform;Retrieval;Time series","Deductive databases;Error correction;Frequency;Multidimensional signal processing;Multimedia databases;Nuclear and plasma sciences;Plasma devices;Plasma materials processing;Plasma waves;Steady-state","information retrieval;physics computing;plasma waves;very large databases","data retrieval;fast subsequence matching;fine-grained segments;multidimensional index;overlapping sections;plasma waveform databases","","3","","11","","","12-14 Sept. 2009","","IEEE","IEEE Conference Publications"
"Digging deep: Software reengineering supported by database reverse engineering of a system with 30+ years of legacy","S. Strobl; M. Bernhart; T. Grechenig; W. Kleinert","Research Group for Industrial Software, Vienna University of Technology","2009 IEEE International Conference on Software Maintenance","20091030","2009","","","407","410","This paper describes the industrial experience in performing database reverse engineering on a large scale software reengineering project. The project in question deals with a highly heterogeneous in-house information system (IS) that has grown and evolved in numerous steps over the past three decades. This IS consists of a large number of loosely coupled single purpose systems with a database driven COBOL application at the centre, which has been adopted and enhanced to expose some functionality over the web. The software reengineering effort that provides the context for this paper deals with unifying these components and completely migrating the IS to an up-to-date and homogeneous platform. A database reverse engineering (DRE) process was tailored to suit the project environment consisting of almost 350 tables and 5600 columns. It aims at providing the developers of the software reengineering project with the necessary information about the more than thirty year old legacy databases to successfully perform the data migration. The application of the DRE process resulted in the development of a high-level categorization of the data model, a wiki based redocumentation structure and the essential data-access statistics.","1063-6773;10636773","POD:978-1-4244-4897-5","10.1109/ICSM.2009.5306293","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306293","","Application software;Collaborative software;Computer industry;Information systems;Information technology;Large-scale systems;Relational databases;Reverse engineering;Software performance;Software reusability","COBOL;data models;database management systems;information retrieval;reverse engineering;software maintenance;systems re-engineering","data model;data-access statistics;database driven COBOL application;database reverse engineering;heterogeneous in-house information system;large scale software reengineering;legacy system;single purpose systems;wiki based redocumentation structure","","2","","8","","","20-26 Sept. 2009","","IEEE","IEEE Conference Publications"
"Acquisition of useful expressions from English research papers","Y. Sakai; K. Sugiki; S. Matsubara","Graduate School of Information Science, Nagoya University, Furo-cho, Chikusa-ku, Nagoya-shi, 464-8601 Japan","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","59","62","This paper proposes a method for extracting useful expressions from English research papers. The method extracts sequences of words from research papers and refine them into phrasal expressions (PEs). We use base-phrases for acquiring such the expressions. The method extracts PEs from the set of sequences of base-phrases by using three kinds of statistical information: frequency, length, and the number of kinds of the succeeding base-phrases. In our experiment using 1,232 research papers, the precision of acquisition at the top-200 was 62.0%. The precision was higher than all of the baselines, and therefore, we confirmed the feasibility of our method.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340948","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340948","","Data mining;Dictionaries;Frequency;Marketing and sales;Natural language processing;Search engines","information retrieval;natural languages;search engines;statistical analysis","English research paper;base-phrase;phrasal expression acquisition;statistical information;word sequence extraction","","0","","7","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Simulation resource selection based on local optimization method in distributed simulation repository","B. Yan; Z. Yang","Department of Computer Science & Engineering, Beijing Institute of Technology, Beijing, China","2009 16th International Conference on Industrial Engineering and Engineering Management","20091204","2009","","","1882","1886","With the development of simulation applications, the Distributed Simulation Resource Repository (DSRR) could manage all kinds of distributed simulation resources and provide transparent access, search, and browsing for users. In recent years, more and more research focus on the construction of DSRR, which includes metadata, dataset, algorithm, model, toolkit, and etc. However, when more and more resources are added into DSRR, users get many same or similar resource result sets in DSRR by semantic searching. Numerous time has been spent on selecting simulation resources from the result sets. According to the features of simulation resources, we setup 5 quality criteria which are the resource price, download speed, the reputation of source, the availability and whether the resource has been accessed. Then the Local Optimization method is applied to get the quality score of each resource. These resources in result set are assorted again by quality score to help user select suitable resource and access it. This method is accomplished by LDAP server and J2EE JNDI interface. The selection time can be reduced by this method.","","POD:978-1-4244-3671-2","10.1109/ICIEEM.2009.5344304","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344304","LDAP;Qos;Service Selection;Simulation Resource","Application software;Computational modeling;Computer science;Computer simulation;Information technology;Optimization methods;Quality of service;Resource management;Technology management;Web services","digital simulation;distributed databases;information resources;information retrieval;optimisation","J2EE JNDI interface;LDAP server;availability criteria;distributed simulation resource repository;download speed;local optimization method;resource price;resource quality score;semantic searching;simulation resource selection;source reputation;transparent access;transparent browsing;transparent searching","","0","","11","","","21-23 Oct. 2009","","IEEE","IEEE Conference Publications"
"A Study of Relation Annotation in Business Environments Using Web Mining","Q. Li; D. He; M. Mao","Sch. of Inf. Sci., Univ. of Pittsburgh, Pittsburgh, PA, USA","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","203","208","Relation annotation (RA) is a process of marking up relations among a set of entities identified from a plain text. RA is important to enterprise applications due to its capability of revealing semantics in business environments. However, RA in business environment is different from that in news domain because the entities involved in the relations in business domain often not just refer to entities like People or Locations, and many business entities still could not be identified by existing entity identification tools. In this paper, we explore RA in business environment using web mining techniques, and propose the Relation Annotation Platform in Business Environments (RAPBE), which can automatically help information workers by annotating business relations in enterprise setting. We evaluated RAPBE using two sample relations that are common in business domain -- COMPANY-LOCATION and COMPANY-PRODUCT. Our experiment results demonstrate the usefulness of RAPBE in relation annotation, and also show that the best method for marking up relations of the entities identifiable by existing entity identification tools is Frequency Weight method, whereas Distant Weight is the best when some entities involved in RA cannot be identified by the information extraction tools.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298616","Bootstrap;Business Environment;Relation Annotation","Companies;Customer relationship management;Data mining;Frequency;Helium;Information science;Navigation;Ontologies;Testing;Web mining","business data processing;data mining;information retrieval;semantic Web","Web mining;business environments;distant weight;entity identification tools;frequency weight method;information extraction tools;relation annotation;semantics","","0","","17","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"The Discovery and Extraction of Query Interfaces Based on Deep Web","Y. Daowen; L. Quan; C. Zhiming; F. Yuchen","Inst. of Comput. Sci. & Technol., Soochow Univ., Suzhou, China","2009 WRI World Congress on Software Engineering","20091110","2009","1","","507","511","With the prevalence of the massive online databases, the Web has become more and more deepened. Up to now, there exists lots of work which has been done in integration of query interface and extraction of query results. Because most Web pages are unstructured and dynamic, the discovery and extraction of query interfaces become the essential question needed to resolve. This paper is mainly a survey about the discovery and extraction of query interfaces. Meanwhile some new suggestions and methods are put forward in this paper.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.129","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319165","","Computer science;Data mining;Distributed databases;HTML;Rendering (computer graphics);Search engines;Software engineering;Uniform resource locators;Web pages;Web sites","Internet;information retrieval systems;user interfaces","deep Web;massive online databases;query interface discovery;query interface extraction","","0","","15","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"Wireless sensor networks for swift bird farms monitoring","A. K. Othman; K. M. Lee; H. Zen; W. A. W. Zainal; M. F. M. Sabri","Department of Electronic Engineering, Universiti Malaysia Sarawak, Kota Samarahan, Malaysia","2009 International Conference on Ultra Modern Telecommunications & Workshops","20091204","2009","","","1","7","This paper provides an in-depth study of wireless sensor network (WSN) application to monitor and control the swift habitat. A set of system is designed and developed which includes the node's hardware, GUI software, sensor network, and interconnectivity for remote data access and management. System architecture is proposed to address the requirements for habitat monitoring. The application driven designed, provides and identifies important areas of work in data sampling, communications and networking. In this monitoring system, a sensor node (MTS400), IRIS and Micaz radio transceivers, and a USB interfaced gateway base station of Crossbow (Xbow) Technology WSN are employed. The Graphical User Interface (GUI) is written using a Laboratory Virtual Instrumentation Engineering Workbench (LabVIEW) along with Xbow Technology drivers provided by National Instrument. This monitoring system is capable of collecting data and presents it in both tables and waveform charts for further analysis. This system is also able to send notification messages by e-mail, whenever changes on the swift habitat at remote sites (swift farms) occur, via the Internet connectivity. Other functions that have been implemented in this system are the database system for record and management purposes; remote access through the internet using LogMeIn software. Finally, this research draws a conclusion that a wireless sensor network for monitoring swift habitat can be effectively used to monitor and manage swift farming industry in Sarawak.","2157-0221;21570221","POD:978-1-4244-3942-3","10.1109/ICUMT.2009.5345571","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5345571","","Application software;Birds;Graphical user interfaces;Hardware;Instruments;Internet;Remote monitoring;Sensor systems;Software development management;Wireless sensor networks","Internet;farming;graphical user interfaces;information retrieval;monitoring;transceivers;wireless sensor networks","IRIS;Internet connectivity;LabVIEW;Micaz radio transceivers;USB interfaced gateway base station;crossbow technology;graphical user interface;laboratory virtual instrumentation engineering workbench;remote data access;remote data management;swift bird farms monitoring;swift habitat;system architecture;wireless sensor networks","","3","","16","","","12-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Applying Collective Intelligence for Search Improvement on Thai Herbal Information","V. Lertnattee; S. Chomya; T. Theeramunkong; V. Sornlertlamvanich","Fac. of Pharmacy, Silpakorn Univ., Nakorn Pathom, Thailand","2009 Ninth IEEE International Conference on Computer and Information Technology","20091117","2009","2","","178","183","Knowledge about herbal medicine can be contributed from experts in several cultures. With the conventional techniques, it is hard to find the way which the experts can build a self-sustainable community for exchanging their information. In this paper, the Knowledge Unifying Initiator for Herbal Information (KUIHerb) is used as a platform for building a web community for collecting the intercultural herbal knowledge with the concept of a collective intelligence. With this system, herb identification, herbal vocabulary and medicinal usages can be collected from this system. KUIHerb provides herbal vocabulary which is dynamically and confidentially applied for searching improvement on the Thai herbal search engine. Three strategies are utilized: (1) providing a set of technical terms in Thai with can be added into the dictionary. These terms are utilized by Thai word segmentation for improving the indexing process (2) A set of synonyms of these technical terms in both Thai and English is built for helping users from a lot of keywords of the same term and (3) a set of keywords from herbal usages can be combined with the name keyword. From the results, information collected from KUIHerb is useful for searching.","","POD:978-0-7695-3836-5","10.1109/CIT.2009.52","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5329112","collective intelligence;herbal information;search engine;web application","Computational intelligence;Computational linguistics;Electronic mail;Information technology;Intelligent structures;Internet;Laboratories;Search engines;Terminology;Vocabulary","Internet;indexing;information retrieval;medical computing;search engines;text analysis","KUIHerb;Knowledge Unifying Initiator for Herbal Information;Thai herbal information;Thai herbal search engine;Thai word segmentation;Web community;collective intelligence;herb identification;herb medicinal usage;herbal vocabulary;indexing process;intercultural herbal knowledge","","2","","10","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Research on Prototype Framework of a Multi-Threading Web Crawler for E-Commerce","W. Yuan","Sch. of Inf. Manage., WuHan Univ., Wuhan, China","2009 International Conference on Management and Service Science","20091030","2009","","","1","5","Web crawlers facilitate the search engine's work by following the hyperlinks in Web pages to automatically download a partial snapshot of the Web. Crawling is the initial and also the most important step during the Web searching procedure. A prototype framework of a multi-threading Web crawler for E-commerce application is proposed, in relationship to the former research of search engine. And the design and implementation of a multi-threading Web crawler is described and discussed. The experiment result demonstrates this prototype of Web crawler has better performance.","","POD:978-1-4244-4638-4","10.1109/ICMSS.2009.5304437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304437","","Costs;Crawlers;Hardware;Information management;Network servers;Prototypes;Search engines;Uniform resource locators;Web pages;Web server","Internet;electronic commerce;information retrieval;multi-threading;search engines","E-commerce;Web page hyperlinks;multithreading Web crawler;search engine","","0","","6","","","20-22 Sept. 2009","","IEEE","IEEE Conference Publications"
"Integrating Structural Data into Methods for Labeling Relations in Domain Ontologies","G. Wohlgenannt; A. Weichselbraun; A. Scharl","Inst. for Inf. Bus., Vienna Univ. of Econ., Vienna, Austria","2009 20th International Workshop on Database and Expert Systems Application","20091117","2009","","","94","98","This paper presents a method for integrating DBpedia data into an ontology learning system that automatically suggests labels for relations in domain ontologies based on large corpora of unstructured text. The method extracts and aggregates verb vectors for semantic relations identified in the corpus. It composes a knowledge base which consists of (i) centroids for known relations between domain concepts, (ii) mappings between concept pairs and the types of known relations, and (iii) ontological knowledge retrieved from DBpedia.Refining similarities between the verb centroids of labeled and unlabeled relations by means of including domain and range constraints applying DBpedia data yields relation type suggestions. A formal evaluation compares the accuracy and average ranking performance of this hybrid method with the performance of methods that solely rely on corpus data and those that are only based on reasoning and external data sources.","1529-4188;15294188","POD:978-0-7695-3763-4","10.1109/DEXA.2009.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337361","data integration;ontology learning;relation labeling;structural data","Labeling;Ontologies","information retrieval;knowledge based systems;ontologies (artificial intelligence)","DBpedia data;corpus data;domain ontologies;knowledge base;ontology learning system;relation labeling;structural data integration;verb centroids;verb vector extraction","","0","","13","","","Aug. 31 2009-Sept. 4 2009","","IEEE","IEEE Conference Publications"
"Extracting company information from the web","M. I Lam; Z. Gong; J. Guo","Faculty of Science and Technology, University of Macau, Macao, PRC","2009 IEEE International Conference on Systems, Man and Cybernetics","20091204","2009","","","3640","3645","As World Wide Web is becoming the most important information repository, increasing amount of information is available. Currently, web search engines can only provide document oriented searches. In order to fully make use of information from the web, some effective and efficient extraction algorithms are definitely desirable. In this paper, some existing achievements are investigated firstly. Then our current technique on web information extraction is discussed in detail. In our approach, rules and patterns are extracted from sample pages through training process, with human involvements. We use both keywords and regular expressions to represent rules and patterns in our system. The keywords work as anchors to locate the positions of the potential information and regular expressions work as validations of the values. In our system, all the extracted information is represented in XML format.","1062-922X;1062922X","POD:978-1-4244-2793-2","10.1109/ICSMC.2009.5346863","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346863","","Cybernetics;Data mining;Database languages;HTML;Humans;Internet;Search engines;Web pages;Web sites;XML","Internet;human factors;information retrieval","Web search engines;World Wide Web;XML format;company information extraction;human involvements;information repository;keywords;training process","","0","","20","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Research of Enterprise Competition Information Intelligent Collecting System Based on Rough Set","Y. Kefei; P. Yu; H. Bin; P. Fang","Coll. of Manage. Sci. & Eng., Nanjing Univ. of Technol., Nanjing, China","2009 WRI World Congress on Software Engineering","20091110","2009","2","","117","121","According to the characteristics of incompleteness, contradictory and extensiveness of the enterprise competition information, rough set theory is used to guide the crawling of Web spider, the gathering efficiency of enterprise completion collection is improved by similarity class expansion and attributes reduction based on discrimination. The achievement of this research has already been applied to the practical construction of information management center of an enterprise so as to offer strategic basis for operation decision of the enterprise.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.348","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319697","","Arm;Databases;Educational institutions;Information analysis;Intelligent systems;Search engines;Set theory;Software engineering;Technology management;Web pages","Internet;business data processing;data mining;information centres;information retrieval;pattern classification;rough set theory;search engines","Web spider crawling;attribute reduction;data mining;enterprise competition information intelligent collection system;enterprise information management center;information gathering;rough set theory;similarity class expansion;strategic operation decision basis","","0","","3","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"Survey and 3D Reconstruction of the St. Orso Capitals in Aosta, through Three-Focal Photogrammetry","P. Salonia; S. Scolastico; A. Marcolongo; T. L. Messina","Inst. for Technol. Appl. to Cultural Heritage, Nat. Res. Council, Rome, Italy","2009 15th International Conference on Virtual Systems and Multimedia","20091030","2009","","","35","40","This paper describes the methodology adopted and the results obtained during the 3D documentation of the medieval capitals of St. Orso Collegiate Church, in Aosta, for reference and digital archiving. This task has been carried out through low cost technology entirely based on digital scanning of high quality images applying stereophotogrammetric principles. This technology allows to obtain point clouds with RGB information and geometries at different levels of complexity, processing a number of images taken with a limited set of constraints, with the use of a special acquisition equipment and through an image matching algorithm. Issue in this research was to obtain the accurate 3D information required to build models of artifacts of a such heritage importance, with a high degree of complexity. In future the 3D model will contribute to create a spatial information system which will assist conservation and restoration activities on the one hand and will be used for tourist information inside the St. Orso Collegiate Church on the other.","","POD:978-0-7695-3790-0","10.1109/VSMM.2009.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306037","Cultural Heritage;digital models;photogrammetry;points clouds;three focal geometry","Clouds;Costs;Councils;Cultural differences;Documentation;Image reconstruction;Information geometry;Laser modes;Multimedia systems;Solid modeling","humanities;image colour analysis;image matching;image reconstruction;information retrieval systems;photogrammetry;stereo image processing","3D documentation;3D reconstruction;RGB information;St. Orso Capitals;St. Orso Collegiate Church;cultural heritage;digital archiving;digital scanning;image matching algorithm;stereophotogrammetric principles;three-focal photogrammetry","","0","","8","","","9-12 Sept. 2009","","IEEE","IEEE Conference Publications"
"Using Wikipedia as a Reference for Extracting Semantic Information from a Text","A. Prato; M. Ronchetti","Dipt. di Ing. e Scienza dell'Inf., Univ. di Trento, Povo di Trento, Italy","2009 Third International Conference on Advances in Semantic Processing","20091023","2009","","","56","61","In this paper we present an algorithm that, using Wikipedia as a reference, extracts semantic information from an arbitrary text. Our algorithm refines a procedure proposed by others, which mines all the text contained in the whole Wikipedia. Our refinement, based on a clustering approach, exploits the semantic information contained in certain types of Wikipedia hyperlinks, and also introduces an analysis based on multi-words. Our algorithm outperforms current methods in that the output contains many less false positives. We were also able to understand which (structural) part of the texts provides most of the semantic information extracted by the algorithm.","","POD:978-1-4244-5044-2","10.1109/SEMAPRO.2009.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291534","Semantic analysis;Wikipedia;clustering;multi-words","Clustering algorithms;Data mining;Encyclopedias;History;Humans;Information analysis;Logic;Ontologies;Statistics;Wikipedia","Web sites;data mining;information retrieval;text analysis","Wikipedia;clustering;data mining;multiword analysis;semantic information extraction;text mining","","1","","11","","","11-16 Oct. 2009","","IEEE","IEEE Conference Publications"
"An adaptive bottom up clustering approach for Web news extraction","J. Chen; S. Shankar; A. Kelly; S. Gningue; R. Rajaravivarma","Computer Science Dept., Queens College, CUNY, Flushing, NY, 11367, USA","2009 18th Annual Wireless and Optical Communications Conference","20091106","2009","","","1","5","An adaptive bottom up Web news extraction approach based on human perception is presented in this paper. The approach simulates how a human perceives and identifies Web news information by using an adaptive bottom up clustering strategy to detect possible news areas. It first detects news areas based on content function, space continuity, and formatting continuity of news information. It further identifies detailed news content based on the position, format, and semantic of detected news areas. Experiment results show that our approach achieves much better performance (in average more than 99% in terms of F1 Value) compared to previous approaches such as tree edit distance and visual wrapper based approaches. Furthermore, our approach does not assume the existence of Web templates in the tested Web pages as required by tree edit distance based approach, nor does it need training sets as required in Visual Wrapper based approach. The success of our approach demonstrates the strength of the perception based Web information extraction methodology and represents a promising approach for automatic information extraction from sources with presentation design for humans.","2379-1268;23791268","POD:978-1-4244-5217-0","10.1109/WOCC.2009.5312904","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5312904","Web news;clustering;component;information extraction","Cadaver;Cities and towns;Computational modeling;Computer science;Data mining;Educational institutions;HTML;Humans;Testing;Web pages","Internet;information retrieval;pattern clustering","Web information extraction methodology;Web news extraction approach;Web template;adaptive bottom-up clustering approach;content function;formatting continuity;human perception;space continuity;tree edit distance-based approach;visual wrapper-based approach","","4","","15","","","1-2 May 2009","","IEEE","IEEE Conference Publications"
"Client-side mobile user profile for content management using data mining techniques","W. Paireekreng; K. W. Wong","School of Information Technology, Murdoch University, Australia, WA, 6150","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","96","100","Mobile device can be used as a medium to send and receive the mobile Internet content. However, there are several limitations using mobile Internet. Content personalisation has been viewed as an important area when using mobile Internet. In order for personalisation to be successful, understanding the user is important. In this paper, we explore the implementation of the user profile at client-side, which may be used whenever user connect to the mobile content provider. The client-side user profile can help to free the provider in performing analysis by using data mining technique at the mobile device. This research investigates the conceptual idea of using clustering and classification of user profile at the client-site mobile. In this paper, we applied K-means and compared several other classification algorithms like TwoStep, Kohenen and anomaly to determine the boundaries of the important factors using information ranking separation.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340939","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340939","","Content management;Context-aware services;Customer satisfaction;Data mining;Demography;Hardware;Mobile computing;Mobile handsets;Natural language processing;Web and internet services","Internet;client-server systems;content management;data mining;information retrieval;mobile computing;pattern classification","K-means classification algorithm;client-side mobile user profile;content management;data mining technique;information ranking separation;mobile Internet content personalisation;mobile device;user profile classification;user profile clustering","","3","2","16","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"A Redundancy Based Term Weighting Approach for Text Categorization","Z. Y. Lu; Y. M. Lin; S. Zhao; J. N. Chen; W. D. Zhu","Coll. of Econ. & Manage., Hebei Polytech. Univ., Tangshan, China","2009 WRI World Congress on Software Engineering","20091110","2009","2","","36","40","With the rapid development of World Wide Web, text categorization has played an important role in organizing and processing large amount of text data. TFmiddotIDF is a simple and quick term weighting method, and widely used in text categorization. But the drawback of TFmiddotIDF is large weight may be assigned to rarely appeared terms in despite of the posterior distribution. This paper presents a redundancy based term weighting method to solve this problem by taking posterior probability distribution into consideration. Experiments on Reuters-21578 and Chinese corpus provide by Computer and Information Technology Data Center of Fudan University show that this weighting method has better performance over TFmiddotIDF.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.191","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319713","","Educational institutions;Engineering management;Frequency measurement;Information technology;Organizing;Probability distribution;Software development management;Software engineering;Text categorization;Web sites","Internet;information retrieval;statistical distributions;text analysis","TF-IDF;World Wide Web;inverse document frequency;posterior probability distribution;redundancy-based term weighting;term frequency;text categorization","","0","","13","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"Entity relation extraction to free text","S. Zhang","Department of Electronic and Communication Engineering of North China Electric Power University, 071003 Baoding, China","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","5","A novel approach of the entity relation extraction is proposed by this paper, it is different from the previous approaches, and the syntactic knowledge extraction is specific section, which automatically extracts the characteristic words and patterns based on hierarchy bootstrapping machine learning. It advocates using a small amount of seed information and a large collection of easily-obtained unlabeled data. Hierarchy bootstrapping makes use of seed words and seed patterns to build a learning program, which extracts more characteristic words using scalar clusters. These characteristic words have semantic similarity with seed words. Then more extraction patterns could be learned automatically and added to the knowledge base, moreover, we also pay attention to semantic and pragmatic knowledge for entity relation extraction. Moreover, the evaluation way belongs to the MUC. According to our experimental results, we can find it is useful method.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313758","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313758","Bootstrapping;Entity Relation Exaction;Information Exaction;Pragmatic Information","Data mining;Feature extraction;Filling;Kernel;Knowledge engineering;Learning systems;Machine learning;Pattern analysis;Power engineering and energy;Text recognition","information retrieval;learning (artificial intelligence);natural language processing;text analysis","characteristic words extraction;entity relation extraction;free text;hierarchy bootstrapping machine learning;scalar clusters;syntactic knowledge extraction","","0","","12","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"Accessing Libraries of Media Art through Metadata","A. Ludtke; B. Gottfried; O. Herzog; G. Ioannidis; M. Leszczukz; V. imko","Technol.-Zentrum Inf., Univ. Bremen, Bremen, Germany","2009 20th International Workshop on Database and Expert Systems Application","20091117","2009","","","269","273","Being faced with digital libraries containing images and video content, means are required that characterize the content for efficient access. In addition, the confluence of media content which is distributed over a number of content providers requires a common and standardized way for searching the content. The usual solution consists in the employment of metadata which describes media content. Among others, issues that arise concern the kind of metadata to be used, how it is to be represented, and how it is to be integrated into the architecture of a portal for accessing the distributed digital archives. The methods presented in this paper have been implemented in the context of the project Gateway to Archives of Media Art (GAMA for short). The objective of this project is to establish a portal for online access to some of the most important digital archives and libraries on media art in Europe.","1529-4188;15294188","POD:978-0-7695-3763-4","10.1109/DEXA.2009.93","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337162","","Art;Data mining;Databases;Employment;Europe;Expert systems;Image analysis;Portals;Production;Software libraries","art;digital libraries;information retrieval systems;meta data;portals","Gateway to Archives of Media Art project;content providers;digital libraries;distributed digital archives;media art accessing libraries;media art content portal;media content;metadata;online access;portal architecture","","2","","13","","","Aug. 31 2009-Sept. 4 2009","","IEEE","IEEE Conference Publications"
"Text mining: Finding hot topics TF∗PDF vs. LSI","J. Katyayani; A. V. Sriharsha; B. Sudhir","Sri Padmavathi Mahila Visva Vidyalayam, Tirupati, India","2009 IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications","20091201","2009","","","526","529","With the vast amount of digital text materials available on the Net, it is almost impractical for people to absorb all related information in a timely manner. This problem has been overcome by erstwhile researchers and scientists of data mining. The efficiency in the methods and exploratory analysis has to be ascertained yet. Document wise term frequencies and inverted frequencies are available to calculate the statistical importance among the documents. Determining the time line importance of the documents plays very essential role than just finding the document's importance. LSI is a basic PCA approach, which is proposed with time-line approach and has been discussed comparatively in this paper.","","POD:978-1-4244-4901-9","10.1109/IDAACS.2009.5342925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342925","IR;Text mining;dimensionality reduction;latent-semantic indexing","Conferences;Data acquisition;Data mining;Educational institutions;Event detection;Explosions;Frequency;Intelligent systems;Large scale integration;Text mining","data mining;database management systems;information retrieval;principal component analysis;text analysis","LSI;PCA approach;TF*PDF;digital text material;dimensionality reduction;document importance;inverted frequency;latent semantic indexing;statistical importance;text database;text mining;text retrieval indexing technique;time line importance","","0","","6","","","21-23 Sept. 2009","","IEEE","IEEE Conference Publications"
"Basics of Concepts Representation for Document Summarization","S. C. Suh; S. I. Saffer; S. G. Anaparthi; N. M. Sirakov","Dept. of Comput. Sci., Texas A&M Univ. - Commerce, Commerce, TX, USA","2009 Fifth International Joint Conference on INC, IMS and IDC","20091113","2009","","","1374","1380","This paper presents three different ways to describe the notion concept. The first one uses the idea of hierarchy and employs a graph to define the connections between attributes and concepts. To enable concepts generation, manipulation or measurement a matrix model is developed. Thus, the entire space of terms could be generated by a set of (linearly) independent terms over a numerical field. The total number of concepts to be generated over a numerical field using a given number of attributes is estimated in this paper. A memory efficient spherical presentation is developed on the above models. An algorithmic implementation of Hierarchy of concepts and attributes (HAC) is developed and experimental results are presented using concepts from the field of education.","","POD:978-1-4244-5209-5","10.1109/NCM.2009.255","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331413","attributes;hierarchy;matrix;spherical model","Application software;Business;Computer science;Computer science education;Internet;Mathematical model;Mathematics;Merging;Search engines;Web search","data mining;graph theory;information retrieval;text analysis;vocabulary","concepts generation;concepts manipulation;document summarization;graph;hierarchy of concepts and attributes;independent terms;matrix model;memory efficient spherical presentation;notion concept","","0","","14","","","25-27 Aug. 2009","","IEEE","IEEE Conference Publications"
"Web behind Web - A Steganographic Web Framework","H. Hioki","Grad. Sch. of Human & Environ. Studies, Kyoto Univ. Kyoto, Kyoto, Japan","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20091117","2009","","","60","63","This paper presents a Web framework as an application of steganography. The framework enables us to stealthily organize a tree of Web objects behind another. The Web objects to be embedded are automatically assigned to cover files appropriately. When embedding is done, we obtain stego-objects those can be uploaded to a Web server as ordinary Web objects. We can retrieve files embedded in stego-objects by a common Web browser via a dedicated proxy.","","POD:978-1-4244-4717-6","10.1109/IIH-MSP.2009.270","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337304","proxy;steganography;web","Engines;File servers;Humans;Payloads;Robustness;Security;Signal processing;Steganography;Web server;World Wide Web","Internet;information retrieval;online front-ends;steganography;tree data structures","Web browser;Web object tree;Web server;Web-behind-Web framework;cover file retrieval;dedicated proxy;steganographic Web framework;stego-object embedding","","0","","4","","","12-14 Sept. 2009","","IEEE","IEEE Conference Publications"
"The Impact of Information upon the Exploitation Sequence of Exhaustible Resources: A Searching Model and the Factor Analysis","G. q. Wang; X. p. Wei","Sch. of Manage., China Univ. of Min. & Technol., Xuzhou, China","2009 International Conference on Management and Service Science","20091030","2009","","","1","4","Many scholars have studied the optimum exploitation sequence of the exhaustible resource, but one of its common characteristics is to establish a planning model and to seek the optimum solution under different bound conditions. This paper establishes a model from the angle of information searching and reveals the motives of the arrangement of exploitation sequence for the contemporary people. This model analyzes the mechanism of technical progress, exploitation reserves, information symmetry and searching cost in the system. The study finds out there is common influence of technology and reserves in the allocation of the exhaustible resources. The best degree of information symmetry in the exploitation sequence system is a special value decided by the increasing technical progress, the increasing exploitation reserves and single phase searching cost.","","POD:978-1-4244-4638-4","10.1109/ICMSS.2009.5304374","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5304374","","Costs;Economic indicators;Electronic mail;Engineering management;Information analysis;Project management;Research and development management;Resource management;Technology management;Technology planning","information management;information retrieval","exhaustible resources;exploitation reserves;factor analysis;information searching;information symmetry;optimum exploitation sequence;planning model;searching model","","0","","12","","","20-22 Sept. 2009","","IEEE","IEEE Conference Publications"
"Reordered tilebars for visual text exploration","V. Thai; S. Handschuh","Digital Enterprise Research Institute, National University of Ireland, Galway, Ireland","2009 IEEE Symposium on Visual Analytics Science and Technology","20091113","2009","","","225","226","The classic TileBars paradigm has been used to show distribution information of query terms in full-text documents. However, when the number of query terms becomes large, it is not an easy task for users to comprehend their distribution within certain parts of a document. In this paper, we present a novel approach to improve the visual presentation of TileBars, in which barycenter heuristic for bigraph crossing minimization is used to reorder TileBars elements. The reordered TileBars can be demonstrated to provide users with better focus and navigation while exploring text documents.","","POD:978-1-4244-5283-5","10.1109/VAST.2009.5333436","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333436","Graphical user interfaces;H.5.2 [Information interfaces and presentation]: User Interfaces","Bars;Bipartite graph;Frequency;Navigation;Ontologies;Pattern analysis;Prototypes;Smoothing methods;User interfaces;Visual analytics","graphical user interfaces;information retrieval;text analysis;visual databases","TileBars;barycenter heuristic;bigraph crossing minimization;distribution information;full-text documents;query terms;visual presentation;visual text exploration","","0","","9","","","12-13 Oct. 2009","","IEEE","IEEE Conference Publications"
"Extracting historical terms based on aligned Chinese-English parallel corpora","X. Li; C. Che; L. H. Liu","Dalian University of Technology Dalian, Liaoning, China","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","6","This paper examines the feasibility of implementing statistic-oriented term extraction and evaluation methods in extracting historical terms from aligned parallel corpora of Chinese historical classics and their translations. It proposes to take transliteration as anchor points to establish sentence-level alignment. It also investigates the approach to extract term translation pairs based on 4000 parallel sentences or segments of sentences from the corpora of the Chinese historical classic Shi Ji (Records of the Historian) and its English translations by two well-known translators. The experimental results indicate that the statistically sound algorithm can successfully extract those terms whose English translations are consistent throughout the corpus and those transliterated pairs, but fails to extract the translations of those terms that are translated differently by the two translators although the translations may be equally qualified in terms of their usage in the English language. The algorithm also fails to extract the top frequency terms which are ambiguous in meaning due to changes of its part of speech. Therefore, this paper suggests insights gained from the linguistic and translation studies perspectives can be integrated with the statistic measurements to improve the extraction and validating results.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313766","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313766","Chinese historical classics;Historical term;extraction;parallel corpora","Channel hot electron injection;Chaos;Data mining;Frequency;Gain measurement;History;Natural language processing;Natural languages;Statistics;Terminology","computational linguistics;information retrieval;language translation;natural language processing;text analysis","Chinese historical classic Shi Ji;Chinese-English parallel corpora;English language;English translations;historical terms;linguistic;sentence-level alignment;statistic-oriented term evaluation;statistic-oriented term extraction","","0","","16","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"Soft-Link Spectral Clustering for Information Extraction","A. Celikyilmaz","Comput. Sci. Div., Univ. of California, Berkeley, CA, USA","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","434","441","Unsupervised spectral clustering methods can yield good performance when identifying crisp clusters with low complexity since the learning algorithm does not rely on finding the local minima of an objective function and rather use spectral properties of the graph. Nonetheless, the performance of such approaches are usually affected by their uncertain parameters. Using the underlying structure of a general spectral clustering method, in this paper a new soft-link spectral clustering algorithm is introduced to identify clusters based on fuzzy k-nearest neighbor approach. We construct a soft weight matrix of a graph by identifying the upper and lower boundaries of learning parameters of the similarity function, specifically the fuzzifier parameter (fuzziness) of the Fuzzy k-Nearest Neighbor algorithm. The algorithm allows perturbations on the graph Laplace during the learning stage by the changes on these learning parameters. With the empirical analysis using an artificial and a real textual entailment dataset, we demonstrate that our initial hypothesis of implementing soft links can improve the classification performance of final outcome.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298647","","Clustering algorithms;Clustering methods;Data mining;Graph theory;Laplace equations;Machine learning;Optimization methods;Partitioning algorithms;Performance analysis;Singular value decomposition","fuzzy set theory;graph theory;information retrieval;matrix algebra;pattern classification;pattern clustering","classification;fuzzy k-nearest neighbor algorithm;graph Laplace;information extraction;soft weight graph matrix;soft-link spectral clustering algorithm","","2","","18","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Enhancing Person Annotation for Personal Photo Management Applications","S. H. Cooray; N. E. O'Connor","CLARITY: Centre for Sensor Web Technol., Dublin City Univ., Dublin, Ireland","2009 20th International Workshop on Database and Expert Systems Application","20091117","2009","","","251","255","This paper addresses a sub-problem of the broad annotation problem, namely ""person annotation"", associated with personal digital photo management and investigates approaches to enhancing person annotation in personal photo management applications. We study a number of approaches to enhance the performance of semi-automatic person annotation using real-life personal photo collections as the test data. To this end, face and body-patch features are employed to describe the appearance of a person as a means to more effectively capture the identities of re-appearing people in personal photo archives. Experiments are carried out to identify a suitable initial annotation method, compare the performances of event-constrained person matching with global person matching, and the effect of the size of initial annotation on the overall performance of person annotation in real-life personal photo archives. The evaluation results, presented in terms of H-hit rate figures, illustrate that using event-constrained person matching with event-based initial annotation proves to be a better performing approach than global person matching for person annotation in personal photo archives. Results also clearly demonstrate the nature of compromise that needs to be made when annotating large photo collections in terms of accuracy against user-interaction.","1529-4188;15294188","POD:978-0-7695-3763-4","10.1109/DEXA.2009.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337462","content-based descriptors;face recognition;person annotation;personal photo management","Content management;Databases;Digital cameras;Expert systems;Face recognition;Paper technology;Performance evaluation;Sensor systems and applications;Technology management;Testing","content-based retrieval;digital photography;face recognition;feature extraction;image matching;image retrieval;information retrieval systems","H-hit rate figure;body-patch feature;content-based descriptor;event-based initial annotation;event-constrained person matching;face recognition;face-patch feature;global person matching;image retrieval;personal digital photo management application;real-life personal photo archive;semiautomatic person annotation enhancement;user interaction","","2","","12","","","Aug. 31 2009-Sept. 4 2009","","IEEE","IEEE Conference Publications"
"Designing a Japanese idiom education support system for overseas' students","K. Yusuke; R. Li; F. Ren","Graduate School of Advanced Technology and Science, The University of Tokushima Minami Josanjima, Tokushima, 7708506, Japan","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","8","The Japanese idioms are phrases which two or more words combine and have the different meaning from the original word. Therefore, learning a Japanese idiom is difficult for Japanese language learners. We are developing a Japanese Idiom Education Support System for Overseas' Students to solve this problem. The main purposes of the system are to raise learner's interest of learning idiom, and to teach the right usage of an idiom. The main functions of this system are the function of Japanese idiom retrieval and teaching Japanese idiom. The learner can input the idiom into the system and the system can retrieve and display an explanation of the idiom. And the learner can input the situation into the system and the system can retrieve the idiom. In this paper, the outline of Japanese Idiom Education Support System for Over-sea's Students, the method of Japanese idiom retrieval, and the result of questionnaire is presented. A questionnaire did the survey on attitudes toward educational support system.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313789","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313789","","Computer science;Computer science education;Cultural differences;Displays;Educational institutions;Educational technology;Fires;Natural languages;Nose;Statistics","computer aided instruction;information retrieval;linguistics;natural languages","Japanese idiom education support system;Japanese idiom retrieval;Japanese idiom teaching;Japanese language learner;overseas students","","1","","13","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"A broker-assisting trust and reputation system based on artificial neural network","B. Zong; F. Xu; J. Jiao; J. Lv","State Key Laboratory for Novel Software Technology, Nanjing 210093, China","2009 IEEE International Conference on Systems, Man and Cybernetics","20091204","2009","","","4710","4715","Due to the dynamic and anonymous nature of open environments, it is critically important for agents to identify trustful cooperators which work consistently as they claim. In the e-services and e-commerce communities, trust and reputation systems are applied broadly as one kind of decision support systems, and aim to cope with the consistency problems caused by uncertain trust relationships. However, challenges still exist: on the one hand, we require more flexible trust computation models to satisfy various personal requirements since agents in these communities are heterogeneous; on the other hand, trust and reputation systems calculate the trustworthiness of agents based on the agents' past behavior. The open environments are dynamic, agents are anonymous and the records about agents' past behavior are distributed in the environments, so agents have to search the required records through the environments due to their lack of valid information. Thus, efficient, scalable and effective information collection strategies are required to address these issues. In this paper we present a distributed trust and reputation system to cope with the challenges. We propose a novel and flexible trust computation model based on artificial neural networks. With the advantages of ANN, our trust model tunes the parameters automatically to adapt to various personal requirements. We propose a broker-assisting information collection strategy based on clustering method. With the support of brokers, subcommunities are managed by reputation mechanism in an efficient and scalable way and help their members collect information with high quality. We show the performance of our trust and reputation system by simulation.","1062-922X;1062922X","POD:978-1-4244-2793-2","10.1109/ICSMC.2009.5346098","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346098","","Artificial neural networks;Clustering methods;Computational modeling;Computer science;Cybernetics;Decision support systems;History;Laboratories;Quality management;USA Councils","artificial intelligence;decision support systems;distributed processing;information retrieval;neural nets;pattern clustering;security of data","ANN;artificial neural network;broker-assisting information collection strategy;broker-assisting trust and reputation system;clustering method;decision support systems;distributed trust and reputation system;e-commerce;e-services;trust computation models","","5","","18","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Information extraction from legal documents","T. T. Cheng; J. L. Cua; M. D. Tan; K. G. Yao; R. E. Roxas","BS Computer Science graduates from the De La Salle University, Manila, Philippines","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","157","162","Legal TRUTHS (turning unstructured texts to helpful structure) is a system that extracts relevant information from Philippine Supreme Court decisions, specifically on criminal cases. We describe here the processes involved in the development of Legal TRUTHS focusing on the issues relating to the domain and the geographical setting of the source documents, and the performance evaluation results are also presented. Pertinent information to be extracted for criminal cases such as the crime, the date and time of commission, the plaintiff, and the penalty were determined from a sample set of documents. Sections of these documents were identified for initial segmentation of the data. Automatic filtering of the data was involved in drawing out relevant information from the texts. From 25 training documents and also the same set for testing, performance showed over-all precision at 91.7%, recall at 99.5%, and F-measure at 95.6%. Testing on another 50 documents showed over-all precision at 84.3%, recall at 95.8%, and F-measure at 91.0%.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340925","","Data mining;Information filtering;Information filters;Law;Legal factors;Natural language processing;Tagging;Testing;Tin;Turning","criminal law;document handling;information retrieval","Philippine Supreme Court decisions;automatic data filtering;criminal cases;helpful structure;information extraction;legal TRUTHS;legal documents;turning unstructured texts","","2","","14","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Ontology-centric, Service-Oriented Enterprise Campaign Management System","W. T. Ang; W. P. Seeto; F. Tan; W. F. Tang; R. Kanagasabai","Inst. for Infocomm Res., Singapore, Singapore","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","442","449","Public relations is an important management staff function in this computer age, and accurate media information retrieval and appropriate measurement for publicity tracking in a campaign management system is of utmost importance. This paper proposes an intelligent campaign management system, called KnowleTracker, based on a semantic service-oriented approach. KnowleTracker has powerful deep mining functions to pull out news and other information that may lie several layers below the front page based on a semantic search for not only the specific keyword, but also the associated concepts that are not part of the keywords. The returned results are presented at different levels of detail and viewed from different angles, and show customers, detailed information of where their news releases are distributed. We demonstrate the utility of KnowleTracker in a real-world scenario through a case study on ""A*STAR Event Campaign Management"", and argue that our system is an effective tool to tackle the complex task of campaign management and tracking. A Web Demo is available at: http://datam.i2r.a-star.edu.sg/est/demo/icsc2009/.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298644","","Conference management;Industrial relations;Internet;Monitoring;Ontologies;Poles and towers;Power system management;Public relations;Technology management;Text mining","commerce;information retrieval;marketing;ontologies (artificial intelligence)","KnowleTracker;intelligent campaign management system;media information retrieval;ontology-centric enterprise campaign management system;publicity tracking;semantic service-oriented approach;service-oriented enterprise campaign management system","","2","","10","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Word Sense Disambiguation Based on Wikipedia Link Structure","A. Fogarolli","Univ. of Trento, Trento, Italy","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","77","82","In this paper an approach based on Wikipedia link structure for sense disambiguation is presented and evaluated. Wikipedia is used as a reference to obtain lexicographic relationships and in combination with statistical information extraction it is possible to deduce concepts related to the terms extracted from a corpus. In addition, since the corpus covers a representation of a part of the real world the corpus itself is used as rdquotraining datardquo for choosing the sense which best fit the corpus.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.7","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298935","Semantics;WSD;Wikipedia","Clustering algorithms;Computer networks;Data mining;Dictionaries;Feature extraction;Navigation;Predictive models;Relays;Training data;Wikipedia","Web sites;encyclopaedias;information retrieval;statistical analysis","Wikipedia link structure;Word sense disambiguation;lexicographic relationships;statistical information extraction;training data","","8","","16","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"The ALICE DAQ online databases","V. Altini; F. Carena; W. Carena; S. Chapeland; V. C. Barroso; F. Costa; R. Divia; M. Frauman; U. Fuchs; I. Makhlyueva; O. Rademakers; D. Rodriguez Navarro; F. Roukoutakis; K. Schossmaier; C. Soos; A. Telesca; P. Vande Vyvre; B. von Haller","Dipartimento di Fisica dell'Universit&#224; and Sezione, INFN, Bary, Italy","2009 16th IEEE-NPSS Real Time Conference","20091110","2009","","","361","365","ALICE (A Large Ion Collider Experiment) is the heavy-ion detector designed to study the physics of strongly interacting matter and the quark-gluon plasma at the CERN Large Hadron Collider (LHC). The ALICE Data Acquisition system (DAQ) is made of a large number of distributed hardware and software components, which rely on several online databases: the configuration database, describing the counting room machines, some detector-specific electronics settings and the DAQ and Experiment Control System runtime parameters; the log database, centrally collecting reports from running processes; the experiment logbook, tracking the run statistics filled automatically and the operator entries; the online archive of constantly updated data quality monitoring reports; the file indexing services, including the status of transient files for permanent storage and the calibration results for offline export; the user guides (Wiki); test databases to check the interfaces with external components; reference data sets used to restore known configurations. With 35 GB of online data hosted on a MySQL server and organized in more than 500 relational tables for a total of 40 million rows, this information is populated and accessible through various frontends, including C library for efficient repetitive access, Tcl/TK GUIs for configuration editors and log browser, HTML/PHP pages for the logbook, and command line tools for scripting and expert debugging. Exhaustive hardware benchmarks have been conducted to select the appropriate database server architecture. Secure access from private and general networks was implemented. Ad-hoc monitoring and backup mechanisms have been designed and deployed. We discuss the implementation of these complex databases and how the inhomogeneous requirements have been addressed. We also review the performance analysis outcome after more than one year in production and show results of data mining from this central information source.","","POD:978-1-4244-4454-0","10.1109/RTC.2009.5322155","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5322155","","Centralized control;Data acquisition;Detectors;Distributed databases;Electronic equipment testing;Hardware;Large Hadron Collider;Physics;Plasmas;Software quality","data acquisition;data mining;information retrieval systems;ion accelerators;physics computing","ALICE DAQ online databases;ALICE Data Acquisition system;C library;CERN;GUI;MySQL server;a large ion collider experiment;ad-hoc monitoring;counting room machines;data mining;database server architecture;detector-specific electronics;distributed hardware components;distributed software components;exhaustive hardware benchmarks;experiment control system runtime parameters;experiment logbook;expert debugging;file indexing services;heavy-ion detector;information source;large hadron collider;log database;quark-gluon plasma;updated data quality monitoring","","0","","10","","","10-15 May 2009","","IEEE","IEEE Conference Publications"
"Design and Performance Evaluation of a Versatile Object-Based File System","Q. Wei; Z. Li; R. V. Arumugam; K. K. Khaing","Data Storage Inst., A*STAR, Singapore, Singapore","2009 Sixth IFIP International Conference on Network and Parallel Computing","20091117","2009","","","133","139","The object-based storage system stripes data across large numbers of object-based storage devices (OSDs) to enable parallel data access. Subsequently, the workload presented to the individual OSD will be quite different from that of general purpose file systems. However, many distributed file systems employ general-purpose file systems as their underlying file system. This paper presents a versatile object-based file system (referred to as V-OBFS), an extent and B+tree based file system designed for use in OSDs. The V-OBFS implements flexible disk layout with multiple block size and differentiated free space management for both small objects and large objects. In addition, proposed V-OBFS enables fast object search and mapping between object ID and physical location with an efficient object namespace management. Our experiments show that our user-level implementation of V-OBFS can efficiently prevent file system fragmentation and outperforms Linux Ext2 and Ext3 by a factor of two or three.","","POD:978-1-4244-4990-3","10.1109/NPC.2009.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328455","file system;multiple block size;namespace;object-based storage","Aggregates;Bandwidth;Costs;File systems;Identity management systems;Image storage;Linux;Memory;Parallel processing;Writing","information retrieval systems;parallel processing","B+tree;differentiated free space management;flexible disk layout;multiple block size;object ID;object namespace management;object-based storage devices;object-based storage system;parallel data access;performance evaluation;system design;versatile object-based file system","","0","","19","","","19-21 Oct. 2009","","IEEE","IEEE Conference Publications"
"Domain Ontology Generation Based on WordNet and Internet","Y. Wang; Z. Zhou","Coll. of Phys. & Eng., Qufu Normal Univ., Qufu, China","2009 International Conference on Management and Service Science","20091030","2009","","","1","5","This paper presents a method of extracting domain ontology based on WordNet and Internet. Domain concepts are extracted from WordNet which contains concepts of almost all the fields. The extracting method is based on the IC-based semantic similarity, this paper gives a new model of semantic similarity which has better performance and is the guarantee of extracting domain concepts more accurately. In order to get the taxonomy relation between the domain concepts, the approach of taxonomy learning from Internet is given, which is based on the mutual information matrix. The generation method makes full use of the WordNet and Internet and can improve the efficiency of domain ontology generation.","","POD:978-1-4244-4638-4","10.1109/ICMSS.2009.5302770","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5302770","","Data mining;Educational institutions;Humans;Intelligent structures;Internet;Mutual information;Ontologies;Physics;Software libraries;Taxonomy","Internet;information retrieval;learning (artificial intelligence);matrix algebra;ontologies (artificial intelligence);text analysis","IC-based semantic similarity;Internet;domain concept extraction;domain ontology generation;mutual information matrix;taxonomy learning;wordnet","","1","","11","","","20-22 Sept. 2009","","IEEE","IEEE Conference Publications"
"A simple approach for Monolingual Event Tracking system in Bengali","A. K. Kolya; A. Ekbal; S. Bandyopadhyay","Department of Computer Science and Engineering, Jadavpur University, Kolkata-700032, India","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","48","53","Real-world applications have to take into consideration both topics and sentiments for precise opinion measurement. Topic and sentiment alignment is crucial for opinion retrieval, extraction, categorization, and aggregation on various issues. In this paper, we have reported a Monolingual Event (or, topic) tracking system for Bengali. The system has been developed based on a newspaper corpus developed from the Web archive of a leading Bengali newspaper. The goal of the system is to determine whether two news documents within a range of dates describe the same event. An event is a vector consisting of person, location, organization, title and date. A particular news document is described as a collection of such event vectors. A particular threshold value has been considered to check whether the number of event vectors of two separate news documents match at least by this threshold. Any particular news document of a date has been selected as the initial story. All the news documents within the preceding 15 and following 15 days have been considered as the target stories (or, documents). Evaluation results have demonstrated the Recall and Precision of 58.93% and 84.62%, respectively. The future works will look for interactions between topics and associated sentiments.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340908","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340908","","Application software;Blogs;Computer science;Discussion forums;Ecosystems;Event detection;Information analysis;Motion pictures;Natural language processing;Thumb","Internet;document handling;electronic publishing;information retrieval;natural languages;tracking","Bengali newspaper;Web archive;categorization;event vector;information retrieval;monolingual event tracking system;new document match;opinion extraction;opinion measurement;opinion retrieval;topic-sentiment alignment","","0","","26","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Extracting similar sub-graphs across PPI networks","V. Fionda; L. Palopoli; S. Panni; S. E. Rombo","Dept. of Mathematics, U. della Calabria, Rende (CS), Italy","2009 24th International Symposium on Computer and Information Sciences","20091023","2009","","","183","188","Singling out conserved modules (corresponding to connected sub-graphs) throughout protein-protein interaction networks of different organisms is a main issue in bioinformatics because of its potential applications in biology. This paper presents a method to discover highly matching sub-graphs in such networks. Sub-graph extraction is carried out by taking into account, on the one side, both protein sequence and network structure similarities and, on the other side, both quantitative and reliability information possibly available about interactions. The method is conceived as a generalization of a known technique, able to discover functional orthologs in interaction networks. Some preliminary experimental results obtained with both synthetic and real data are also accounted for in the paper.","","POD:978-1-4244-5021-3","10.1109/ISCIS.2009.5291845","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291845","","Bioinformatics;Biological information theory;Cells (biology);Cellular networks;Computational biology;Data mining;Evolution (biology);Mathematics;Organisms;Protein sequence","bioinformatics;graph theory;information retrieval;proteins","bioinformatics;network structure similarities;protein sequence;protein-protein interaction networks;similar subgraph extraction","","0","","28","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Searching Semantic Resources for Complex Selectional Restictions to Support Lexical Acquisition","M. Taylor; L. Carlson; S. Fontaine; S. Poisson","MITRE Corp., McLean, VA, USA","2009 Third International Conference on Advances in Semantic Processing","20091023","2009","","","92","97","Natural language processing systems are increasingly using ontologies and other large-scale semantic resources to support Verb Sense Disambiguation (VSD) and other applications. One of the ways in which these resources can be used is to identify the selectional restrictions on verb arguments needed for sense distinction. However, manually navigating such resources can be difficult and inefficient due to their size and complexity. In this paper, we present a process for automatically searching through an ontology to determine appropriate concepts for expressing selectional restrictions on verb sense. The goal of this research is to semi-automate the development of a semantically rich lexicon to support high-precision information extraction.","","POD:978-1-4244-5044-2","10.1109/SEMAPRO.2009.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291528","concept search;selectional restrictions;supervised learning","Data mining;Large-scale systems;Length measurement;Natural language processing;Navigation;Ontologies;Robustness;Sun;Supervised learning;Taxonomy","computational linguistics;information retrieval;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence)","complex selectional restrictions;high-precision information extraction;large-scale semantic resource searching;lexical acquisition support;natural language processing systems;ontologies;semantically rich lexicon;supervised learning;verb arguments;verb sense disambiguation","","0","","14","","","11-16 Oct. 2009","","IEEE","IEEE Conference Publications"
"Information Search Based on Test Mining and EXTJS","L. Yang; Y. Zhang; H. Li","Sch. of Inf. Eng., JDZ Ceramic Inst., China","2009 WRI World Congress on Software Engineering","20091110","2009","1","","386","391","One of information search based on text mining and EXTJS technology is provided, which uses the relative principles in the field of text mining and segmentation technology to preprocess and transform the information from internet. The problem of large data dimension caused by the redundancy characters extracted from the text is resolved and it improves the performance of the FE algorithm. Finally, the visual windows of results and easy operation web page interaction interface of management are showed and the burden of the CPU and memory of the server brought by the computing of the text mining is relieved by the new technology of EXTJS. The information decision system of the ceramic company testify that this method have the characters of Intelligent Behavior, precise result, beautiful and convenient interaction interface, easy operation and synchronization of running.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319134","feature extraction;information search;segmentation technology;test mining","Ceramics;Computer interfaces;Data mining;Internet;Iron;Memory management;Technology management;Testing;Text mining;Web pages","Internet;ceramic industry;data mining;feature extraction;information retrieval;text analysis","EXTJS technology;Extent JavaScript;Internet;ceramic company;feature extraction algorithm;information decision system;information search;large data dimension;segmentation technology;test mining;text mining;web page interaction interface","","0","","8","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"Improving Prediction Accuracy Using Entropy Weighting in Collaborative Filtering","H. J. Kwon; T. H. Lee; J. H. Kim; K. S. Hong","Sch. of Inf. & Commun. Eng., Sungkyunkwan Univ., Suwon, South Korea","2009 Symposia and Workshops on Ubiquitous, Autonomic and Trusted Computing","20091110","2009","","","40","45","In this paper, we evaluate performance of existing similarity measurement metric and propose a novel method using user's preferences information entropy to reduce MAE in memory-based collaborative recommender systems. The proposed method applies a similarity of individual inclination to traditional similarity measurement methods. We experiment on various similarity metrics under different conditions,which include an amount of data and significance weighting from n/10 to n/60, to verify the proposed method. As a result, we confirm the proposed method is robust and efficient from the viewpoint of a sparse data set, applying existing various similarity measurement methods and significance weighting.","","POD:978-1-4244-4902-6","10.1109/UIC-ATC.2009.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319264","","Accuracy;Collaboration;Collaborative work;Computer errors;Conferences;Filtering;Information entropy;Pervasive computing;Recommender systems;Robustness","entropy;information filters;information retrieval system evaluation","MAE;collaborative filtering;entropy weighting;information entropy;memory-based collaborative recommender systems;performance evaluation;prediction accuracy;significance weighting;similarity measurement metric;sparse data set","","2","1","19","","","7-9 July 2009","","IEEE","IEEE Conference Publications"
"2009 CCPR Keynotes","H. Uszkoreit","Saarland Univ., Saarbrucken, Germany","2009 Chinese Conference on Pattern Recognition","20091204","2009","","","i","iv","Minimally supervised machine learning methods based on bootstrapping are an attractive approach to advanced information extraction. Complex patterns signalling relevant semantic relations in free texts can be detected in this way. However, the potential and limitations of such methods are not yet sufficiently understood. We have systematically analyzed a bootstrapping approach. The starting point of the analysis is a pattern-learning graph, which is a subgraph of the bipartite graph representing all connections between linguistic patterns and relation instances exhibited by the data. It is shown that the performance of such general learning framework for actual tasks is dependent on certain properties of the data and on the seed construction. However, the greatest improvements can be obtained through the systematic learning of negative patterns.","","POD:978-1-4244-4199-0","10.1109/CCPR.2009.5344160","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5344160","","","computational linguistics;graph theory;information retrieval;learning (artificial intelligence);text analysis","bipartite subgraph;bootstrapping approach;free text detection;information extraction;minimally supervised machine learning method;positive-negative pattern learning graph;relation extraction;seed construction;semantic relation","","0","","","","","4-6 Nov. 2009","","IEEE","IEEE Conference Publications"
"Search and retrieval of multimedia objects over a distributed P2P network for mobile devices","D. Giakoumis; M. Lazaridis; A. Axenopoulos; D. Tzovaras; J. Trnkoczy; G. Paravati; A. Sanna; F. Lamberti; P. Di Torino; G. Hassapis","INFORMATICS AND TELEMATICS INSTITUTE","IEEE Wireless Communications","20091103","2009","16","5","42","49","Search for and delivery of multimedia content will be of great importance in the years to come. Common data formats and appropriate architectures that will pave the way toward this challenging task must be defined. In this article a framework enabling mobile device users to search for and retrieve multimedia objects over a distributed P2P-based network is presented. The framework was an outcome of the research that took place within the VICTORY European project. The proposed concept is to develop a P2P-based application that, having as a basis the state of the art in the field of 3D search algorithms, would be able to deliver search and retrieval functionalities to users on the move through mobile devices. Following a service-oriented approach, a framework able to deliver these functionalities to mobile user agents in a device- and application-independent way has been implemented.","1536-1284;15361284","","10.1109/MWC.2009.5300301","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5300301","","Computer architecture;Computer networks;Content based retrieval;Distributed computing;IP networks;Middleware;Mobile communication;Search engines;Solids;User interfaces","information retrieval;mobile computing;multimedia communication;multimedia computing;peer-to-peer computing","3D search algorithms;VICTORY European project;distributed peer-to-peer network;mobile devices;multimedia object retrieval;multimedia object search;service oriented approach","","1","","10","","","October 2009","","IEEE","IEEE Journals & Magazines"
"Consumer Information Search Behavior with Time Tolerance","X. Li; M. Huang; X. Li","Coll. of Basic Sci., Huazhong Agric. Univ., Wuhan, China","2009 International Conference on Management and Service Science","20091030","2009","","","1","4","Based on perceived trust and search time, this paper develops a consumer utility model that describes consumer information search behavior in online retailer. The model reveals that consumer utility is direct proportion to perceived trust in online retailer and inverse proportion to search time. Furthermore, consumer utility increases as search time increases within time threshold, while consumer utility decreases as search time increases. Finally, the time threshold of consumer information search is inverse proportional to price.","","POD:978-1-4244-4638-4","10.1109/ICMSS.2009.5303433","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5303433","","Companies;Costs;Decision making;Educational institutions;Information processing;Insurance;Internet;Marketing and sales;Profitability;Utility theory","consumer behaviour;information retrieval;pricing;retail data processing;utility theory","consumer information search behavior;consumer utility model;online retailer;pricing;search time tolerance","","0","","16","","","20-22 Sept. 2009","","IEEE","IEEE Conference Publications"
"A Method to Estimate Object Existence Based on Temporal Analysis of Web Pages for Digital Map Credibility","D. Kitayama; R. Lee; K. Sumiya","Sch. of Human Sci. & Environ., Univ. of Hyogo, Himeji, Japan","2009 20th International Workshop on Database and Expert Systems Application","20091117","2009","","","261","265","Digital maps are widely used these days. However, they often display geographical objects that are no longer present or do not display newly created geographical objects because the various map suppliers update their maps at different times. In other words, map media contain wrong information. Users who believe that a digital map contains accurate information may encounter problems when going somewhere using an incorrect digital map. We propose a calculation method to determine the degree of existence of an object for geographical objects displayed on digital maps in order to establish a map's credibility. Specifically, we first extract temporal Web pages from Web archives by searching for each geographical object as query. Next, we calculate the degree of existential support using the update history of extracted temporal Web pages. Finally, we calculate the degree of existence of an object based on all degrees of support. We display map content with a valid object to establish map credibility.","1529-4188;15294188","POD:978-0-7695-3763-4","10.1109/DEXA.2009.63","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337161","Digital maps;Information credibility;Temporal analysis;Web archives","Companies;Data mining;Databases;Displays;Expert systems;History;Humans;Information analysis;Object detection;Web pages","Internet;cartography;geographic information systems;information retrieval","Web archive;Web pages temporal analysis;digital map credibility;geographical objects display;map media;object existence estimation method;temporal Web pages extraction","","0","","13","","","Aug. 31 2009-Sept. 4 2009","","IEEE","IEEE Conference Publications"
"Research and Improvement of Search Engine Based on Lucene","Y. Zhang; J. l. Li","Coll. of Comput. & Commun., Lanzhou Univ. of Technol., Lanzhou, China","2009 International Conference on Intelligent Human-Machine Systems and Cybernetics","20091117","2009","2","","270","273","Lucene is currently and has been for a few years, the most popular free Java full-text retrieval library. Firstly, this article analyzes Lucence system structure, indexing mechanism, searching mechanism; secondly, it studies the sorting technology and how to adjust indexing performance; finally, the article presents a new retrieval sorting algorithm.","","POD:978-0-7695-3752-8","10.1109/IHMSC.2009.191","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5335990","indexing;maxMergeDocs;mergeFactor;new algorithm;satisfaction degree;searching;terms","Data mining;Databases;Educational institutions;Indexes;Indexing;Java;Packaging;Performance analysis;Search engines;Sorting","Java;indexing;information retrieval;search engines;sorting","Java full-text retrieval library;Lucence system structure analysis;indexing mechanism;indexing performance;new retrieval sorting algorithm;search engine;searching mechanism;sorting technology","","1","","6","","","26-27 Aug. 2009","","IEEE","IEEE Conference Publications"
"The Topic Similarity Computation Model Based Information Granularity","X. Liyong; D. Yanrong; X. Na; P. Caiyan; G. Liwei; K. Yan","Hebei Normal Univ. of Sci. & Technol., Qinhuangdao, China","2009 WRI World Congress on Software Engineering","20091110","2009","2","","12","15","Topic similarity computation model is a research hotspot in fields of information retrieval and text classification, etc. From the view of information granularity, this paper integrates traditional content topic identification with event topic identification, and presents a new topic similarity computation model, i.e., first carrying out content identification, secondly event identification. Finally, the model is evaluated by experiments.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319717","Concept Space;Information Extraction;Information Granularity;TDT;Topic Identification","Computational modeling","information retrieval;text analysis","content topic identification;event topic identification;information granularity;information retrieval;text classification;topic similarity computation model","","0","","6","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"A 7-Layer Model for Modernizing the World: A Step Towards a Hi-Tech World","R. Vaid; R. B. Patel","Dept. of Comput. Eng., M.M. Univ. Mullana (Ambala), Mullana, India","2009 International Conference on Advances in Recent Technologies in Communication and Computing","20091117","2009","","","840","843","In this paper we present a 7-layer model which consists of peer systems arranged logically in a hierarchical fashion. Every layer of the system is integrated with PMADE and mobile agents. This system is integrated with at least seven agents one at each layer. This system allocates 17 digit hexadecimal ID to every individual who is the citizen of the country which is a member of this system. It provides two types of services to every citizen. First type of service is shared between the citizens of the entire system provided at country layer and other type of service is shared between the citizens of a single country provided at state layer.","","POD:978-1-4244-5104-3","10.1109/ARTCom.2009.103","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328261","E-governance;E-services;Hi-Tech world;Information management;PMADE;mobile agent technology","Engineering management;Environmental management;Government;Information management;Management information systems;Mobile agents;Peer to peer computing;Quality management;Technology management;Web and internet services","fault tolerant computing;government data processing;information retrieval;mobile agents","7-layer model;E-governance;Hi-Tech world;PMADE;fault tolerant system;hexadecimal ID;information retrieval;mobile agent","","0","","9","","","27-28 Oct. 2009","","IEEE","IEEE Conference Publications"
"Ontological User Profiling and Language Modeling for Personalized Information Services","C. F. So; C. C. L. Lai; R. Y. K. Lau","Dept. of Inf. Syst., City Univ. of Hong Kong, Kowloon, China","2009 IEEE International Conference on e-Business Engineering","20091201","2009","","","559","564","With the explosive growth of the information and services deployed to the Web, there is a pressing need to develop an effective service for personalized services discovery and recommendation. Despite personalized services discovery methods have been studied by researchers before, few attempts have been made to explore ontological user profiling and probabilistic language modeling approach for service contextualization and service ranking. This paper makes a novel contribution in terms of developing an agent and ontology based user profiling mechanism to improve the service discovery processes. In particular, a novel probabilistic language modeling approach is developed to conduct service contextualization and service ranking. Our initial experimental results show that the proposed service personalization approach is promising when compared with a baseline system.","","POD:978-0-7695-3842-6","10.1109/ICEBE.2009.89","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342055","Domain Ontology;Language Modeling;Ontological User Profiling;Services Discovery;Services Personalization;Web Services","Clustering algorithms;Context awareness;Context modeling;Context-aware services;Explosives;History;Ontologies;Semantic Web;Taxonomy;Web services","Web services;information retrieval;ontologies (artificial intelligence);probability;user modelling","World Wide Web;agent development;ontological user profiling;personalized information service discovery;probabilistic language modeling;service contextualization;service ranking","","0","","26","","","21-23 Oct. 2009","","IEEE","IEEE Conference Publications"
"CRF-based active learning for Chinese named entity recognition","L. Yao; C. Sun; S. Li; X. Wang; X. Wang","Computer Science Department, HITSGS ShenZhen, China","2009 IEEE International Conference on Systems, Man and Cybernetics","20091204","2009","","","1557","1561","Conditional random fields (CRFs) have been used for many sequence labeling tasks and got excellent results. Further, the supervised model strongly depends on the huge training data. Active learning is a different way rather than relying on a large amount random sampling. However, random sampling constructively participates in the optimal choosing training examples. Based on different query strategies, active learning can combine with other machine learning methods to reduce the annotation cost while maintaining the accuracy. This paper proposes a new active learning strategy based on information density (ID) integrated with CRFs for Chinese named entity recognition (NER). On Sighan bakeoff 2006 MSRA NER corpus, an F1 score of 77.2% is achieved by using only 10,000 labeled training sentences chosen by the proposed active learning strategy.","1062-922X;1062922X","POD:978-1-4244-2793-2","10.1109/ICSMC.2009.5346315","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346315","active learning;conditional random field;information density;named entity recognition","Computer science;Costs;Cybernetics;Hidden Markov models;Labeling;Learning systems;Machine learning;Sampling methods;Training data;USA Councils","graph theory;information retrieval;learning (artificial intelligence);natural language processing","CRF-based active learning strategy;Chinese named entity recognition;conditional random fields;information density;machine learning methods;query strategies;random sampling;supervised model","","1","","16","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"A preliminary study of comparative and evaluative questions for business intelligence","N. Rose; T. Lim; P. Saint-Dizier; B. Gay; R. E. Roxas","De La Salle Univ. in Manila, Manila, Philippines","2009 Eighth International Symposium on Natural Language Processing","20091201","2009","","","35","41","Comparative and evaluative question answering (QA) systems provide objective answers to questions that involve comparisons and evaluations based on a quantifiable set of criteria. As evaluations involve inferences and computations, answers are not lifted from source text. This entails the need for correct semantic interpretation of comparative expressions, converting them to quantifiable criteria before data can be obtained from source text, processing these information, and formulating natural language answers from the result of the processing. As business intelligence (BI) requires comparisons and interpretations of seemingly unrelated facts, a QA system for this domain would be beneficial. This paper presents a study of some comparative and evaluative questions that are raised in the domain of business intelligence. How these questions are processed is also discussed.","","POD:978-1-4244-4138-9","10.1109/SNLP.2009.5340910","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5340910","","Bismuth;Books;Companies;Costs;Measurement standards;Natural language processing;Natural languages;Terminology;Transportation","competitive intelligence;information retrieval;natural languages","business intelligence;comparative expression;natural language answer;question answering system;semantic interpretation","","0","","13","","","20-22 Oct. 2009","","IEEE","IEEE Conference Publications"
"Automatic segmentation of clinical texts","E. Apostolova; D. S. Channin; D. Demner-Fushman; J. Furst; S. Lytinen; D. Raicu","College of Computing and Digital Media, DePaul University, Chicago, IL 60604","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20091113","2009","","","5905","5908","Clinical narratives, such as radiology and pathology reports, are commonly available in electronic form. However, they are also commonly entered and stored as free text. Knowledge of the structure of clinical narratives is necessary for enhancing the productivity of healthcare departments and facilitating research. This study attempts to automatically segment medical reports into semantic sections. Our goal is to develop a robust and scalable medical report segmentation system requiring minimum user input for efficient retrieval and extraction of information from free-text clinical narratives. Hand-crafted rules were used to automatically identify a high-confidence training set. This automatically created training dataset was later used to develop metrics and an algorithm that determines the semantic structure of the medical reports. A word-vector cosine similarity metric combined with several heuristics was used to classify each report sentence into one of several pre-defined semantic sections. This baseline algorithm achieved 79% accuracy. A support vector machine (SVM) classifier trained on additional formatting and contextual features was able to achieve 90% accuracy. Plans for future work include developing a configurable system that could accommodate various medical report formatting and content standards.","1094-687X;1094687X","CD-ROM:978-1-4244-3296-7","10.1109/IEMBS.2009.5334831","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334831","","","data structures;health care;information retrieval;support vector machines;text analysis","baseline algorithm;clinical narratives;clinical text automatic segmentation;free-text clinical narratives;hand-crafted rules;healthcare departments;information extraction;information retrieval;medical report formatting;medical report segmentation system;pathology;radiology;support vector machine classifier;word-vector cosine similarity metric","Algorithms;Artificial Intelligence;Documentation;Information Storage and Retrieval;Medical Records;Natural Language Processing;Pattern Recognition, Automated;Semantics","3","","15","","","3-6 Sept. 2009","","IEEE","IEEE Conference Publications"
"Bernoulli's principle of insufficient reason and conservation of information in computer search","W. A. Dembski; R. J. Marks","Professor of Philosophy, Southwestern Baptist Theological Seminary, Fort Worth, Texas","2009 IEEE International Conference on Systems, Man and Cybernetics","20091204","2009","","","2647","2652","Conservation of information (COI) popularized by the no free lunch theorem is a great leveler of search algorithms, showing that on average no search outperforms any other. Yet in practice some searches appear to outperform others. In consequence, some have questioned the significance of COI to the performance of search algorithms. An underlying foundation of COI is Bernoulli's Principle of Insufficient Reason(PrOIR) which imposes of a uniform distribution on a search space in the absence of all prior knowledge about the search target or the search space structure. The assumption is conserved under mapping. If the probability of finding a target in a search space is p, then the problem of finding the target in any subset of the search space is p. More generally, all some-to-many mappings of a uniform search space result in a new search space where the chance of doing better than p is 50-50. Consequently the chance of doing worse is 50-50. This result can be viewed as a confirming property of COI. To properly assess the significance of the COI for search, one must completely identify the precise sources of information that affect search performance. This discussion leads to resolution of the seeming conflict between COI and the observation that some search algorithms perform well on a large class of problems.","1062-922X;1062922X","POD:978-1-4244-2793-2","10.1109/ICSMC.2009.5346119","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5346119","Bernoulli's principle of insufficient reason;Bernoulli's principle of non-sufficient reason;Bertrand's paradox;active information;conservation of generalization performance;conservation of information;endogenous information;equal distribution of ignorance;evolutionary search;no free lunch theorem;principle of indifference;uniform distribution","Computer science;Cybernetics;Entropy;Information resources;Optimization methods;Robustness;USA Councils","information retrieval;search problems","computer search;conservation of information;no free lunch theorem;principle of insufficient reason;search algorithms;search target;uniform search space distribution","","7","","70","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Research about the Application of Web Mining in Distance Education Platform","Y. Qu; L. Zhong; H. Zou; C. Wang","Coll. of Math. Phys. & Inf. Eng., Zhejiang Normal Univ., Jinhua, China","2009 International Conference on Scalable Computing and Communications; Eighth International Conference on Embedded Computing","20091201","2009","","","508","513","With the development of Internet, distance education platforms are very popular in today's society, but there are quite a few problems existent, such as the inadequate utilization of network teaching resources and the lack of individuation of the existed distance education platforms. To solve these problems have become the key of designing a good distance education platform. Web mining refers to the process of extracting useful data and information from Web sites or Web pages. In this paper we mainly discuss how to make use of Web mining technology to improve distance education platforms. We will introduce Web mining and its application in distance education platforms and propose a model of Web mining process in distance education platforms.","","POD:978-0-7695-3825-9","10.1109/EmbeddedCom-ScalCom.2009.98","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341783","distance education platform;teaching resource;web content mining;web mining;web structure mining;web usage mining","Computer networks;Data mining;Distance learning;Education;Educational technology;Embedded computing;Multimedia systems;Physics computing;Web mining;Web pages","Internet;Web sites;computer aided instruction;data mining;distance learning;information retrieval;teaching","Internet;Web mining;Web site;data extraction;distance education platform;network teaching resource","","0","","5","","","25-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"Track me! a web based location tracking and analysis system for smart phone users","M. A. Bayir; M. Demirbas; A. Cosar","Ubiquitous Comput. Lab., SUNY - Univ. at Buffalo, Buffalo, NY, USA","2009 24th International Symposium on Computer and Information Sciences","20091023","2009","","","117","122","Mobility information of cell phone users is very important for wide range of applications, including context-based search and advertising, early warning systems, city-wide sensing applications such as air pollution exposure estimation and traffic planning. With the inclusion of new technologies in the cell phone hardware such as built-in GPS and 802.11 supports, mobility information are easily captured, managed and forwarded to a remote system via opportunistic connections over Internet. However, it is very difficult to use these low level location data for practical applications due to lack of sufficient information including high level location and temporal data. In order to solve this problem, we propose a Web based mobility analysis system which collects location data from cell phone users via opportunistic Internet connections and convert these low level location data to high level mobility information as well as adding a temporal dimension. In our experiments, we have illustrated the benefits of our systems on the reality mining data set which contains 350 K hours of real cell tower connection data.","","POD:978-1-4244-5021-3","10.1109/ISCIS.2009.5291863","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5291863","cellphones;mobility analysis;web based information system","Advertising;Air pollution;Alarm systems;Cellular phones;Global Positioning System;Hardware;Information analysis;Internet;Smart phones;Technology management","Internet;data mining;information retrieval;mobility management (mobile radio)","Web based location analysis system;Web based location tracking system;air pollution exposure estimation;cell phone users;city-wide sensing applications;context-based search;early warning systems;mobility information;opportunistic Internet connections;reality mining data set;smart phone users;traffic planning","","2","1","13","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Neighborhood graph and learning discriminative distance functions for clinical decision support","A. Tsymbal; S. K. Zhou; M. Huber","The Corporate Technology Division, Siemens AG, Erlangen 91058, Germany","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","20091113","2009","","","5617","5620","There are two essential reasons for the slow progress in the acceptance of clinical case retrieval and similarity search-based decision support systems; the especial complexity of clinical data making it difficult to define a meaningful and effective distance function on them and the lack of transparency and explanation ability in many existing clinical case retrieval decision support systems. In this paper, we try to address these two problems by introducing a novel technique for visualizing inter-patient similarity based on a node-link representation with neighborhood graphs and by considering two techniques for learning discriminative distance function that help to combine the power of strong ""black box"" learners with the transparency of case retrieval and nearest neighbor classification.","1094-687X;1094687X","CD-ROM:978-1-4244-3296-7","10.1109/IEMBS.2009.5333784","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333784","","","decision support systems;graph theory;information retrieval;medical information systems;transparency","case retrieval;clinical case retrieval;clinical data making;clinical decision support system;inter-patient similarity;learning discriminative distance function;nearest neighbor classification;neighborhood graph;node-link representation;similarity search","Algorithms;Artificial Intelligence;Decision Support Systems, Clinical;Decision Support Techniques;Discriminant Analysis","2","2","19","","","3-6 Sept. 2009","","IEEE","IEEE Conference Publications"
"An exploration of unintended online private information disclosure in educational institutions across four countries","O. Oh; R. Chakraborty; H. R. Rao; S. Upadhyaya","Management Science and Systems, School of Management, University at Buffalo, 325 Jacobs Management Building, NY 14260-4000, USA","2009 eCrime Researchers Summit","20091201","2009","","","1","11","Advanced Google search queries can be used to extract sensitive information from Websites that can potentially be exploited for malice. The purpose of this paper is to identify the existence of unintended private information disclosure possibilities in educational institutions across four countries through advanced search techniques such as Google Hacking. Google Hacking is a technique of retrieving information that may not be intended for public retrieval using advanced Google search operators. The focus is on the country level comparison of Excel files which contain unintended private information. For this exploratory study, we used relevant Google hacking search queries to retrieve Excel spreadsheet files which contain personally identifiable information from higher education institutions of India, US, South Korea, and Romania. Our analysis of these retrieved files establishes that each country shows (1) a different perception of privacy, (2) a different practice of information sharing and (3) a different degree of vulnerability and exposure. Finally, based on our findings, we recommend policy requirements which can be used to prevent higher education institutions from accidentally exposing individuals to potential cybercrimes through Google Hacking.","2159-1237;21591237","POD:978-1-4244-4625-4","10.1109/ECRIME.2009.5342606","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342606","Google hacking;Higher Education Institution;Unintended private information disclosure;Web Crawlers;Webmaster;eCrime;robots.txt","Clustering algorithms;Computer security;Credit cards;Educational institutions;Informatics;Information security;Information technology;Internet;Laboratories;Uniform resource locators","Web sites;data privacy;educational institutions;information retrieval;search engines","Google hacking;Google search query;Web site;data privacy;educational institution;higher education institution;information retrieval;information sharing;search engine;sensitive information extraction;unintended online private information disclosure","","2","","31","","","Sept. 20 2009-Oct. 21 2009","","IEEE","IEEE Conference Publications"
"Reusable SOA Assets Identification Using E-Business Patterns","I. Elgedawy","Northern Cyprus Campus, Comput. Eng. Dept., Middle East Tech. Univ., Mersin, Cyprus","2009 World Conference on Services - II","20091030","2009","","","33","40","Keyword-based search techniques are currently used to identify assets packaged for reuse. Unfortunately, these techniques are not suitable for searching large asset repositories, as they are known of having low precision and recall. One way to improve the search accuracy, is to identify reusable assets using their architecture models, since models based on similar architecture decisions, will most probably lead to developing similar solutions. However, informality and heterogeneity in designing architecture models remains a major obstacle for this identification approach. To overcome this problem, this paper proposes use of e-business patterns for reusable asset identification instead of architectural models. As e-business patterns are common to every designer, they could be used as an architecture design reference for comparing assets architecture layouts. Therefore, we propose a model to capture such architecture layouts in a machine understandable format using a graph of the adopted business, integration, and application e-business patterns, thereby assets could be rapidly identified based on the matching status of their architecture layout graphs. We believe the proposed approach provides better precision and recall when compared to unstructured keyword-based approaches.","","POD:978-1-4244-5303-0","10.1109/SERVICES-2.2009.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305988","E-business;Identification;Patterns;Resuable Assets;SOA","Application software;Computer architecture;Context-aware services;Packaging;Pattern analysis;Pattern matching;Semiconductor optical amplifiers;Service oriented architecture;Software reusability;Vocabulary","electronic commerce;information retrieval;software architecture;software reusability","architecture layout graphs;e-business patterns;keyword-based search techniques;reusable SOA assets identification","","2","","10","","","21-25 Sept. 2009","","IEEE","IEEE Conference Publications"
"Study on Quality Safety Traceability Systems for Cereal and Oil Products","S. Liu; H. Zheng; H. Meng; H. Hu; J. Wu; C. Li","Agric. Inf. Inst., Chinese Acad. of Agric. Sci., Beijing, China","2009 WRI World Congress on Software Engineering","20091110","2009","1","","163","166","Food companies and governments in many countries are putting increasing emphasis on establishment of food traceability systems. Food traceability has become an effective way in food safety management. Aimed at food safety problems of cereal and oil products existing in the production, processing, warehousing, distribution and other links in the supply chain, this paper firstly proposes a new traceability framework combines the information flow with critical control points and quality indicators. Then it introduces traceability database design and data access mode to realize the framework. In practice, code design for tracing is a challenge, so this paper put forward a code system based on UCC/EAN-128 standard. Middleware and electronic terminal design are also briefly introduced to accomplish traceability system for cereal and oil products.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.350","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319079","RFID;cereal and oil products;electronic terminal;supply chain;traceability;tracking","Code standards;Databases;Government;Middleware;Petroleum;Product safety;Production;Supply chains;User-generated content;Warehousing","food products;food safety;information retrieval;middleware","UCC/EAN-128 standard;cereal products;critical control points;data access mode;electronic terminal design;food companies;food safety management;information flow;middleware;oil products;quality indicators;quality safety traceability systems;traceability database design","","5","","17","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"The Altinn Case Study: Proposal for a Large-Scale Public-Key Biometric Infrastructure","B. B. Mjaaland; D. Gligoroski; S. J. Knapskog","Dept. of Telematics, Norwegian Univ. of Sci. & Technol. (NTNU), Trondheim, Norway","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20091117","2009","","","1066","1071","We have developed a method for generation of a large database of public keys based on biometric data extracted from fingerprints of the users, without the need to use stored templates. In this short paper we show the application of our method in a case study for Altinn - a Norwegian state official Web portal, giving services to millions of customers in a 24/7 manner. To cope with variations in the sample, the key generation method allows minutiae points to change slightly without affecting the resulting key. Thus, the database does not only hold one public key for each user, but also keys that are likely (with high probability) to be generated if there is a small sample error in the future. This high probability is based on critical minutiae points.","","POD:978-1-4244-4717-6","10.1109/IIH-MSP.2009.102","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337546","PKI;biocryptics;biometric encryption;biometrics;fingerprint;public key","Bioinformatics;Biometrics;Cryptography;Fingerprint recognition;Iris;Large-scale systems;Proposals;Public key;Signal generators;Telematics","fingerprint identification;information retrieval;portals;probability;public key cryptography","Altinn case study;Web portal;fingerprints extraction;key generation method;large-scale public-key biometric infrastructure","","0","","20","","","12-14 Sept. 2009","","IEEE","IEEE Conference Publications"
"Evolution of the Probability of an MPM (Multiple Primary Malignancy) after a First Colon Tumor","A. Cavallo; C. Dodaro; A. Renda","Dept. Ing. dell'Inf., Second Univ. of Napoli, Aversa, Italy","2009 IEEE International Conference on Bioinformatics and Biomedicine","20091201","2009","","","252","256","Starting from a database containing over 30-years observations on multiple primary malignacies (SEER Program), the most probable tumor following a first colon cancer is determined, based on premises such as sex and age at first diagnosis of the patient. Decision trees and randomized algorithms are used in order to extract significant data from the whole database. Moreover, tree structure varies during the years, hence a dynamic decision tree is obtained by connecting decision trees computed on 5-years patient cohorts. The resulting probability for each district is presented by using a linguistic fuzzy interpolator. Thus, a linguistic tool for second tumor forecasting is obtained.","","POD:978-0-7695-3885-3","10.1109/BIBM.2009.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5341793","Decision Trees;Fuzzy Inference Systems;Multiple Primary Malignancy (MPM);System Identification","Breast neoplasms;Cancer;Colon;Data mining;Databases;Decision trees;Esophagus;Fuzzy systems;Medical diagnostic imaging;Patient monitoring","biological organs;cancer;decision trees;fuzzy systems;information retrieval;medical diagnostic computing;medical information systems;tumours","data extraction;dynamic decision tree;first colon tumor;linguistic fuzzy interpolator system;multiple primary malignancy;patient diagnosis;probability;randomized algorithms;second tumor forecasting","","0","","10","","","1-4 Nov. 2009","","IEEE","IEEE Conference Publications"
"Scientific Quality Towards Specific Topics","H. Wu; Y. Pei","Sch. of Inf. Sci. & Eng., Yunnan Univ., Kunming, China","2009 2nd International Conference on Biomedical Engineering and Informatics","20091030","2009","","","1","5","The studies of citations are comprehensively carried out with the increasing electronically citation data on the Web. Most of the metrics observe scientific quality in a global view instead of in multiple fine-grained views. In this paper, we suggest to apply Topic Model and adaptive PageRank algorithm to assess the relative importance of scientific objects including articles, authors, conferences and journals. The scientific quality is measured by an aggregation PageRank metric towards some topics. This metric considers the impact of a paper both in global view and local view. The experiments on ACL Anthology bibliographic corpus show our method is a useful measure to observe scientific quality on multi-views.","1948-2914;19482914","POD:978-1-4244-4132-7","10.1109/BMEI.2009.5305784","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305784","","Bibliometrics;Citation analysis;Data engineering;Graph theory;Information science;Lakes;Linear discriminant analysis;Paper technology;Statistics;Telecommunication traffic","Internet;citation analysis;information retrieval","ACL Anthology bibliographic corpus;Web;adaptive PageRank algorithm;electronic citation data;scientific quality;topic model","","0","","8","","","17-19 Oct. 2009","","IEEE","IEEE Conference Publications"
"Event Extraction from Turkish Football Web-casting Texts Using Hand-crafted Templates","D. Tunaoglu; Ö. Alan; O. Sabuncu; S. Akpinar; N. K. Cicekli; F. N. Alpaslan","METU Technopolis, Orbim Corp., Ankara, Turkey","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","466","472","In this paper, we present a domain specific information extraction approach. We use manually formed templates to extract information from unstructured documents where grammatical and syntactical errors occur frequently. We applied our approach to primarily Turkish unstructured soccer Web-casting texts. Compared to automated approaches we achieve high precision-recall rates (97% - 85%). In addition to that, unlike automated approaches we do not use part-of-speech taggers, parsers, phrase chunkers or that kind of a linguistic tool. As a result, our approach can be applied to any domain or any language without the necessity of successful linguistic tools. The drawback of our approach is the time spent on crafting the templates. We also propose the means to decrease that time.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298635","Data Mining;Semantic Web","Buildings;Cities and towns;Computer errors;Data mining;Games;Information analysis;Intelligent systems;Internet;Ontologies;Search engines","Internet;information retrieval;sport;text analysis","Turkish football Web-casting text;domain specific information extraction;event extraction;grammatical error;hand-crafted template;syntactical error","","2","","13","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Community Support for Disabled Bus Riders: What Can We Do?","A. Kawaguchi; C. Chan","City Coll., Comput. Sci. Dept., City Univ. of New York, New York, NY, USA","2009 International Conference on Computing, Engineering and Information","20091117","2009","","","100","103","This paper discusses the implementation of one type of information systems for the New York City bus transit service, as a case study to provide value-added transportation services for the people in need of mobile wheelchairs. Information technology is a key for finding out flexible transportation services especially for disabled people. Useful information supplies psychological reassurance to these vulnerable people to make them feel more safe and secure. Residents in metropolitan areas increasingly view the convenience of public transportation, and they are becoming used to supply and retrieve the information gathered for regional community. The accessibility improvement needs the exact same type of the cooperation of transportation companies and regional residents. The widespread use of mobile wheelchairs has a socioeconomic impact. The significance of this research for the longer-term goals lies in its implications for adaptation of this kind of intelligent models into the future welfare activities.","","POD:978-0-7695-3538-8","10.1109/ICC.2009.51","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328884","assistive database system;assistive information system;disabled bus rider;mobile wheelchair;route planner","Aging;Cities and towns;Computer science;Educational institutions;Information systems;Information technology;Mobile computing;Psychology;Road transportation;Wheelchairs","handicapped aids;information retrieval systems;traffic information systems;transportation","community support;disabled bus riders;flexible transportation services;information retrieval;information systems;mobile wheelchairs;public transportation;value-added transportation services","","2","","9","","","2-4 April 2009","","IEEE","IEEE Conference Publications"
"GARDI : A Self-Regulating Framework for Digital Libraries","C. Gurrin; T. Aarflot; D. Johansen","Dept. of Comput. Sci., Univ. of Tromso, Tromso, Norway","2009 Ninth IEEE International Conference on Computer and Information Technology","20091117","2009","1","","305","310","The ever increasing quantities of personal digital media being captured and stored in digital libraries has posed a new problem, that of automatically managing large quantities of heterogeneous personal digital data. In this paper, we propose a new digital library framework (GARDI) which is designed to be self-regulating, in that it automatically optimizes the storage and retrieval of personal digital data. GARDI is positively evaluated in the extremely challenging domain of human digital memories.","","POD:978-0-7695-3836-5","10.1109/CIT.2009.137","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328045","Gardi;digital library;digital object lifecycle;human digital memory","Computer science;Conference management;Design optimization;Humans;Information technology;Nonhomogeneous media;Software libraries;Storage automation;Technology management;Videoconference","digital libraries;information retrieval;information storage","GARDI;data retrieval;data storage;digital libraries;heterogeneous personal digital data;human digital memories;optimisation;self-regulating framework","","0","","14","","","11-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"Digital Reconstruction of a Historical Kabuki Theater","K. Furukawa; R. Akama; C. Hirose; K. Hachimura","Coll. of Image Arts & Sci., Ritsumeikan Univ., Kyoto, Japan","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20091117","2009","","","1160","1163","A purpose of this research is digitally reconstruct old Kabuki theater in Japan during the Edo period. Kabuki theaters and Kabuki played were important features of cultures in the Edo period. Reconstructing these by 3D CG helps us to represent the spectacle of Edo culture. In this research, we tried to reconstruct historial theater, Kanamaru-za that was refurbished in 1976, but conveys the structure, mechanisms and atmosphere of theaters in Edo period. This is the first step for restoring cultures of performing arts in that period.","","POD:978-1-4244-4717-6","10.1109/IIH-MSP.2009.306","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337567","3D CG;Kabuki Theater;Keywords - Digital archives","Art;Atmosphere;Character generation;Cities and towns;Cultural differences;Educational institutions;Image reconstruction;Image restoration;Painting;Production","art;computer graphics;digital libraries;history;image reconstruction;information retrieval systems","3D CG image;Edo period arts;Edo period culture;Japan;Kanamaru-za setup;digital archives;digital historical Kabuki theater reconstruction","","1","","6","","","12-14 Sept. 2009","","IEEE","IEEE Conference Publications"
"Reader emotion classification of news headlines","Y. Jia; Z. Chen; S. Yu","Institute of Computational Linguistics, Peking University & Key Laboratory of Computational Linguistics, Ministry of Education Beijing, China","2009 International Conference on Natural Language Processing and Knowledge Engineering","20091106","2009","","","1","6","Emotion classification of text is very important in applications like emotional text-to-speech (TTS) synthesis, human computer interaction, etc. Past studies on emotion classification focus on the writer's emotional state conveyed through the text. This research addresses the reader's emotions provoked by the text. The classification of documents into reader emotion categories has novel applications. One of them is to integrate reader emotion classification into a Web search engine to allow users to retrieve documents that contain relevant contents and at the same time produce proper emotions. Another is for Web sites to organize contents according to reader emotion categories and provide users a convenient browse. In this paper, we explore sentence level emotion classification. Firstly, we extract news headlines and related reader emotion information from the Web. Then we classify news headlines into reader emotion categories using support vector machine (SVM), and examine classification performance under different feature settings. Experiments show that certain feature combinations achieve good results.","","POD:978-1-4244-4538-7","10.1109/NLPKE.2009.5313762","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5313762","Emotion classification;news headlines;support vector machine (SVM)","Application software;Computational linguistics;Computer science education;Human computer interaction;Laboratories;Search engines;Speech synthesis;Support vector machine classification;Support vector machines;Web search","Web sites;emotion recognition;human computer interaction;information retrieval;search engines;speech synthesis;support vector machines","Web search engine;Web sites;document retrieval;emotional text-to-speech synthesis;human computer interaction;news headlines;reader emotion classification;support vector machine","","1","","15","","","24-27 Sept. 2009","","IEEE","IEEE Conference Publications"
"A Brain-Actuated Human Computer Interface for Google Search","H. Xu; T. Qian; B. Hong; X. Gao; S. Gao","Dept. of Biomed. Eng., Tsinghua Univ., Beijing, China","2009 2nd International Conference on Biomedical Engineering and Informatics","20091030","2009","","","1","4","A brain-actuated human computer interface for Google search is proposed in this study. Aiming at increasing system simplicity and flexibility, a steady-state visual evoked potential based system was developed by integrating simplified EEG cap, compact wireless EEG amplifier, flexible visual stimulator and user-friendly software design. A practical application for Google search which involved character input and cursor control was designed and tested on this system. 10 out of 12 subjects accomplished the task and achieved an average accuracy of 92.0%.","1948-2914;19482914","POD:978-1-4244-4132-7","10.1109/BMEI.2009.5305708","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305708","","Application software;Computer interfaces;Electroencephalography;Human computer interaction;Network servers;Software design;Software systems;Steady-state;Uniform resource locators;Web pages","brain-computer interfaces;electroencephalography;information retrieval","EEG cap;Google search;brain-actuated human computer interface;flexible visual stimulator;steady-state visual evoked potential based system;user-friendly software design;wireless EEG amplifier","","0","","9","","","17-19 Oct. 2009","","IEEE","IEEE Conference Publications"
"Operations on Spaces of Information","I. Oliver; S. Boldyrev","Res. Center, Nokia, Helsinki, Finland","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","267","274","Space-based computing environments are based around numerous, discrete sets of information with internal reasoning, interaction and semantics. These spaces of information can be treated as first-class entities and thus have direct interactions between themselves as spaces. We develop and formalise a simple set of base operations between individual spaces that preserves the integrity of the information contained within those spaces through larger grained semantic structures (eg: RDF molecules) and historical information about past interactions. We then show how this forms the basis of extended and possible-world queries and how such features are manifested in common scenarios.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.102","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298630","agents;semantic web;space-based computing","Application software;Calendars;History;Information management;Internet;Privacy;Resource description framework;Semantic Web;Social network services;Tree graphs","information dissemination;information retrieval systems;query formulation;semantic Web","extended world queries;historical information;information space;larger grained semantic structures;possible world queries;semantic Web;space based computing","","2","15","38","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Extracting Environmental Information for Improved Web Service Matching and Identification","K. Kannan; N. C. Narendra; L. Ramaswamy","IBM India Res. Lab., Bangalore, India","2009 World Conference on Services - II","20091030","2009","","","79","86","The current approaches to automatic identification of Web services for a given set of requirements involve matching based on the interface information published by the service providers. However, due to concerns about exposing of internals of Web services, this information is typically limited, thus rendering Web service matching quite difficult in practice. In this paper, we argue that service matching can be significantly enhanced by harnessing the environmental information about the Web services. Towards this end, we explore two crucial questions: (i) what environmental information would improve service matching, and (ii) how much of that information can be revealed without violating the fundamental tenet of service-oriented architecture that the internal implementation details of Web services are not available to Web service consumers? This paper provides a precise characterization of environmental information of Web services. We motivate our approach via real-life Web service search portals, by showing how environmental information can improve the precision and recall of service matching process. We then show how Web service providers can extract this information from the designs of the Web services, and publish them along with their interfaces. We illustrate our ideas with a realistic running example, and demonstrate it via a proof-of concept prototype.","","POD:978-1-4244-5303-0","10.1109/SERVICES-2.2009.33","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5305982","","Algorithm design and analysis;Data mining;Portals;Protocols;Prototypes;Rendering (computer graphics);Semantic Web;Service oriented architecture;Unified modeling language;Web services","Web services;environmental science computing;information retrieval;pattern matching;software architecture","Web service identification;Web service matching;environmental information extraction;proof-of concept prototype;service-oriented architecture","","1","","26","","","21-25 Sept. 2009","","IEEE","IEEE Conference Publications"
"The impact of limited search procedures for systematic literature reviews — A participant-observer case study","B. Kitchenham; P. Brereton; M. Turner; M. Niazi; S. Linkman; R. Pretorius; D. Budgen","School of Computing and Mathematics, Keele University, Stoke-on-Trent ST5 5BG, UK","2009 3rd International Symposium on Empirical Software Engineering and Measurement","20091106","2009","","","336","345","This study aims to compare the use of targeted manual searches with broad automated searches, and to assess the importance of grey literature and breadth of search on the outcomes of SLRs. We used a participant-observer multi-case embedded case study. Our two cases were a tertiary study of systematic literature reviews published between January 2004 and June 2007 based on a manual search of selected journals and conferences and a replication of that study based on a broad automated search. Broad searches find more papers than restricted searches, but the papers may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers; if publication bias is not an issue; or if they are assessing research trends in research methodologies.","1949-3770;19493770","POD:978-1-4244-4842-5","10.1109/ESEM.2009.5314238","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5314238","","Aggregates;Computer science;Councils;Guidelines;Mathematics;Particle measurements;Physics computing;Software engineering;Software libraries;Software measurement","digital libraries;information retrieval;research libraries;reviews;software engineering","SLR;broad automated search procedure;digital library;grey literature;participant-observer multicase embedded case study;publication bias;research trend;software engineering;systematic literature review;targeted manual journal search","","6","","37","","","15-16 Oct. 2009","","IEEE","IEEE Conference Publications"
"Comparison of footwear retrieval systems for synthetic and real shoe marks","F. Cervelli; F. Dardi; S. Carrato","Dept. Electrical, Electronic and Information Engineering (DEEI), University of Trieste, Via Valerio 10, 34100, Italy","2009 Proceedings of 6th International Symposium on Image and Signal Processing and Analysis","20091030","2009","","","684","689","Shoe marks found on crime scenes can lead to the identification of the culprit, thus it is important to find the make and model of the footwear that left the marks. Some semi-automatic and automatic systems have been proposed in literature for the purpose, but they have been all tested on synthetic shoe marks, i.e. sole prints with added synthetic noise. Here we make a comparison of some of the methods reported in literature, both on synthetic and on real shoe marks coming from crime scenes: this last comparison has never been done before. Moreover we propose a new matching algorithm, based on the Mahalanobis distance, which is also compared to the other methods. Results show that simulated shoe marks are not suited to test a footwear retrieval system aimed at finding the shoe make and model of a shoe mark found on the crime scene.","1845-5921;18455921","POD:978-953-184-135-1","10.1109/ISPA.2009.5297631","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5297631","","Active noise reduction;Electroencephalography;Filter bank;Footwear;Frequency estimation;Hearing aids;Information analysis;Internal combustion engines;Noise reduction;Signal processing","footwear;footwear industry;information retrieval","Mahalanobis distance;footwear retrieval systems;matching algorithm;real shoe marks;semiautomatic systems;synthetic shoe marks","","2","","20","","","16-18 Sept. 2009","","IEEE","IEEE Conference Publications"
"Document Summarization and Information Extraction for Generation of Presentation Slides","K. Gokul Prasad; H. Mathivanan; T. V. Greetha; M. Jayaprakasam","Dept. of Comput. Sci. & Eng., Anna Univ., Chennai, India","2009 International Conference on Advances in Recent Technologies in Communication and Computing","20091117","2009","","","126","128","In this paper, a semi automated technique to generate slide presentations from english text documents is proposed. The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP (Natural Language Processing). The technique involves an information extractor and a slide generator, which combines certain NLP methods such as segmentation, chunking, summarization etc.., with certain special linguistic features of the text such as the ontology of the words, noun phrases found, semantic links, sentence centrality etc., In order to aid the language processing task, two tools can be utilized namely, <i>MontyLingua</i> which helps in chunking and <i>Doddle</i> helps in creating an ontology for the input text represented as an OWL (Ontology Web Language) file. The process of the technique comprises of extracting text, creating an ontology, identifying important phrases for bullets and generating slides.","","POD:978-1-4244-5104-3","10.1109/ARTCom.2009.74","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5328052","chunking;information extraction;ontology;segmentation;summarization","Aggregates;Communications technology;Computer science;Data mining;Explosions;Natural languages;OWL;Ontologies;Pattern matching;Seminars","information retrieval;knowledge representation languages;natural language processing;text analysis","Doddle;MontyLingua;NLP method;OWL file;Ontology Web Language;document summarization;english text document;information extraction;natural language processing;ontology creation;presentation slide generation;text extraction","","2","","9","","","27-28 Oct. 2009","","IEEE","IEEE Conference Publications"
"Semantic Text Mining with Linked Data","Z. Huang; H. Chen; T. Yu; H. Sheng; Z. Luo; Y. Mao","Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","2009 Fifth International Joint Conference on INC, IMS and IDC","20091113","2009","","","338","343","Linked Data is an open data space that emerges from the publication and interlinking of structured data on the Web using the Semantic Web technologies. How to utilize this wealth of data is currently a focused research theme of the Semantic Web community. In this paper, we aim to utilize Linked Data to generate semantic annotations for frequent patterns extracted from textual documents. First, we extract semantic relations from textual documents and merge them into a set of semantic graphs. Then, we apply a frequent subgraph discovery algorithm on the set of graphs to generate frequent patterns. Finally, we annotate the discovered patterns using Linked Data. Our approach can be applied in such domains as terrorist network analysis and biological network analysis. The efficacy of our approach is demonstrated through an empirical experiment that discovers and validates relationships between political figures from large number of news on the Web.","","POD:978-1-4244-5209-5","10.1109/NCM.2009.131","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5331702","Entity Extraction;Frequent Subgraph;Linked Data;Massive Knowledge Relation;Semantic Graph;Semantic Graph Fusion","Computer science;Data engineering;Data mining;Educational institutions;OWL;Resource description framework;Semantic Web;Space technology;Text mining;Web pages","data mining;graph theory;information retrieval;semantic Web;text analysis","biological network analysis;frequent subgraph discovery algorithm;linked data;open data space;pattern extraction;semantic Web technologies;semantic annotations;semantic graphs;semantic text mining;structured data interlinking;structured data publication;terrorist network analysis;text documents","","1","","14","","","25-27 Aug. 2009","","IEEE","IEEE Conference Publications"
"Audio Segmentation in AAC Domain for Content Analysis","R. Zhu; H. Ai; R. Hu","Nat. Eng. Res. Center for Multimedia Software, Wuhan Univ., Wuhan, China","2009 5th International Conference on Wireless Communications, Networking and Mobile Computing","20091030","2009","","","1","4","We focus the attention on the audio scene segmentation in AAC domain for audio-based multimedia indexing and retrieval applications. In particular, a MFCC extraction method is proposed, which is adaptive to the window switch in AAC encoding process, and independent of the audio sampling frequency. We discuss the fusion method of MFCC features, which came from different window type in order to keep the balance of the frequency and temporal resolution. A series of experiments via the probability distribution of MFCC were implemented to test the effective in audio scene segmentation. The experimental results show that such approach based on compression domain can approach the performance of the system based on PCM audio, and the CPU overload decreased dramatically. It is meaningful to the real time analysis of audio content.","2161-9646;21619646","POD:978-1-4244-3691-0","10.1109/WICOM.2009.5301778","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5301778","","Application software;Audio coding;Content based retrieval;Encoding;Indexing;Layout;Mel frequency cepstral coefficient;Probability distribution;Sampling methods;Switches","audio coding;data compression;discrete cosine transforms;feature extraction;indexing;information retrieval;multimedia computing;sensor fusion;signal resolution;signal sampling;statistical distributions","AAC compression domain;AAC encoding process;CPU overload;MDCT domain;MFCC feature extraction algorithm;MFCC fusion method;PCM audio;adaptive window switch;audio sampling frequency;audio scene segmentation;audio-based multimedia indexing;audio-based multimedia retrieval;frequency resolution;probability distribution;real-time audio content analysis;temporal resolution","","0","","9","","","24-26 Sept. 2009","","IEEE","IEEE Conference Publications"
"Integration of mutual information and text mining methods for extracting gene-gene interactions from gene expression data","D. H. Millis; J. L. Solka; L. K. Matukumalli","Department of Bioinformatics and Computational Biology, George Mason University, Manassas, Virginia","2009 IEEE International Conference on Bioinformatics and Biomedicine Workshop","20091113","2009","","","357","357","Mutual information algorithms have been used for the identification of gene-gene interactions in gene expression data. These methods have been hindered by a high false-positive rate. We explored the use of free-text abstracts as an additional source of information for assessing the biological relevance of predicted gene interactions. Our results suggest that the performance of a mutual information algorithm on this task can be enhanced by using text mining methods to refine the initial set of predictions.","","POD:978-1-4244-5121-0","10.1109/BIBMW.2009.5332076","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5332076","gene expression;gene-gene interactions;latent semantic analysis;mutual information;text mining","Abstracts;Algorithm design and analysis;Bioinformatics;Data mining;Databases;Gene expression;Humans;Information analysis;Mutual information;Text mining","biology computing;data mining;genetics;information analysis;information retrieval;text analysis","biological relevance;free-text abstracts;gene expression data;gene-gene interactions;high false-positive rate;latent semantic analysis;mutual information algorithms;text mining methods","","0","","4","","","1-4 Nov. 2009","","IEEE","IEEE Conference Publications"
"Supporting Personalized Information Exploration through Subjective Expert-created Semantic Attributes","C. Hampson; O. Conlan","Knowledge & Data Eng. Group, Trinity Coll. Dublin, Dublin, Ireland","2009 IEEE International Conference on Semantic Computing","20091030","2009","","","384","389","Ordinary users are finding it increasingly difficult to explore the large volumes of diverse data they encounter in their everyday lives. Techniques based on data mining algorithms are useful but they tend to be too complex for casual users to work with effectively. Furthermore, these techniques don't allow the user to engage with the information using semantics meaningful to them. Semantically enriched and personalized data exploration is seen as an essential step to support such users. Moreover, by allowing these users to leverage and personalize the subjective insights and knowledge of experts, more relevant and useful information can be discovered and interesting correlations drawn. In order to support these domain specific explorations, a prototype architecture named SARA (Semantic Attribute Reconciliation Architecture) has been built, and its underlying methodology, implementation and initial evaluation are described within this paper.","","POD:978-1-4244-4962-0","10.1109/ICSC.2009.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5298658","Domain Experts;Information Exploration;Personalization;Semantic Attributes;Subjectivity","Control systems;Data engineering;Data mining;Data structures;Educational institutions;Humans;Machine learning algorithms;Neural networks;Probability;Prototypes","data analysis;information retrieval","personalized information exploration;semantic attribute reconciliation architecture;semantically enriched data exploration;subjective expert-created semantic attribute","","7","","18","","","14-16 Sept. 2009","","IEEE","IEEE Conference Publications"
"Media Caching Support for Mobile Transit Clients","H. Gomaa; G. Messier; R. Davies; C. Williamson","Dept. of Electr. & Comput. Eng., Univ. of Calgary, Calgary, AB, Canada","2009 IEEE International Conference on Wireless and Mobile Computing, Networking and Communications","20091110","2009","","","79","84","In this paper, we consider the design of caching infrastructure to enhance the client-perceived performance of mobile wireless clients retrieving multimedia objects from the Internet. We consider three primary issues: location of the cache, size of the cache, and management policy for the cache. We consider both infrastructure-oriented caching at the Access Point (AP), as well as peer-assisted caching at the mobile clients. Simulation is used as the methodology for evaluation and comparison of caching strategies. The simulation results show that AP caching is generally more effective than client-side caching, that adequate performance is achievable with a mix of rather modest AP and client-side caches, and that Least Frequently Used (LFU) is the most effective cache replacement policy. Additional simulation experiments show that our results are robust across different request generation rates and client turnover rates.","2160-4886;21604886","POD:978-0-7695-3841-9","10.1109/WiMob.2009.23","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325310","Caching;Mobility;Multimedia;Simulation;Wireless Networks","IP networks;Internet;Mobile communication;Mobile computing;Network servers;Peer to peer computing;Robustness;Streaming media;Wireless LAN;Wireless networks","Internet;information retrieval;mobile computing;multimedia communication;storage management","Internet;cache management policy;cache replacement policy;client turnover rates;infrastructure-oriented caching;least frequently used policy;media caching;mobile transit clients;mobile wireless clients;multimedia object retrieval;peer-assisted caching;request generation rates","","11","3","18","","","12-14 Oct. 2009","","IEEE","IEEE Conference Publications"
"TV Commercial Detection Based on Shot Change and Text Extraction","L. Meng; Y. Cai; M. Wang; Y. Li","Automobile Transp. Command Dept., Acad. of Mil. Transp., Tianjin, China","2009 2nd International Congress on Image and Signal Processing","20091030","2009","","","1","5","A new algorithm is proposed for TV commercial detection from color video in this paper. High shot change frequency and ""still"" shots of the trademark information, which are two basic characteristics of TV commercial, are exploited to distinguish commercials from general programs. Our commercial retrieval system based on shot change and text detection is realized through a slide window. First, histogram difference is computed on consecutive images and then the four common shot transitions including cut, dissolve, fade in/out and wipe are detected. Our text detection algorithm based on maximum gradient difference, allows fast filtering of scan lines without text. A commercial is determined either if its shot change frequency is satisfied or if its trademark information is detected. Performance evaluation shows that our algorithm performs well with relatively high detection accuracy for different kinds of programs when we involved the text detection, which is an important contribution of this paper.","","POD:978-1-4244-4129-7","10.1109/CISP.2009.5302320","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5302320","","Change detection algorithms;Data mining;Detection algorithms;Filtering;Frequency;Gunshot detection systems;Histograms;Performance evaluation;TV;Trademarks","image colour analysis;information retrieval;television;text analysis;video signal processing","TV commercial detection;color video;commercial retrieval system;maximum gradient difference;shot change frequency;text detection;text extraction","","4","","12","","","17-19 Oct. 2009","","IEEE","IEEE Conference Publications"
"The Research on Service-Oriented Geospatial Portal","X. g. Yu; X. p. Yu","Hubei Univ. of Econ., Wuhan, China","2009 WRI World Congress on Software Engineering","20091110","2009","3","","286","290","Traditional methods of application development through tightly coupled components can no longer satisfy increased demands and urgently needed geospatial applications. In this paper, service-oriented architecture and spatial Web services are utilized to design and develop a geospatial portal. Geospatial portals use Web services to publish available geospatial data and processing services, help applications find them and invoke services or retrieve data. It enables geoprocessing interoperability that makes it possible to exchange heterogeneous geographic information content and share a wide variety of geospatial services over the World Wide Web. This article describes the architecture, development and deployment related to the portal.","","POD:978-0-7695-3570-8","10.1109/WCSE.2009.112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319410","Web Services;geospatial portal;interoperability;service-oriented architecture","Application software;Buildings;Geographic Information Systems;ISO;Portals;Research and development management;Service oriented architecture;Software development management;Software engineering;Web services","Web services;geographic information systems;information retrieval;open systems;portals","GIS;World Wide Web;application development;data retrieval;geoprocessing interoperability;geospatial data publishing;heterogeneous geographic information exchange;service-oriented architecture;service-oriented geospatial portal design;spatial Web service;tightly-coupled component","","0","","13","","","19-21 May 2009","","IEEE","IEEE Conference Publications"
"A Method of Managing Distributed Contents in Heterogeneous Servers","M. Shiba","Ryukoku Univ., Otsu, Japan","2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20091117","2009","","","1148","1151","Various types of content servers are used to run digital archives and digital museums in many sites. These content servers have different interfaces and ways to keep their contents. Inappropriately managed content servers prevent efficient use of contents in different servers. This paper proposes a solution to this inefficiency by describing a peer-to-peer based content management system that integrates distributed contents in two or more content servers. Our proposed system has a mechanism for controlling content servers and creating content control data for managing contents. The proposed system manages the content servers and the contents with these data over a peer-to-peer network. With this management method, the proposed system provides scalable and efficient use of the contents by integrating the servers and the contents.","","POD:978-1-4244-4717-6","10.1109/IIH-MSP.2009.253","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5337584","content management system;distributed system;peer-to-peer","Conference management;Content management;Control systems;Cultural differences;File servers;Java;Libraries;Network servers;Peer to peer computing;Signal processing","Internet;content management;exhibitions;file servers;humanities;information retrieval systems;peer-to-peer computing","digital archive;digital museum;distributed content management;heterogeneous server;peer-to-peer system","","0","","2","","","12-14 Sept. 2009","","IEEE","IEEE Conference Publications"
