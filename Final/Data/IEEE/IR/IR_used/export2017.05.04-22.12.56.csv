"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7348327,7345372,7340797,7343096,7344509,7345273,7345397,7341447,7344924,7338171,7345370,7345328,7339037,7334843,7339012,7334801,7335488,7339220,7336270,7337033,7339194,7336153,7334027,7333871,7332556,7334712,7335361,7333448,7328537,7334640,7333387,7333885,7335381,7332153,7331984,7330184,7137654,7328437,7328516,7324058,7327014,7322549,7323027,7323006,7326915,7279139,7325497,7325721,7322676,7321279,7321464,7321461,7321230,7321252,7317372,7319270,7320184,7318447,7312713,7314021,7314709,7313677,7313133,7312286,7311988,7312295,7307808,7307475,7310720,7311201,7311829,7192694,7192728,7300986,7300616,7300949,7300956,7300954,7301141,7306755,7307051,7301166,7306774,7306991,7301027,7300814,7306952,7300960,7299974,7298018,7298752,7297441,7298015,7297443,7296338,7292713,7293031,7289565,6942231,6926845",2017/05/04 22:12:56
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Negative-Based Sampling for Multimedia Retrieval","H. Y. Ha; S. C. Chen; M. L. Shyu","Sch. of Comput. & Inf. Sci., Florida Int. Univ., Miami, FL, USA","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","64","71","Nowadays, in such a high-tech living lifestyle, profusion of multimedia data are produced and propagated around the world. To identify meaningful semantic concepts from the large amount of data, one of the major challenges is called the data imbalance problem. Data imbalance occurs when the number of positive instances (i.e., instances which contain the target concept) is greatly less than the number of negative instances (i.e., instances which do not contain the target concept). In other words, the ratio between positive and negative instances is extremely low. Rebalancing the dataset is usually proposed to resolve the problem by sampling or data pruning. In this paper, we propose a sampling method which consists of three stages, namely selecting features to identify the negative instances, producing negative ranking scores, and performing sampling. The method is compared with some other existing methods on the TRECVID dataset and is demonstrated to have better performance.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300956","FC-MST;Feature Selection;Multimedia;Sampling","Correlation;Feature extraction;Multimedia communication;Sampling methods;Semantics;Training data;Videos","feature selection;information retrieval;multimedia computing;sampling methods","TRECVID dataset;data imbalance problem;data pruning;data sampling;dataset rebalancing;feature selection;meaningful semantic concepts;multimedia data;multimedia retrieval;negative instance identification;negative ranking score;negative-based sampling","","","","42","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Cloud based Privacy Preserving efficient Document Storage and Retrieval Framework","G. Raghuraman; P. Nilangekar; P. Vijay; K. Premkumar; S. Mukherjee","Department of Computer Science, Anna University, Chennai, India","2015 IEEE International Conference on Intelligent Computer Communication and Processing (ICCP)","20151102","2015","","","519","525","The amount of data in the digital universe is growing at a rate of forty percent annually. This growth coupled with the power of the internet motivates several individuals and enterprises to store their data on the cloud. The cloud guarantees reliable storage and provides computational capabilities at low costs. However, the cloud also exposes the data and its users to various privacy and security vulnerabilities. We propose a document retrieval framework which searches on the encrypted data stored on the cloud while ensuring that the confidentiality of the data is not compromised. The privacy of the data during search and retrieval is ensured by deploying Privacy Preserving n-keyword search scheme. We have also investigated and implemented a Key Exchange Mechanism to ensure access control, thus providing a holistic solution encompassing authorization, access control and data privacy.","","Electronic:978-1-4673-8200-7; POD:978-1-4673-8201-4; USB:978-1-4673-8199-4","10.1109/ICCP.2015.7312713","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312713","","Cloud computing;Data privacy;Encryption;Indexes;Keyword search;Mathematical model","authorisation;cloud computing;cryptography;data privacy;information retrieval","access control;authorization;cloud based privacy preserving n-keyword search scheme;data encryption;data privacy;document retrieval;document storage;key exchange mechanism","","1","","19","","","3-5 Sept. 2015","","IEEE","IEEE Conference Publications"
"Identifying miRNA-mediated signaling subpathways by integrating paired miRNA/mRNA expression data with pathway topology","A. G. Vrahatis; G. N. Dimitrakopoulos; A. K. Tsakalidis; A. Bezerianos","Department of Computer Engineering and Informatics, University of Patras, 26500, GR","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","3997","4000","In the road for network medicine the newly emerged systems-level subpathway-based analysis methods offer new disease genes, drug targets and network-based biomarkers. In parallel, paired miRNA/mRNA expression data enable simultaneously monitoring of the micronome effect upon the signaling pathways. Towards this orientation, we present a methodological pipeline for the identification of differentially expressed subpathways along with their miRNA regulators by using KEGG signaling pathway maps, miRNA-target interactions and expression profiles from paired miRNA/mRNA experiments. Our pipeline offered new biological insights on a real application of paired miRNA/mRNA expression profiles with respect to the dynamic changes from colostrum to mature milk whey; several literature supported genes and miRNAs were recontextualized through miRNA-mediated differentially expressed subpathways.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7319270","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319270","","Bioinformatics;Biology;Dairy products;Diseases;Network topology;Regulators;Topology","RNA;bioinformatics;biological techniques;information retrieval systems;molecular biophysics","KEGG signaling pathway maps;miRNA regulators;miRNA-mediated signaling subpathways;miRNA-target interactions;microRNA;paired miRNA-mRNA expression data;pathway topology","","","","23","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Approach to building a web-based expert system interface and its application for software provisioning in clouds","E. Pyshkin; A. Kuznetsov","Institute of Computing and Control, Peter the Great St. Petersburg Polytechnic University, Russia, 195251","2015 Federated Conference on Computer Science and Information Systems (FedCSIS)","20151109","2015","","","343","354","This paper focuses on a generalized approach to providing user interface to a web-based expert system (WBES). We examine MVC and MVP design patterns used traditionally to construct a web application user interface. In order to leverage the strength of the MVC/MVP design patterns we propose a special ontology representing a user communication domain. We describe a self-service networked infrastructure for automatic deployment of command line interface (CLI) applications. We demonstrate how to apply the proposed ontology for the design of a WBES aimed at supporting client software re-execution in clouds. In particular, we address the problems existing in the area of software development for music information retrieval algorithms implementation.","","Electronic:978-8-3608-1065-1; POD:978-1-4799-6747-6; USB:978-8-3608-1067-5","10.15439/2015F142","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321464","","Cloud computing;Computer architecture;Expert systems;Ontologies;Software algorithms;User interfaces","cloud computing;expert systems;information retrieval;music;ontologies (artificial intelligence);software engineering;user interfaces","CLI application;MVC;MVP;WBES;Web-based expert system interface;command line interface applications;music information retrieval algorithm;software development;software provisioning;software re-execution;user communication domain;user interface","","","","32","","","13-16 Sept. 2015","","IEEE","IEEE Conference Publications"
"Smart Maps through Semantic Web, Social Media, and Sentiment Analysis","J. A. B. Perera; D. Zhang; M. Lu","Dept. of Comput. Sci., California State Univ., Sacramento, CA, USA","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","49","56","In this paper we describe a new approach to creating rich, dynamic and customized maps for business or leisure activities, and demonstrate how the approach can be implemented through a prototype system. The approach is aimed at changing the way we map the world by providing a meaningful and personalized context that is augmented with the semantic web, social media integration and sentiment analysis. In our approach, smart search for an entity on a map is assisted through the semantic web. Once a map entity is identified, real-time and dynamic information about properties of the entity is gathered from social media and is integrated into the map. Sentiment analysis with regard to the map entity can be conducted and its results displayed or used as filters. The main benefit of the proposed approach is two-fold: it allows users to define their own user experience or context by selecting specific properties they want to display on their maps (ratings, comments, pictures and other specific business information), and it organizes interactive maps through the hierarchy of entities/markers/layers/timeframes. We also compare our approach with related work.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300954","DBpedia;Factual;map applications;semantic web;sentiment analysis;social media","Companies;Databases;Media;Prototypes;Semantic Web;Sentiment analysis","information retrieval;semantic Web;social networking (online);text analysis","semantic Web;sentiment analysis;smart map;smart search;social media","","","","23","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"SDPA: Sensor Data Processing Architecture for Modeling Semantic Data from Sensor Steams","S. Seo; S. Chun; B. Oh; K. H. Lee","Dept. of Comput. Sci., Yonsei Univ., Seoul, South Korea","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","9","16","With the rapid deployment of a number of sensors, it is crucial to efficiently manage their data streams with heterogeneous properties. To achieve various sensor applications such as discovery and mashup, a method of retrieving meaningful information from raw sensor data is required. However, it is hard to analyze and represent the sensor data since sensors generate streaming data of different patterns and continuously transmit the observations to servers in real-time. In this paper, we propose a sensor data processing architecture to retrieve meaningful information from raw sensor data. In particular, we adopt a machine leaning strategy for sensor data analysis. Semantic sensor data are modeled based on ontologies. The processed semantic data construct a semantic knowledge base, which allows a user to make the best use of sensor information. We present an evaluation of our approach by using real-world datasets and experimental results.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300949","data processing architecture;ontology;semantic data representation;sensor network","Data analysis;Data models;Hidden Markov models;Ontologies;Probability;Real-time systems;Semantics","data analysis;information retrieval;learning (artificial intelligence);ontologies (artificial intelligence);sensor placement","SDPA;data stream management;information retrieval;machine leaning strategy;ontology;raw sensor data;semantic data modelling;semantic knowledge base;semantic sensor data modelling;sensor data analysis;sensor data processing architecture;sensor deployment;sensor streams","","","","23","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Automatic Mapping Magnetic Resonance Images into Multimedia Database Using SIFT","J. L. Reynoso Munoz; A. D. Cuevas Rasgado; F. Garcia Lamont; A. Guzman Arenas","Univ. Autonoma del Estado de Mexico, Toluca, Mexico","IEEE Latin America Transactions","20151119","2015","13","8","2709","2714","This paper focuses on the representation of magnetic resonances of different parts of the human body, such as knees, spinal column, arms, elbows, etc., using ontologies. First, it maps the resonance images in a multimedia database. Then, automatically, using the SIFT pattern recognition algorithm, descriptors of the images stored in the database are extracted in order to recover useful data for the user; it uses the ontologies as an artificial intelligence tool and, in consequence, reduces generation of useless data. Why do we think this is an interesting task? Because, if the user requires information about any topics or (s)he has some illness or needs to undergo magnetic resonance, this tool will show him/her images and text to convey a better understanding, helping to obtain useful conclusions. Artificial intelligence techniques are used, such as machine learning, knowledge representation, and pattern recognition. The ontological relations introduced here are based on the common representation of language, using definition dictionaries, Roget's thesaurus, synonym dictionaries, and other resources The system generates an output in the OM ontological language [1]. This language represents a structure where our system adds the data scanned by the SIFT algorithm. The tests have been made in Spanish; however, thanks to the portability of our system, it is possible to extend the method to any language.","1548-0992;15480992","","10.1109/TLA.2015.7332153","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332153","Artificial Intelligence;Magnetic Resonance;Multimedia Database;Ontology;Pattern Recognition","Magnetic resonance;Multimedia communication;Multimedia databases;Ontologies;Pattern recognition;Streaming media","biomedical MRI;data reduction;feature extraction;information retrieval;learning (artificial intelligence);medical image processing;multimedia databases;ontologies (artificial intelligence)","OM ontological language;Roget's thesaurus;SIFT pattern recognition algorithm;arms;artificial intelligence techniques;automatic mapping magnetic resonance images;data recovery;definition dictionaries;elbows;feature extraction;human body;knees;knowledge representation;language representation;machine learning;multimedia database;spinal column;synonym dictionaries","","","","","","","Aug. 2015","","IEEE","IEEE Journals & Magazines"
"Large scale cross-media data retrieval based on Hadoop","Wenchen Cheng; Jiang Qian; Zhicheng Zhao; Fei Su","Beijing University of Posts and Telecommunications, China","2015 11th International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness (QSHINE)","20151123","2015","","","133","138","With the rapid development of the Internet and speedy increase of the data size, there are more and more data intensive applications which often involve hundreds of megabytes of data. It is important and necessary to obtain the retrieval results from cross-media data quickly and accurately. Large scale cross-media data retrieval based on Hadoop is proposed to speed up the retrieval in this paper. We divide cross-media feature extraction and cross-media retrieval into paralleled pipeline, and implement with the combination of the HDFS, HBase and MapReduce framework. To verify the performance of the proposed method, comparisons with stand-alone mode on different sizes of the image dataset are conducted, and the experimental results demonstrate the good performances of proposed method, which sharply decreases time-consuming, and meanwhile keeps the same query precision.","","Electronic:978-1-6319-0063-1; POD:978-1-4799-8217-2","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332556","Hadoop;MapReudce;cross-media;image retrieval","Chlorine;Data mining;Feature extraction","Internet;data handling;distributed processing;information retrieval","Hadoop;Internet;MapReduce framework;cross media data;data megabytes;image dataset;large scale cross media data retrieval;paralleled pipeline","","","","24","","","19-20 Aug. 2015","","IEEE","IEEE Conference Publications"
"Applying a hybrid IGA-SimE algorithm to a multimedia retrieval system","H. Takenouchi; M. Tokumaru","Fukuoka Institute of Technology, Japan","2015 IEEE 7th International Conference on Awareness Science and Technology (iCAST)","20151102","2015","","","60","65","In this study, we propose a multimedia retrieval system framework using the Kansei retrieval agent (KRA) model to retrieve multimedia data. Our previous study involved recommender systems using the KRA model with reciprocal evaluation of each KRA and a hidden KRA model to optimize agent parameters; however, our previous method uses an interactive genetic algorithm (IGA) to optimize these parameters. Therefore, the performance and runtimes decreases with larger inputs. Then, in the present study, we use a hybrid IGA and simulated evolution (SimE) algorithm to improve optimization performance. Moreover, our proposed method employs rearrangement evaluation of multimedia data to reduce the evaluation load of users. Furthermore, we have verified the effectiveness of the proposed method via a numerical simulation using a pseudo-user that imitates user preferences. Our simulation results show that optimization performance of the proposed method is substantially higher than that of our previous method.","2325-5986;23255986","Electronic:978-1-4673-7658-7; POD:978-1-4673-7659-4; USB:978-1-4673-7657-0","10.1109/ICAwST.2015.7314021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314021","","Encoding;Genetic algorithms;Genetics;Motion pictures;Multimedia communication;Resource management","genetic algorithms;information retrieval;multimedia computing;numerical analysis;recommender systems","KRA model;Kansei retrieval agent model;agent parameter;hybrid IGA-SimE algorithm;interactive genetic algorithm;multimedia data;multimedia retrieval system;numerical simulation;optimization performance;pseudo-user;rearrangement evaluation;reciprocal evaluation;recommender system;simulated evolution algorithm","","","","11","","","22-24 Sept. 2015","","IEEE","IEEE Conference Publications"
"Tamil phoneme classification using contextual features and discriminative models","Karpagavalli S.; Chandra E","Department of Computer Science, PSGR Krishnammal College for Women, Coimbatore 641 004, India","2015 International Conference on Communications and Signal Processing (ICCSP)","20151112","2015","","","0564","0568","The speech recognition systems may be designed based on any one of the sub-word unit phoneme, tri-phone and syllable. The phonemes are a set of base-forms for representing the unique sounds in a particular language. In supervised phoneme classification, the segmentation of phoneme, features and class label are given and the goal is to classify the phoneme. Phoneme classification and recognition can be useful in applications such as spoken document retrieval, named entity extraction, out-of-vocabulary detection, language identification, and spoken term detection. In trained speech, each phoneme occurs clearly in speech waveform. In spontaneous speech, due to co-articulation effect, influence of adjacent phonemes is present in each phoneme where left and right context frame information plays vital role in accurate phoneme classification. In the proposed work, three discriminative classifiers like Multilayer Perceptron, Naive Bayes and Support Vector Machine are used to classify 25 phonemes of Tamil language. The approximate boundaries of phoneme identified using Spectral Transition Measure (STM). After segmentation, Mel Frequency Cepstral Co-Efficient (MFCC) of 9 frames including 4 left context frames, 1 centre frame corresponding to the phoneme and 4 right context frames are extracted and used as input to classifiers. Tamil word dataset prepared to cover 25 phonemes of the language. The performance of the classifiers are analysed and results are presented.","","CD-ROM:978-1-4799-8080-2; Electronic:978-1-4799-8081-9; POD:978-1-4799-8082-6","10.1109/ICCSP.2015.7322549","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322549","Multilayer Perceptron;Naive Bayes;Phoneme Classification;Spectral Transition Measure;Support Vector Machine","Artificial neural networks;Manuals;Robustness;Support vector machines;Training","Bayes methods;information retrieval;multilayer perceptrons;signal classification;speech recognition;support vector machines","MFCC;Naive Bayes method;STM;Tamil language;Tamil phoneme classification;contextual features;discriminative models;entity extraction;language identification;mel frequency cepstral coefficient;multilayer perceptron;out-of-vocabulary detection;phoneme segmentation;spectral transition measure;speech recognition system;spoken document retrieval;spoken term detection;supervised phoneme classification;support vector machine","","","","15","","","2-4 April 2015","","IEEE","IEEE Conference Publications"
"Multimedia data mining using deep learning","P. Wlodarczak; J. Soar; M. Ally","Faculty of Business, Education, Law and Arts, University of Southern Queensland, Toowoomba, Australia","2015 Fifth International Conference on Digital Information Processing and Communications (ICDIPC)","20151112","2015","","","190","196","Due to the large amounts of Multimedia data on the Internet, Multimedia mining has become a very active area of research. Multimedia mining is a form of data mining. Data mining uses algorithms to segment data to identify useful patterns and to make predictions. Despite the successes in many areas, data mining remains a challenging task. In the past, multimedia mining was one of the fields where the results were often not satisfactory. Multimedia Data Mining extracts relevant data from multimedia files such as audio, video and still images to perform similarity searches, identify associations, entity resolution and for classification. As the mining techniques have matured, new techniques were developed. A lot of progress has been made in areas such as visual data mining and natural language processing using deep learning techniques. Deep learning is a branch of machine learning and has been used among other on Smartphones for face recognition and voice commands. Deep learners are a type of artificial neural networks with multiple data processing layers that learn representations by increasing the level of abstraction from one layer to the next. These methods have improved the state-of-the-art in multimedia mining, in speech recognition, visual object recognition, natural language processing and other areas such as genome mining and predicting the efficacy of drug molecules. This paper describes some of the deep learning techniques that have been used in recent research for multimedia data mining.","","CD-ROM:978-1-4673-6831-5; Electronic:978-1-4673-6832-2; POD:978-1-4673-6833-9","10.1109/ICDIPC.2015.7323027","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323027","artificial neural networks;data mining;deep learning;multimedia data mining;natural language processing;visual data mining","Backpropagation;Data mining;Feature extraction;Machine learning;Multimedia communication;Training;Visualization","Internet;data mining;information retrieval;learning (artificial intelligence);multimedia computing;neural nets;pattern classification","Internet;artificial neural networks;association identification;audio files;classification;data segmentation;deep learning;entity resolution;machine learning;multimedia data mining;multimedia files;multiple data processing layers;relevant data extraction;similarity search;still images;useful pattern identification;video files","","2","","29","","","7-9 Oct. 2015","","IEEE","IEEE Conference Publications"
"Automated analysis of line plots in documents","R. R. Nair; N. Sankaran; I. Nwogu; V. Govindaraju","Department of Computer Science and Engineering, University at Buffalo, NY 14260-1660, USA","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","20151123","2015","","","796","800","Information graphics, such as graphs and plots, are used in technical documents to convey information to humans and to facilitate greater understanding. Usually, graphics are a key component in a technical document, as they enable the author to convey complex ideas in a simplified visual format. However, in an automatic text recognition system, which are typically used to digitize documents, the ideas conveyed in a graphical format are lost. We contend that the message or extracted information can be used to help better understand the ideas conveyed in the document. In scientific papers, line plots are the most commonly used graphic to represent experimental results in the form of correlation present between values represented on the axes. The contribution of our work is in the series of image processing algorithms that are used to automatically extract relevant information, including text and plot from graphics found in technical documents. We validate the approach by performing the experiments on a dataset of line plots obtained from scientific documents from computer science conference papers and evaluate the variation of a reconstructed curve from the original curve. Our algorithm achieves a classification accuracy of 91% across the dataset and successfully extracts the axes from 92% of line plots. Axes label extraction and line curve tracing are performed successfully in about half the line plots as well.","","Electronic:978-1-4799-1805-8; POD:978-1-4799-1806-5","10.1109/ICDAR.2015.7333871","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333871","","Accuracy;Image color analysis;Three-dimensional displays","document image processing;feature extraction;image classification;information retrieval","automatic information extraction;axes label extraction;classification accuracy;document based information retrieval;image processing algorithms;line curve tracing;line plots","","","","19","","","23-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Context-Aware Trust Aided Recommendation via Ontology and Gaussian Mixture Model in Big Data Environment","Z. Yu; C. Chen; X. Zheng; W. Ding; D. Chen","Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","2014 International Conference on Service Sciences","20151029","2014","","","85","90","With the development of big data, the data size becomes bigger and bigger, which makes users consume enormous time to find the items that they might like from abundant options. Recommender systems are expected to help users find interested items. However, most existing recommendation methods do not take into account any additional contextual information with a reasonable complexity. This paper aims to propose a context-aware recommender system by incorporating context-aware technology into recommendation. The context-aware approach is based on ontology and Gaussian Mixture Model. The recommendation analysis is implemented by trust aided probabilistic matrix factorization approach. The evaluation shows that the proposed approach has a good effect in recommendation quality.","2165-3828;21653828","CD-ROM:978-1-4799-4332-6; Electronic:978-1-4799-4330-2; POD:978-1-4799-4329-6","10.1109/ICSS.2014.44","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312295","Context-Aware;Gaussian Mixture Mode;Ontology;Recommender system;Trust","Big data;Context;Gaussian mixture model;Matrix decomposition;Ontologies;Probabilistic logic;Recommender systems","Big Data;Gaussian processes;information retrieval;matrix decomposition;mixture models;ontologies (artificial intelligence);probability;recommender systems;ubiquitous computing","Gaussian mixture model;big data environment;complexity;context-aware technology;context-aware trust aided recommendation;contextual information;data size;ontology;recommendation analysis;recommendation quality;recommender systems;trust aided probabilistic matrix factorization approach","","","","21","","","22-23 May 2014","","IEEE","IEEE Conference Publications"
"A New Context Information Selecting Method","X. Li; G. Li","Fac. of Inf. Sci. & Technol., Dalian Maritime Univ., Dalian, China","2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics","20151123","2015","1","","316","318","This paper proposes a new method of selecting context information. The method utilizes ontologies and Formal Concept Analysis to require the context information users need. It can be applied in context search engines and other aspects regarding context-aware technology. The paper also proposes the experiment and assessment of the method to prove this method is really beneficial for context search engines.","","Electronic:978-1-4799-8646-0; POD:978-1-4799-8647-7","10.1109/IHMSC.2015.120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334712","Formal Concept Analysis;context search engines;context-aware technology;ontologies","Context;Formal concept analysis;Lattices;Ontologies;Search engines;Semantic Web;Yttrium","formal concept analysis;information retrieval;ontologies (artificial intelligence);search engines;ubiquitous computing","context information selecting method;context information users need;context search engines;context-aware technology;formal concept analysis;ontologies","","","","13","","","26-27 Aug. 2015","","IEEE","IEEE Conference Publications"
"Device-independent on demand synchronization in the Unico file system","J. Schroeder; M. J. Feeley","Department of Computer Science, University of British Columbia, 201-2366 Main Mall, Vancouver, Canada, V6T1Z4","2015 International Conference and Workshop on Computing and Communication (IEMCON)","20151203","2015","","","1","7","This paper presents Unico, a storage platform that presents users with a single view of all their data, independently of the device they are using. Each of the user's devices has full and up-to-date information about the entire data structure and metadata, and is able to retrieve any file transparently as needed. File content is efficiently distributed among the user's own devices according to where it is used, with no need for centralized storage and no data stored in devices that are not owned by the user. Peers communicate with each other directly to maintain a consistent state. Users also have no need to know where the data is stored, or to remember in which device the latest update to a file has been performed. Unico is able to work with devices with different storage characteristics, and provides mechanisms for applications to adapt to data availability.","","Electronic:978-1-4799-6908-1; POD:978-1-4799-6909-8","10.1109/IEMCON.2015.7344509","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344509","Distributed computing;Middleware;Mobile storage","Cloud computing;File systems;Metadata;Peer-to-peer computing;Portable computers;Synchronization","cloud computing;data structures;information retrieval;meta data;storage management;synchronisation","Unico file system;centralized storage;data structure;demand synchronization;file content;file retrieval;metadata;storage platform","","","","15","","","15-17 Oct. 2015","","IEEE","IEEE Conference Publications"
"Census-Based Cost on Gradients for Matching under Illumination Differences","C. Stentoumis; A. Amditis; G. Karras","Inst. of Commun. & Comput. Syst., Athens, Greece","2015 International Conference on 3D Vision","20151130","2015","","","224","231","Stereo-matching is an indispensable process of dense 3D information extraction for a wide range of applications. Relevant methods rely on cost functions and optimization algorithms for estimating accurate disparities. This work analyses a novel cost for stereo matching under radiometric differences in the stereo-pair, which is based on a modification of the widely used census transformation. It is proposed to define the census on image x and y gradients. The modified census (MC) on gradients is evaluated as an independent matching cost in the presence of severe radiometric differences. For this, the original and the modified census transformation (CT) are implemented in three different aggregation schemes, namely fixed rectangular windows, adaptive cross-based support regions and semi-global matching. It is shown that the MC can provide better results in the cases of local radiometric differences, such as different illumination conditions. Thus, this approach can extend the inherent capability of the original CT to address global monotonic radiometric differences.","","Electronic:978-1-4673-8332-5; POD:978-1-4673-8333-2","10.1109/3DV.2015.79","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335488","census;disparity;gradients;radiometric differences;semi-global matching;stereo","Three-dimensional displays","image matching;information retrieval;optimisation;stereo image processing","adaptive cross-based support regions;census-based cost;dense 3D information extraction;global monotonic radiometric differences;gradients;illumination conditions;illumination differences;modified census transformation;optimization algorithms;semi-global matching;stereo-matching;stereo-pair","","1","","29","","","19-22 Oct. 2015","","IEEE","IEEE Conference Publications"
"Extracting map information from trajectory and social media data","J. Li; H. Dai; Z. o. Yuan; Q. Qin; H. Jiang","College of Geoscience and Surveying Engineering, China University of Mining and Technology, Beijing, 100083","2015 2nd IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services (ICSDM)","20151015","2015","","","18","21","Existing surveying methods are either labor intensive or highly costly and have a long updating cycle, which hinders the timely update of maps. In view of these problems, this paper proposes a framework of extracting digital map information from raw geospatial big data. The framework consists of four steps: data preprocessing, mathematical modeling, information extraction and map post-processing. Extracting map information based on the proposed framework is low-cost and has a short update cycle. A case study is illustrated to show the effectiveness of the framework.","","Electronic:978-1-4799-7749-9; POD:978-1-4799-7750-5","10.1109/ICSDM.2015.7298018","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298018","Data mining;Map extraction;Spatial big data","Big data;Data mining;Media;Roads;Spatial databases;Trajectory;Vehicles","Big Data;cartography;data mining;information retrieval","data preprocessing step;digital map information extraction;geospatial Big Data;information extraction step;map post-processing step;mathematical modeling step;social media data;surveying methods;trajectory data","","","","11","","","8-10 July 2015","","IEEE","IEEE Conference Publications"
"Research of CBR retrieval method based on rough set theory","H. Li; Y. Song; X. Li; Q. Liu; Y. Zhu","School of Computer Science & Technology, Beijing Institute of Technology, Beijing, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20151130","2015","","","990","993","Case retrieval is the key technology of case-based reasoning (CBR), directly affect the efficiency and quality of CBR. For the measurement issues of similar cases, using rough set theory to determine the importance of attributes and to distribute the rational weights of each property. Taking the improved nearest neighbor method which is based on the combination of Hamming distance and Euclidean distance to solve case similarity, improve the accuracy and efficiency of case matching.","2327-0586;23270586","CD-ROM:978-1-4799-8351-3; Electronic:978-1-4799-8353-7; POD:978-1-4799-8354-4","10.1109/ICSESS.2015.7339220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339220","cbr;retrieval method;rough set theory;similarity","Cognition;Courseware;Euclidean distance;Hamming distance;Knowledge representation;Set theory;Yttrium","case-based reasoning;information retrieval;rough set theory","CBR retrieval method;Euclidean distance;Hamming distance;case matching accuracy;case matching efficiency;case retrieval;case similarity;case-based reasoning;nearest neighbor method;rational weight distribution;rough set theory","","","","3","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"Effectiveness of social media text classification by utilizing the online news category","P. Jotikabukkana; V. Sornlertlamvanich; O. Manabu; C. Haruechaiyasak","School of ICT, Sirindhorn International Institute of Technology, Thammasat University, Pathum Thani 12121, Thailand","2015 2nd International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","20151123","2015","","","1","5","Social media text can illustrate significant information of our real social situation. It can show the direction of real-time social movement. However, it has its own characteristics such as using short text and informal language, many unstructured information and argot. This kind of text is hard to classify and difficult to analyze to extract the useful information. In this paper, we propose an effective technique to classify the social media text by utilizing the initial keywords from well-formed sources of data, such as online news. Term frequency-inverse document frequency weighting technique (TF-IDF) and Word Article Matrix (WAM) are used as main methods in this research. We use the extracted keywords from the well-formed source as a main factor to do experiment on Twitter messages. We found a set of the social media keywords can represent the essence of social events and can be used to classify the text effectively.","","Electronic:978-1-4673-8143-7; POD:978-1-4673-8144-4","10.1109/ICAICTA.2015.7335361","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335361","Twitter;Word Article Matrix (WAM);keywords extraction;social media;term frequency-inverse document frequency (TF-IDF)","Hafnium;Manganese","classification;information resources;information retrieval;matrix algebra;social networking (online);social sciences computing;text analysis","TF-IDF;Twitter messages;WAM;argot;informal language;initial keywords utilization;online news category utilization;real social situation;real-time social movement;social events;social media text classification effectiveness;term frequency-inverse document frequency weighting technique;useful information extraction;word article matrix","","","","11","","","19-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Using disturbance records to automate the diagnosis of faults and operational procedures in power generators","M. Moreto; J. G. Rolim","Federal University of Santa Catarina, Brazil","IET Generation, Transmission & Distribution","20151116","2015","9","15","2389","2397","Nowadays, it is a common practice in power generation utilities to monitor the generation units using digital fault recorders. As the disturbance records are generally analysed and stored at a central office or control centre, it has become difficult for engineers to analyse all this data. Some of the main steps in developing automated diagnosis tools to help in this task are the segmentation and feature extraction of the recorded signals and decision making. This study presents a methodology to extract meaningful information from each segment of a disturbance signal. In the approach described in this study, the segmentation is performed by an extended complex Kalman filter. The main features extracted from each segment are symmetrical components at fundamental frequency of voltage and current signals. Feature extraction uses root-mean-square values to obtain the symmetrical components of the three phase quantities. This methodology focuses on offline analysis of fault recorder data of power generators and it is developed not only to fault analysis, but also to verify normal operational procedures, from which result most of the disturbance records. This study also describes an expert system that can be used to automatically classify each record into known categories, focusing the engineer's attention to the most relevant occurrences.","1751-8687;17518687","","10.1049/iet-gtd.2014.0785","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328437","","","Kalman filters;data analysis;fault diagnosis;feature extraction;information retrieval;nonlinear filters;power engineering computing;power generation faults;power generation reliability;power supply quality;statistical analysis","data analysis;digital fault recorders;disturbance records;disturbance signal segmentation;extended complex Kalman filter;fault diagnosis automation;feature extraction;generation unit monitoring;information extraction;operational procedure automation;phase quantities;power generation utilities;power generators;power supply quality;power supply reliability;root-mean-square values","","","","","","","11 19 2015","","IET","IET Journals & Magazines"
"LDA based topic modeling of journal abstracts","P. Anupriya; S. Karpagavalli","PSGR Krishnammal College for Women, Coimbatore, India","2015 International Conference on Advanced Computing and Communication Systems","20151112","2015","","","1","5","Topic modeling is a powerful technique for unsupervised analysis of large document collections. Topic models conceive latent topics in text using hidden random variables, and discover that structure with posterior inference. Topic models have a wide range of applications like tag recommendation, text categorization, keyword extraction and similarity search in the broad fields of text mining, information retrieval, statistical language modeling. In this work, a dataset with 200 abstracts fall under four topics are collected from two different domain journals for tagging journal abstracts. The document models are built using LDA (Latent Dirichlet Allocation) with Collapsed Variational Bayes and Gibbs sampling. Then the built model is used to extract appropriate tags for abstracts. The performance of the built models are analyzed by the evaluation measure perplexity and observed that Gibbs sampling outperforms CV B0 sampling. Tags extracted by two algorithms remains almost the same.","","Electronic:978-1-4799-6438-3; POD:978-1-4799-6439-0","10.1109/ICACCS.2015.7324058","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324058","Gibbs sampling;Latent Dirichlet Allocation;Topic modeling;laborative Variational Bayes","Algorithm design and analysis;Analytical models;Approximation methods;Bayes methods;Computational modeling;Tagging;Training","Bayes methods;Monte Carlo methods;data mining;information retrieval;recommender systems;statistical analysis;text analysis","Gibbs sampling;LDA based topic modeling;collapsed variational Bayes;hidden random variables;information retrieval;journal abstracts;keyword extraction;large document collections;latent Dirichlet allocation;similarity search;statistical language modeling;tag recommendation;text categorization;text mining;unsupervised analysis","","","","11","","","5-7 Jan. 2015","","IEEE","IEEE Conference Publications"
"Automatic data extraction of websites using data path matching and alignment","Y. C. Chu; C. C. Hsu; C. J. Lee; Y. T. Tsai","Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan, R.O.C.","2015 Fifth International Conference on Digital Information Processing and Communications (ICDIPC)","20151112","2015","","","60","64","Since most of web pages contain their main information in data records, extracting data records enables one to obtain and integrate data from diverse sources of Internet. Therefore, data extraction of web pages has been a popular research issue in the last decade. The paper aims to automatically extract data records from web pages and identify items from those extracted records. The proposed approach utilizes Data Path Matching to effectively extract data records and Data Path Code Alignment to efficiently identify data items. Experimental results reveal that the method can extract data effectively.","","CD-ROM:978-1-4673-6831-5; Electronic:978-1-4673-6832-2; POD:978-1-4673-6833-9","10.1109/ICDIPC.2015.7323006","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323006","DOM (Document Object Model);Data Path;Web Mining;Web data extraction","Data mining;HTML;Information filters;Visualization;Web pages;Yttrium","Web sites;data integration;data mining;information retrieval;pattern matching","Internet;Web mining;Web pages;Websites;automatic data extraction;data integration;data items identification;data path code alignment;data path matching;data records extraction","","1","","15","","","7-9 Oct. 2015","","IEEE","IEEE Conference Publications"
"Judicial precedents search supported by natural language processing and clustering","M. A. Calambás; A. Ordóñez; A. Chacón; H. Ordoñez","Intelligent Management Systems Group, Fundaci&#x00F3;n Universitaria de Popay&#x00E1;n, Cauca, Colombia","2015 10th Computing Colombian Conference (10CCC)","20151123","2015","","","372","377","The judicial precedent facilitates decision-making by judges based on previous sentences. However, legal professionals should look among a large number of documents sentences serve as support for its ongoing cases. This work shows the progress in developing a system that uses technologies of natural language processing and grouping (clustering) to optimize the process of search and analysis of these court documents.","","Electronic:978-1-4673-9464-2; POD:978-1-4673-9465-9","10.1109/ColumbianCC.2015.7333448","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333448","Document clustering;judicial document retrieval;judicial natural language processing;legal ontology;named entity recognition","","document handling;information retrieval;law administration;natural language processing;pattern clustering","clustering;court documents;judicial precedent search;legal professionals;natural language processing","","","","16","","","21-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"The data retrieval optimization from the perspective of evidence-based medicine","V. Dobrynin; J. Balykina; M. Kamalov; A. Kolbin; E. Verbitskaya; M. Kasimova","Saint-Petersburg State University, Universitetskaya nab., 7-9, Russia","2015 Federated Conference on Computer Science and Information Systems (FedCSIS)","20151109","2015","","","323","328","The paper is devoted to classification of MEDLINE abstracts into categories that correspond to types of medical interventions - types of patient treatments. This set of categories was extracted from Clinicaltrials.gov web site. Few classification algorithms were tested includingMultinomial Naive Bayes, Multinomial Logistic Regression, and Linear SVM implementations from sklearn machine learning library. Document marking was based on the consideration of abstracts containing links to the Clinicaltrials.gov Web site. As the result of an automatical marking 3534 abstracts were marked for training and testing the set of algorithms metioned above. Best result of multinomial classification was achieved by Linear SVM with macro evaluation precision 70.06%, recall 55.62% and F-measure 62.01%, and micro evaluation precision 64.91%, recall 79.13% and F-measure 71.32%.","","Electronic:978-8-3608-1065-1; POD:978-1-4799-6747-6; USB:978-8-3608-1067-5","10.15439/2015F130","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321461","","Biology;Databases;Drugs;Entropy;Support vector machines;Testing;Training","Bayes methods;Web sites;information retrieval;learning (artificial intelligence);patient treatment;regression analysis;support vector machines","Clinicaltrials.gov web site;classification algorithm;data retrieval optimization;document marking;evidence-based medicine;linear SVM;multinomial logistic regression;multinomial naive Bayes;patient treatment;sklearn machine learning library","","1","","11","","","13-16 Sept. 2015","","IEEE","IEEE Conference Publications"
"How distributed geodata solutions improve emergency management efficiency","A. Ajmar; S. Balbo; P. Boccardo; F. Pongelli; F. Stompanato","ITHACA, via P.C. Boggio 61, 10138 Torino, Italy","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","4844","4847","Being prepared for natural and complex emergencies is a top priority for the UN World Food Programme (WFP). In order to efficiently provide adequate support, WFP has a strong local presence, with more than 80 Country Offices (CO) around the world. The WFP Emergency Preparedness team, in strict cooperation with leading academic institutions and technology experts, develops innovative early warning systems and rapid impact analysis tools and products. An emergency response phase requires an extremely well-organized communication between different actors, providing information timely and in an immediately understandable and not misleading format. WFP acquires, analyzes, distributes and displays data and information, gathered either on the field or retrieved from global monitoring systems. Data retrieved from global monitoring systems are managed centrally at WFP HQ, but need to be accessible to local offices. Similarly, data acquired on the field by local staff should be available at headquarters level for further and global analysis. This organizational model requires the set-up and maintenance of an effective solution for data managing and sharing: the adoption of a common data model further improve data sharing mechanisms, data interpretation and analysis. Additionally, Standard symbology rules and automated map templates helps in enforcing a brand perception and in increasing output quality, timeliness and readability.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7326915","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326915","Emergency Response;Remote Sensing;Understanding the Earth for a Safer World","Emergency services;Metadata;Monitoring;Organizations;Standards;Synchronization","data analysis;emergency management;geographic information systems;information retrieval","CO;UN World Food Programme;WFP HQ;WFP emergency preparedness team;academic institutions;automated map templates;brand perception;country offices;data acquisition;data analysis;data display;data distribution;data interpretation;data management;data model;data retrieval;data sharing mechanism improvement;distributed geodata;early warning systems;emergency management efficiency improvement;emergency response phase;global analysis;global monitoring systems;impact analysis tools;information gathering;local offices;local presence;local staff;natural complex emergencies;organizational model;output quality;readability;standard symbology rules;technology experts;timeliness","","","","6","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Improving storage capacity by distributed exact deduplication systems","C. Barca; D. C. Barca; C. Mara; P. Anghelescu; B. Gavriloaia; R. Vizireanu; R. Craciunescu; O. Fratu","Faculty of Electronics, Communications and Computers, University of Pitesti, Roumania","2015 7th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","20151026","2015","","","C-11","C-16","The topic of data deduplication has received lately a lot of attention for its storage reduction functionality. Data deduplication essentially refers to the elimination of redundant data, leaving only one copy of the data to be stored, and is meant to reduce the pain regarding the exponential data growth in backup or archiving centers. Most existing state-of-the-art deduplication systems rely on approximate deduplication in order to achieve high-performance. Unfortunately, these studies are usually conducted and tested on single-host systems. Although their authors claim that the design can be easily applied on multi-node systems, we have not seen yet an extension that enacts that - they lack of trust. Thus, in a world where data deduplication storage systems are continuously struggling in providing the required throughput and disk capacities necessary to store and retrieve data within reasonable times, we are handled the task to design a distributed deduplication systems that will achieve efficiency, scalability and throughput at a petascale capacity level. In this paper we present a proof-of-concept design that one can use to implement such a system: A Distributed Exact Deduplication System, which we believe it will cross the boundaries towards a new generation of backup and archiving systems.","","Electronic:978-1-4673-6647-2; POD:978-1-4673-6648-9","10.1109/ECAI.2015.7301141","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301141","distributed exact deduplication;hashed fingerprints;in-line deduplication;indexing;load balancing","Containers;Fingerprint recognition;Indexes;Metadata;Random access memory;Scalability;Throughput","information retrieval systems;information storage;records management","archiving systems;backup systems;distributed exact deduplication systems;multinode systems;petascale capacity level;proof-of-concept design;storage capacity improvement;storage reduction functionality","","","","28","","","25-27 June 2015","","IEEE","IEEE Conference Publications"
"Contextual Sub-network Extraction in Contextual Social Networks","X. Zheng; Y. Wang; M. A. Orgun","Dept. of Comput., Macquarie Univ., Sydney, NSW, Australia","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","119","126","Predicting the trust between a source participant and a target participant in a social network is important in many applications, e.g., assessing the recommendation from a target participant from the perspective of a source participant. In general, social networks contain participants, the links and trust relations between them and the contextual information for their interactions. All such information has important influence on trust prediction. However, predicting the trust between two participants based on the whole network is ineffective and inefficient. Thus, prior to trust prediction, it is necessary to extract a small-scale contextual network that contains most of the important participants as well as trust and contextual information. However, extracting such a sub-network has been proved to be an NP-Complete problem. To solve this challenging problem, we propose a social context-aware trust sub-network extraction model to search near-optimal solutions effectively and efficiently. In our proposed model, we first present the important factors that affect the trust between participants in OSNs. Then, we define a utility function to measure the trust factors of each node in a social network. At last, we design an ant colony algorithm with a newly designed mutation process for sub-network extraction. The experiments, conducted on two popular datasets of Epinions and Slashdot, demonstrate that our approach can extract those sub-networks covering important participants and contextual information while keeping a high density rate. Our approach is superior to the state-of-the-art approaches in terms of the quality of extracted sub-networks within the same execution time.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.365","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345273","Sub-network extraction;Trust;Trust prediction","Context;Context modeling;Data mining;Predictive models;Reliability;Search problems;Social network services","ant colony optimisation;computational complexity;information retrieval;security of data;social networking (online);ubiquitous computing","Epinions dataset;NP-complete problem;OSN;Slashdot dataset;ant colony algorithm;contextual social networks;contextual subnetwork extraction;execution time;social context-aware trust subnetwork extraction model;trust prediction","","","","30","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Detecting Data Semantic: A Data Leakage Prevention Approach","S. Alneyadi; E. Sithirasenan; V. Muthukkumarasamy","Sch. of Inf. & Commun. Technol., Griffith Univ., Gold Coast, QLD, Australia","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","910","917","Data leakage prevention systems (DLPSs) are increasingly being implemented by organizations. Unlike standard security mechanisms such as firewalls and intrusion detection systems, DLPSs are designated systems used to protect in use, at rest and in transit data. DLPSs analytically use the content and surrounding context of confidential data to detect and prevent unauthorized access to confidential data. DLPSs that use content analysis techniques are largely dependent upon data fingerprinting, regular expressions, and statistical analysis to detect data leaks. Given that data is susceptible to change, data fingerprinting and regular expressions suffer from shortcomings in detecting the semantics of evolved confidential data. However, statistical analysis can manage any data that appears fuzzy in nature or has other variations. Thus, DLPSs with statistical analysis capabilities can approximate the presence of data semantics. In this paper, a statistical data leakage prevention (DLP) model is presented to classify data on the basis of semantics. This study contributes to the data leakage prevention field by using data statistical analysis to detect evolved confidential data. The approach was based on using the well-known information retrieval function Term Frequency-Inverse Document Frequency (TF-IDF) to classify documents under certain topics. A Singular Value Decomposition (SVD) matrix was also used to visualize the classification results. The results showed that the proposed statistical DLP approach could correctly classify documents even in cases of extreme modification. It also had a high level of precision and recall scores.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.464","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345372","Data leakage prevention;Data semantics;Singular Value Decomposition;Statistical analysis","Data models;Fingerprint recognition;Robustness;Security;Semantics;Singular value decomposition;Statistical analysis","information retrieval;security of data;singular value decomposition;statistical analysis","DLPS;SVD matrix;TF-IDF;confidential data context;content analysis technique;data fingerprinting;data leakage prevention approach;information retrieval function;regular expression;singular value decomposition matrix;standard security mechanism;statistical analysis;statistical data DLP model;statistical data leakage prevention model;term frequency-inverse document frequency;unauthorized access","","","","23","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"A Real-Time Grid Enabled Test Bed for Sharing and Searching Documents among Universities","S. M. D. Kumar; A. Jumnal","Dept. of Comput. Sci. & Eng., Univ. Visvesvaraya Coll. of Eng., Bangalore, India","2015 Second International Conference on Advances in Computing and Communication Engineering","20151026","2015","","","604","609","In recent years, the world has witnessed growing technologies in the area of high performance computing. Grid computing has made it possible to aggregate distributed CPU, memory, storage and data to provide the full potential for educational application to share the knowledge that can be attainable on any single system. However, the literature shows that the potential of grid resources for educational purpose is not being fully utilized yet. In this paper, we develop a real-time grid enabled infrastructure test bed that allows students, faculty and researchers to share and gain knowledge in their area of interest by using e-learning, searching and distributed repository services among universities from anywhere anytime. This project uses the Globus toolkit 5.2.5 (GTK) software that provides grid middleware with resource access, discovery and management, data movement, security, and so forth. In addition, the infrastructure test bed also enables users to discover these services and interact with them using the grid portal. This project have a potential impact on future educational institution's research activities.","","Electronic:978-1-4799-1734-1; POD:978-1-4799-1735-8","10.1109/ICACCE.2015.94","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306755","Distributed Repository;E-Learning;Globus Toolkit;Grid Computing;Searching","Electronic learning;Middleware;Portals;Protocols;Security;Servers","document handling;educational computing;educational institutions;grid computing;information retrieval;middleware;portals","Globus toolkit 5.2.5 software;data movement;distributed data aggregation;distributed repository service;document searching;document sharing;e-learning;educational application;educational institution research activity;educational purpose;faculty;grid computing;grid middleware;grid portal;grid resources;high performance computing;knowledge sharing;real-time grid enabled infrastructure test bed;resource access;resource discovery;resource management;security;service discovery;students;universities","","","","22","","","1-2 May 2015","","IEEE","IEEE Conference Publications"
"TV Advertisement Search Based on Audio Peak-Pair Hashing in Real Environments","H. G. Kim; H. S. Cho; J. Y. Kim","Dept. of Electron. Convergence Eng., Kwangwoon Univ., Seoul, South Korea","2015 5th International Conference on IT Convergence and Security (ICITCS)","20151008","2015","","","1","4","This paper proposes a robust TV advertisement search based on audio fingerprinting in real environments. To obtain prominent audio peak pairs against different types of distortions, an adaptive thresholding method based on a median filter and peak-picking update is applied. Using the prominent audio peak-pair hashing, the proposed audio fingerprinting algorithm improves the robustness of the TV advertisement search against noise, pitch-shifting, and time-stretching, and delivers high identification accuracy in spite of short queries. The experimental results show that the proposed method has respectable results compared to other methods.","","Electronic:978-1-4673-6537-6; POD:978-1-4673-6538-3","10.1109/ICITCS.2015.7293031","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293031","","Databases;Distortion;Fingerprint recognition;Noise;Robustness;TV;Time-frequency analysis","advertising;audio coding;cryptography;information retrieval;median filters","TV advertisement search;adaptive thresholding method;audio fingerprinting;audio peak-pair hashing;median filter;peak-picking update;television advertisement","","","","5","","","24-27 Aug. 2015","","IEEE","IEEE Conference Publications"
"Preventing Access to Residual Data Exposed by Packet Expansion Operations","R. Duncan; K. Ross; J. Frandeen; A. Chorro-Rivas","CloudShield Technol., Leidos Co., Sunnyvale, CA, USA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","20151130","2015","","","855","860","Computer packets are often expanded, e.g., to insert headers for local network routing. This act of enlarging packets creates opportunities for illicit access to residual data in the buffer holding the packet. The condition arises when the newly allocated area stretches past the packet's previous end, exposing data left by previous, lengthier packets. This residual data might contain sensitive data, proprietary protocols or classified information that a malicious program can forward or log for illicit use. Unwanted access to such data can be prevented by overwriting the area or by requiring an immediate, complete fill of the expansion area by the application. The paper analyzes basic characteristics of the problem. In addition, it compares performance consequences of various solutions by implementing them as specialized bytecode instructions handled by microcode.","","Electronic:978-1-4799-8937-9; POD:978-1-4799-8938-6","10.1109/HPCC-CSS-ICESS.2015.204","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336270","network processing;packet processing;residual data;security","Buffer storage;Conferences;Context;Filling;Payloads;Security;Writing","buffer storage;data protection;information retrieval;network routing;packet switching;pattern classification;protocols","buffer storage;bytecode instruction;computer packet expansion operation;illicit access;information classification;local network routing;malicious program;microcode;proprietary protocol;residual data access prevention","","","","15","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"A User-Relationship-Based Cache Replacement Strategy for Mobile Social Network","Q. Xing; J. Wang; Y. Li; Y. Han","Beijing Key Lab. on Integration & Anal. of Large-scale Stream Data, North China Univ. of Technol., Beijing, China","2015 Ninth International Conference on Frontier of Computer Science and Technology","20151102","2015","","","376","381","In recent years, mobile applications grew rapidly with the development of Android and iOS platforms. Most of applications on these smart phones generate and make use of user-generated data. When users need relative data, it is not very realistic to request from the server every time. So a suitable cache technology is required. Traditional cache technologies pay more attention on data access time, frequency or data space, but do not consider data generators' relationships with each other. In mobile social network environment, data access is closely related to users' relationships, so this factor is suitable to be used in cache technologies. In this paper, we proposed a user-relationship-based cache replacement strategy. We combined user relationship with the classic cache algorithm LRU. Not only the access times of each data blocks are considered, but also users' relationships are computed by the closeness between data requesters and generators. The experiment results show that our replacement strategy can improve the cache hit ratio in mobile social environment.","2159-6301;21596301","CD-ROM:978-1-4673-9294-5; Electronic:978-1-4673-9295-2; POD:978-1-4673-9296-9","10.1109/FCST.2015.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314709","cache replacement;mobile computing;social network","Mobile applications;Mobile communication;Mobile computing;Servers;Twitter;Wireless communication","Android (operating system);cache storage;iOS (operating system);information retrieval;mobile computing;smart phones;social networking (online);user interfaces","Android platforms;cache replacement strategy;data access time;iOS platforms;mobile social network;smart phones;user-relationship","","","","12","","","26-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Secure Sketch Search for Document Similarity","C. Orencik; M. Alewiwi; E. Savas","Fac. of Eng. & Natural Sci., Sabanci Univ., Istanbul, Turkey","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","1102","1107","Document similarity search is an important problem that has many applications especially in outsourced data. With the wide spread of cloud computing, users tend to outsource their data to remote servers which are not necessarily trusted. This leads to the problem of protecting the privacy of sensitive data. We design and implement two secure similarity search schemes for textual documents utilizing locality sensitive hashing techniques for cosine similarity. While the first one provides very fast search time results and a decent level of privacy, the second method enjoys enhanced security properties such as hiding the search and access patterns but with higher latency.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.489","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345397","Confidentiality;Document Similarity;Locality Sensitive Hashing;Privacy","Cloud computing;Encryption;Privacy;Protocols;Servers","cryptography;data privacy;information retrieval;text analysis","access patterns hiding;cosine similarity;document similarity search;locality sensitive hashing techniques;privacy;search hiding;secure similarity search schemes;sketch search security;textual documents","","","","13","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Entity Linking and Name Disambiguation in Chinese Micro-Blogs","L. Li; Y. Guo; Y. Xiang; X. Xu; W. Zeng","Dept. of Comput. Sci., Southwest Univ., Chongqing, China","2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops","20151026","2014","","","838","843","The amounts of data on social networks have been increasing sharply with the development of Web 2.0. Extracting social media content for the construction and extension of the knowledge base, mainly through remove ambiguities of entities from microblogs, has attracted attention from both academia and industry. Understanding Chinese microblogs is challenging because of the inherent features of Chinese language, the in- formal usage of the language and the wide variety of content it covers. In this paper, we focus on entity disambiguation in Chinese microblogs. A Web crawler is first developed to collect relevant information from both Baidu Encyclopedia and Chinese Wikipedia. The creation of the entity dictionary is based on the exiting mapping rules obtained from Baidu search engine by the newly developed crawler. A novel disambiguation strategy including a clustering algorithm based on Newman fast algorithm is proposed along with a label disambiguation algorithm. We then evaluate our approach on the Chinese microblog data set. The experimental result achieved 89.34% in terms of accuracy, which is 4.35% better than the best result of 84.99% (of all participating teams). Our approach is promising in identifying entity links and discovering the potential links in Chinese microblogs.","","CD-ROM:978-1-4799-7645-4; Electronic:978-1-4799-7646-1; POD:978-1-4799-7647-8","10.1109/UIC-ATC-ScalCom.2014.142","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307051","Chinese microblog;Clustering;En- tity disambiguation;Entity linking","Accuracy;Clustering algorithms;Conferences;Dictionaries;Encyclopedias;Joining processes;Knowledge based systems","Internet;encyclopaedias;information retrieval;natural language processing;pattern clustering;search engines;social networking (online)","Baidu encyclopedia;Baidu search engine;Chinese Wikipedia;Chinese language;Chinese microblog data set;Newman fast algorithm;Web 2.0 development;Web crawler;clustering algorithm;entity disambiguation strategy;entity linking strategy;knowledge base;label disambiguation algorithm;mapping rules;relevant information collection;social media content extraction;social networks","","","","17","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Automatic lexico-syntactic classification of noun-adjective relations for Romanian language","M. Lazăr; D. Militaru","Military Equipment and Technologies Research Agency, Bucharest, Romania","2015 7th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","20151026","2015","","","S-85","S-88","The natural language processing became one of the most important fields of artificial intelligence because is related to the area of human-computer interaction using human languages (natural language generation, question answering, machine translation, etc.) or speech understanding (language modeling).To model the relations between words it is necessary to find the syntactic and semantic relations between them. Starting from the property/attribute-holder relation between nouns and adjectives, extracted from Romanian translation of Orwell's novel “Nineteen eighty-four” (part of Multext-East [1]), this paper presents the results of automatic lexico-syntactic classification using three classification methods: decision trees, k nearest neighbors and naïve Bayes.","","Electronic:978-1-4673-6647-2; POD:978-1-4673-6648-9","10.1109/ECAI.2015.7301166","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301166","Naïve Bayes;Romanian language;decision trees;k-NN;lexico-syntactic pattern","Accuracy;Context;Decision trees;Pragmatics;Semantics;Syntactics;Training","decision trees;human computer interaction;language translation;natural language processing;question answering (information retrieval)","HCI;Romanian language;artificial intelligence;attribute-holder relation;automatic lexico-syntactic classification;decision trees;human languages;human-computer interaction;k nearest neighborsand naïve Bayes;language modeling;machine translation;natural language generation;natural language processing;noun-adjective relations;question answering;speech understanding","","","","23","","","25-27 June 2015","","IEEE","IEEE Conference Publications"
"Exploring multi-language resources for unsupervised spoken term discovery","B. Ludusan; A. Caranica; H. Cucu; A. Buzo; C. Burileanu; E. Dupoux","Laboratoire de Sciences Cognitives et Psycholinguistique, EHESS/ENS/CNRS, Paris, France","2015 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)","20151203","2015","","","1","6","With information processing and retrieval of spoken documents becoming an important topic, there is a need of systems performing automatic segmentation of audio streams. Among such algorithms, spoken term discovery allows the extraction of word-like units (terms) directly from the continuous speech signal, in an unsupervised manner and without any knowledge of the language at hand. Since the performance of any downstream application depends on the goodness of the terms found, it is relevant to try to obtain higher quality automatic terms. In this paper we investigate whether the use input features derived from of multi-language resources helps the process of term discovery. For this, we employ an open-source phone recognizer to extract posterior probabilities and phone segment decisions, for several languages. We examine the features obtained from a single language and from combinations of languages based on the spoken term discovery results attained on two different datasets of English and Xitsonga. Furthermore, a comparison to the results obtained with standard spectral features is performed and the implications of the work discussed.","","DVD:978-1-4673-7559-7; Electronic:978-1-4673-7560-3; POD:978-1-4673-7561-0","10.1109/SPED.2015.7343096","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343096","multi-language resources;posteriorgrams;spoken term discovery","Context;Feature extraction;Hidden Markov models;Mel frequency cepstral coefficient;Speech;Speech recognition","audio streaming;feature extraction;information retrieval;natural language processing;probability;spectral analysis;speech processing;unsupervised learning","English;Xitsonga;automatic audio stream segmentation;continuous speech signal;information processing;language knowledge;multilanguage resources;open-source phone recognizer;phone segment decisions;posterior probability extraction;spectral features;spoken document retrieval;spoken term discovery;unsupervised spoken term discovery;word-like unit extraction","","1","","26","","","14-17 Oct. 2015","","IEEE","IEEE Conference Publications"
"Archiving and analysis of electroencephalograms in Ukrainian Grid: The first application","V. O. Gaidar; O. O. Sudakov","Faculty of Radiophysics, Electronics and Computer Systems, National Taras Shevchenko University of Kyiv, Glushkova prosp., 2, Kyiv","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","20151203","2015","2","","961","965","Distributed database and analysis tools for electroencephalography (EEG) data archiving and procession in Ukrainian Grid Infrastructure were proposed, implemented and applied for separation of signal sources. The database is being populated with human scalp EEG and rat intracranial EEG data now. Resources consuming algorithms for removal of electromyogram and electro-oculogram artifacts from human scalp EEG were applied using the database. Applied algorithms are based on nonlinear blind source separation techniques with the Second Order Blind Identification and asymptotically optimal weights. Applied algorithms proved to be efficient for preliminary EEG data procession and are being implemented for distributed application in Grid.","","CD-ROM:978-1-4673-8358-5; Electronic:978-1-4673-8361-5; POD:978-1-4673-8362-2","10.1109/IDAACS.2015.7341447","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7341447","blind source separation;database;electroencephalogram;electromyogram;grid;oculogram","Blind source separation;Databases;Electrodes;Electroencephalography;Electromyography;Electrooculography;Medical diagnostic imaging","blind source separation;electroencephalography;grid computing;information retrieval systems;medical signal processing","EEG data procession;Ukraine;data archiving;distributed database;electro-oculogram artifacts;electroencephalography;grid infrastructure;nonlinear blind source separation techniques;rat intracranial EEG data;second order blind identification;signal sources separation","","","","20","","","24-26 Sept. 2015","","IEEE","IEEE Conference Publications"
"High-throughput analysis of tissue-based biomarkers in digital pathology","Y. R. Van Eycke; O. Debeir; L. Verset; P. Demetter; I. Salmon; C. Decaestecker","Laboratories of Image, Signal processing and Acoustics, Universit&#x00E9; Libre de Bruxelles, Brussels, Belgium","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","7732","7735","By simultaneously processing a large number of tissue samples, the tissue microarray (TMA) technology allows standardized screening of protein expression using immunohistochemistry thereby providing a very efficient way for tissue-based biomarker analysis. Nowadays, whole slide imaging is becoming standard in digital pathology and enables image sharing, archiving and also processing. In this paper, we present methods for processing TMA images in order to correctly identify the numerous tissue samples and to register images from consecutive TMA sections.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7320184","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320184","","Accuracy;Biological tissues;Biomarkers;Immune system;Pathology;Proteins;Registers","biochemistry;biological tissues;biomedical optical imaging;image registration;information retrieval systems;medical image processing;proteins","TMA image processing;digital pathology;high-throughput analysis;image archiving;image registration;image sharing;immunohistochemistry;protein expression;standardized screening;tissue microarray technology;tissue samples;tissue-based biomarker analysis;whole slide imaging","","1","","11","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Indonesian medical retrieval case based on knowledge association rule similarity","W. Suwarningsih; I. Supriana; A. Purwanti","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Jl. Ganesa 10 Bandung, Indonesia","2015 International Conference on Control, Electronics, Renewable Energy and Communications (ICCEREC)","20151130","2015","","","142","147","In Case Based Reasoning (CBR), retrieval phase is one of the important phases in view of the dependence of the overall effectiveness of the CBR system on the case retrieval stage. To run the process of finding a new case, CBR systems typically utilize knowledge similarity called Similarity-Based Reasoning (SBR) in which the knowledge encoded in the form of term measures is used to calculate the similarity between a new case with the old one. In this paper, we attempt to build a new concept of similarity text for retrieval case on Indonesian medical sentences. The method to be used is the knowledge base of similarity in which the stage included: (i) the utilization of the knowledge for decision-case association, and (ii) the association extraction of knowledge by creating a rule based on attribute data to generate a subset of cases and solutions in a number of cases. The test results then showed the highest values found in the case of the active sentence form at 88.18% for precision, 88.23% for recall and 89.12% for F1-Measure.","","CD-ROM:978-1-4799-8974-4; Electronic:978-1-4799-8975-1; POD:978-1-4799-8976-8","10.1109/ICCEREC.2015.7337033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337033","Case Based Reasoning;Indonesian medical sentence;Similarity Based Reasoning;Text Similarity","Association rules;Cognition;Context;Head;Knowledge based systems;Renewable energy sources;Semantics","case-based reasoning;data mining;information retrieval;medical information systems;text analysis","CBR system;Indonesian medical retrieval case;Indonesian medical sentences;attribute data;case based reasoning;decision-case association;knowledge association extraction;knowledge association rule similarity;similarity text","","1","","33","","","27-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Central Architecture Framework for e-Governance System in India Using ICT Infrastructure","M. Kumar; R. Bhatt; K. S. Vaisla","Dept. of CSE, THDC Inst. of Hydro. Eng. & Tech., Tehri, India","2015 Second International Conference on Advances in Computing and Communication Engineering","20151026","2015","","","708","713","There are many countries involve in the field of e-Governance through the ICT (Information and Communication Technology) which is play important role in the world of internet and mobile because the growth of Information Technology is going extreme level. The effective model of e-Governance can change the scenario of the information accessing power from the internet and mobile that is needed to the empower for the user of government department or citizen. This paper is discussed and represents the effective architecture framework of e-Governance for India which includes the Center, all States and respective District and gram panchayat through the ICT infrastructure. This paper is also discuss is the central architecture for the Indian government which is control the all government department of whole states of the India. The main objective of this paper is to create or implement the architecture framework for the e-Governance that is beneficial or cost effective for the Indian government. Through this model all government organization and agencies will interact effectively and conveniently share the data and information.","","Electronic:978-1-4799-1734-1; POD:978-1-4799-1735-8","10.1109/ICACCE.2015.53","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306774","Architectural Framework for e-Governance using ICT infrastructure;ICT(Information and Communication Technology) infrastructure;e-Governance Infrastructure","Data models;Government;Information and communication technology;Internet;Knowledge engineering;Wide area networks","Internet;government data processing;information retrieval;mobile computing","Gram Panchayat;ICT infrastructure;Indian government;Internet;central architecture framework;citizen;data sharing;e-governance system;government department;information access;information and communication technology;information sharing;mobile computing","","","","11","","","1-2 May 2015","","IEEE","IEEE Conference Publications"
"An Uncertainty-Aware Approach for Exploratory Microblog Retrieval","M. Liu; S. Liu; X. Zhu; Q. Liao; F. Wei; S. Pan","Tsinghua University","IEEE Transactions on Visualization and Computer Graphics","20151027","2016","22","1","250","259","Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.","1077-2626;10772626","","10.1109/TVCG.2015.2467554","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192694","microblog data;mutual reinforcement model;uncertainty modeling;uncertainty propagation;uncertainty visualization","Data models;Data visualization;Monte Carlo methods;Tagging;Twitter;Uncertainty;Visual analytics","Web sites;data visualisation;information retrieval","composite visualization;exploratory microblog retrieval;flow map;graph node;graph visualization;hashtags retrieval;high-quality microblog data retrieval;mutual reinforcement rank;ranking technique;salient posts retrieval;uncertainty glyph;uncertainty-aware approach;uncertainty-aware visual analytics approach","0","4","","55","","20150812","Jan. 31 2016","","IEEE","IEEE Journals & Magazines"
"LWCS: A large-scale web page classification system based on anchor graph hashing","Y. Zheng; C. Sun; C. Zhu; X. Lan; X. Fu; W. Han","College of Computer, National University of Defense Technology, Changsha, Hunan, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20151130","2015","","","90","94","Nowadays, while we are enjoying the convenience brought by such a huge repository of online web information, we may come across difficulties in finding the web pages we want related to particular information we are searching for. Hence, it is essential to classify web documents to facilitate the search and retrieval of pages. Existing algorithms work well with a small quantity of web pages, whereas, they become slow and even non-effective while dealing with a large scale of web pages. Recently, some of these algorithms were adapted to distributed platforms which boosted their classification speeds effectively. However, due to high dimensions of web page features, the parallel classifiers were still trained with limited capacity training sets. In addition, these methods didn't improve the classification itself, merely boosted by high computing performance of distributed platforms. So oriented to large-scale web page classification, we propose to integrate anchor graph hashing with K-Nearest Neighbour(KNN) classifier to reduce the pages' original feature dimensions. The hash value of each page is used for training and classification instead of the original vectors. Experimental comparison with the original KNN on a large dataset demonstrates the efficacy of our proposed method.","2327-0586;23270586","CD-ROM:978-1-4799-8351-3; Electronic:978-1-4799-8353-7; POD:978-1-4799-8354-4","10.1109/ICSESS.2015.7339012","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339012","Chinese web page classification;anchor graph hashing;automatic classification;k-nearest neighbours;large-scale web page classification","Classification algorithms;Crawlers;Internet;Manuals;Training;Web pages;Yttrium","Internet;file organisation;graph theory;information retrieval;pattern classification","KNN classifier;LWCS;anchor graph hashing;k-nearest neighbour classifier;large-scale Web page classification system;page retrieval;page search","","","","14","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"Vector representation of emotion flow for popular music","C. H. Chung; H. Chen","National Taiwan University, Taiwan","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","20151203","2015","","","1","6","The flow of emotion expressed by music through time is a useful feature for music information indexing and retrieval. In this paper, we propose a novel vector representation of emotion flow for popular music. It exploits the repetitive verse-chorus structure of popular music and connects a verse (represented by a point) and its corresponding chorus (another point) in the valence-arousal emotion plane. The proposed vector representation visually gives users a snapshot of the emotion flow of a popular song in an intuitive and instant manner, more effective than the point and curve representations of music emotion flow. Because many other genres also have repetitive music structure, the vector representation has a wide range of applications.","","Electronic:978-1-4673-7478-1; POD:978-1-4673-7479-8","10.1109/MMSP.2015.7340797","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340797","Affective content;emotion flow;music emotion representation;music structure","Bridges;Feature extraction;Instruments;Multiple signal classification;Music;Predictive models;Training","information retrieval;music","emotion flow;music information indexing;music information retrieval;popular music;vector representation","","","","36","","","19-21 Oct. 2015","","IEEE","IEEE Conference Publications"
"Part-of-speech labeling for Reuters database","R. Creţulescu; A. David; D. Morariu; L. Vinţan","Comput. Sci. & Electr. Eng. Dept., &#x201C;Lucian Blaga&#x201D; Univ. of Sibiu, Sibiu, Romania","2015 19th International Conference on System Theory, Control and Computing (ICSTCC)","20151109","2015","","","117","122","Even if the Vector Space Model used for document representation in information retrieval systems integrates a small quantity of knowledge it continues to be used due to its computational cost, speed execution and simplicity. We try to improve this document representation by adding some syntactic information such as the parts of speech. In this paper, we have evaluated three different tagging algorithms in order to select the most suitable tagger for using it to tag the Reuters dataset. In this work, we have evaluated the taggers using only five different parts of speech: noun, verb, adverb, adjective and others. We considered these particular tags being the most representative for describing the documents into these parts of speech space.","","Electronic:978-1-4799-8481-7; POD:978-1-4799-8482-4; USB:978-1-4799-8480-0","10.1109/ICSTCC.2015.7321279","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321279","Documents Representation;Part of Speech;Tagging Algorithms;Vector Space Model","Accuracy;Labeling;Speech;Syntactics;Tagging;Testing;Training","document handling;information retrieval","Reuters database;Reuters dataset;document representation;information retrieval systems;part-of-speech labeling;tagging algorithms;vector space model","","","","16","","","14-16 Oct. 2015","","IEEE","IEEE Conference Publications"
"Crawling Online Social Networks","F. Erlandsson; R. Nia; M. Boldt; H. Johnson; S. F. Wu","","2015 Second European Network Intelligence Conference","20151109","2015","","","9","16","Researchers put in tremendous amount of time and effort in order to crawl the information from online social networks. With the variety and the vast amount of information shared on online social networks today, different crawlers have been designed to capture several types of information. We have developed a novel crawler called SINCE. This crawler differs significantly from other existing crawlers in terms of efficiency and crawling depth. We are getting all interactions related to every single post. In addition, are we able to understand interaction dynamics, enabling support for making informed decisions on what content to re-crawl in order to get the most recent snapshot of interactions. Finally we evaluate our crawler against other existing crawlers in terms of completeness and efficiency. Over the last years we have crawled public communities on Facebook, resulting in over 500 million unique Facebook users, 50 million posts, 500 million comments and over 6 billion likes.","","Electronic:978-1-4673-7592-4; POD:978-1-4673-7593-1","10.1109/ENIC.2015.10","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321230","crawling;mining;online social media;online social networks","Crawlers;Facebook;Feeds;Informatics;Sampling methods;Silicon compounds","information retrieval;social networking (online)","Facebook;SIN Crawler Engine;SINCE;online social networks crawling","","1","","25","","","21-22 Sept. 2015","","IEEE","IEEE Conference Publications"
"Semantic similarity for sequenced shingles","F. Konaka; T. Miura","Dept. of Advanced Sciences, HOSEI University, Kajinocho 3-7-2, Koganei, Tokyo, Japan","2015 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","20151130","2015","","","12","17","In this investigation, we introduce new kinds of sentence similarity, called Euclid similarity and Levenshtein similarity, to capture both word sequences and semantic aspects. This is especially useful for Semantic Textual Similarity (STS) so that we could retrieve SNS texts, short sentences or something including collocations. We show the usefulness of our approach by some experimental results.","","Electronic:978-1-4673-7788-1; POD:978-1-4673-7789-8; USB:978-1-4673-7787-4","10.1109/PACRIM.2015.7334801","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334801","Euclid Similarity;Levenshtein Similarity;Semantic Textual Similarity","Blogs;Context;Dictionaries;Electronic publishing;Internet;Semantics","information retrieval;text analysis","Euclid similarity;Levenshtein similarity;SNS texts;information retrieval;semantic textual similarity;sentence similarity;shingles sequence;word sequences","","","","11","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Optimized Search-and-Compute Circuits and Their Application to Query Evaluation on Encrypted Data","J. H. Cheon; M. Kim; M. Kim","Department of Mathematical Sciences, Seoul National University, Seoul, Korea","IEEE Transactions on Information Forensics and Security","20151112","2016","11","1","188","199","Private query processing on encrypted databases allows users to obtain data from encrypted databases in such a way that the users' sensitive data will be protected from exposure. Given an encrypted database, users typically submit queries similar to the following examples: 1) How many employees in an organization make over U.S. $100000? 2) What is the average age of factory workers suffering from leukemia? Answering the questions requires one to search and then compute over the relevant encrypted data sets in sequence. In this paper, we are interested in efficiently processing queries that require both operations to be performed on fully encrypted databases. One immediate solution is to use several special-purpose encryption schemes simultaneously; however, this approach is associated with a high computational cost for maintaining multiple encryption contexts. Another solution is to use a privacy homomorphic scheme. However, no secure solutions have been developed that satisfy the efficiency requirements. In this paper, we construct a unified framework to efficiently and privately process queries with search and compute operations. For this purpose, the first part of our work involves devising several underlying circuits as primitives for queries on encrypted data. Second, we apply two optimization techniques to improve the efficiency of these circuit primitives. One technique involves exploiting single-instruction-multiple-data (SIMD) techniques to accelerate the basic circuit operations. Unlike general SIMD approaches, our SIMD implementation can be applied even to a single basic operation. The other technique is to use a large integer ring (e.g., Z<sub>2</sub>t) as a message space rather than a binary field. Even for an integer of k bits with k > t, addition can be performed using degree 1 circuits with lazy carry operations. Finally, we present various experiments performed by varying the considered parameters, such as the query type and the number of tuple- .","1556-6013;15566013","","10.1109/TIFS.2015.2483486","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279139","Encrypted databases;Homomorphic encryption;Private query processing;homomorphic encryption;private query processing","Encryption;Optimization;Polynomials;Query processing;Servers","cryptography;data protection;optimisation;parallel processing;query processing;question answering (information retrieval)","SIMD techniques;degree 1 circuits with lazy carry operations;encrypted databases;large integer ring;message space;optimized search-and-compute circuits;privacy homomorphic scheme;private query processing;query evaluation;question answering;single-instruction-multiple-data techniques;special-purpose encryption schemes;user sensitive data protection","","1","","45","","20150928","Jan. 2016","","IEEE","IEEE Journals & Magazines"
"Speeding up the file access of large compressed NIfTI neuroimaging data","Z. Rajna; A. Keskinarkaus; V. Kiviniemi; T. Seppänen","Biomedical Engineering Research Group, Department of Computer Science and Engineering, ITEE, University of Oulu, 90014 Finland","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","20151105","2015","","","654","657","A method and implementation are presented to achieve a thousand fold speed-up for seeking of large files in a commonly used compressed neuroimaging data format NIfTI. Such technologies are not currently available in this research field while they would make the everyday work for hundreds of researchers and experts much smoother and faster. The method includes the creation of a novel index structure for the compressed data in order to achieve the speed-up. With random seek simulations, we demonstrate that a speed-up of over hundred up to even five thousand can be reached compared to the currently available implementations. By configuring the index structure properly, one can set an operating point which optimizes the efficiency as speed-up versus index size according to the requirements by the user. For example, a thousand fold speed-up can be achieved with an index size of only about two percent of the original compressed data.","1094-687X;1094687X","DVD:978-1-4244-9270-1; Electronic:978-1-4244-9271-8; POD:978-1-4244-9269-5","10.1109/EMBC.2015.7318447","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318447","","Hospitals;Indexing;Libraries;Neuroimaging;Random access memory;Testing","biomedical MRI;data compression;information retrieval;medical information systems;neurophysiology","file access;index size;index structure;large compressed NIfTI neuroimaging data format;original compressed data;random seek simulations;speed-up","","","","17","","","25-29 Aug. 2015","","IEEE","IEEE Conference Publications"
"Domain specific audio indexing using linguistic information","L. Pandey; K. Nathwani; S. Kaur; I. Husain; R. Pathak; G. Singh; S. Tiwari; R. M. Hegde","Dept. of Electrical Engineering, Indian Institute of Technology, Kanpur, India","2014 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)","20151026","2014","","","000364","000369","In this paper a novel methodology for indexing domain specific audio archives using linguistic information present in the speech signal is discussed. The audio indexing system is phone based and can work under limited training data conditions. A training data set that captures the linguistic information within Hindi language at the syllable level is first developed. A reduced phone set is then derived from the super syllabic set of the Hindi language. The system is then bootstrapped at the phone level with domain specific data. The audio indexing itself is then performed using a novel sliding phone protocol technique. The performance of such a audio indexing system is then evaluated for Indian parliament speech and read news. The proposed bootstrapping method with sliding phone search provides reasonable improvements in phone recognition accuracy and in terms of search retrieval efficiency when compared to conventional methods.","2162-7843;21627843","CD-ROM:978-1-4799-1811-9; Electronic:978-1-4799-1812-6; POD:978-1-4799-1813-3","10.1109/ISSPIT.2014.7300616","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300616","","Accuracy;Indexing;Pragmatics;Protocols;Speech;Speech recognition","audio signal processing;computer bootstrapping;data analysis;indexing;information retrieval;protocols;speech processing","Hindi language;Indian parliament speech;bootstrapping method;domain specific audio indexing;linguistic information;phone level;phone recognition accuracy;read news;reduced phone set;search retrieval efficiency;sliding phone protocol technique;sliding phone search;speech signal;syllable level;training data set","","","","20","","","15-17 Dec. 2014","","IEEE","IEEE Conference Publications"
"An automatic eHealth platform for cardiovascular and cerebrovascular disease detection","X. Mou; X. Wang; Z. Wu; X. Wang; M. Zhou","College of Information Science and Technology, Beijing Normal University, Beijing, Beijing, P.R.C","2015 International Symposium on Bioelectronics and Bioinformatics (ISBB)","20151203","2015","","","63","66","Cardiovascular and cerebrovascular disease has become the number-one killer to human life in the world We study and design an aided diagnosis eHealth platform for cerebrovascular and cerebrovascular disease detection with 3-tier architecture. The architecture has three layers: User Interface Layer, Business Logic Layer and Data Access Layer. We employ several key technologies for the platform We use a novel statistical cerebrovascular segmentation algorithm with particle swarm optimization to segment the cerebral vascular. We apply a multiscale enhancement and dynamic balloon tracking (MSCAR-DBT) method to segment the heart vascular. We propose Ball B-Spline curve to reconstruct the blood vessels. We use ray-casting volume rendering with compute unified device architecture (CUDA). Experiments on 108 patients' computed tomography data or magnetic resonance imaging data stored in the system verify the feasibility and validity of each model we propose. We also test the platform on several hospitals in Beijing and receive a positive feedback from doctors.","","Electronic:978-1-4673-6609-0; POD:978-1-4673-6610-6; USB:978-1-4673-6608-3","10.1109/ISBB.2015.7344924","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344924","Ball B-Spline curve;Cardiovascular;Cerebrovascular;EHealth platform;Statistical vascular segmentation;Volume rendering","Diseases;Image segmentation;Medical diagnostic imaging;Rendering (computer graphics);Splines (mathematics);Three-dimensional displays","biomedical MRI;blood vessels;brain;cardiovascular system;computerised tomography;diseases;image enhancement;image reconstruction;image segmentation;information retrieval;medical image processing;neurophysiology;parallel architectures;particle swarm optimisation;user interfaces","3-tier architecture;Ball B-Spline curve;CUDA;aided diagnosis eHealth platform;automatic eHealth platform;blood vessel reconstruction;business logic layer;cardiovascular disease detection;cerebral vascular segment;cerebrovascular disease detection;compute unified device architecture;data access layer;dynamic balloon tracking MSCAR-DBT method;heart vascular;magnetic resonance imaging data storage;multiscale enhancement;particle swarm optimization;patient computed tomography data;ray-casting volume rendering;statistical cerebrovascular segmentation algorithm;user interface layer","","","","13","","","14-17 Oct. 2015","","IEEE","IEEE Conference Publications"
"Bridging the gap for retrieving DBpedia data","A. S. Ismail; H. Al-Feel; H. M. O. Mokhtar","Information Systems Department Faculty of Computers and Information, Fayoum University Fayoum, Egypt","2015 Forth International Conference on e-Technologies and Networks for Development (ICeND)","20151123","2015","","","1","5","DBpedia is nowadays considered one of the main projects in the World Wide Web that extracts and enriches Wikipedia data in a structured form. Also, it is considered the central hub for the Linked Open Data. Querying DBpedia using big data approaches such as Hive-QL is regarded as one of the new techniques to solve the shortcomings of SPARQL; the main query language of DBpedia and the Semantic Web. Nevertheless, despite the speed of Hive-QL compared to SPARQL, it has a stability problem. Our paper presents a new architecture and implementation for querying DBpedia using Shark query language in addition to Hive-QL. As a result of this work, An obvious decrease in execution time, as well as, an increase in the degree of stability have been attained.","","CD-ROM:978-1-4799-8450-3; Electronic:978-1-4799-8451-0; POD:978-1-4799-8452-7","10.1109/ICeND.2015.7328537","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328537","Big Data;DBpedia;Hadoop;Hive;MapReduce;Semantic Web;Shark;Spark","Big data;Data mining;Database languages;Encyclopedias;Semantic Web;Sparks","Big Data;Web sites;database management systems;information retrieval;query languages;semantic Web","Big Data approach;DBpedia data retrieval;DBpedia querying;Hive-QL;Linked Open Data;SPARQL;Shark query language;Wikipedia data;World Wide Web;central hub;semantic Web","","","","33","","","21-23 Sept. 2015","","IEEE","IEEE Conference Publications"
"Exploiting Pipelined Encoding Process to Boost Erasure-Coded Data Archival","J. Huang; Y. Wang; X. Qin; X. Liang; S. Yin; C. Xie","Wuhan National Lab. for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Parallel and Distributed Systems","20151007","2015","26","11","2984","2996","This paper addresses an issue of erasure-coded data archival, where (k + r; k) erasure codes are employed to archive rarely accessed replicas. The traditional synchronous encodingprocess neither leverages the existence of replicas, nor handles encoding operations in a decentralized manner. To overcome these drawbacks, we exploit pipelined encoding processes to boost the data archival performance on storage clusters. First, we propose two data layouts called [D + P]<sub>cd</sub> and [3X]<sub>cd</sub> by applying a chained-declustering mechanism to both Mirrored RAID-5 and triplication redundancy groups. Second, in light of the [D + P]<sub>cd</sub> and [3X]<sub>cd</sub> layouts, we design two archiving schemes named DP and 3X, which exhibit the following three salient features: (i) exploiting data locality-two or three local blocks are read by each involved node for encoding; (ii) decentralized computation load-encoding operations are distributed among k nodes; and (iii) parallel archival processing-two or three encoding pipelines are simultaneously deployed to generate parity blocks. We implement both the DPand 3X schemes and three existing solutions (i.e., SynE, DE, and RapidRAID) in a real-world storage cluster. Experimental results show that our archival schemes outperform the other three solutions in terms of archiving time by a factor of at least 3.41 in a nine-node storage cluster. The experiments strongly indicate that the performance bottleneck of SynE lies in its block-receiving stage; it is disk I/O rather than network traffic that dominates archiving time for both the DE and RapidRAID schemes.","1045-9219;10459219","","10.1109/TPDS.2014.2366113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942231","Erasure-coded storage cluster;data archival;pipelined encoding;power efficiency","Bandwidth;Distributed databases;Educational institutions;Encoding;Layout;Pipelines;Redundancy","codes;information retrieval systems;pattern clustering;pipeline processing;storage management","3X archiving scheme;DP archiving scheme;Mirrored RAID-5;SynE;boost erasure-coded data archival;chained-declustering mechanism;data layouts;data locality;decentralized computation load-encoding operations;disk I/O;parallel archival processing;pipelined encoding process;storage clusters;synchronous encoding process;triplication redundancy groups","","1","","42","","20141031","Nov. 1 2015","","IEEE","IEEE Journals & Magazines"
"Concept Detection in Multimedia Web Resources About Home Made Explosives","G. Kalpakis; T. Tsikrika; F. Markatopoulou; N. Pittaras; S. Vrochidis; V. Mezaris; I. Patras; I. Kompatsiaris","Inf. Technol. Inst., CERTH, Thessaloniki, Greece","2015 10th International Conference on Availability, Reliability and Security","20151019","2015","","","632","641","This work investigates the effectiveness of a state-of-the-art concept detection framework for the automatic classification of multimedia content, namely images and videos, embedded in publicly available Web resources containing recipes for the synthesis of Home Made Explosives (HMEs), to a set of predefined semantic concepts relevant to the HME domain. The concept detection framework employs advanced methods for video (shot) segmentation, visual feature extraction (using SIFT, SURF, and their variations), and classification based on machine learning techniques (logistic regression). The evaluation experiments are performed using an annotated collection of multimedia HME content discovered on the Web, and a set of concepts, which emerged both from an empirical study, and were also provided by domain experts and interested stakeholders, including Law Enforcement Agencies personnel. The experiments demonstrate the satisfactory performance of our framework, which in turn indicates the significant potential of the adopted approaches on the HME domain.","","Electronic:978-1-4673-6590-1; POD:978-1-4673-6591-8","10.1109/ARES.2015.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299974","concept detection;concept-based multimedia retrieval;home made explosives;visual feature extraction","Feature extraction;Multimedia communication;Semantics;Streaming media;Videos;Visualization","Internet;content management;explosives;feature extraction;image classification;image segmentation;information resources;information retrieval;law;learning (artificial intelligence);multimedia computing;national security;regression analysis;transforms;video signal processing","HMEs;Law Enforcement Agencies personnel;SIFT;SURF;automatic multimedia content classification;concept detection;home made explosives;images;logistic regression;machine learning technique;multimedia HME content discovery;multimedia Web resources;publicly available Web resources;semantic concept;shot segmentation;video segmentation;visual feature extraction","","2","","41","","","24-27 Aug. 2015","","IEEE","IEEE Conference Publications"
"A Hybrid Model for Experts Finding in Community Question Answering","H. Li; S. Jin; S. LI","Sch. of Comput., Nat. Univ. of Defense Technol., Changsha, China","2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","20151029","2015","","","176","185","As a means to share knowledge, the community question answering (CQA) service provides users a chance to obtain or provide help by raising or answering questions. After a question is posted, the system must find an appropriate individual to answer this question. Several approaches have recently been proposed to find experts in CQA. In this paper, a new method to find experts in CQA is proposed by considering user post contents, answer votes, ratio of best answers, and user relation. The votes are used in post relation analysis to calculate user authority. The user's knowledge score can be calculated through topic analysis. Considering that a question usually includes many trivial words, an accurate distribution is nearly impossible to obtain with LDA. To solve this problem, vocabulary is extended by including the link information shown in a question, the top 10 relevant words from Wikipedia are provided for each tag. Tag-LDA models the user topic distribution and predicts the topic distribution of new questions. An experiment is conducted on Stack Overflow dataset, which is the world's largest computer programming CQA site. Experimental results showed approximately 2.97% to 7.79% performance improvement in nDCG@N metrics.","","Electronic:978-1-4673-9200-6; POD:978-1-4673-9201-3","10.1109/CyberC.2015.87","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307808","Community question answering;Expert finding;Link analysis;Tag semantic enrichment","Computers;Encyclopedias;Internet;Java;Knowledge discovery;Semantics;Web pages","question answering (information retrieval);text analysis","CQA service;Stack Overflow dataset;Tag-LDA model;Wikipedia;answer votes;community question answering service;computer programming CQA site;expert finding;knowledge sharing;link information;nDCG@N metrics;post relation analysis;topic analysis;user authority;user knowledge score;user post content;user relation;user topic distribution;vocabulary","","1","","22","","","17-19 Sept. 2015","","IEEE","IEEE Conference Publications"
"Social networks: Analysis for integrated social profiles","R. Cognini; D. Falcioni; A. Polzonetti","University of Camerino, Italy","2015 Internet Technologies and Applications (ITA)","20151105","2015","","","68","72","In recent years, the diffusion of online social networks has been facilitated by the introduction of Web 2.0 which, while enabling users to retrieve information on the web, also contributes to the creation of others. The great success obtained by some of them, has contributed to the dissemination of a large number of “content aggregators”, applications that enable users to see, in real-time, social updates coming from different social networks to which they are subscribed, providing them with the possibility to interact with others as if they were actually connected to the social website. Although there are many applications of this type, none of them can visualize integrated statistics related to a set of social profiles provided by the user. The main aims of this work are the study of the reputation problem within social networks, the definition of analysis metrics and models, and the creation of web applications through which registered users may not only view social updates in real-time, but also daily collected statistical data, related to all their social profiles. In particular, a page has been created containing a general overview related to the user selected, resuming all his/her quantitative information collected, as well as, providing “visibility” and “productivity” indicators, and the possibility of visualizing temporal diagrams about his/her social activity carried out on different social networks. This paper begins by providing a background of online social networks. It then goes on to outline the main contribution provided through the realization of the project, describing how the analysis of the data provided by the different social network APIs has been conduced, presenting also the key features of the web application created. Finally, it provides some conclusions about the project, evaluating the results obtained and suggesting some interesting developments that could be done in the future.","","Electronic:978-1-4673-9557-1; POD:978-1-4799-8037-6","10.1109/ITechA.2015.7317372","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317372","Reputation;Social Analysis;Social Metrics;Social Networking;Social Visibility","Analytical models;Correlation;Visualization","application program interfaces;data visualisation;information dissemination;information retrieval;social networking (online);statistical analysis","Web 2.0;Web applications;analysis metrics;content aggregators;information retrieval;integrated social profiles analysis;integrated statistics;online social networks;productivity indicators;quantitative information;social Website;social activity;social network API;social updates;temporal diagram visualization;visibility indicators","","","","12","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Person attributes extraction in profiles based on SVM and pattern","Z. Zhu; Y. Sun","School of Information Engineering, Minzu University of China, Beijing, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20151130","2015","","","203","206","This paper is an exploration to find a way to get the person attributes in profiles. Considering those attributes exists in large volume of unstructured data, and it is very difficult to gain in a short time. So, we use a method combing the pattern and SVM to extract the person attributes. Firstly, we collect many raw profiles in websites by our configurable crawler. Secondly, we use statistic methods to do pre-processing works include lexical analysis and name recognition. Thirdly, we build the patterns, which can use in model to extract the person attributes. Also we generalize those patterns to SVM features. Finally, we use SVM assisted with pattern-based method to predict the person attributes. The results prove the method is effective and the data we extracted is useful in building specific-areas' expert database and information retrieval.","2327-0586;23270586","CD-ROM:978-1-4799-8351-3; Electronic:978-1-4799-8353-7; POD:978-1-4799-8354-4","10.1109/ICSESS.2015.7339037","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339037","Expert database;Pattern-based extraction;Person attributes extraction;SVM","Data mining;Databases;Education;Employment;Feature extraction;Kernel;Support vector machines","information retrieval;social networking (online);statistical analysis;support vector machines","SVM based extraction;SVM features;Websites;configurable crawler;information retrieval;lexical analysis;name recognition;pattern based extraction;pattern-based method;person attribute extraction;profiles;raw profiles","","","","13","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"Skyline Based Search Result Diversification Method","Y. Wenke; W. Shanshan","Sci. & Technol. on Inf. Syst. Eng. Lab., Nanjing, China","2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics","20151123","2015","1","","11","15","There are high redundancy and low diversity problems existing in current search engines. To solve this problem, a skyline based search result diversification method was proposed. The search results were sorted in accordance with skyline level, domination degree and coverage. In order to reduce the time cost, a bitmap based skyline level and domination degree computing algorithm was designed. Experimental results show that the proposed method can achieve better performance in terms of search results diversity and skyline level computing time cost.","","Electronic:978-1-4799-8646-0; POD:978-1-4799-8647-7","10.1109/IHMSC.2015.215","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334640","bitmap;coverage;diversification;domination degree;skyline;skyline leve","Bismuth;Distributed databases;Mathematical model;Redundancy;Search engines;Search problems;Sorting","information retrieval;search engines","bitmap based skyline level;domination degree computing algorithm;search engines;skyline based search result diversification method;skyline level computing time cost reduction","","","","15","","","26-27 Aug. 2015","","IEEE","IEEE Conference Publications"
"Aatish - A New Profile-Based Recommendation Services for Mobile Telecom Network Subscribers","M. Saravanan; P. Manoj; G. B. Smitha; V. Lakshmi","Ericsson Res. India Ericsson India Private Ltd., Chennai, India","2015 Second European Network Intelligence Conference","20151109","2015","","","160","164","Current mobile telecom networks generate massive amounts of subscribers' data consisting of observations on duration of call, time of call, type of call, plans subscribed and other details in addition to network data. Due to the ever increasing degree in complexity of understanding behavioural pattern of subscribers in networks, coupled with specific constraints related to subscribers' preferences, the traditional recommendation approaches are not efficient in meeting customer demands. A new recommendation paradigm is required that uses techniques that exploit user-specific information based on time, location, recent interest, etc., rather than common techniques, to prioritize recommendation. Our work presents the design and specification of Aatish, a profile-based recommendation system as a solution to increase the probability of acceptance of recommendations by individuals. In our approach, recommendation services are modelled using a newly proposed policy-based engine that combines information from subscriber's behavioural aspects of Mobile Telecom Networks (MTN) and Social Media (SM) inputs. Services are also associated with requirements, that is, set of propositions that characterize the execution environment of the service. The architecture of Aatish illustrates policy enhancement that takes into account the context as well as content-aware situations and user preferences that can impact the performances and functionalities of efficient recommendation system. Our study starts by extracting user information from CDRs and social media to generate sequences of recommendations. After observing response from users for the recommendations, appropriate actions are selected, re-ranked and recommended so that the engendered recommendations match recent needs of the users. Moreover, subscriber's ranking preferences have been derived based on the behavioural pattern of users in terms of their mobile phone usage (Recency, Frequency, Monetary) for the benefit of impro- ing the loyalty scores.","","Electronic:978-1-4673-7592-4; POD:978-1-4673-7593-1","10.1109/ENIC.2015.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321252","Policy Engine;Profile-based Policy Recommendation;RFM Model;Social Network and Social Media Analysis","Algorithm design and analysis;Engines;Feeds;Media;Mobile communication;Resource description framework;Telecommunications","information retrieval;mobile communication;recommender systems;social networking (online);telecommunication computing","Aatish;content-aware situations;mobile telecom network subscribers;policy-based engine;profile-based recommendation services;social media;user information extraction","","","","17","","","21-22 Sept. 2015","","IEEE","IEEE Conference Publications"
"Event extraction for collective knowledge in multimedia digital EcoSystem","M. A. Abebe; F. Getahun; S. Asres; R. Chbeir","Addis Ababa University, Addis Ababa, Ethiopia","AFRICON 2015","20151119","2015","","","1","5","Emerging technologies like Internet and Web services enable a new form of collaboration. An Internet based collaborative environment allows actors with similar profile or interest to publish and share multimedia content. This results in the availability of massive, distributed, heterogeneous and fast-moving streamed multimedia content. In addition, most of the data in the Web describes events associated to people, activities and locations. An event describing a situation might be initiated by a user, followed by a number of users within a specified time period. Extracting events from user contributed multimedia content in collaborative environments is challenging due to three reasons: 1)the content is heterogeneous in source, size and format; 2)users might use different vocabulary to describe the same event; 3)the collaboration environment contains non-events. In this paper, we have proposed an event extraction approach that will be used to build an event-based collective knowledge management framework that assists the retrieval of multimedia contents from various social media sources. This approach is accompanied by experimental results with future works.","","Electronic:978-1-4799-7498-6; POD:978-1-4799-7499-3","10.1109/AFRCON.2015.7331984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331984","","Collaboration;Ecosystems;Media;Multimedia communication;Noise measurement;Streaming media;Videos","Web services;electronic publishing;groupware;information retrieval;knowledge acquisition;knowledge management;multimedia computing;social networking (online)","Internet based collaborative environment;Web services;event extraction;event-based collective knowledge management;fast-moving streamed multimedia content;multimedia content publishing;multimedia content retrieval;multimedia content sharing;multimedia digital ecosystem;social media sources;vocabulary","","","","25","","","14-17 Sept. 2015","","IEEE","IEEE Conference Publications"
"An indexing method using sensing information on integrated sensor networks","R. Kawasumi; T. Yoshihisa; T. Hara; S. Nishio","Graduated School of Information Science and Technology, Osaka University, Japan","2015 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","20151130","2015","","","252","257","For integrated sensor networks, some methods to faster retrieve the sensor network site that has the queried mobile sensor data have been proposed. In these methods, the systems create the indexes for the site retrieval based on the locations at that sensor data are obtained. The systems can much faster retrieve the queried data by creating the indexes also based on the sensing information such as the times at that sensor data are obtained and the queried ranges. Hence, we propose a spatiotemporal index creation method considering sensing information such as the locations and the time. In our proposed method, an index server manages the range of the locations and the times of the mobile sensor data that each site has. For the management, the index server constructs tree structured indexes. Our evaluation show that our proposed method can reduce the retrieval time compared with conventional indexing methods.","","Electronic:978-1-4673-7788-1; POD:978-1-4673-7789-8; USB:978-1-4673-7787-4","10.1109/PACRIM.2015.7334843","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334843","Index;Integrated sensor networks;P2P;Spatiotemporal data","Indexing;Mobile communication;Mobile computing;Sensors;Servers;Spatiotemporal phenomena","indexing;information retrieval;mobile computing;peer-to-peer computing;wireless sensor networks","P2P;indexing method;integrated sensor networks;mobile sensor data;sensing information;site retrieval;spatiotemporal index creation method","","","","9","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Randomly encrypted key generation algorithm against side channel attack in cloud computing","M. B. Rashid; N. Islam; A. A. M. Sabuj; S. Waheed; M. B. A. Miah","Department of Information and Communication Technology, Mawlana Bhashani Science and Technology University, Santosh-1902, Tangail, Bangladesh","2015 International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)","20151029","2015","","","1","5","Cloud computing offers both services that provide resources over the Internet and economic benefits for using these resources. Economic benefit plays a vital role both for users and providers of cloud. The users pay as they use on the other hand resources of providers never remain idle. The cloud users hive away their valuable data in the cloud and perform their activity with the data. The security of these valuable data should be ensured. The side channel attack is one of the common attacks in cloud computing. Attackers use a malicious virtual machine to retrieve data from the cloud. In this paper, implements a randomly encrypted key generation algorithm against side channel attack in cloud computing. As anyone can use the cloud, it makes easy for the attackers to attack the desired users data. Also, a process has been developed that generates random keys to encrypt data of the cloud users at the time of storage and transmission. The attackers may find the encrypted data when they grab the users data that may no further value for them.","","Electronic:978-1-4673-6676-2; POD:978-1-4673-6677-9","10.1109/ICEEICT.2015.7307475","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307475","Cloud Computing;Random Key;Side Channel Attack;Virtual Machine","Encryption;IP networks;Linux;Monitoring;Reliability;Software","cloud computing;cryptography;information retrieval;operating systems (computers);virtual machines","Internet;cloud computing;economic benefits;generation algorithm;malicious virtual machine;randomly encrypted key generation algorithm;retrieve data;side channel attack","","","","17","","","21-23 May 2015","","IEEE","IEEE Conference Publications"
"Information extraction of the history evolution based on hybrid convolution tree kernel","C. Tian; M. Lin; Siriguleng","Coll. of Comput. & Inf. Eng., Inner Mongolia Normal Univ., Hohhot, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","20151130","2015","","","878","881","A hybrid convolution tree kernel is applied to extract of the history evolution information. The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicate-argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments. The Predicate-Arguments Feature (PAF) kernel was extracted and decomposed into Constituent Structure kernel and Path kernel. The linear combination of Constituent Structure kernel and Path kernel improve the accuracy and efficiency in this task. The experimental results show this method performs better.","2327-0586;23270586","CD-ROM:978-1-4799-8351-3; Electronic:978-1-4799-8353-7; POD:978-1-4799-8354-4","10.1109/ICSESS.2015.7339194","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339194","Convolution Tree Kernel;History Evolution;Information Extraction","Convolution;Data mining;Feature extraction;History;IP networks;Kernel;Syntactics","convolution;history;information retrieval;trees (mathematics)","PAF kernel;constituent structure kernel;history evolution information extraction;hybrid convolution tree kernel;linear combination;path kernel;predicate-argument feature kernel;predicate-argument link features;syntactic structure features","","","","16","","","23-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"VisKE: Visual knowledge extraction and question answering by visual verification of relation phrases","F. Sadeghi; S. K. Divvala; A. Farhadi","University of Washington, Seattle, United States","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","20151015","2015","","","1456","1464","How can we know whether a statement about our world is valid. For example, given a relationship between a pair of entities e.g., `eat(horse, hay)', how can we know whether this relationship is true or false in general. Gathering such knowledge about entities and their relationships is one of the fundamental challenges in knowledge extraction. Most previous works on knowledge extraction have focused purely on text-driven reasoning for verifying relation phrases. In this work, we introduce the problem of visual verification of relation phrases and developed a Visual Knowledge Extraction system called VisKE. Given a verb-based relation phrase between common nouns, our approach assess its validity by jointly analyzing over text and images and reasoning about the spatial consistency of the relative configurations of the entities and the relation involved. Our approach involves no explicit human supervision thereby enabling large-scale analysis. Using our approach, we have already verified over 12000 relation phrases. Our approach has been used to not only enrich existing textual knowledge bases by improving their recall, but also augment open-domain question-answer reasoning.","1063-6919;10636919","Electronic:978-1-4673-6964-0; POD:978-1-4673-6965-7; USB:978-1-4673-6963-3","10.1109/CVPR.2015.7298752","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298752","","Cognition;Computational modeling;Data mining;Detectors;Feature extraction;Knowledge based systems;Visualization","inference mechanisms;knowledge acquisition;natural language processing;question answering (information retrieval)","VisKE;large-scale analysis;open-domain question-answer reasoning;question answering;spatial consistency;text-driven reasoning;verb-based relation phrase;visual knowledge extraction;visual relation phrase verification","","9","","41","","","7-12 June 2015","","IEEE","IEEE Conference Publications"
"Classification of research efforts in dynamic/big data analytics","L. Turiy","Palmer School of Library and Information Science Long Island University Brookville, NY, USA","2015 12th International Conference & Expo on Emerging Technologies for a Smarter World (CEWIT)","20151203","2015","","","1","6","The recent explosion in Dynamic (a.k.a., ""Big"") Data Analytics<sup>1</sup> research provides a massive amount of software capabilities, published papers, and conference proceedings that make it difficult to sift through and inter-relate it all. This paper proposes a trial classification scheme with several orthogonal dimensions of classification. These dimensions include stages of application, challenges, solution origins, specialization of technologies, purpose, ownership (business type), data processing (batch vs. streaming), and data types applied to (structured, semi-structured and unstructured). The full list of determined categories in each dimension is presented. The classification scheme is intentionally made to be not too complex, to help anyone entering the expanding world of Big Data Analytics, by helping them gain a better understanding of the applicability of various tools and capabilities that are available, and how they contrast and synergize amongst each another. Additionally, this work can help with creation of educational materials, demarcation of the domain, and encourage full research coverage in big data analytics, as well as enable discovery and articulation of common principles and solutions. The research topics used in testing this classification scheme are retrieved from the top 20 most relevant papers of Scopus online database, which is aiming to be the largest repository of the peer-reviewed literature, as well as by reviewing examples of similar past classification attempts.","","Electronic:978-1-4673-7865-9; POD:978-1-4673-7866-6","10.1109/CEWIT.2015.7338171","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338171","Big Data;Big Data Analytics;Scopus;bibliometric analysis;classification","Bibliometrics;Big data;Business;Data analysis;Data visualization;Databases;Software","Big Data;data analysis;information retrieval;pattern classification;software tools","Big Data analytics;software capability;software tool;topic retrieval;trial classification scheme","","","","15","","","19-20 Oct. 2015","","IEEE","IEEE Conference Publications"
"A Case for Rigorous Workload Classification","A. Wildani; I. F. Adams","","2015 IEEE 23rd International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems","20151119","2015","","","146","149","Traditional workload labels such as ""archival"" and ""HPC"" are poorly understood and inconsistently applied. As usage of systems has evolved, the language to describe this usage has stagnated. To better understand how workload type translates into system design requirements, we use a combination of longitudinal analysis and statistical feature extraction to categorize workload traces and study how the properties of classical workload types, such as the ""write-once, read-maybe"" assumption for archives, have evolved over time. Once this step is complete, we intend to move to active classification of workloads to replace these broad, poorly specified categories with quantitative metrics that can be used to improve metrics such as power, availability, and performance by mathematically relating storage algorithms with workload properties.","1526-7539;15267539","Electronic:978-1-4673-7720-1; POD:978-1-4673-7721-8","10.1109/MASCOTS.2015.32","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7330184","data mining;storage;workload analysis","Adaptation models;Analytical models;Computational modeling;Feature extraction;Labeling;Measurement;Taxonomy","information retrieval systems;statistical analysis;storage management","HPC;archival;longitudinal analysis;rigorous workload classification;statistical feature extraction","","","","20","","","5-7 Oct. 2015","","IEEE","IEEE Conference Publications"
"Raga identification of carnatic music using iterative clustering approach","H. Daniel; A. Revathi","Communication Systems, Saranathan College of Engineering, Tiruchirapalli, Tamilnadu, India","2015 International Conference on Computing and Communications Technologies (ICCCT)","20151008","2015","","","19","24","This paper proposes a method to identify the arohana-avarohana of carnatic raga. Carnatic raga is broadly classified as melakarta (parent) and janya (child) raga. Arohana-avarohana of 10 different ragas is collected from 16 different singers. 16 audio data are collected for each raga. 11 among the 16 are used in the training phase and the remaining 5 are used for testing. The acoustic feature, MFCC which is increasingly used in music information retrieval, is used as the feature. Clustering model with 256 clusters is developed for all the 10 different ragas. To perform testing, the 5 test data of each raga are concatenated. Then, the sequence of training vectors (feature vectors) are divided into segments of L=100 feature vectors with 90 vectors overlapping between them. The minimum distance is calculated between each test vector and centroid of clusters. Average of the minimum distances for each segment is found out. The segment belongs to the model which has minimum of averages. To improve to accuracy, each of the ten ragas is sub classified as either parent or child group and testing is done under each group.","","Electronic:978-1-4799-7623-2; POD:978-1-4799-7624-9","10.1109/ICCCT2.2015.7292713","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7292713","Arohana-Avarohana;Clustering methods;Mel Frequency Cepstral Coefficients (MFCC);Vector quantization (VQ)","Accuracy;Computational modeling;Data mining;Feature extraction;Mel frequency cepstral coefficient;Testing;Training","acoustic signal processing;information retrieval;iterative methods;music;pattern clustering","MFCC;acoustic feature;arohana-avarohana identification;carnatic music;carnatic raga;clustering model;iterative clustering approach;janya raga;melakarta raga;music information retrieval;raga identification","","","","13","","","26-27 Feb. 2015","","IEEE","IEEE Conference Publications"
"An Efficient Data Selection Policy for Search Engine Cache Management","X. Dong; R. Li; H. He; X. Gu; M. Sarem; M. Qiu; K. Li","Sch. of Comput. Sci. & Technol., Huazhong Univ. of Sci. & Technol., Wuhan, China","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","20151130","2015","","","122","127","Caching is an effective optimization in search engine. The data selection policy plays a key role in caching, which places the data to be cached in memory. However, the current data selection policies are not suitable to the hybrid storage architecture with solid state disks (SSDs), which have gradually replaced hard disk drives (HDDs) in search engines. In this paper, we present an Efficient Data Selection policy (EDS) for search engine cache management, which views cache media as a knapsack, and views results and posting lists as items. The best benefit can be computed by greedy algorithms. In order to verify the effectiveness, we carry out a series of experiments to study essential factors of data selection in different architectures, including HDD, SSD, and SSD-based hybrid storage architecture, which uses SSD as a secondary cache for memory. The experimental results demonstrate that the proposed policy improves the hit ratio by 20.04% and the retrieval performance on HDD, SSD, and hybrid architecture by 31.98%, 28.72% and 23.24%, respectively.","","Electronic:978-1-4799-8937-9; POD:978-1-4799-8938-6","10.1109/HPCC-CSS-ICESS.2015.216","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336153","cache management;data selection;hybrid storage architecture;search engine;solid state disk","Cache storage;Computer science;Electronic mail;Memory management;Search engines;Solids","cache storage;greedy algorithms;information retrieval;search engines","EDS;HDD;SSD-based hybrid storage architecture;cache media;efficient data selection policy;greedy algorithms;hard disk drives;hit ratio improvement;hybrid architecture improvement;knapsack problem;retrieval performance improvement;search engine cache management;secondary cache;solid state disks","","","","15","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"SW-POR: A Novel POR Scheme Using Slepian-Wolf Coding for Cloud Storage","T. P. Thao; L. C. Kho; A. O. Lim","Japan Adv. Inst. of Sci. & Technol., Nomi, Japan","2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops","20151026","2014","","","464","472","Cloud computing is a service by which the clients can outsource their data to the cloud storage server to deal with the local storage limitation. However, the cloud storage providers are untrustworthy, which can lead to several security challenges, such as data availability, data integrity, and data confidentiality. To mitigate the issues of data availability and data integrity, a novel Slepian-Wolf-based Proof of Retrievability (SW-POR) scheme is proposed to enable the client to check whether the distributed data stored in the cloud servers is intact or not. The proposed SW-POR scheme not only can obtain an optimal coded block size, but also it can provide the exact-repair feature and low complexity. In this paper, the security analysis and efficiency analysis are provided. Simulation results show that the SW-POR scheme can accomplish a significant improvement in computation time.","","CD-ROM:978-1-4799-7645-4; Electronic:978-1-4799-7646-1; POD:978-1-4799-7647-8","10.1109/UIC-ATC-ScalCom.2014.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306991","Cloud Storage;Network Coding;Proof of Retrievability;Slepian Wolf Coding","Cloud computing;Conferences;Encoding;Indexes;Maintenance engineering;Network coding;Servers","cloud computing;data integrity;information retrieval;security of data;storage management","SW-POR scheme;Slepian-Wolf coding;Slepian-Wolf-based proof of retrievability scheme;cloud storage;data availability;data confidentiality;data integrity;security challenges","","","","24","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Machine Learning for Imbalanced Datasets of Recognizing Inference in Text with Linguistic Phenomena","M. Y. Day; C. C. Tsai","Dept. of Inf. Manage., Tamkang Univ., Taipei, Taiwan","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","562","568","Recognizing inference in text (RITE) plays an important role in the answer validation modules for a Question Answering (QA) system. The problem of class imbalance has received increased attention in the machine learning community. In recent years, several attempts have been made on the linguistic phenomena analysis, however, little is known about the effects of imbalanced datasets with linguistic phenomenon in recognizing inference in text. The objective of this paper is to provide an empirical study on learning imbalanced datasets of recognizing inference in text with linguistic phenomena for a better understanding of the effects of imbalanced datasets with linguistic phenomenon in recognizing inference in text. In this paper, we proposed an analysis of imbalanced datasets of recognizing inference in text with linguistic phenomena using NTCIR 11 RITE-VAL gold standard dataset and development dataset. The experimental results suggest that the distribution of imbalanced datasets of recognizing inference in text with linguistic phenomenon could be dramatically varied on the performance of a machine learning classifier.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.99","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301027","Imbalanced Datasets;Linguistic Phenomena;Machine Learning;Recognizing Inference in Text;Textual Entailment","Accuracy;Gold;Pragmatics;Semantics;Standards;Text recognition;Yttrium","inference mechanisms;learning (artificial intelligence);linguistics;pattern classification;question answering (information retrieval);text analysis","QA system;RITE;imbalanced dataset;linguistic phenomena analysis;machine learning classifier;question answering system;recognizing inference in text","","","","29","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine","M. Puppala; T. He; S. Chen; R. Ogunti; X. Yu; F. Li; R. Jackson; S. T. C. Wong","Houston Methodist Hospital Research Institute","IEEE Transactions on Biomedical Engineering","20151119","2015","62","12","2776","2786","Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.","0018-9294;00189294","","10.1109/TBME.2015.2450181","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654","Clinical Data Warehouse;Clinical data warehouse;Cohort Identification;Natural Language Processing;Outcomes Research;Readmission Risk;Smartphone Health App;cohort identification;natural language processing (NLP);outcomes research;readmission risk;smartphone health app","Data mining;Data warehouses;Databases;Hospitals;Informatics;Natural language processing","data analysis;data integration;data mining;data warehouses;decision support systems;electronic health records;health care;information retrieval","HMH;Houston Methodist Hospital system;METEOR EDW;SIA layers;clinical decision support systems;clinical informatics area;clinical research training;cohort identification;cost containment;data access;data analysis;data integration;data mining;electronic health records;enterprise analytics platform;enterprise data warehouse;enterprise health informatics environment;enterprise level;evidence-based medicine;health analytics;healthcare industry;healthcare institutions;healthcare market;hypothesis testing;informatics applications;integrated clinical informatics environment;integrated delivery networks;methodist environment for translational enhancement and outcomes research;preliminary evaluation;risk prediction;software intelligence and analytics layer;usability analysis","0","6","","27","","20150626","Dec. 2015","","IEEE","IEEE Journals & Magazines"
"A conceptual design of a web information extraction and data analysis learning framework","C. H. Tseng; Y. H. Chen; Y. R. Jiang","Department of Information, Management, Nanhua University, Chiayi County, R.O.C.","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","20151015","2015","","","124","127","The Web is flooded with data. However, there is a huge gap between data and information. Collecting, normalization, and analyzation are required steps to transform data into information. However, HTML is document-centric rather than data-centric. Extracting large amounts of data from the Web is a time consuming and tedious task, but information technologies can only provide little help, especially when users lack of domain knowledge. In this research, the conceptual design of a Web information extraction and data analysis framework is proposed. The framework helps data analysts go through the required steps. Furthermore, our design is suitable for inexperienced beginners in data analyzation field since some assistant modules have been considered.","","CD-ROM:978-1-4673-8268-7; Electronic:978-1-4673-8270-0; POD:978-1-4673-8271-7","10.1109/UMEDIA.2015.7297441","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7297441","Data Analyzation;Data Mining;Information Extraction;Learning","Data analysis;Data mining;Grammar;Software agents;Web pages","Internet;data analysis;hypermedia markup languages;information retrieval;learning (artificial intelligence)","HTML;Web data analysis learning framework;Web information extraction","","","","12","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"RSAE: Ranked Keyword Search over Asymmetric Encrypted Cloud Data","C. Guo; Q. Song; R. Zhuang; B. Feng","Software Sch., Dalian Univ. of Technol., Dalian, China","2015 IEEE Fifth International Conference on Big Data and Cloud Computing","20151029","2015","","","82","86","Cloud computing becomes more and more popular and is applied into many practical applications because of much cheaper and more powerful features. In cloud system, users can outsource local data to the cloud servers to lighten their local storage and computing resource loads, which products a new industry method of use-on-demand, pay-on-use. However outsourcing data to cloud servers makes sensitive information centralized into the server, which brings great challenge to protecting sensitive information privacy. For privacy preserving, the user encrypts sensitive data before outsourcing. Traditional searchable encryption methods make it possible for users to securely conduct keyword search over encrypted data and finally retrieve the most relevant Top-N files among the whole data. In this paper, we systematically propose a scheme to solve the problem of how to securely and efficiently retrieve the Top-N files by keyword-based searching over encrypted data by asymmetric encrypted.","","CD-ROM:978-1-4673-7182-7; Electronic:978-1-4673-7183-4; POD:978-1-4673-7184-1","10.1109/BDCloud.2015.11","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7310720","Top-N files retrieval;asymmetric encryption;cloud data;identity based encryption;keyword search;searchable encryption","Cascading style sheets;Cloud computing;Encryption;Keyword search;Servers","cloud computing;cryptography;data privacy;information retrieval","RSAE;Top-N file retrieval;asymmetric encrypted cloud data;cloud computing;data outsourcing;pay-on-use method;ranked keyword search;searchable encryption methods;sensitive information privacy;use-on-demand method","","1","","24","","","26-28 Aug. 2015","","IEEE","IEEE Conference Publications"
"Enhancing property and type detection for a QA system over linked data","B. Nicula; S. Ruseti; T. Rebedea","Computer Science Department, University Politehnica of Bucharest Bucharest, Romania","2015 14th RoEduNet International Conference - Networking in Education and Research (RoEduNet NER)","20151029","2015","","","167","172","In this paper we discuss the detection of concepts for question answering systems using Linked Data. The emphasis will be put on the detection of types and properties. We will present different methods that are currently being used for these tasks and the improvements proposed to improve the performance of QA systems. Both modules have been trained by analyzing a large subset of Wikipedia pages which have a correspondent in the DBpedia [1] knowledge base. The paper also presents the preliminary test results for both modules which show that the proposed methods have a good performance and should be integrated in the next version of our QA system over Linked Data.","2068-1038;20681038","Electronic:978-1-4673-8180-2; POD:978-1-4673-8181-9","10.1109/RoEduNet.2015.7311988","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311988","information extraction;linked data;natural language processing;property detection;question answering","Decision support systems;Quality of service","information retrieval systems;question answering (information retrieval)","DBpedia knowledge base;Linked Data;QA system;Wikipedia page;question answering system","","","","12","","","24-26 Sept. 2015","","IEEE","IEEE Conference Publications"
"QUINTA: A question tagging assistant to improve the answering ratio in electronic forums","F. Charte; A. J. Rivera; M. J. del Jesus; F. Herrera","Dept. Artificial Intelligence and Computer Science, University of Granada, Spain","IEEE EUROCON 2015 - International Conference on Computer as a Tool (EUROCON)","20151102","2015","","","1","6","The Web is broadly used nowadays to obtain information about almost any topic, from scientific procedures to cooking recipes. Electronic forums are very popular, with thousands of questions asked and answered every day. Correctly tagging the questions posted by users usually produces quicker and better answers by other users and experts. In this paper a prototype of a system aimed to assist the users while tagging their questions is proposed. To accomplish this task, firstly the text of each post is processed to produce a multilabel dataset, then a lazy nearest neighbor multilabel classification algorithm is used to predict the tags on new posts. The obtained results are promising, opening the door to the developing of a full automated system for this task.","","Electronic:978-1-4799-8569-2; POD:978-1-4799-8570-8","10.1109/EUROCON.2015.7313677","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313677","","Computer science;Electronic mail;Measurement;Pipelines;Prediction algorithms;Tagging;Text mining","Web sites;data mining;pattern classification;question answering (information retrieval);text analysis","QUINTA;answering ratio improvement;electronic forums;lazy nearest neighbor multilabel classification algorithm;multilabel dataset;post-text processing;question tagging assistant;tag prediction","","2","","32","","","8-11 Sept. 2015","","IEEE","IEEE Conference Publications"
"Measuring the Ranking Capability of SWA System","O. Shurrab; I. Awan","","2015 3rd International Conference on Future Internet of Things and Cloud","20151026","2015","","","165","172","The analysts need timely and accurate information to conduct proactive action over complex situations. Typically, there are thousands of reported activities in real time operation, although, to direct the analysts attentions to the most important one, researchers have designed multiple levels of situational awareness (SWA). Each process lends itself to ranking the most important activities into a predetermined order. According to our best knowledge, less attention has been given to the performance evaluation with regards to the prioritisation stage. Specifically, the performance metric, ""The Activity of Interest Scores"" has not considered corner cases of different situational assessments needs and configurations. Originally, it had not been designed for measuring the capability of the SWA system. In this paper, we have proposed a new performance metric, as well as a guidance case study for measuring the ranking capability of SWA systems. Our initial result shows that, The Ranking Capability Score has provided an appropriate scoring scheme for different ranking capabilities of SWA systems.","","Electronic:978-1-4673-8103-1; POD:978-1-4673-8104-8","10.1109/FiCloud.2015.98","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300814","","Atmospheric measurements;Computer science;Cyberspace;Particle measurements;Real-time systems;Risk management","information retrieval","SWA system;performance evaluation;performance metric;ranking capability score;situational awareness","","","","23","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Cooperative Web Caching Using Dynamic Interest-Tagged Filtered Bloom Filters","H. Alexander; I. Khalil; C. Cameron; Z. Tari; A. Zomaya","","IEEE Transactions on Parallel and Distributed Systems","20151007","2015","26","11","2956","2969","Although cooperative Web caching has been widely researched, comparatively little has been done to reduce inter-proxy network overhead whilst allowing for a high percentage of requested documents to be retrieved from the cache. Alleviating these issues can substantially reduce Web traffic, increase scalability and enhance a user's browsing experience. This paper introduces a novel cache sharing system employing data structures called Dynamic Interest-Tagged Filtered Bloom Filters (DITFBFs). DITFBFs are capable of representing the cache content of a proxy in a compact form, which is then shared with other proxies in the cooperative Web caching system. What distinguishes the proposed system from others is that DITFBFs only represent the portion of a proxy's cache content that will be of interest to another proxy. This then results in a reduction of inter-proxy overhead. Experimental simulations indicate that, when compared with existing protocols, the proposed system is capable of multiple improvements. Namely, lowering the number of remote cache search messages by at least 60 percent, decreasing user-perceived latency by at least 65 percent and appreciably reducing the overall inter-proxy network overhead. The proposed system accomplishes this whilst maintaining a cache hit ratio as high as the other protocols.","1045-9219;10459219","","10.1109/TPDS.2014.2363458","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926845","Bloom Filters;Cache Performance;Cooperative Caching;Peer-to-Peer;Summary Cache;Web Caching;Web caching;bloom filters;cache performance;cooperative caching;peer-to-peer;summary cache","Arrays;Educational institutions;Information filters;Protocols;Scalability;Servers","Internet;cache storage;data structures;groupware;information retrieval;online front-ends;user interfaces","DITFBF;Web traffic;cache sharing system;cooperative Web caching;data structures;document retrieval;dynamic interest-tagged filtered Bloom filters;inter-proxy network;user browsing experience","","1","","31","","20141016","Nov. 1 2015","","IEEE","IEEE Journals & Magazines"
"Wearable vision for retrieving architectural details in augmented tourist experiences","S. Alletto; D. Abati; G. Serra; R. Cucchiara","Universit&#x00E0; degli Studi di Modena e Reggio Emilia, Via Vignolese 905, 41125 Modena - Italy","2015 7th International Conference on Intelligent Technologies for Interactive Entertainment (INTETAIN)","20151112","2015","","","134","139","The interest in cultural cities is in constant growth, and so is the demand for new multimedia tools and applications that enrich their fruition. In this paper we propose an egocentric vision system to enhance tourists' cultural heritage experience. Exploiting a wearable board and a glass-mounted camera, the visitor can retrieve architectural details of the historical building he is observing and receive related multimedia contents. To obtain an effective retrieval procedure we propose a visual descriptor based on the covariance of local features. Differently than the common Bag of Words approaches our feature vector does not rely on a generated visual vocabulary, removing the dependence from a specific dataset and obtaining a reduction of the computational cost. 3D modeling is used to achieve a precise visitor's localization that allows browsing visible relevant details that the user may otherwise miss. Experimental results conducted on a publicly available cultural heritage dataset show that the proposed feature descriptor outperforms Bag of Words techniques.","","Electronic:978-1-6319-0061-7; POD:978-1-4799-8377-3","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325497","computer vision;egocentric vision;enhanced tourist experience;smart guides","Buildings;Cameras;Cultural differences;Histograms;Image color analysis;Three-dimensional displays;Visualization","architecture;computer vision;history;image sensors;information retrieval;multimedia computing;solid modelling;wearable computers","3D modeling;architectural details retrieval;augmented tourist experiences;bag of words approaches;cultural cities;cultural heritage experience;egocentric vision system;feature descriptor;generated visual vocabulary;glass-mounted camera;historical building;local feature covariance;multimedia contents;multimedia tools;precise visitor localization;visual descriptor;wearable board;wearable vision","","","","24","","","10-12 June 2015","","IEEE","IEEE Conference Publications"
"Gaussian Mixture Model-Based Subspace Modeling for Semantic Concept Retrieval","C. Chen; M. L. Shyu; S. C. Chen","Dept. of Electr. & Comput. Eng., Univ. of Miami, Coral Gables, FL, USA","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","258","265","Data mining and machine learning methods have been playing an important role in searching and retrieving multimedia information from all kinds of multimedia repositories. Although some of these methods have been proven to be useful, it is still an interesting and active research area to effectively and efficiently retrieve multimedia information under difficult scenarios, i.e., detecting rare events or learning from imbalanced datasets. In this paper, we propose a novel subspace modeling framework that is able to effectively retrieve semantic concepts from highly imbalanced datasets. The proposed framework builds positive subspace models on a set of positive training sets, each of which is generated by a Gaussian Mixture Model (GMM) that partitions the data instances of a target concept (i.e., the original positive set of the target concept) into several subsets and later merges each subset with the original positive data instances. Afterwards, a joint-scoring method is proposed to fuse the final ranking scores from all such positive subspace models and the negative subspace model. Experimental results evaluated on a public-available benchmark dataset show that the proposed subspace modeling framework is able to outperform peer methods commonly used for semantic concept retrieval.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.50","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300986","Gaussian Mixture Model (GMM);Semantic Concept Retrieval;Subspace Modeling","Data models;Gaussian mixture model;Mathematical model;Semantics;Training;Training data","Gaussian processes;data mining;information retrieval;learning (artificial intelligence)","GMM;Gaussian mixture model-based subspace modeling;data mining;information retrieval;information search;joint-scoring method;machine learning;multimedia information;multimedia repositories;semantic concept retrieval","","1","","50","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Method of refining of document relevance in search engines based on a user's local behavioral description","M. Zagirnyak; P. Kostenko","Kremenchuk Mykhailo Ostrohradskyi National University, Kremenchuk, Ukraine","2015 16th International Conference on Computational Problems of Electrical Engineering (CPEE)","20151123","2015","","","242","244","Huge information content in the World Wide Web leads to redundancy of search result that can reach hundred thousands of pages. Due to this an end user sometimes finds necessary materials not in the top ten links. Method of local refining of document relevance is proposed in the paper. The proposed method is based on user's local behavioral description. Its main module comprises collecting the data from user's Web-surfing and monitoring the Web-resources retrieved in response to her/his web search query which are the most interesting for the user. Local search results are grounded on the base search engine results page and keyword search query. The research results indicate the 15% increase in search results quality.","","Electronic:978-6-1760-7804-3; POD:978-1-4673-7937-3","10.1109/CPEE.2015.7333387","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333387","document relevance;index;rating;search engine;search result","Decision support systems","information retrieval;search engines","Web-resource retrieval;document relevance refining;search engines;user local behavioral description;web search query;world wide web","","","","5","","","2-5 Sept. 2015","","IEEE","IEEE Conference Publications"
"Manage Web Educational Knowledge Based on Knowledge Dimension Model","J. Peng; D. Jiang; X. Zhang","Inf. Technol. Center, Tsinghua Univ., Beijing, China","2014 International Conference on Service Sciences","20151029","2014","","","31","36","With the development of open online courses, the quantity of educational knowledge on the internet is growing fast. Building tree-structured catalogue and establishing full text search engine based on keywords are two main methods for information retrieval on a website. However, managing knowledge from various websites is still difficult. Due to the heterogeneous tree structure and diverse description format, it is difficult to integrate key information from different websites. This paper introduced a knowledge dimension model to solve the problems. By analyzing the dimensions of tree structure and description text, all the basic information about the materials was extracted and put into a shareable knowledge dimension model in order to manage the public materials on websites and build a knowledge base for knowledge utilization. This paper also used online courses data to verify the validity of the method.","2165-3828;21653828","CD-ROM:978-1-4799-4332-6; Electronic:978-1-4799-4330-2; POD:978-1-4799-4329-6","10.1109/ICSS.2014.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312286","knoweldge integration;knowledge dimension model;knowledge management;web educational knowledge","Courseware;Knowledge based systems;Knowledge engineering;Knowledge management;Media;Organizations","Internet;Web sites;computer aided instruction;information retrieval;knowledge based systems;knowledge management;search engines","Internet;Web educational knowledge management;Web site;description text;diverse description format;full text search engine;heterogeneous tree structure;information retrieval;knowledge base;knowledge dimension model;knowledge utilization;open online courses;public material management;tree-structured catalogue","","","","6","","","22-23 May 2014","","IEEE","IEEE Conference Publications"
"iASK: a distributed Q&A system incorporating social community and global collective intelligence","G. Liu; H. Shen","Department of Electrical and Computer Engineering, Clemson University, Clemson, SC 29631, USA","2015 IEEE International Conference on Peer-to-Peer Computing (P2P)","20151116","2015","","","1","10","Traditional Web-based Question and Answer (Q&A) Web sites cannot easily solve non-factual questions to match askers' preference. Recent research efforts begin to study social-based Q&A systems that rely on an asker's social friends to provide answers. However, this method cannot find answerers for a question not belonging to the asker's interests. To solve this problem, we propose a distributed Q&A system incorporating both social community intelligence and global collective intelligence, named as iASK. iASK improves the response latency and answer quality in both the social domain and global domain. It uses a neural network based friend ranking method to identify answerer candidates by considering social closeness and Q&A activities. To efficiently identify answerers in the global user base, iASK builds a virtual server tree that embeds the hierarchical structure of interests, and also maps users to the tree based on user interests. To accurately locate the cooperative experts, iASK has a fine-grained reputation system to evaluate user reputation based on their cooperativeness and expertise. Experimental results from large-scale trace-driven simulation and realworld daily usages of the iASK prototype show the superior performance of iASK. It achieves high answer quality with 24% higher accuracy, short response latency with 53% less delay and effective cooperative incentives with 16% more answers compared to other social-based Q&A systems.","","Electronic:978-1-5090-0300-6; POD:978-1-5090-0301-3","10.1109/P2P.2015.7328516","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328516","","Artificial intelligence;Delays;Neural networks;Prototypes;Quality of service;Servers;Social network services","neural nets;question answering (information retrieval);social networking (online)","answer quality;answer quality improvement;asker preference;cooperative experts;distributed Q-and-A system;distributed question and answer system;fine-grained reputation system;global collective intelligence;global user base;hierarchical structure;iASK;neural network based friend ranking method;nonfactual questions;response latency improvement;social closeness;social community intelligence;social-based Q-and-A systems;user interests;user mapping;user reputation evaluation;virtual server tree","","","","26","","","21-25 Sept. 2015","","IEEE","IEEE Conference Publications"
"SMAP data and services at the NASA DAACs","A. Leon; S. J. S. Khalsa; S. Leslie","National Snow and Ice Data Center Distributed Active Archive Center","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","150","152","The NASA Distributed Active Archive Centers (DAACs) at the Alaska Satellite Facility and the National Snow and Ice Data Center Data provide synergistic and seamless data access and support for the data products coming from NASA's Soil Moisture Active Passive (SMAP) satellite mission. We describe the tools and services available to data users for discovering, accessing, visualizing and analyzing SMAP data.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7325721","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325721","data access;data management;data visualization;soil moisture","Distributed databases;NASA;Orbits;Radar;Radiometry;Satellites;Soil moisture","data analysis;data visualisation;information retrieval","Alaska Satellite Facility;NASA DAAC;NASA Distributed Active Archive Centers;NASA Soil Moisture Active Passive satellite mission;National Snow and Ice Data Center Data;SMAP data;data access;data products;data users","","","","2","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Proofs of Encrypted Data Retrievability with Probabilistic and Homomorphic Message Authenticators","D. Liu; J. Zic","CSIRO, Marsfield, NSW, Australia","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","897","904","When users store their data on a cloud, they may concern on whether their data is stored correctly and can be fully retrieved. Proofs of Retrivability (PoR) is a cryptographic concept that allows users to remotely check the integrity of their data without downloading. This check is usually done by attaching data with message authenticators that contain data integrity information. The existing PoR schemes consider only the retrievability of unencrypted data and their message authenticators are usually deterministic. In this paper, we propose a PoR scheme that is built over homomorphic encryption schemes. Our PoR scheme can prove the retrievability of homomorphically encrypted data by generating probabilistic and homomorphic message authenticators. Moreover, the homomorphically encrypted data can be processed by the cloud directly and our PoR scheme can verify the integrity of such outsourced computations over ciphertexts. A prototype of our scheme is implemented to evaluate its performance.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.462","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345370","Homomorphic Encryption;Integrity;Outsourcing;Proofs of Retrivability","Australia;Blood pressure;Cloud computing;Encryption;Probabilistic logic;Prototypes","cloud computing;cryptography;information retrieval;probability","PoR;Proofs of Retrivability;ciphertexts;cryptographic concept;data integrity information;encrypted data retrievability;homomorphic message authenticators;message authenticators;outsourced computations;probabilistic message authenticators;unencrypted data","","1","","14","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"A study on keyword detection using weighted similarity and character sequence for low-resolution medical documents","M. Kawamura; H. Kawanaka; S. Doi; T. Suzuki; H. Takase; S. Tsuruoka","Graduate School of Engineering, Mie University, Tsu, Japan","2015 International Conference on Informatics, Electronics & Vision (ICIEV)","20151123","2015","","","1","5","By the diffusion of Hospital Information Systems, many medical documents have been computerized. In addition, most of paper documents before computerization have been also scanned and archived as document images. These were usually converted to text data by using document analysis techniques and Optical Character Reader (OCR) and archived for medical document retrieval. However, the resolutions of some documents are not sufficient for character recognition because of storage spaces, scanning regulations and so on. Therefore, we cannot search desired keywords in the documents, as a result, these documents are not still used effectively in medical document retrieval systems. In this study, we discuss a keyword detection and extraction methods for these document images. As the first step of this study, this paper proposes a method to detect and extract desired words from these documents by using weighted dissimilarity and character sequence. Evaluation experiments using actual medical documents are conducted to discuss the effectiveness of the proposed method.","","Electronic:978-1-4673-6902-2; POD:978-1-4673-6903-9","10.1109/ICIEV.2015.7334027","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334027","Document Image Processing;Document Retrieval Systems;Keyword Extraction Method;Low-resolution Image","Accuracy;Biomedical imaging;Character recognition;Databases;Hospitals;Image resolution;Optical character recognition software","document image processing;information retrieval;medical information systems;optical character recognition","OCR;character sequence;document analysis technique;hospital information system;keyword detection method;keyword extraction method;low-resolution medical document;medical document retrieval system;optical character reader;weighted similarity","","","","14","","","15-18 June 2015","","IEEE","IEEE Conference Publications"
"Date field extraction from handwritten documents using HMMs","R. Mandal; P. P. Roy; U. Palz; M. Blumenstein","School of Information and Communication Technology, Griffith University, Queensland, Australia","2015 13th International Conference on Document Analysis and Recognition (ICDAR)","20151123","2015","","","866","870","Automatic document interpretation and retrieval is an important task to access handwritten digitized document repositories. In documents, the date is an important field and it has various applications such as date-wise document indexing/retrieval. In this paper a framework has been proposed for automatic date field extraction from handwritten documents. In order to design the system, sliding window-wise Local Gradient Histogram (LGH)-based features and a character-level Hidden Markov Model (HMM)-based approach have been applied for segmentation and recognition. Individual date components such as month-word (month written in word form i.e. January, Jan, etc.), numeral, punctuation and contraction categories are segmented and labelled from a text line. Next, a Histogram of Gradient (HoG)-based features and a Support Vector Machine (SVM)- based classifier have been used to improve the results obtained from the HMM-based recognition system. Subsequently, both numeric and semi-numeric regular expressions of date patterns have been considered for undertaking date pattern extraction in labelled components. The experiments are performed on an English document dataset and the encouraging results obtained from the approach indicate the effectiveness of the proposed system.","","Electronic:978-1-4799-1805-8; POD:978-1-4799-1806-5","10.1109/ICDAR.2015.7333885","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333885","","Hidden Markov models;Writing","document image processing;feature extraction;gradient methods;handwriting recognition;hidden Markov models;information retrieval;support vector machines","English document dataset;HMM;Hidden Markov Model;Histogram of Gradient;HoG;LGH;SVM;automatic date field extraction;automatic document interpretation;automatic document retrieval;date field extraction;date pattern extraction;date wise document indexing-retrieval;handwritten digitized document repositories;handwritten documents;local gradient histogram;support vector machine","","","","20","","","23-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Making knowledge discovery services scalable on clouds for big data mining","D. Talia","DIMES-University of Calabria & DtoK Lab Srl Rende (CS), Italy","2015 2nd IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services (ICSDM)","20151015","2015","","","1","4","The amount of digital data is increasing beyond any previous estimation and data stores and sources are more and more pervasive and distributed. Professionals and scientists need advanced data analysis tools and services coupled with scalable architectures to support the extraction of useful information from big data repositories. Cloud computing systems offer an effective support for addressing both the computational and data storage needs of big data mining and parallel knowledge discovery applications. In fact, complex data mining tasks involve data- and compute-intensive algorithms that require large and efficient storage facilities together with high performance processors to get results in acceptable times. In this paper we introduce the topic and the main research issues. We discuss how to make knowledge discovery services scalable and present the Data Mining Cloud Framework designed for developing and executing distributed data analytics applications as workflows of services. In this environment we use data sets, analysis tools, data mining algorithms and knowledge models that are implemented as single services that can be combined through a visual programming interface in distributed workflows to be executed on Clouds. The main features of the programming interface are described and performance evaluation of knowledge discovery applications are reported.","","Electronic:978-1-4799-7749-9; POD:978-1-4799-7750-5","10.1109/ICSDM.2015.7298015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298015","Big Data;Cloud computing;Data mining;Data mining cloud framework","Big data;Cloud computing;Computational modeling;Data analysis;Data mining;Knowledge discovery;Programming","Big Data;cloud computing;data analysis;data mining;information retrieval;visual programming","big data mining;big data repositories;cloud computing systems;compute-intensive algorithm;data analysis tools;data mining cloud framework;data-intensive algorithm;digital data;distributed data analytics applications;distributed workflows;high performance processors;information extraction;knowledge discovery services;parallel knowledge discovery applications;performance evaluation;scalable architectures;storage facilities;visual programming interface","","","","7","","","8-10 July 2015","","IEEE","IEEE Conference Publications"
"Transit Trip Planners: Real-Time Strategy-Based Path Recommendation","A. Nuzzolo; A. Comi","Dept. of Enterprise Eng., &#x201C;Tor Vergata&#x201D; Univ. of Rome, Rome, Italy","2015 IEEE 18th International Conference on Intelligent Transportation Systems","20151102","2015","","","196","201","In order to improve the effectiveness of information provided to travelers of a transit network, the new generation of trip planners should give recommendations taking into account several factors, such as network unreliability and presence of diversion nodes where path decision can be made according to the occurrences of random events. In this context, travelers have not to rely on a single selected path, but they have to use a strategy, i.e. set of rules that allow travelers to reach the destination maximizing their expected utility. The availability of real-time predictive information requires the traditional optimal hyper-path approaches to be overcame and new ones to be developed. Further, as the values of path attributes forecasted are random variables and therefore, also with an information system, the uncertainty is not completely overcome. This paper explores some aspects of providing path recommendations in unreliable network, proposing a methodology for defining real-time optimal strategies, that combine predictive and expected values of path attributes.","2153-0009;21530009","Electronic:978-1-4673-6596-3; POD:978-1-4673-6597-0","10.1109/ITSC.2015.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313133","path recommendation;real-time information;travel strategy;trip planner","Decision making;Decision theory;Information systems;Random variables;Real-time systems;Uncertainty;Vehicles","information retrieval;recommender systems;traffic information systems","diversion nodes;information system;network unreliability;optimal hyperpath approach;path attribute forecasting;path decision making;random variables;real-time optimal strategy;real-time predictive information;real-time strategy-based path recommendation;transit network;transit trip planner;travelers;unreliable network","","1","","25","","","15-18 Sept. 2015","","IEEE","IEEE Conference Publications"
"The implementing of the Internet of things concepts for the continuous provision of informational services for vehicle drivers and passengers","S. Popov; M. Kurochkin; L. Kurochkin; V. Glazunov","Telematics department, St. Petersburg State Polytechnical University, Russia, Politekhnicheskaja 29, 195251","2015 1st International Conference on Telematics and Future Generation Networks (TAFGEN)","20151008","2015","","","1","5","This article deals with concepts of implementing the Internet of things that help both vehicle drivers and passengers get the continuous access to information services. The main feature of the proposed solution is the designing of the intelligent model of the virtual objects interaction that reflects the entity of the traffic network itself. The model provides the technology that helps to form the access map to the cloud services. Cloud client-server architecture has been chosen as an implementation area. This means that any mobile subscriber can get an actual list of available subnets for exchanging messages at any time. In this case the mobile subscriber receives all the necessary settings to access those available subnets. This particular article offers the implementation of the Internet of things concepts for building the continuous channel of message exchange between mobile subscribers. The offered approach allows to lower significantly the load of the data transmission networks, due to the physical division of system and user data flows.","","Electronic:978-1-4799-7315-6; POD:978-1-4799-7316-3","10.1109/TAFGEN.2015.7289565","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7289565","","Data communication;Internet;Mobile communication;Mobile computing;Roads;Telematics;Vehicles","Internet of Things;client-server systems;cloud computing;data communication;driver information systems;electronic data interchange;information retrieval;information services;mobile computing","Internet of Things;access map;cloud client-server architecture;cloud service;continuous channel;continuous information service access;continuous informational service provision;data transmission network;intelligent model;message exchange;mobile subscriber;passengers;traffic network;user data flow;vehicle drivers;virtual object interaction","","","","13","","","26-28 May 2015","","IEEE","IEEE Conference Publications"
"Influencing Factors Analysis of People's Answering Behaviours on Social Network Based Questions","Z. Sun; W. Rong; Y. Shen; Y. Ouyang; C. Li; Z. Xiong","Sch. of Comput. Sci. & Eng., Beihang Univ., Beijing, China","2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops","20151026","2014","","","196","203","Recently the social question/answering systems have become a natural way for people to get information from their social connections. Despite its popularity in knowledge sharing, its development still faces several challenges, among which a notable one is that a majority of questions get no response. How to improve the response ratio and reduce the number of unanswered social questions has become a challenging problem. Though some research in this context has been well studied, intrinsic factors that reveal profiles of social answerers are still less understood. In this paper, we try to discover: what kind of person is likely to be an answerer? To answer this question, we study multiple hypothetical factors from the perspectives of social and personal analysis. A set of experiments are conducted to identify the relationship between these factors and the action of answering. We present a comprehensive description alongside our analysis of behavioural patterns of answerers in the scenario of social information seeking.","","CD-ROM:978-1-4799-7645-4; Electronic:978-1-4799-7646-1; POD:978-1-4799-7647-8","10.1109/UIC-ATC-ScalCom.2014.83","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306952","Answering Behaviour;Factor Analysis;Question/Answering;Social Network","Conferences;Context;Correlation;Eigenvalues and eigenfunctions;Media;Principal component analysis;Social network services","question answering (information retrieval);social networking (online)","factors analysis;knowledge sharing;people answering behaviours;social information seeking;social network based questions;social question-answering systems","","","","27","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Mobilizing digital museum with Chinese digital archive","Y. N. Lien; H. C. Jang; T. C. Tsai; P. J. Kuo; C. L. Hu","Department of Computer Science, National Chengchi University, Taipei City, Taiwan (R.O.C.)","2015 8th International Conference on Ubi-Media Computing (UMEDIA)","20151015","2015","","","134","139","Many museums have been digitalizing their collections and disseminated them to the world through the Internet. On the other hand, cloud computing and mobile communication technologies will further enhance the penetration capability of digital information, enabling mobile users surfing the wave of information world at anytime and anywhere. Nevertheless, both technologies have their own limitations, especially in handling Chinese artifacts that may hinder their applicability to the digital museums. We use the National Palace Museum (NPM) as a reference museum for the application of cloud computing and mobile communication technologies to mobilize digital museums with Chinese digital archives. A set of design guidelines specific to the NPM's services is formulated and a few prototype systems are implemented based on these guidelines: (1) Qingming Painting and Mao Gong Ding Inscription graphical exhibition and edutainment systems; (2) Mobile Digital Museum Explorer. By mobilizing, the NPM and other museums will be able to lift their world-class services to another level for the benefit of all.","","CD-ROM:978-1-4673-8268-7; Electronic:978-1-4673-8270-0; POD:978-1-4673-8271-7","10.1109/UMEDIA.2015.7297443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7297443","Digital archiving;Mobile computing","Bandwidth;Cloud computing;Mobile communication;Mobile computing;Mobile handsets;Painting;Web pages","cloud computing;information retrieval systems;mobile communication;mobile computing;museums","Chinese artifacts;Chinese digital archive;Internet;Mao Gong Ding Inscription graphical exhibition;NPM services;Qingming painting;cloud computing;digital information;edutainment systems;mobile communication technology;mobile digital museum explorer;national palace museum","","","","12","","","24-26 Aug. 2015","","IEEE","IEEE Conference Publications"
"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations","K. Wongsuphasawat; D. Moritz; A. Anand; J. Mackinlay; B. Howe; J. Heer","University of Washington","IEEE Transactions on Visualization and Computer Graphics","20151027","2016","22","1","649","658","General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.","1077-2626;10772626","","10.1109/TVCG.2015.2467191","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7192728","User interfaces;exploratory analysis;information visualization;mixed-initiative systems;visualization recommendation","Browsers;Compass;Data visualization;Encoding;Grammar;Image color analysis;Visualization","data visualisation;question answering (information retrieval);recommender systems","Voyager;automatically-generated visualizations;data variable coverage;manual chart construction;mixed-initiative system;question-answering;recommended chart faceted browsing;visual encodings;visualization recommendations","","9","","48","","20150812","Jan. 31 2016","","IEEE","IEEE Journals & Magazines"
"Improved retrieval of sea ice thickness from SMOS and CryoSat-2","L. Kaleschke; X. Tian-Kunze; N. Maaß; R. Ricker; S. Hendricks; M. Drusch","Institute for Oceanography, University of Hamburg","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20151112","2015","","","5232","5235","We investigate the potential of a synergetic combination of data from ESA's SMOS and CryoSat-2 mission for sea ice thickness retrieval. SMOS and CryoSat-2 provide complementary information because of their different spatio-temporal sampling and resolution, and because of the complementary uncertainty due to the fundamental difference of the radiometric and altimetric measurement principle. The main limitations of the ice thickness retrieval depend on the emission e-folding depth and the vertical resolution of the effective radar pulse-length, respectively. It is shown that the combination of SMOS and CryoSat-2 considerably reduces the uncertainty with respect to the products derived from the single sensors. The RMS error is reduced from 76 to 66 cm and the squared correlation coefficient increases from 0.47 to 0.61 in comparison to validation data of NASA's Operation IceBridge campaign, 2013. Furthermore, we demonstrate the applicability of the Optimal Interpolation method for the generation of a combined product based on weekly CryoSat-2 averages.","2153-6996;21536996","Electronic:978-1-4799-7929-5; POD:978-1-4799-7930-1; USB:978-1-4799-7928-8","10.1109/IGARSS.2015.7327014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7327014","","Ice thickness;Interpolation;Salinity (Geophysical);Sea ice;Sea measurements;Uncertainty","geographic information systems;geophysical techniques;information retrieval;radar altimetry;sea ice;thickness measurement","ESA CryoSat-2 mission;ESA SMOS mission;NASA Operation IceBridge campaign;RMS error;altimetric measurement principle;effective radar pulse-length;emission e-folding depth;optimal interpolation method;radiometric measurement principle;sea ice thickness retrieval;spatiotemporal sampling;squared correlation coefficient","","1","","6","","","26-31 July 2015","","IEEE","IEEE Conference Publications"
"Application of First-Order Logic Inference in Vietnamese Question Answering System","H. V. Nguyen; D. T. Nguyen","Fac. of Comput. Sci., Univ. of Inf. Technol., Ho Chi Minh City, Vietnam","2015 6th International Conference on Intelligent Systems, Modelling and Simulation","20151029","2015","","","10","15","The aim of this research is to develop a Question-Answering (QA) system which is based on a First-Order Logic (FOL) inference mechanism to determine answers for Vietnamese questions about outage schedule of electricity and water in Ho Chi Minh City, Vietnam. In this paper, firstly we focus on parsing the syntax, and secondly on representing the semantics, which are built in form of FOL formulas, of many types of usual Vietnamese questions. Finally, based on the FOL formulas of questions, the system finds out the information in knowledge base, and performs some inferences on time and location to answer user's questions. In experiment, the precision of our Vietnamese QA system is 92.67% with 300 testing questions about schedule of power outage and cut water.","2166-0662;21660662","Electronic:978-1-4799-8258-5; POD:978-1-4799-8259-2","10.1109/ISMS.2015.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311201","Vietnamese language processing;computational semantic;first-order logic inference;question answering;semantic representation","Knowledge based systems;Power system faults;Power system restoration;Schedules;Semantics;Syntactics","formal logic;grammars;inference mechanisms;natural language processing;programming language semantics;question answering (information retrieval)","FOL formulas;FOL inference mechanism;Ho Chi Minh City;Vietnamese QA system;Vietnamese question-answering system;Vietnamese questions;cut water;electricity outage schedule;first-order logic inference;knowledge base;power outage;semantics;syntax parsing;water outage schedule","","","","10","","","9-12 Feb. 2015","","IEEE","IEEE Conference Publications"
"Secure digital archiving in post-quantum era","V. Clupek; L. Malina; V. Zeman","Department of Telecommunications, Brno University of Technology, Brno, Czech Republic","2015 38th International Conference on Telecommunications and Signal Processing (TSP)","20151012","2015","","","622","626","This article introduces a solution of secure digital archiving in the post-quantum era. The basic tool at secure digital archiving of the electronic documents is the signature schemes, which are used at creation of the certificates and the timestamps. This article deals with the question of security of the signature schemes in the post-quantum era and introduces the post-quantum signature schemes, which will be resistant to the attacks leading by both conventional and quantum computers. The conventional signature schemes, based on factorization or discrete logarithm problem, in case of implementing the Shor algorithm on a quantum computer will be easy to break through. The main point of this article is the proposal of the solution of secure digital archiving with using the secure post-quantum signature schemes.","","Electronic:978-1-4799-8498-5; POD:978-1-4799-8499-2; USB:978-1-4799-8497-8","10.1109/TSP.2015.7296338","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296338","Digital Signature;Post-Quantum Cryptography;Secure Digital Archiving;Security","Computers;Digital signatures;Lattices;Public key;Quantum computing","digital signatures;document handling;information retrieval systems;quantum computing","Shor algorithm;attack resistance;certificates;discrete logarithm problem;electronic document;factorization;quantum computer;secure digital archiving;secure post-quantum signature scheme;timestamps","","","","15","","","9-11 July 2015","","IEEE","IEEE Conference Publications"
"The Urban Data Re-use and Integration Platform for Australia: Design, Realisation, and Case Studies","R. O. Sinnott; C. Bayliss; A. Bromage; G. Galang; Y. Gong; P. Greenwood; G. Jayaputera; D. Marques; L. Morandini; M. Nino-Ruiz; G. Nogoorani; H. Pursultani; R. Rabanal; M. Sarwar; W. Voorsluys; I. Widjaja","Australian Urban Res. Infrastruct. Network, Univ. of Melbourne, Melbourne, VIC, Australia","2015 IEEE International Conference on Information Reuse and Integration","20151026","2015","","","90","97","Many/most of the challenges facing urban researchers and indeed policy makers relate to discovery of, access to and subsequent use of heterogeneous urban data sets. These data are typically held by a myriad of government agencies (local, State and Federal), industry and academic research organisations. The Australian Urban Research Infrastructure Network (AURIN) project commenced in 2010 and was tasked specifically with addressing this issue for the Australian urban research community. Specifically it was tasked with developing an e-Infrastructure providing seamless, secure live access to an extensive, extensible, and diverse collection of (typically) pre-existing data from autonomous and definitive data providers, where here ""live"" implies that the data is held by organizations and programmatic access to the data in situ is supported. The AURIN platforms currently provides access to over 1800 data sets from over 60 major organisations across Australia with over 100 targeted tools offering best practice analytics and visualization of data. These data sets can be combined in a multitude of ways reflecting urban research challenges/needs. This paper describes the design of the platform and its support for data re-use, data integration and associated data analytics and visualisation. We illustrate the flexibility of the system through case studies highlighting typical aggregate-level data re-use and integration of data from multiple independent agencies and in supporting novel disaggregated (unit-level) data re-use.","","Electronic:978-1-4673-6656-4; POD:978-1-4673-6657-1","10.1109/IRI.2015.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300960","Data Analytics;Data Integration;Data Re-use;Urban Research;e-Infrastructure","Australia;Cities and towns;Distributed databases;Geospatial analysis;Metadata;Sociology;Statistics","data analysis;data integration;data visualisation;information retrieval;town and country planning","AURIN project;Australian Urban Research Infrastructure Network project;Australian urban research;aggregate-level data reuse;best practice analytics;data analytics;data integration;data visualisation;data visualization;e-infrastructure;government agencies;heterogeneous urban data set;policy making;programmatic data access;seamless secure live access;urban data reuse","","","","23","","","13-15 Aug. 2015","","IEEE","IEEE Conference Publications"
"Ahead context Qa: Interactive communications spawning through accumulation propaganda","R. S. Shalini; S. J. Samuel","Department of Information technology, Faculty of Computing, Sathyabama University, Chennai, India","2015 International Conference on Communications and Signal Processing (ICCSP)","20151112","2015","","","1115","1117","Interactively freak out information to the user is generally play a vital role. Existent QA is usually enough to be useful only in plain text format providing to the user. In this project we connote textual data with the appropriate media to recommend a inspired by the response. It has three components of our system Construal median scrap, Inquiry localization, Data pick up and Launch. The Construal median scrap is used to select a variety of responses to the receiver. Inquiry localization is used for extracting keywords from the source in question is widely used in the investigation. Data pick up and Launch is used for choose the correct answer and the result is used to retrieve the data and launching. We use Stemming algorithm, Naive Bayes classifier algorithm and Ranking algorithm. We will increase the contribution of community responses. Data unwittingly, any user can get information immediately. Our nigh is to deal with complicated questions. Queries are generated on the basis of the information we collect upright multimedia image and video with multimedia search engines.","","CD-ROM:978-1-4799-8080-2; Electronic:978-1-4799-8081-9; POD:978-1-4799-8082-6","10.1109/ICCSP.2015.7322676","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322676","CQA;medium selection;question answering;reranking","Engines;Integrated optics;Optical imaging","multimedia systems;question answering (information retrieval);search engines;text analysis","Naive Bayes classifier algorithm;accumulation propaganda;ahead context QA technique;construal median scrap;data launch;data pick up;data retrieval;inquiry localization;interactive communication spawning;multimedia image;multimedia search engine;multimedia video;plain text format;ranking algorithm;receiver;source keyword extraction;textual data connotation","","","","8","","","2-4 April 2015","","IEEE","IEEE Conference Publications"
"Automated user documentation system for Android","F. B. Manolache; Y. Li; O. Rusu","Carnegie Mellon University Pittsburgh, Pennsylvania, USA","2015 14th RoEduNet International Conference - Networking in Education and Research (RoEduNet NER)","20151029","2015","","","63","67","Documenting android projects and synchronizing the help pages with the current stage of the project is a time consuming task, especially for complex projects developed by larger teams. Keeping the website of the project synchronized with the current development is even more difficult, since usually the two components are maintained by different people or teams. This paper proposes an automated documentation system based on a wiki that is filled by contributors, and a software system that extracts the info from the wiki and builds a set of html help pages that are incorporated automatically in the project with every new version. The documentation system is not project specific, and offers the following advantages over maintaining the help pages by hand: the help system automatically reflects the current content of the wiki; the help system of the app is independent of the network; every version of the software keeps a set of help pages specific to the development stage of that version. The implementation uses python and is available as open source.","2068-1038;20681038","Electronic:978-1-4673-8180-2; POD:978-1-4673-8181-9","10.1109/RoEduNet.2015.7311829","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7311829","android;documentation;help;wiki","Browsers;Decision support systems;Radio frequency;Uniform resource locators","Android (operating system);Web sites;help systems;information retrieval;project management;public domain software;system documentation;text analysis","Android project documentation;Python;Website;automated user documentation system;help page synchronization;html help pages;open source;project development;software system;wiki content;wiki information extraction","","","","12","","","24-26 Sept. 2015","","IEEE","IEEE Conference Publications"
"Bin Encoding: A User-Centric Secure Full-Text Searching Scheme for the Cloud","M. A. Will; R. K. L. Ko; I. H. Witten","Cyber Security Lab., Univ. of Waikato, Hamilton, New Zealand","2015 IEEE Trustcom/BigDataSE/ISPA","20151203","2015","1","","563","570","Permitting users to search encrypted documents presents cloud storage providers with interesting challenges. Existing solutions target large corporations rather than individual users of the cloud. In order to serve all users, we propose a way of shifting most of the computational complexity from the client to the cloud by building and managing the index there, while ensuring that only the client can access the plaintext. This allows more sophisticated indexing and search ranking schemes to be implemented, including approximate search with multiple errors. Our method uses a many-to-one encoding scheme called ""Bin Encoding"", and this paper analyses its cryptographic strength against letter-frequency and dictionary attacks.","","Electronic:978-1-4673-7952-6; POD:978-1-4673-7953-3","10.1109/Trustcom.2015.420","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345328","cloud computing;cyber security;data privacy;encrypted search;full-text search;information security;trapdoor functions;user-centric security","Cloud computing;Computer security;Cryptography;Dictionaries;Electronic mail;Encoding;Indexes","cloud computing;cryptography;full-text databases;indexing;information retrieval","approximate search;bin encoding;cloud storage provider;computational complexity;cryptographic strength analysis;dictionary attack;encrypted document searching;index building;index management;letter-frequency attack;many-to-one encoding scheme;plaintext access;search ranking;user-centric secure full-text searching scheme","","","","27","","","20-22 Aug. 2015","","IEEE","IEEE Conference Publications"
"Live demonstration: Curating multi-domain investigations of electrochemical sensors for healthcare applications with a flexible hardware-software platform","S. Boling; A. J. Mason","Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA","2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)","20151207","2015","","","1","1","A software package will be demonstrated for designing, organizing, and executing sophisticated real-world sensor interrogation protocols, archiving and annotating collected data, and performing differential analysis of experimental results. Users may describe the steps and components of an electronic experiment in abstract terms and may save and share protocol templates for later specialization and reuse. Once sufficiently specialized, an experiment may be executed on real hardware, yielding an organized and metadata-rich set of results tailored to the users' specifications. Results may be inspected at many degrees of detail, annotated, and compared with the outcomes of related experiments. Researchers can use this system to raise confidence in their conclusions, diagnose methodological problems, share their work, and determine new avenues of investigation. The presented framework is flexible enough to improve the automation and reproducibility of many sensor characterization tasks. During the demonstration, the system will be used to run a real protocol related to the authors' target application, namely electrochemical gas sensing for environmental health monitoring.","","Electronic:978-1-4799-7234-0; POD:978-1-4799-7235-7; USB:978-1-4799-7233-3","10.1109/BioCAS.2015.7348327","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348327","","Digital signal processing;Hardware;Protocols;Sensors;Software;Voltmeters","electrochemical sensors;environmental monitoring (geophysics);gas sensors;health care;information retrieval systems;medical diagnostic computing;medical disorders;meta data;patient diagnosis;patient monitoring","data annotation;data archiving;diagnose methodological problems;differential analysis;electrochemical gas sensor;electronic experiment;environmental health monitoring;flexible hardware-software platform;healthcare applications;metadata;organized data;protocol templates;real-world sensor interrogation protocols;software package;target application","","","","","","","22-24 Oct. 2015","","IEEE","IEEE Conference Publications"
"A framework of fundamental news summarization to determine the direction of foreign exchange rate using adaptive indicator scheme","Y. T. Samuel; D. H. Widyantoro; A. I. Wuryandari","School of Electrical Eng. & Inf., Bandung Institute of Technology, Bandung, Indonesia","2015 2nd International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","20151123","2015","","","1","6","Fundamental indicator news have a strong influence on the movement of foreign exchange rates. There were a lot of fundamental indicator news on the internet that rapidly changes, but to read all of it and make conclusion need a lot of time and effort. Therefore a system is needed to summarize the news. In this paper we propose a framework to generate a scheme that summarize factors affecting the fundamental indicator, where these factors may adapt automatically. The approach used in this system based on 3 (three) phases: 1. News Classification. At this stage, classification process is done by looking at the factors that affect each indicator. The process of obtaining these factors is done by using information extraction approach. 2 Generating scheme based on the Influencing factors, where the scheme can adapt automatically to the factors that affect an indicator. Methods in determining the scheme is based on the extraction of information. 3. Fills the scheme. At this stage, the scheme will be filled with appropriate information so that it will produce the summary of news which related to specific indicators.","","Electronic:978-1-4673-8143-7; POD:978-1-4673-8144-4","10.1109/ICAICTA.2015.7335381","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335381","Forex movement;Information Extraction;News Clasification;News Summarization;Rule Extraction","Classification algorithms;Data mining;Economic indicators;Exchange rates;Feature extraction;Pragmatics;Unemployment","Internet;electronic publishing;exchange rates;information retrieval;pattern classification","Internet;adaptive indicator scheme;foreign exchange rate direction;fundamental indicator news;fundamental news summarization framework;generating scheme;information extraction;information extraction approach;news classification process","","","","29","","","19-22 Aug. 2015","","IEEE","IEEE Conference Publications"
