"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7872791,7868324,7861163,7858187,7854192,7846612,7845085,7840686,7836454,7838246,7837873,7837921,7836651,7836781,7830233,7829763,7831339,7828548,7829912,7752857,7822008,7824887,7820422,7822007,7822636,7816648,7813697,7816490,7816530,7817061,7817031,7817091,7817142,7600398,7782321,7804496,7805067,7803072,7803112,7801549,7800261,7785763,7785776,7790302,4463120,7784223,4462897,4463242,4463113,7777783,7772288,7772307,7758051,7755127,7755154,7749008,7743804,7744426,7728006,7740368,7737996,7733216,7727289,7729737,7730832,7730107,7729226,7729170,7730655,7725637,7725366,7603473,7603518,7603278,7565634,7588959,7585527,7585111,7577578,7578614,7523302,7576527,7574678,7575143,7571832,7571833,7308086,7559555,7559627,7559617,7557941,7442564,7549387,7545037,7545055,7541487,7535082,7501538,7526963,7526940",2017/05/04 20:46:32
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Text mining from biomedical domain using a full parser","P. Govindarajan; K. S. Ravichandran","SASTRA UNIVERSITY, Kumbakonam","2016 International Conference on Inventive Computation Technologies (ICICT)","20170119","2016","2","","1","9","Text mining is vital for knowledge cultivation, keeping this in perspective we have focused on developing a system which uses a full parser for analyzing the text, grammar towards the biomedical arena. We proposed a preprocessor to overcome the shortcomings of full parsing and modules to handle the partial outcome. The developed system, not only has the viability to be maintained easily, but also can adapt itself for a particular domain. In the primary experiment, out of 131 argument structures extracted from 96 sentences, 32 were extractable, 33 with ambiguity and the remaining 66 (non-extractable) for which partial result was determined. The work produced better result than the other full parser with reduced count of failure in extraction and ambiguity.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7824887","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7824887","Analyzing;Argument structure;Biomedical domain;Extraction;Full parser;Preprocessor;Structured data;Text mining","Feature extraction;Grammar;Information retrieval;Pattern matching;Pragmatics;Syntactics;Text mining","data mining;data structures;grammars;medical computing;text analysis","argument structures;biomedical domain;full parser;grammar;knowledge cultivation;text mining","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Mobile Augmented Reality: Placing Labels Based on Gaze Position","A. McNamara; C. Kabeerdoss","Dept. of Visualization, Texas A&M Univ., College Station, TX, USA","2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)","20170202","2016","","","36","37","The arrangement of Augmented Reality (AR) labels on mobile displays can cause problems when the labels over populate the screen space. This can result in occlusion, not only of real scene, but also of other labels annotating the scene. We are developing view management systems that arrange and display AR content based on user attention. We present a pilot user study that tests speed and accuracy on a simple information retrieval task on simulated AR scenarios. We varied the presentation in two ways: a naive approach and a gaze-based approach. The results did not show statistical differences between the two methods, yet results from this study provide valuable insight for subsequent studies.","","Electronic:978-1-5090-3740-7; POD:978-1-5090-3741-4","10.1109/ISMAR-Adjunct.2016.0033","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836454","","Atmospheric measurements;Augmented reality;Human computer interaction;Information retrieval;Mobile communication;Visualization","augmented reality;information retrieval;mobile computing","AR content;gaze position;gaze-based approach;information retrieval;mobile augmented reality labels;mobile displays;naive approach;user attention;view management systems","","","","","","","19-23 Sept. 2016","","IEEE","IEEE Conference Publications"
"Multimedia Hashing and Networking","W. Liu; T. Zhang","Tencent AI Lab","IEEE MultiMedia","20160805","2016","23","3","75","79","This department discusses multimedia hashing and networking. The authors summarize shallow-learning-based hashing and deep-learning-based hashing. By exploiting successful shallow-learning algorithms, state-of-the-art hashing techniques have been widely used in high-efficiency multimedia storage, indexing, and retrieval, especially in multimedia search applications on smartphone devices. The authors also introduce Multimedia Information Networks (MINets) and present one paradigm of leveraging MINets to incorporate both visual and textual information to reach a sensible event coreference resolution. The goal is to make deep learning practical in realistic multimedia applications.","1070-986X;1070986X","","10.1109/MMUL.2016.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535082","Multimedia Information Networks;big data;data analysis;hashing;multimedia;networking","Information retrieval;Learning systems;Machine learning;Multimedia communication;Visualization","file organisation;learning (artificial intelligence);multimedia systems;social networking (online)","MINets;deep-learning-based hashing;multimedia hashing;multimedia information networks;multimedia networking;shallow-learning-based hashing;textual information;visual information","","","","","","","July-Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Knowledge-Based Content Linking for Online Textbooks","R. Meng; S. Han; Y. Huang; D. He; P. Brusilovsky","Sch. of Inf. Sci., Univ. of Pittsburgh, Pittsburgh, PA, USA","2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)","20170116","2016","","","18","25","Although the volume of online educational resources has dramatically increased in recent years, many of these resources are isolated and distributed in diverse websites and databases. This hinders the discovery and overall usage of online educational resources. By using linking between related subsections of online textbooks as a testbed, this paper explores multiple knowledge-based content linking algorithms for connecting online educational resources. We focus on examining semantic-based methods for identifying important knowledge components in textbooks and their usefulness in linking book subsections. To overcome the data sparsity in representing textbook content, we evaluated the utility of external corpuses, such as more textbooks or other online educational resources in the same domain. Our results show that semantic modeling can be integrated with a term-based approach for additional performance improvement, and that using extra textbooks significantly benefits semantic modeling. Similar results are obtained when we applied the same approach to other domains.","","Electronic:978-1-5090-4470-2; POD:978-1-5090-4471-9","10.1109/WI.2016.0014","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817031","content linking;knowledge component;online corpus;textbook","Adaptation models;Context;Information retrieval;Joining processes;Knowledge based systems;Semantics;Standards","Web sites;educational aids;knowledge based systems;semantic Web;semantic networks","Web sites;data sparsity;external corpus utility;knowledge-based content linking;online educational resources;online textbook linking;semantic modeling;semantic-based methods;textbook content representation","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Research on text structuralization in medical field","Xiangwu Ding; Xihua Zhang","School of Computer Science and Technology, Donghua University, Shanghai, China","2016 2nd International Conference on Cloud Computing and Internet of Things (CCIOT)","20170302","2016","","","155","161","Transforming the non-structured medical text data into structured data is the basis of the processing and analysis of medical data. The effect of general-purpose word segmentation tools recognizing terminology is not ideal, which greatly affects the accuracy of the word segmentation, and further influences the result of text structuralization. In view of above problems, this paper puts forward a method of discovering new words based on word embedding. It uses Google open source word vector tool word2vec to train text and map the words into abstracted n-dimensional vector space. We can get the latent semantic relations between words and words in the corpus. And then combining the information entropy and word frequency, we can find new words. Finally, we design information extraction rules to get the key information according to the new words, and organize them into structured data. Experimental results on real medical data show that the accuracy is improved by 10% compared to traditional method, and the time is saved by 18% compared to traditional method.","","Electronic:978-1-4673-9822-0; POD:978-1-4673-9823-7","10.1109/CCIOT.2016.7868324","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7868324","Chinese word segmentation;Information entropy;Word embedding;Word2vec","Algorithm design and analysis;Information entropy;Information retrieval;Semantics;Syntactics;Text analysis;Training","data analysis;medical administrative data processing;text analysis;vectors;word processing","Google open source word vector tool;abstracted n-dimensional vector space;general-purpose word segmentation tools recognizing terminology;information entropy;information extraction rules;latent semantic relations;medical data analysis;medical field;nonstructured medical text data;structured data;text structuralization;text training;word frequency;word mapping;word2vec","","","","","","","22-23 Oct. 2016","","IEEE","IEEE Conference Publications"
"Energy-Efficient Distributed Topology Control Algorithm for Low-Power IoT Communication Networks","G. Yi; J. H. Park; S. Choi","Department of Multimedia Engineering, Dongguk University, Seoul, South Korea","IEEE Access","20170123","2016","4","","9193","9203","Topology control is one of the significant research topics in traditional wireless networks. The primary purpose of topology control ensures the connectivity of wireless nodes participated in the network. Low-power Internet of Things communication networks look like wireless network environments in which the main communication devices are wireless devices with limited energy like battery. In this paper, we propose a distributed topology control algorithm by merging the combinatorial block design from a design theory with the multiples of 2. The proposed technique especially focuses on asynchronous and asymmetric neighbor discovery. The concept of block design is used to generate the neighbor discovery schedule when a target duty cycle is given. In addition, the multiples of 2 are applied to overcome the challenge of the block design and support asymmetric operation. We analyze the worst case discovery latency and energy consumption numerically by calculating the total number of slots and wake-up slots based on the given duty cycle. It shows that our proposed method has the smallest total number of slots and wake-up slots among existing representative neighbor discovery protocols. The numerical analysis represents the proposed technique find neighbors quickly with minimum battery power compared with other protocols for distributed topology control. For future research direction, we could perform a simulation study or real experiment to investigate the best parameter for choosing the multiple of a certain number.","2169-3536;21693536","","10.1109/ACCESS.2016.2630715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752857","Internet of things;asymmetric duty cycle;asynchronous neighbor discovery;low-power IoT communication;machine to machine communications","Asymmetric neighbor discovery;Energy efficiency;Information retrieval;Internet of things;Low power electronics;Machine-to-machine communication;Wireless networks","Internet of Things;numerical analysis;radio networks;telecommunication control;telecommunication network topology","Internet of Things communication networks;asymmetric operation support;block design;combinatorial block design;communication devices;design theory;distributed topology control;energy efficient distributed topology control algorithm;low power IoT communication networks;neighbor discovery schedule;numerical analysis;wireless devices;wireless network environments;wireless networks;wireless nodes","","","","","","20161122","2016","","IEEE","IEEE Journals & Magazines"
"A Bidirectional Hierarchical Skip-Gram model for text topic embedding","Suncong Zheng; Hongyun Bao; J. Xu; Yuexing Hao; Zhenyu Qi; H. Hao","Institute of Automation, Chinese Academy of Sciences, 100190, Beijing, China","2016 International Joint Conference on Neural Networks (IJCNN)","20161103","2016","","","855","862","Taking advantage of the large scale corpus on the web to effectively and efficiently mine the topics within texts is an essential problem in the era of big data. We focus on the problem of learning text topic embedding in an unsupervised manner, which enjoys the properties of efficiency and scalability. Text topic embedding represents words and documents in a semantic topic space, in which the words and documents with similar topic will be embedded close to each other. When compared with conventional topic models, which implicitly capture the document-level word co-occurrence patterns, text topic embedding alleviates the data sparsity problem and captures the semantic relevance between different words and documents. To model text topic embedding, we propose a Bidirectional Hierarchical Skip-Gram model (BHSG) based on skip-gram model. BHSG includes two components: semantic generation module to learn semantic relevance between texts and topic enhance module to produce the text topic embedding based on text embedding learned in the former module. We evaluated our method on two kinds of topic-related tasks: text classification and information retrieval. The experimental results on four public datasets and one dataset we provide all demonstrate that our proposed method can achieve a better performance.","","","10.1109/IJCNN.2016.7727289","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727289","","Computational modeling;Context;Data models;Information retrieval;Neural networks;Semantics;Syntactics","Big Data;Internet;classification;data mining;information retrieval;learning (artificial intelligence);text analysis","BHSG;World Wide Web;bidirectional hierarchical skip-gram model;big data;data sparsity;information retrieval;large scale corpus;learning;semantic topic space;text classification;text topic embedding;topic mining","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Inferring the Functions of Proteins from the Interrelationships between Functional Categories","K. Taha","Electrical & Computer Engineering Department, Khalifa University, Abu Dhabi, UAE (e-mail: kamal.taha@kustar.ac.ae).","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2016","PP","99","1","1","This study proposes a new method to determine the functions of an unannotated protein. The proteins and amino acid residues mentioned in biomedical texts associated with an unannotated protein p can be considered as characteristics terms for p, which are highly predictive of the potential functions of p. Similarly, proteins and amino acid residues mentioned in biomedical texts associated with proteins annotated with a functional category f can be considered as characteristics terms of f. We introduce in this paper an information extraction system called IFP_IFC that predicts the functions of an unannotated protein p by representing p and each functional category f by a vector of weights. Each weight reflects the degree of association between a characteristic term and p (or a characteristic term and f). First, IFP_IFC constructs a network, whose nodes represent the different functional categories, and its edges the interrelationships between the nodes. Then, it determines the functions of p by employing random walks with restarts on the mentioned network. The walker is the vector of p. Finally, p is assigned to the functional categories of the nodes in the network that are visited most by the walker. We evaluated the quality of IFP_IFC by comparing it experimentally with two other systems. Results showed marked improvement.","1545-5963;15455963","","10.1109/TCBB.2016.2615608","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585111","Protein function prediction;biomedical text;information extraction;protein annotation;text mining","Amino acids;Databases;Feature extraction;Information retrieval;Proteins;Semantics","","","","","","","","20161006","","","IEEE","IEEE Early Access Articles"
"Object-oriented information extraction and evaluation of seismic damage of buildings using very high spatial resolution imagery","Y. Zhao; J. Zhang; L. Yao","China University of Geosciences in Beijing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","2853","2856","Earthquake is one of the most serious natural disasters in the world. With the continuous improvement of the spatial resolution of remote sensing image, it contains more abundant information, which can better reflect the features of geometric structure and texture information. The operation of the image is not only rely on the image element, but also use the object to carry on the operation. This paper proposes a fast building damage information extraction and evaluation of technology process which can quickly extract and evaluate the disaster area. Using the method of object oriented extracts the buildings of Yushu disaster area based on the pre-earthquake and post-earthquake QuickBird image. It has a high accuracy of 88.53% and 90.21%, respectively, and extracts the regional change information, whose change area is 15923.52m<sup>2</sup> accounted for 68.16% of the entire study area that assesses for moderate to severe disaster areas. This rapid seismic damage information extraction is a kind of practical and effective method, which is worth popularizing widely.","","","10.1109/IGARSS.2016.7729737","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729737","Building damage Extraction and evaluation;Object-oriented","Buildings;Data mining;Earthquakes;Image edge detection;Image segmentation;Information retrieval;Remote sensing","buildings (structures);disasters;earthquakes;feature extraction;geophysical image processing;remote sensing;seismology","China;Yushu disaster area;building damage information extraction;building seismic damage evaluation;natural disaster;object-oriented information extraction;post-earthquake QuickBird image;preearthquake QuickBird image;remote sensing image;texture information;very high spatial resolution imagery","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"MeGS: Partitioning Meaningful Subgraph Structures Using Minimum Description Length","S. Goebl; A. Tonch; C. Böhm; C. Plant","LMU Munich, Munich, Germany","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","889","894","How can we fully structure a graph into pieces of meaningful information? Into structures that provide us with insights and carry a meaning beyond simple clustering. How can we also exploit these patterns to compress the graph for fast transmission and easier storage? In many applications of graph analysis like network analysis or medical information extraction we are searching for special patterns. Here, it is not sufficient to extract only parts of the relevant information in a graph, but to understand the complete underlying structure. Therefore, we propose our algorithm MeGS (Partitioning Meaningful Subgraph Structures using Minimum Description Length) to fully understand how a graph is constructed. The most common primitives (clique, hub, tree, bipartite, and sparse) serve as models to split a graph into meaningful structures. Using the principle of Minimum Description Length (MDL) structure types and counts are determined by the best fitting model. These structures achieve the best compression of the adjacency matrix. As result, every node is part of exactly one structure and has an interpretable context. No unknown areas remain in the graph. The higher a model compresses its section of the graph, the stronger its match with the corresponding structural assumption. MeGS, a fast and parameter-free split-and-merge algorithm, automatically finds the optimal structures achieving the best compression. We compare to state-of-the-art algorithms to prove MeGS' ability for interpretation and compression.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837921","Graph Mining;Minimum Description Length","Channel coding;Data mining;Entropy;Information retrieval;Partitioning algorithms;Vegetation","data compression;data mining;graph theory;matrix algebra;pattern clustering","MDL structure types;MeGS;adjacency matrix compression;best fitting model;graph analysis;information extraction;meaningful subgraph structure partitioning algorithm;medical information extraction;minimum description length;network analysis;parameter-free split-and-merge algorithm;simple clustering","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Optimizing dependency parsing throughput","A. Weichselbraun; N. Süsstrunk","Department of Information, University of Applied Sciences Chur, Pulverm&#252;hlestrasse 57, Switzerland","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","20160801","2015","01","","511","516","Dependency parsing is considered a key technology for improving information extraction tasks. Research indicates that dependency parsers spend more than 95% of their total runtime on feature computations. Based on this insight, this paper investigates the potential of improving parsing throughput by designing feature representations which are optimized for combining single features to more complex feature templates and by optimizing parser constraints. Applying these techniques to MDParser increased its throughput four fold, yielding Syntactic Parser, a dependency parser that outperforms comparable approaches by factor 25 to 400.","","Electronic:978-9-8975-8164-9; POD:978-1-5090-1967-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526963","Dependency Parsing;Natural Language Processing;Performance Optimization;Throughput","Encoding;Feature extraction;Information retrieval;Optimization;Runtime;Syntactics;Throughput","","","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"String matching algorithms for reteriving information from desktop — Comparative analysis","S. Vijayarani; R. Janani","Department of Computer Science, Bharathiar University, Coimbatore","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","6","Information retrieval is the method of retrieving the knowledge relevant to an issue of curiosity. It locates the relevant documents, on the premise of user's question which includes keywords or example files. Probably the most acquainted application of information retrieval system is search engine corresponding to Web search, Desktop search, Federated search, Mobile search, Enterprise search and Social search. This research work mainly focused on the desktop search. Desktop search is the specified variant of enterprise search, where the information foundations are the files stored on a personal computer, together with email and websites established on content analysis. Content Analysis is a group of manual or computer based approaches for contextualized explanations of documents. To analyze the content the different text pattern matching algorithms are used and it is used to discover all the existences of a limited set of patterns inside an input document. Commonly these algorithms are used in several applications that include information security bio-informatics, plagiarism detection, text mining and document matching. String matching is essential for finding text patterns that are in online and offline. String matching algorithm is used to matches the pattern precisely or about in the input document. The main objective of this research work is to analyze the performance of existing string matching algorithms. For this comparison there are four algorithms are used namely, Two way algorithm, Colussi algorithm, Optimal mismatch algorithm and Maximal shift algorithm. From this analysis it is observed that the Colussi string matching algorithm gives the better result.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830233","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830233","Colussi algorithm;Desktop Search;Maximal shift algorithm;Optimal mismatch algorithm;String matching;Two way algorithm","Algorithm design and analysis;Information retrieval;Pattern matching;Performance analysis;Text mining;Time complexity;Time measurement","Internet;Web sites;bioinformatics;data mining;electronic mail;information retrieval;mobile computing;search engines;security of data;string matching","Colussi algorithm;Web search;Web sites;bioinformatics;content analysis;desktop search;document matching;e-mail;enterprise search;federated search;information retrieval;information security;knowledge retrieval;maximal shift algorithm;mobile search;optimal mismatch algorithm;plagiarism detection;search engine;social search;string matching;text mining","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Two New Term Weighting Methods for Router Syslogs Anomaly Detection","T. Tan; S. Gao; W. Yang; Y. Song; C. Lin","Sch. of Math. Sci., Univ. of Chinese Acad. of Sci., Beijing, China","2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)","20170126","2016","","","1454","1460","A Router's syslogs are a sequence of events observed and logged by the router. They have been widely used in the system security field. This paper focuses on detecting anomalous behaviors of routers by analyzing router syslogs. For syslog data pre-processing, hierarchical clustering based on counting cousin distance between event patterns is used to cluster events. In order to construct a time series, the length for a time window is set and every time window has a score related to the event clusters in it. Instead of simply treating every event cluster equally in a time window, we assign weights to every event cluster by using four term weighting methods. Two of them-inverse document frequency (IDF) and residual inverse document frequency (RIDF) methods - are widely used in the information retrieval field. Due to their drawbacks, this paper proposes two new weighting methods. The first method, IDFVAR, is a modification of the RIDF method and takes data distribution, frequency, and several additional factors into consideration. In the second method, IDFJMP, JMP value is proposed to evaluate the degree of sudden change of an event cluster. In order to compare those term weighting methods, experiments are done on an estimated data set. Then, we detect anomalous behaviors by using a method derived from standard deviation on the chosen time series. Finally, we conduct experiments on real router syslogs.","","Electronic:978-1-5090-4297-5; POD:978-1-5090-4298-2","10.1109/HPCC-SmartCity-DSS.2016.0207","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828548","anomaly detection;log analysis;network security;router syslog;term weighting method;time series","Conferences;Electronic mail;IP networks;Information retrieval;Roads;Standards;Time series analysis","Internet;computer network security;statistical analysis;telecommunication network routing;telecommunication traffic;time series","Internet traffic;RIDF;hierarchical clustering;residual inverse document frequency;router syslog anomaly detection;syslog data preprocessing;system security;term weighting method;time series","","","","","","","12-14 Dec. 2016","","IEEE","IEEE Conference Publications"
"An exploration on lexical analysis","Farhanaaz; V. Sanju","Department of Computer Science, Aditya Institute of Management Studies and Research, Bangalore, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","20161124","2016","","","253","258","The word lexical in lexical analysis, its meaning is extracted from the word “lexeme”. Lexeme is an abstract unit of morphological analysis in linguistics. A lexical analyser is used in various applications like text editors, information retrieval system, pattern recognition programs and language compilers. But we limit our discussion in this paper to language compilers. Lexical analyser being the first phase of the compilation process it deals with the processing of input language. Discussion is also extended to multi-core environment.","","CD:978-1-4673-9936-4; DVD:978-1-4673-9937-1; Electronic:978-1-4673-9939-5; POD:978-1-4673-9940-1; USB:978-1-4673-9938-8","10.1109/ICEEOT.2016.7755127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755127","Finite Automata;Lexeme;Regular Expression;Tokenization;parallel lexical Analyser","Automata;Computational modeling;Computer science;Electronic mail;Information retrieval;Optimization;Pragmatics","program compilers","compilation process;input language processing;language compilers;lexical analysis;linguistics;morphological analysis abstract unit;multicore environment","","","","","","","3-5 March 2016","","IEEE","IEEE Conference Publications"
"Sparse representation based subpixel information extraction framework for hyperspectral remote sensing imagery","R. Feng; D. He; Y. Zhong; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","7026","7029","Sparse representation theory has become a powerful tool since it can obtain the sparsest or the unique solution for the underdetermined problem with the development of linear algebra, optimization, scientific computing and more. As subpixel information extraction encountered in hyperspectral remote sensing, which contains many mixed pixels, are famous under-determined ill-posed problem. In addition, there is no unified model to conquer the problems with the subpixel analysis techniques, i.e., spectral unmixing and subpixel mapping. To cope with this under-determined problem, a unified sparse subpixel information extraction framework was proposed in this paper, which connects sparse unmixing and sparse subpixel mapping methods in a unified theoretical system as a serious of sparse regression problem. The experimental results with hyperspectral images indicate that the proposed sparse representation framework outperforms the previous subpixel analysis approaches, hence, provides an effective option for subpixel information extraction idea for hyperspectral remote sensing imagery.","","","10.1109/IGARSS.2016.7730832","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730832","Sparse representation;hyperspectral image;remote sensing;subpixel information extraction","Hyperspectral imaging;Information retrieval;Libraries;Optimization;Standards","geophysical techniques;hyperspectral imaging;remote sensing","hyperspectral remote sensing imagery;linear algebra;linear optimization;mixed pixels;scientific computing;sparse regression problem;sparse representation theory;sparse subpixel mapping method;spectral unmixing method;subpixel analysis approaches;subpixel analysis techniques;under-determined ill-posed problem;unified sparse subpixel information extraction framework;unified theoretical system","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"View-based text representation","C. Koleejan; X. Gao","School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand","2016 IEEE Congress on Evolutionary Computation (CEC)","20161121","2016","","","263","270","Document clustering is useful for many research areas such as Text Mining and Information Retrieval. Therefore, it is desirable to be able to cluster documents accurately. The clustering quality depends not only on the clustering algorithm used but also on the way text is represented in the algorithm. Text is typically represented using the All-Words Vector Space Model in text mining applications. However, this representation's effectiveness is limited when clustering by a specific criterion or view, such as geographical location, and also tends to be computationally expensive due to not reducing the feature space. This paper presents a new representation, the View-Based Vector Space Model, which models a document differently depending on a given view, and is computationally efficient. The basic representation requires predefined ontologies to be available. In order to improve the usefulness of this model, a new text representation learning method which uses Particle Swarm Optimisation to construct ontologies automatically given a set of documents is also presented.","","Electronic:978-1-5090-0623-6; POD:978-1-5090-0624-3; USB:978-1-5090-0622-9","10.1109/CEC.2016.7743804","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743804","","Clustering algorithms;Computational modeling;Information retrieval;Ontologies;Optimization;Particle swarm optimization;Text mining","learning (artificial intelligence);ontologies (artificial intelligence);particle swarm optimisation;pattern clustering;text analysis","all-words vector space model;document clustering;feature space;geographical location;information retrieval;ontologies;particle swarm optimisation;text mining;text representation learning method;view-based text representation;view-based vector space model","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"A data-driven approach to chord similarity and chord mutability","D. Bountouridis; H. V. Koops; F. Wiering; R. C. Veltkamp","Dept. of Inf. & Comput. Sci., Utrecht Univ., Utrecht, Netherlands","2016 IEEE Second International Conference on Multimedia Big Data (BigMM)","20160818","2016","","","275","278","Assessing the relationship between chord sequences is an important ongoing research topic in the fields of music cognition and music information retrieval. Heuristic and cognitive models of chord similarity have been investigated but none has aimed to capture the collective perception of chord similarity from a large dataset of user-generated content. Devising a largescale experiment to gather sufficient data from human subjects has always been a major stumbling block. We present a novel chord similarity model based on a large amount of crowd-sourced transcriptions from a popular automatic chord estimation service. We show that our model outperforms heuristic-based models in a song identification task. Secondly, a model of chord mutations based on a large amount of crowd-sourced cover songs transcriptions is introduced. From crowd-sourced data, we create substitution matrices that capture the perceived similarity and mutability between chords. These results show that modelling the collective perception can not only substitute alternative, sophisticated models but also further enhance performance in various music information retrieval tasks.","","Electronic:978-1-5090-2179-6; POD:978-1-5090-2180-2","10.1109/BigMM.2016.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545037","","Computational modeling;Context;Context modeling;Mathematical model;Matrices;Music information retrieval;User-generated content","data mining;information retrieval;music","automatic chord estimation service;chord mutability;chord mutations;chord sequences;chord similarity model;cognitive models;crowd-sourced cover songs transcriptions;crowd-sourced data;crowd-sourced transcriptions;heuristic-based models;music cognition;music information retrieval tasks;song identification task;user-generated content","","","","","","","20-22 April 2016","","IEEE","IEEE Conference Publications"
"A Novel System Architecture for Ontology Matching","X. Xue; J. Wang; A. Ren","Fujian Provincial Key Lab. of Big Data Min. & Applic., Fujian Univ. of Technol., Fuzhou, China","2016 12th International Conference on Computational Intelligence and Security (CIS)","20170119","2016","","","100","102","In recent years, many ontology matching systems have been developed to deal with the semantic heterogeneity problem in ontology engineering by taking into account various aspects of this problem. Due to the high heterogeneity of ontologies, a combination of many methods is necessary to correctly determine the semantic correspondences between ontology elements. Moreover, difficulties can arise not only inside matcher combination but also in the mapping extraction process which might directly affect the quality of the final alignment. In this paper, we propose a novel ontology matching system architecture, and on this basis, we further present the details of two kernel models inside, i.e. ontology matching module and mapping extraction module. The experimental results show that the mean recall and precision of our ontology matching system are generally high, and the mean f-measure of the alignments obtained by our proposal outperforms all other state of the art matching systems.","","Electronic:978-1-5090-4840-3; POD:978-1-5090-4841-0","10.1109/CIS.2016.0031","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820422","mapping extraction;matcher combination;ontology matching system","Feature extraction;Information retrieval;Kernel;Ontologies;Proposals;Semantics;Systems architecture","ontologies (artificial intelligence);pattern matching","mapping extraction process;mean f-measure;mean recall;ontology elements;ontology engineering;ontology matching system architecture;precision;semantic correspondences;semantic heterogeneity","","","","","","","16-19 Dec. 2016","","IEEE","IEEE Conference Publications"
"Stemming versus multi-words indexing for Arabic documents classification","M. S. E. Bazzi; T. Zaki; D. Mammass; A. Ennaji","IRF-SIC Laboratory, Faculty of sciences, Ibn Zohr University, Agadir, Morocco","2016 11th International Conference on Intelligent Systems: Theories and Applications (SITA)","20161208","2016","","","1","5","Documents indexing is the main step in a conventional document classification or information retrieval framework. This study aims to highlight the influence of features' type on the efficiency of a classification system. Empirical results on Arabic dataset reveal that the choice of extracted feature's type has a significant impact on conserving semantic information and improving classification accuracy, especially with the morphological complexity of the Arabic language. Precision, recall, F-measure and accuracy are the metrics adopted to compare the efficiency of the proposed indexing system.","","Electronic:978-1-5090-5781-8; POD:978-1-5090-5782-5","10.1109/SITA.2016.7772288","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7772288","TF-IDF;documents classification;multi-words;semantic;text indexing","Feature extraction;Indexing;Information retrieval;Semantics;Support vector machines;Text categorization;Text mining","classification;document handling;indexing;information retrieval;natural language processing","Arabic dataset;Arabic documents classification;Arabic language;F-measure;document classification;documents indexing;information retrieval framework;morphological complexity;multiwords indexing;stemming","","","","","","","19-20 Oct. 2016","","IEEE","IEEE Conference Publications"
"A high-performance persistent identification concept","F. Berber; P. Wieder; R. Yahyapour","Gesellschaft fur wissenschaftliche Datenverarbeitung Gottingen, Gottingen, Germany","2016 IEEE International Conference on Networking, Architecture and Storage (NAS)","20160825","2016","","","1","10","The immense research dataset growth requires new strategies and concepts for an appropriate handling at the corresponding research data repositories. This is especially true for the concept of Persistent Identifiers (PIDs), which is designed to provide a persistent identification layer on top of the address-based resource retrieval methodology of the current Internet. For research datasets, which are referenced, further processed and transferred, such a persistent identification concept is highly crucial for ensuring a long-term scientific exchange. Often, each individual research dataset stored in a research data repository, is registered at a particular PID registration agency in order to be assigned a globally unique PID. However, for the explosive growth of research datasets this concept of registering each individual research dataset is in terms of the performance highly inappropriate. Therefore, the focus of this work is on a concept for enabling a high-performance persistent identification of research datasets. Recent research data repositories often are equipped with a built-in naming component for assigning immutable and internally unique identifiers for their incoming research datasets. Thus, the core idea in this work is to enable these internal identifiers to be directly resolvable at the well-known global PID resolution systems. This work will therefore provide insight into the implementation of this idea into the well-known Handle System. Finally, in this work, we will provide an experimental evaluation of our proposed concept.","","Electronic:978-1-5090-3315-7; POD:978-1-5090-3316-4","10.1109/NAS.2016.7549387","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7549387","","Access control;Information resources;Information retrieval","data handling;information retrieval;persistent objects;storage management","Internet;PID registration agency;address-based resource retrieval methodology;data repository;dataset growth;global PID resolution systems;handle system;high-performance persistent identification;internal identifiers;long-term scientific exchange;persistent identification layer;research datasets","","","","","","","8-10 Aug. 2016","","IEEE","IEEE Conference Publications"
"Thematic information extraction in high-resolution remote sensing image based on weighted PCA and VBICA","L. Liu; C. Li; Y. Lei; J. Zhao; X. Sun","School of Computer Engineering and Science, Shanghai University, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","760","765","The thematic information extraction has been a difficult problem in high-resolution remote sensing application. Principal component analysis (PCA) is able to extract data's independent features on the basis of the second-order statistics, the variational Bayesian independent component analysis (VBICA) not only overcome the inconsistency between the standard ICA model and remote sensing image but also decrease the computational complexity. In view of the characteristics of high-resolution remote sensing, a thematic information extraction method based on weighted PCA and VBICA is presented in this article, and IKONOS high-resolution remote sensing image experiments are performed. The result shows that the classification accuracy of proposed method reaches 78.30% under certain conditions with the suitable number of eigenvectors and weighted values.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846612","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846612","High-resolution remote sensing;IKONOS image;PCA;VBICA;information extraction","Bayes methods;Earth;Feature extraction;Information retrieval;Principal component analysis;Remote sensing;Training","Bayes methods;computational complexity;feature extraction;geophysical image processing;higher order statistics;image resolution;independent component analysis;principal component analysis;remote sensing;variational techniques","IKONOS high-resolution remote sensing image;VBICA;computational complexity;data independent feature extraction;principal component analysis;second-order statistics;standard ICA model;thematic information extraction method;variational Bayesian independent component analysis;weighted PCA","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"Performance analysis and simulation of 3D information deriving from InSAR with different baselines","H. Zhang; G. Jin; Q. Xu; Y. Zhang","Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","20161024","2016","","","1921","1925","Interferometric Synthetic Aperture Radar (InSAR) can derive accurate 3D information with image acquiring all time and all weather. But the accuracy of derived 3D information is heavily determined by the length of perpendicular baseline. We know that the longer baseline is, the more accurate Digital Elevation Model (DEM) can be derived in condition of other parameters are the same. Meanwhile the phase unwrapping is more difficult and the processing of InSAR is getting more complicated. Especially the length of baseline cannot longer than critical baseline. On the contrary, if the baseline is shorter, the processing of InSAR is easier, but the accuracy of derived DEM is lower. In order to show the performance of 3D information deriving from InSAR with different baselines, interferograms of InSAR with three different baselines are simulated and 3D information are derived in this paper. The frequencies and the coherence maps of their fringes are apparently different. The non-fuzzy height value and critical baseline are deduced to analyze the impact of baseline to InSAR performance theoretically. With the experimental results and the theoretical analysis, the facts in processing difficulty and accuracy for single baseline InSAR in mountainous areas are demonstrated, as can explain why the multi-baseline InSAR technique should be adopted to derive accurate DEM in mountainous areas robustly.","","","10.1109/FSKD.2016.7603473","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603473","InSAR;accuracy;baseline;simulation;three dimensional information","Antennas;Coherence;Geometry;Information retrieval;Robustness;Solid modeling;Three-dimensional displays","geophysical image processing;radar imaging;radar interferometry;synthetic aperture radar","3D information;InSAR performance;coherence maps;critical baseline;derived DEM accuracy;digital elevation model;interferometric synthetic aperture radar;mountainous area;multibaseline InSAR technique;nonfuzzy height value;perpendicular baseline;phase unwrapping;theoretical analysis","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conference Publications"
"Using relation similarity on open information extraction-based event template extraction","A. Romadhony; D. H. Widyantoro; A. Purwarianti","School of Electrical Engineering and Informatics Bandung, Institute of Technology, Bandung, Indonesia","2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20170309","2016","","","341","346","Automatic template extraction has been studied intensively in order to perform information extraction without predefined template. Several existing studies utilized the similar preprocessing techniques which are applied in Open Information Extraction (Open IE) paradigm system. We investigate the use of Open IE results to build the automatic event template extraction. In this study, we adapt the clustering based approach for template extraction, and propose to add the relation similarity information in the clustering function. We compare the clusters quality of the Open IE based system and non-Open IE based system and also with the use of relation similarity function using document classification metric. The experimental result shows that the performance of Open IE based system is comparable with the non-Open IE based system and the relation similarity information is able to improve the clusters quality.","","Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7","10.1109/ICACSIS.2016.7872791","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872791","Cluster Evaluation;Event Template Extraction;Open Information Extraction;Relation Similarity;phrase-to-phrase","Clustering methods;Data mining;Data preprocessing;Filtering;Information retrieval;Semantics;Training data","classification;document handling;pattern clustering","Open IE paradigm system;automatic template extraction;clustering based approach;document classification metric;event template extraction;open information extraction;relation similarity","","","","","","","15-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Automatic summarization of the Arabic documents using NMF: A preliminary study","A. A. Mohamed","Prince Sattam bin Abdulaziz University, Kingdom of Saudi Arabia","2016 11th International Conference on Computer Engineering & Systems (ICCES)","20170119","2016","","","235","240","The exponential growth of the Internet produces a huge amount of documents online. Finding the desired documents from amongst these huge resources is a difficult task. This problem is known as “Information Overloading”. Automatic Text Summarization techniques (ATS) try to solve this problem by extracting the essential sentences that cover most of the main issues in the document. So the user will spend less time and effort to identify the main ideas of the document. Research in this field in the Arabic language is relatively new compared with the available research in English. This paper presents a preliminary study that investigates the effectiveness of using Non negative Matrix Factorization (NMF) algorithm to summarize the Arabic documents. The researcher of the present study has built an Arabic corpus of 150 documents manually and conducted extensive experiments by using different sentences scoring algorithms and term weighting schemes. The performance of the proposed algorithm has been measured, and the extensive experiments have shown that the NMF algorithm yields promising results.","","Electronic:978-1-5090-3267-9; POD:978-1-5090-3268-6; USB:978-1-5090-3266-2","10.1109/ICCES.2016.7822007","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822007","Arabic Text Summarization;Information Retrieving;Natural Language Processing (NLP);Non negative Matrix Factorization (NMF);Text Mining","Entropy;Information retrieval;Mathematical model;Matrix decomposition;Pragmatics;Semantics;Text mining","Internet;matrix decomposition;natural language processing;text analysis","ATS;Arabic corpus;Arabic documents;Arabic language;English;Internet;NMF;automatic text summarization techniques;information overloading;nonnegative matrix factorization;sentences scoring algorithms;term weighting schemes","","","","","","","20-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"The reverse doubling construction","J. F. Viaud; K. Bertet; C. Demko; R. Missaoui","Laboratory L3i, University of La Rochelle, France","2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)","20160801","2015","01","","350","357","It is well known inside the Formal Concept Analysis (FCA) community that a concept lattice could have an exponential size in the data. Hence, the size of concept lattices is a critical issue in the presence of large real-life data sets. In this paper, we propose to investigate factor lattices as a tool to get meaningful parts of the whole lattice. These factor lattices have been widely studied from the early theory of lattices to more recent work in the FCA field. This paper contains two parts. The first one gives background about lattice theory and formal concept analysis, and mainly compatible sub-contexts, arrow-closed sub-contexts and congruence relations. The second part presents a new decomposition called “reverse doubling construction” that exploits the above three notions used for the doubling convex construction investigated by Day. Theoretical results and their proofs are given as well as an illustrative example.","","Electronic:978-9-8975-8164-9; POD:978-1-5090-1967-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526940","Arrow Closed Subcontext;Arrow Relation;Compatible Subcontext;Concept Lattice;Congruence Relation;Doubling Convex;Factor Lattice","Context;Databases;Formal concept analysis;Information retrieval;Knowledge discovery;Lattices;Upper bound","","","","","","","","","12-14 Nov. 2015","","IEEE","IEEE Conference Publications"
"Binary Adaptive Embeddings From Order Statistics of Random Projections","D. Valsesia; E. Magli","Politecnico di Torino, Torino, Italy","IEEE Signal Processing Letters","20170105","2017","24","1","111","115","We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space.","1070-9908;10709908","","10.1109/LSP.2016.2639036","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782321","Binary embeddings;random projections","DH-HEMTs;Distortion;Geometry;Hamming distance;Information retrieval;Quantization (signal);Random variables","signal processing;statistical analysis","analytical standpoint;binary adaptive embeddings;binary embedding;order statistics;random projections;reduced dimensionality space;reference signal","","","","","","20161213","Jan. 2017","","IEEE","IEEE Journals & Magazines"
"Classification and Retrieval of Medical Images in an Integrated Healthcare Environment","A. B. Jose; M. do Carmo dos Reis; J. F. Camapum; H. S. Carvalho; D. F. Vasconcelos; A. F. da Rocha; T. M. G. de A. Barbosa","Department of Electrical Engineering, University of Bras&#237;lia, 70919-970 Brasil. phone: +55 61 2735977; fax: +55 61 2746651; e-mail: abjucg@gmail.com","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","20161215","2006","","","4889","4892","This work presents a new approach for classification and retrieval of echocardiographic images from textual information of the anatomical structures and diagnosis features. These textual attributes will be acquired from the electronic medical report generated in an integrated healthcare environment. The medical report is provided by a specialist in the area during the analysis of the medical image stored in a PACS environment. Such innovation guarantees a more accurate classifier and a better optimization of the medical work, since the medical report and the attributes for the medical image classifier will be created at the same time. The system is being developed in the University Hospital of the University of Brasilia","1557-170X;1557170X","CD:1-4244-003303; Paper:1-4244-0032-5","10.1109/IEMBS.2006.260355","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4462897","","Anatomical structure;Biomedical imaging;Hospitals;Image analysis;Image retrieval;Information retrieval;Medical diagnostic imaging;Medical services;Picture archiving and communication systems;Technological innovation","PACS;echocardiography;health care;image classification;image retrieval;medical image processing","PACS environment;diagnosis features;echocardiography;electronic medical report;integrated healthcare environment;medical image analysis;medical image classification;medical image classifier;medical image retrieval;textual anatomical structure information","Brazil;Computer Graphics;Computer Security;Database Management Systems;Delivery of Health Care;Delivery of Health Care, Integrated;Hospital Information Systems;Humans;Information Storage and Retrieval;Medical Records Systems, Computerized;Programming Languages;Radiology Information Systems;Software;Systems Integration;User-Computer Interface","","","","","","Aug. 30 2006-Sept. 3 2006","","IEEE","IEEE Conference Publications"
"Edge-Based Registration-Noise Estimation in VHR Multitemporal and Multisensor Images","Y. Han; F. Bovolo; L. Bruzzone","Center for Information and Communication Technology, Fondazione Bruno Kessler, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","20160805","2016","13","9","1231","1235","Even after coregistration, very high resolution (VHR) multitemporal images acquired by different multispectral sensors (e.g., QuickBird and WordView) show a residual misregistration due to dissimilarities in acquisition conditions and in sensor properties. Residual misregistration can be considered as a source of noise and is referred to as registration noise (RN). Since RN is likely to have a negative impact on multitemporal information extraction, detecting and reducing it can increase multitemporal image processing accuracy. In this letter, we propose an approach to identify RN between VHR multitemporal and multisensor images. Under the assumption that dominant RN mainly exists along boundaries of objects, we propose to use edge information in high frequency regions to estimate it. This choice makes RN detection less dependent on radiometric differences and thus more effective in VHR multisensor image processing. In order to validate the effectiveness of the proposed approach, multitemporal multisensor data sets are built including QuickBird and WorldView VHR images. Both qualitative and quantitative assessments demonstrate the effectiveness of the proposed RN identification approach compared to the state-of-the-art one.","1545-598X;1545598X","","10.1109/LGRS.2016.2577719","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501538","Coregistration;multitemporal multisensor data;registration noise (RN);very high resolution (VHR) images","Image edge detection;Image sensors;Information retrieval;Radiometry;Sensors;Spatial resolution","feature extraction;geophysical image processing;image registration;remote sensing","QuickBird VHR images;VHR multisensor image processing;VHR multisensor images;VHR multitemporal images;WorldView VHR images;acquisition conditions;edge information;edge-based registration-noise estimation;multitemporal image processing;multitemporal information extraction;registration noise;residual misregistration;sensor properties","","","","","","20160628","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Natural language processing pipeline for temporal information extraction and classification from free text eligibility criteria","G. Parthasarathy; A. Olmsted; P. Anderson","Department of Computer Science, College of Charleston, Charleston, SC","2016 International Conference on Information Society (i-Society)","20170216","2016","","","120","121","Automation of information extraction from eligibility criteria will provide a breakthrough in effective utilization of information for patient search in clinical databases. A majority of eligibility criteria contain temporal information associated with medical conditions and events. This project creates a novel natural language processing (NLP) pipeline for extraction and classification of temporal information as historic, current and planned from free-text eligibility criteria. The pipeline uses pattern learning algorithms for extracting temporal information and trained Random Forest classifier for classification. The pipeline achieved an accuracy of 0.82 in temporal data detection and classification with an average precision of 0.83 and recall of 0.80 in temporal data classification.","","Electronic:978-1-9083-2061-2; POD:978-1-5090-2545-9; USB:978-1-9083-2062-9","10.1109/i-Society.2016.7854192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7854192","Eligibility criteria;Natural Language Processing;Random Forest Classifier;Temporal Information Extraction","Classification algorithms;Data mining;Information retrieval;Natural language processing;Pipelines;Training;Training data","electronic health records;feature extraction;information retrieval;learning (artificial intelligence);natural language processing;pattern classification","clinical databases;free text eligibility criteria;information classification;information extraction automation;medical conditions;natural language processing pipeline;patient search;pattern learning algorithms;temporal data detection;temporal information extraction;trained random forest classifier","","","","","","","10-13 Oct. 2016","","IEEE","IEEE Conference Publications"
"Multiple Ordinal Regression by Maximizing the Sum of Margins","O. C. Hamsici; A. M. Martinez","Qualcomm Research, San Diego, CA, USA","IEEE Transactions on Neural Networks and Learning Systems","20160915","2016","27","10","2072","2083","Human preferences are usually measured using ordinal variables. A system whose goal is to estimate the preferences of humans and their underlying decision mechanisms requires to learn the ordering of any given sample set. We consider the solution of this ordinal regression problem using a support vector machine algorithm. Specifically, the goal is to learn a set of classifiers with common direction vectors and different biases correctly separating the ordered classes. Current algorithms are either required to solve a quadratic optimization problem, which is computationally expensive, or based on maximizing the minimum margin (i.e., a fixed-margin strategy) between a set of hyperplanes, which biases the solution to the closest margin. Another drawback of these strategies is that they are limited to order the classes using a single ranking variable (e.g., perceived length). In this paper, we define a multiple ordinal regression algorithm based on maximizing the sum of the margins between every consecutive class with respect to one or more rankings (e.g., perceived length and weight). We provide derivations of an efficient, easy-to-implement iterative solution using a sequential minimal optimization procedure. We demonstrate the accuracy of our solutions in several data sets. In addition, we provide a key application of our algorithms in estimating human subjects' ordinal classification of attribute associations to object categories. We show that these ordinal associations perform better than the binary one typically employed in the literature.","2162-237X;2162237X","","10.1109/TNNLS.2015.2477321","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7308086","Computer vision;direct attribute prediction (DAP);object classification;ordinal regression;sequential minimal optimization (SMO);support vector machine (SVM)","Accuracy;Information retrieval;Iterative methods;Learning systems;Optimization;Support vector machines;Training","computer vision;image classification;regression analysis;support vector machines","attribute associations;classifiers;computer vision;decision mechanisms;direction vectors;human preference estimation;multiple ordinal regression algorithm;object categories;object classification;ordered classes;ordinal associations;ordinal classification;ordinal regression problem;ordinal variables;sequential minimal optimization;sum-of-margins maximization;support vector machine algorithm","","","","","","20151027","Oct. 2016","","IEEE","IEEE Journals & Magazines"
"IXAmed-IE: On-line medical entity identification and ADR event extraction in Spanish","A. Casillas; A. D. de Ilarraza; K. Fernandez; K. Gojenola; M. Oronoz; A. Pérez; S. Santiso","IXA group (http://ixa.si.ehu.eus). Computer Engineering Faculty; P. Manuel Lardizabal, 1; 20018 Donostia, Spain","2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20170119","2016","","","846","849","This work presents an on-line system developed for medical information extraction. The goal is to provide a web-based service addressed to the medical community for the efficient processing of electronic health records in Spanish and support the clinical decision making process. This tool assists in the identification of medical entities as well as cause-effect reactions in order to help professionals both to prevent and to document adverse drug events. So far, the prototype is in its early stage of testing and validation by experts from the Galdakao-Usansolo and Basurto hospitals from the Basque Sanitary System (Osakidetza).","","Electronic:978-1-5090-1611-2; POD:978-1-5090-1612-9","10.1109/BIBM.2016.7822636","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822636","","Diseases;Drugs;Electronic medical records;Hospitals;Information retrieval;Prototypes;Semantics","Web sites;decision making;electronic health records;medical computing","ADR event extraction;Basque Sanitary System;Basurto hospitals;Galdakao-Usansolo hospitals;IXAmed-IE;clinical decision making process;electronic health records;medical community;medical information extraction;on-line medical entity identification;web-based service","","","","","","","15-18 Dec. 2016","","IEEE","IEEE Conference Publications"
"Enhanced Topic Identification Algorithm for Arabic Corpora","A. Alsaad; M. Abbod","Dept. of Electron. & Comput. Eng., Brunel Univ., Uxbridge, UK","2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim)","20160926","2015","","","90","94","During the past few years, the construction of digitalized content is rapidly increasing, raising the demand of information retrieval, data mining and automatic data tagging applications. There are few researches in this field for Arabic data due to the complex nature of Arabic language and the lack of standard corpora. In addition, most work focuses on improving Arabic stemming algorithms, or topic identification and classification methods and experiments. No work has been conducted to include an efficient stemming method within the classification algorithm, which would lead to more efficient outcome. In this paper, we propose a new approach to identify significant keywords for Arabic corpora. That is done by implementing advanced stemming and root extraction algorithm, as well as Term Frequency/Inverse Document Frequency (TFIDF) topic identification method. Our results show that combining advanced stemming, root extraction and TFIDF techniques, lead to extracting a highly significant terms represented by Arabic roots. These roots weights higher TFIDF values than terms extracted without the use of advanced stemming and root extraction methods. Decreasing the size of indexed words and improving the feature selection process.","","","10.1109/UKSim.2015.77","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576527","root extraction; feature selection; topic identification; natural language processing; data mining; text mining","Algorithm design and analysis;Classification algorithms;Feature extraction;Information retrieval;Internet;Text categorization","data mining;information retrieval;natural language processing;pattern classification;text analysis","Arabic corpora;Arabic data;Arabic language;Arabic roots;Arabic stemming algorithms;TFIDF techniques;automatic data tagging applications;classification algorithm;data mining;digitalized content construction;efficient stemming method;feature selection process;information retrieval;root extraction algorithm;root extraction methods;term frequency-inverse document frequency topic identification method;topic identification algorithm enhancement","","","","","","","25-27 March 2015","","IEEE","IEEE Conference Publications"
"SparseHash: Embedding Jaccard coefficient between supports of signals","D. Valsesia; S. M. Fosson; C. Ravazzi; T. Bianchi; E. Magli","Politecnico di Torino - DET, Italy","2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","20160926","2016","","","1","6","Embeddings provide compact representations of signals to be used to perform inference in a wide variety of tasks. Random projections have been extensively used to preserve Euclidean distances or inner products of high dimensional signals into low dimensional representations. Different techniques based on hashing have been used in the past to embed set similarity metrics such as the Jaccard coefficient. In this paper we show that a class of random projections based on sparse matrices can be used to preserve the Jaccard coefficient between the supports of sparse signals. Our proposed construction can be therefore used in a variety of tasks in machine learning and multimedia signal processing where the overlap between signal supports is a relevant similarity metric. We also present an application in retrieval of similar text documents where SparseHash improves over MinHash.","","","10.1109/ICMEW.2016.7574678","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574678","Embedding;Jaccard coefficient;MinHash;random projections;sparse matrices","Europe;Geometry;Indexes;Information retrieval;Measurement;Signal processing;Sparse matrices","file organisation;information retrieval;learning (artificial intelligence);signal representation;sparse matrices;text analysis","Euclidean distance;Jaccard coefficient;SparseHash;machine learning;multimedia signal processing;random projection;set similarity metric;signal representation;sparse matrix;text documents retrieval","","2","","","","","11-15 July 2016","","IEEE","IEEE Conference Publications"
"Information extraction for scholarly digital libraries","K. Williams; J. Wu; Z. Wu; C. L. Giles","Information Sciences and Technology, Pennsylvania State University, University Park, PA 16802, USA","2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)","20160905","2016","","","287","288","Scholarly documents contain many data entities, such as titles, authors, affiliations, figures, and tables. These entities can be used to enhance digital library services through enhanced metadata and enable the development of new services and tools for interacting with and exploring scholarly data. However, in a world of scholarly big data, extracting these entities in a scalable, efficient and accurate manner can be challenging. In this tutorial, we introduce the broad field of information extraction for scholarly digital libraries. Drawing on our experience in running the Cite-SeerX digital library, which has performed information extraction on over 7 million academic documents, we argue for the need for automatic information extraction, describe different approaches for performing information extraction, present tools and datasets that are readily available, and describe best practices and areas of research interest.","","Electronic:978-1-4503-4229-2; POD:978-1-5090-5254-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559627","Information extraction;digital libraries;scholarly big data","Best practices;Big data;Data mining;Information retrieval;Libraries;Metadata;Tutorials","digital libraries;information retrieval;meta data","Cite-SeerX digital library;automatic information extraction;enhanced metadata and","","","","","","","19-23 June 2016","","IEEE","IEEE Conference Publications"
"Generating Headline Candidates for News Articles","N. Okumura; T. Miura","Dept..of Adv. Sci., HOSEI Univ., Koganei, Japan","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","20161219","2016","","","355","360","In this investigation, we propose a new method to estimate headlines to news articles. Very often, in news articles, headlines contain characteristic expressions specific to their contents. However, conventional approaches may extract keywords or patterns from article bodies, and put them into well-forms. However we can hardly obtain the characteristic expressions. Here we examine both news articles and the headlines separetely, give bridge between the two documents in such a way that similar articles carry similar headlines. To do that we examine latent semantics over articles to decide similarity, and generate headline candidates semi-automatically.","","Electronic:978-1-5090-3207-5; POD:978-1-5090-3208-2","10.1109/IRI.2016.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785763","Headline Estimation;News Articles;Query by Latent Semantics","Dogs;Electronic mail;Estimation;Feature extraction;Information retrieval;Matrix decomposition;Semantics","natural language processing;query processing","characteristic expressions;headline candidate generation;headline estimation;keyword extraction;latent semantics;news articles;pattern extraction;query","","","","","","","28-30 July 2016","","IEEE","IEEE Conference Publications"
"An Active Learning Approach to Audio-to-Score Alignment Using Dynamic Time Warping","C. H. Chuan","Sch. of Comput., Univ. of North Florida, Jacksonville, FL, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","796","799","We propose an integrated system using active learning for audio-to-score alignment. Audio-to-score alignment is a fundamental task in music information retrieval. Although various machine learning techniques have been applied to this task, it is not the case for active learning. To show how beneficial active learning is in audio-to-score alignment, we demonstrate a system that integrates it with dynamic time warping, a commonly used algorithm for time series alignment. We propose a simple parametric model for selecting queries-a crucial step in active learning. We evaluate the system using synthesized audio as well as real performances. The alignment accuracy is improved with a range from 20% to 50% using only less than 10% query instances, a promising result that hopefully can inspire the creation of a collaborative framework between human and machine for audio-to-score alignment in the future.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0142","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838246","active learning;audio-to-score alignment;dynamic time warping","Acoustics;Algorithm design and analysis;Feature extraction;Instruments;Music information retrieval;Spectrogram;Uncertainty","audio signal processing;learning (artificial intelligence);music;query processing","active learning approach;audio synthesis;audio-to-score alignment;dynamic time warping;integrated system;machine learning techniques;music information retrieval;query instances;query selection;simple parametric model;time series alignment","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Query-Based Evolutionary Graph Cuboid Outlier Detection","A. Dalmia; M. Gupta; V. Varma","","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","85","92","Graph-OLAP is an online analytical framework which allows us to obtain various projections of a graph, each of which helps us view the graph along multiple dimensions and multiple levels. Given a series of snapshots of a temporal heterogeneous graph, we aim to find interesting projections of the graph which have anomalous evolutionary behavior. Detecting anomalous projections in a series of such snapshots can be helpful for an analyst to understand the regions of interest from the temporal graph. Identifying such semantically related regions in the graph allows the analyst to derive insights from temporal graphs which enables her in making decisions. While most of the work on temporal outlier detection is performed on nodes, subgraphs and communities, we are the first to propose detection of evolutionary graph cuboid outliers. Further, we perform this detection in a query sensitive manner. Thus, an evolutionary graph cuboid outlier is a projection (or cuboid) of a snapshot of the temporal graph such that it contains an unexpected number of matches for the query with respect to other cuboids both in the same snapshot as well as in the other snapshots. Identifying such outliers is challenging because (1) the number of cuboids per snapshot could be large, and (2) number of snapshots could itself be large. We model the problem by predicting the outlier score for each cuboid in each snapshot. We propose to build subspace ensemble regression models to learn (a) the behavior of a cuboid across different snapshots, and (b) the behavior of all the cuboids in a given snapshot. Experimental results on both synthetic and real datasets show the effectiveness of the proposed algorithm in discovering evolutionary graph cuboid outliers.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0020","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836651","Evolutionary Graph Cuboid Outliers;Graph Projection Outliers;Information Networks;Outlier Detection","Conferences;Data mining;Information retrieval;Lattices;Organizations;Predictive models;Semantics","data mining;evolutionary computation;graph theory;learning (artificial intelligence);query processing;regression analysis","graph nodes;graph projections;graph-OLAP;online analytical framework;outlier score prediction;query-based evolutionary graph cuboid outlier detection;real datasets;subspace ensemble regression model;synthetic datasets;temporal heterogeneous graph;temporal outlier detection","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Semantic Bootstrapping: A Theoretical Perspective","W. Wu; H. Li; H. Wang; K. Q. Zhu","Microsoft Research, Redmond, WA","IEEE Transactions on Knowledge and Data Engineering","20170110","2017","29","2","446","457","Knowledge acquisition is an iterative process. Most previous work has focused on bootstrapping techniques based on syntactic patterns, that is, each iteration finds more syntactic patterns for subsequent extraction. However, syntactic bootstrapping is incapable of resolving the inherent ambiguities in the syntactic patterns. The precision of the extracted results is thus often poor. On the other hand, semantic bootstrapping bootstraps directly on knowledge rather than on syntactic patterns, that is, it uses existing knowledge to understand the text and acquire more knowledge. It has been shown that semantic bootstrapping can achieve superb precision while retaining good recall. Nonetheless, the working mechanism of semantic bootstrapping remains elusive. In this paper, we present a detailed analysis of semantic bootstrapping from a theoretical perspective. We show that the efficiency and effectiveness of semantic bootstrapping can be theoretically guaranteed. Our experimental evaluation results substantiate the theoretical analysis.","1041-4347;10414347","","10.1109/TKDE.2016.2619347","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600398","Algorithm;big data;information extraction;semantic bootstrapping","Cats;Data mining;Electronic mail;Information retrieval;Semantics;Syntactics","computational linguistics;knowledge acquisition","information extraction;knowledge acquisition;semantic bootstrapping","","","","","","20161019","Feb. 1 2017","","IEEE","IEEE Journals & Magazines"
"Information extraction in statistics indicator tables using rule generalizations and ontology","M. R. Bastian; A. Purwarianti","School of Electrical Engineering and Informatics, Bandung Institute of Technology","2016 International Conference on Information Technology Systems and Innovation (ICITSI)","20170220","2016","","","1","6","The main problem of rule-based information extraction technique is that the extraction rules tend to be specifically designed for specific information or document structure; hence it cannot be directly used in another without some proper modifications. Semi-structured documents like tables present another challenge to information extraction; since there are no standards on how to design it, the structure of the tables can be varying. Statistics indicator is a source of information that use tables as a means of data presentation. Statistics indicators also have a relationship concept that must be carefully identified and extracted. Generalization rules attempt to reduce effort in the extraction rule modification process by creating extraction rules in general terms. Combined with ontology, the rules can also extract the relationship between indicators. The output of this information extraction system is a database that keeps not only the data itself but also the relationship concept between indicators.","","Electronic:978-1-5090-2449-0; POD:978-1-5090-2450-6","10.1109/ICITSI.2016.7858187","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858187","general rule;information extraction;ontology;rule-based;statistics indicators;tables","Companies;Context;Data mining;Databases;Information retrieval;Layout;Ontologies","document handling;generalisation (artificial intelligence);information retrieval;knowledge based systems;ontologies (artificial intelligence);statistics","data presentation;document structure;generalizations;information extraction database;ontology;rule-based information extraction technique;statistics indicator tables","","","","","","","24-27 Oct. 2016","","IEEE","IEEE Conference Publications"
"A modified language modeling method for authorship attribution","S. Vazirian; M. Zahedi","Kharazmi International Campus, Shahrood University of Technology, Shahrood, Iran","2016 Eighth International Conference on Information and Knowledge Technology (IKT)","20161212","2016","","","32","37","This paper presents an approach to a closed-class authorship attribution (AA) problem. It is based on language modeling for classification and called modified language modeling. Modified language modeling aims to offer a solution for AA problem by Combinations of both bigram words weighting and Unigram words weighting. It makes the relation between unseen text and training documents clearer with giving extra reward of training documents; training document including bigram word as well as unigram words. Moreover, IDF value multiplied by related word probability has been used, instead of removing stop words which are provided by Stop words list. we evaluate Experimental results by four approaches; unigram, bigram, trigram and modified language modeling by using two Persian poem corpora as WMPR-AA2016-A Dataset and WMPR-AA2016-B Dataset. Results show that modified language modeling attributes authors better than other approaches. The result on WMPR-AA2016-B, which is bigger dataset, is much better than another dataset for all approaches. This may indicate that if adequate data is provided to train language modeling the modified language modeling can be a good solution to AA problem.","","Electronic:978-1-5090-4335-4; POD:978-1-5090-4336-1","10.1109/IKT.2016.7777783","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7777783","Authorship Attribution;Authorship Identification;Language Modeling;Text Processing","Data models;Information retrieval;Probabilistic logic;Probability;Semantics;Smoothing methods;Training","literature;text analysis","AA problem;IDF value;Persian poem corpora;WMPR-AA2016-A dataset;WMPR-AA2016-B dataset;bigram word weighting;closed-class authorship attribution problem;language modeling training;modified language modeling method;training document;training documents;unigram word weighting;word probability","","","","","","","7-8 Sept. 2016","","IEEE","IEEE Conference Publications"
"IQAS: Inference question answering system for handling temporal inference","Z. Neji; M. Ellouze; L. H. Belguith","Computer Science Mir@cl, FSEGS, Sfax, Tunisia","2016 International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)","20160922","2016","","","1","5","One of the most crucial problems in any Natural Language Processing (NLP) task is the representation of time. This includes applications such as Information Retrieval techniques (IR), Information Extraction (IE) and Question/answering systems (QA). This paper deals with temporal information involving several forms of inference in Arabic language.","","","10.1109/INISTA.2016.7571832","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571832","Arabic Language;Natural language processing;several forms of inference;temporal inference","Computer science;Data mining;Encyclopedias;Information retrieval;Internet;Knowledge discovery;Natural language processing","inference mechanisms;natural language processing;question answering (information retrieval)","Arabic language;IQAS;NLP;inference question answering system;information extraction;information retrieval;natural language processing;temporal inference handling;time representation","","","","","","","2-5 Aug. 2016","","IEEE","IEEE Conference Publications"
"TIR over Egyptian Hieroglyphs","E. Iglesias-Franjo; J. Vilares","Facultade de Inf., Dept. de Comput., Grupo Lengua y Soc. de la Inf. (LYS), Univ. da Coruna, A Coruna, Spain","2016 27th International Workshop on Database and Expert Systems Applications (DEXA)","20170116","2016","","","198","203","This work presents an Information Retrieval system specifically designed to manage Ancient Egyptian hieroglyphic texts taking into account their peculiarities at both the lexical and encoding level for its application in Egyptology and Digital Heritage. The tool has been made freely available to the research community under a free license and, to the best of our knowledge, is the first tool of its kind.","","Electronic:978-1-5090-3635-6; POD:978-1-5090-3636-3","10.1109/DEXA.2016.050","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816648","Archaeology;Digital Heritage;Egyptology;Text Retrieval;hieroglyphs;manuel de codage","Encoding;History;Indexing;Information retrieval;Standards;Writing","history;information retrieval","Ancient Egyptian hieroglyphic texts;TIR;digital heritage;egyptology;text information retrieval system","","","","","","","5-8 Sept. 2016","","IEEE","IEEE Conference Publications"
"Reinforcement Learning Interfaces for Biomedical Database Systems","I. Rudowsky; O. Kulyba; M. Kunin; S. Parsons; T. Raphan","Department of Computer and Information Science, Brooklyn College, City University of New York, USA. phone: 718-951-5000 x2062; fax: 718-951-4489; e-mail: rudowsky@brooklyn.cuny.edu","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","20161215","2006","","","6269","6272","Studies of neural function that are carried out in different laboratories and that address different questions use a wide range of descriptors for data storage, depending on the laboratory and the individuals that input the data. A common approach to describe non-textual data that are referenced through a relational database is to use metadata descriptors. We have recently designed such a prototype system, but to maintain efficiency and a manageable metadata table, free formatted fields were designed as table entries. The database interface application utilizes an intelligent agent to improve integrity of operation. The purpose of this study was to investigate how reinforcement learning algorithms can assist the user in interacting with the database interface application that has been developed to improve the performance of the system","1557-170X;1557170X","CD:1-4244-003303; Paper:1-4244-0032-5","10.1109/IEMBS.2006.260484","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463242","","Cities and towns;Data analysis;Database systems;Humans;Information retrieval;Intelligent agent;Laboratories;Learning;Memory;Relational databases","cooperative systems;learning (artificial intelligence);medical computing;medical information systems;meta data;neurophysiology;relational databases;user interfaces","biomedical database systems;data integrity;data storage;database interface application;free formatted fields;intelligent agent;manageable metadata table;neural function;nontextual data;reinforcement learning interfaces;relational database","Algorithms;Computational Biology;Computer Communication Networks;Computer Graphics;Computer Simulation;Database Management Systems;Databases, Factual;Humans;Learning;Medical Informatics Applications;Programming Languages;Reinforcement (Psychology);Software;Software Design;User-Computer Interface","","","","","","Aug. 30 2006-Sept. 3 2006","","IEEE","IEEE Conference Publications"
"Supporting News Article Understanding by Detecting Subject-Background Event Relations","S. Tanaka; A. Jatowt; K. Tanaka","Kyoto Univ., Kyoto, Japan","2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)","20170116","2016","","","256","263","Typically, news articles mention not just one but multiple events. These events can be classified into subject or background events. The former are events that the article is written about, while the latter are additional events referred to in order to explain the background of the subject events (e.g., causal relations, circumstances or the consequences of the main event). Background events are considered to play an important role in helping to understand articles. In this paper, we first propose to classify content of news articles into subject or background event descriptions. In the second part of the paper, we demonstrate a novel solution for improving the news article search. Based on the subject and background relationship structure between events and articles, our method outputs news articles that help with understanding of a given target article.","","Electronic:978-1-5090-4470-2; POD:978-1-5090-4471-9","10.1109/WI.2016.0044","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817061","","Context;Feature extraction;Frequency measurement;Information retrieval;Strips;Support vector machines;Visualization","information retrieval;pattern classification","news article classification;news article search;news article understanding;subject-background event relations detection","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Towards automatic generation of relevance judgments for a test collection","M. Makary; M. Oakes; F. Yamout","RIILP, University of Wolverhampton, Wolverhampton, UK","2016 Eleventh International Conference on Digital Information Management (ICDIM)","20170126","2016","","","121","126","This paper represents a new technique for building a relevance judgment list for information retrieval test collections without any human intervention. It is based on the number of occurrences of the documents in runs retrieved from several information retrieval systems and a distance based measure between the documents. The effectiveness of the technique is evaluated by computing the correlation between the ranking of the TREC systems using the original relevance judgment list (qrels) built by human assessors and the ranking obtained by using the newly generated qrels.","","Electronic:978-1-5090-2641-8; POD:978-1-5090-2642-5","10.1109/ICDIM.2016.7829763","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829763","Evaluation;document distance;occurrences;qrels;relevance judgments;test collections","Buildings;Correlation;Harmonic analysis;Indexing;Information retrieval;NIST;System performance","information retrieval;text analysis","TREC systems;Text Retrieval Conference;automatic relevance judgment generation;distance based measure;information retrieval test collections;qrels;relevance judgment list","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"Development of thai text-mining model for classifying ICD-10 TM","P. Jatunarapit; K. Piromsopa; C. Charoeanlap","Department of Orthopaedics Faculty of Medicine, Chulalongkorn University, Bangkok, Thailand","2016 8th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","20170223","2016","","","1","6","This paper presents a model for classifying ICD-10 TM using machine learning and information retrieval. The scope of this research take systematic approach for translating diagnosis from medical records to ICD-10 TM is proposed. First, an information retrieval is used to find similarity word in Thai and English diagnose. Then, machine learning approach is applied to classify ICD-10 TM by training models using Naïve Bayes algorithm. The result shows that our proposed approach can accurately classify ICD-10 TM in Thai-English diagnose at 81.41%.","","DVD:978-1-5090-2044-7; Electronic:978-1-5090-2047-8; POD:978-1-5090-2048-5","10.1109/ECAI.2016.7861163","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7861163","ICD-10 TM;IR;Text mining;Thai diagnosis;machine learning","Classification algorithms;Diseases;Information retrieval;Medical diagnostic imaging;Semantics;Support vector machines;Text mining","Bayes methods;data mining;electronic health records;information retrieval;learning (artificial intelligence);medical diagnostic computing;natural language processing;pattern classification;text analysis","English medical diagnosis;ICD-10 TM classification;Naïve Bayes algorithm;Thai medical diagnosis;Thai text-mining model;information retrieval;machine learning;medical record diagnosis translation","","","","","","","June 30 2016-July 2 2016","","IEEE","IEEE Conference Publications"
"Cosine similarity to determine similarity measure: Study case in online essay assessment","A. R. Lahitani; A. E. Permanasari; N. A. Setiawan","Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Jl. Grafika No. 2 Kampus UGM, Yogyakarta 55281, Indonesia","2016 4th International Conference on Cyber and IT Service Management","20160929","2016","","","1","6","Development of technology in educational field brings the easier ways through the variety of facilitation for learning process, sharing files, giving assignment and assessment. Automated Essay Scoring (AES) is one of the development systems for determining a score automatically from text document source to facilitate the correction and scoring by utilizing applications that run on the computer. AES process is used to help the lecturers to score efficiently and effectively. Besides it can reduce the subjectivity scoring problem. However, implementation of AES depends on many factors and cases, such as language and mechanism of scoring process especially for essay scoring. A number of methods implemented for weighting the terms from document and reaching the solutions for handling comparative level between documents answer and expert's document still defined. In this research, we implemented the weighting of Term Frequency - Inverse Document Frequency (TF-IDF) method and Cosine Similarity with the measuring degree concept of similarity terms in a document. Tests carried out on a number of Indonesian text-based documents that have gone through the stage of pre-processing for data extraction purposes. This process results is in a ranking of the document weight that have closesness match level with expert's document.","","","10.1109/CITSM.2016.7577578","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577578","Automated Essay Scoring (AES);Cosine Similarity;TF-IDF","Algorithm design and analysis;Context;Data mining;Feature extraction;Frequency measurement;Information retrieval;Media","document handling;educational administrative data processing","Indonesian text-based document;TF-IDF method;automated essay scoring;cosine similarity;data extraction;educational field;inverse document frequency;online essay assessment;similarity measure;subjectivity scoring problem;term frequency","","","","","","","26-27 April 2016","","IEEE","IEEE Conference Publications"
"A Domain Specific Data Management Architecture for Protein Structure Data","Y. Wang; R. Sunderraman; H. Tian","Computer Science Department, Georgia State University, 34 Peachtree Street, Suite, 1454, GA 30303, USA ywang17@student.gsu.edu","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","20161215","2006","","","5751","5754","In this paper, we propose an architecture that extends the Object-Oriented Database (OODB) system architecture by adding domain specific additional layers to manage protein structure data. The two layers introduced above OODB are Protein-QL, domain-specific query language and Protein-OODB, a domain-specific data layer. This architecture is designed specifically for the protein domain, but it is the first step in building a general Bio-OODBMS for biological applications. Three internal data types are defined for the primary, secondary, and tertiary protein structures, respectively, to simplify queries in Protein-QL. This enables the domain scientists to easily formulate data requests. We use lambda-DB as the back-end database to implement Protein-QL. Queries in Protein-QL are compiled into OQL which are then executed against the database. In order to make the underlying OODB system (lambda-DB) more powerful, we introduce additional constraints to check the integrity of protein data","1557-170X;1557170X","CD:1-4244-003303; Paper:1-4244-0032-5","10.1109/IEMBS.2006.259892","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463113","Domain Specific Object-Oriented DataBase;Lambda-DB Constraint Language;Protein Query Language","Bioinformatics;Biology;Buildings;Conference management;Database languages;Information retrieval;Object oriented databases;Proteins;Relational databases;User interfaces","biochemistry;biology computing;data integrity;molecular biophysics;object-oriented databases;proteins;query languages","Bio-OODBMS;Protein-QL;back-end database;data management architecture;domain-specific data layer;domain-specific query language;lambda-DB constraint language;object-oriented database system;primary protein structure;protein data integrity;protein query language;protein structure data;protein-OODB;secondary protein structure;tertiary protein structure","Algorithms;Amino Acid Sequence;Computational Biology;Databases, Protein;Programming Languages;Protein Conformation;Protein Structure, Secondary;Protein Structure, Tertiary;Proteins;Proteomics;Software;Structural Homology, Protein;User-Computer Interface","2","","","","","Aug. 30 2006-Sept. 3 2006","","IEEE","IEEE Conference Publications"
"A novel public security cases knowledge navigation system based topic maps","J. Chen; Q. Fang; Y. Cheng; P. Gong; F. Hao; J. Ma; J. You; B. Liu","School of Computer Science, Hubei University of Technology, Wuhan, China, 430068","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","20161024","2016","","","2178","2183","The information construction plays an essential role in the area of public security. However, the “Information Silo” phenomenon in Public Security Department has become a bottleneck of the development of public security. The paper proposes a novel approach to implement a public security knowledge navigation system, utilizing the information extraction to obtain the ontology for the topic maps of public security cases. And the topic maps integrate useful information to analyze and find relations among cases not only assistant investigators to systematize public security information resources, but also contribute to fully master all of information related to the participates. In particular, the paper proposed a visual navigation for the topic maps which can improve the efficiency and accuracy of public security staffs.","","","10.1109/FSKD.2016.7603518","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603518","public security;topic maps information extraction","Data mining;Databases;Hidden Markov models;Information retrieval;Navigation;Ontologies;Security","ontologies (artificial intelligence);security of data","information resources;information silo phenomenon;ontology;public security case knowledge navigation system;public security department;public security staffs;visual navigation","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conference Publications"
"Exploiting network analysis to investigate topic dynamics in the digital library evaluation domain","L. Papachristopoulos; M. Sfakakis; N. Kleidis; G. Tsakonas; C. Papatheodorou","Digital Curation Unit, IMIS, &#x2018;Athena&#x2019; RC, Athens, Greece","2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)","20160905","2016","","","267","268","The multidimensional nature of digital libraries evaluation domain and the amount of scientific production published on the field hinders and disorientates the interested researchers who contemplate to focus on the specific domain. These communities need guidance in order to exploit the considerable amount of data and the diversity of methods effectively as well as to identify new research goals and develop their plans for future works. This poster investigates the core topics of the digital library evaluation field and their impact by applying topic modeling and network analysis on a corpus of the JCDL, ECDL/TDPL and ICADL conferences proceedings in the period 2001-2013.","","Electronic:978-1-4503-4229-2; POD:978-1-5090-5254-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559617","Digital library evaluation;network analysis;topic modeling","Analytical models;Information retrieval;Libraries;Metadata;Resource management;Semantics;Text categorization","digital libraries","digital library evaluation domain;network analysis;scientific production;topic dynamics;topic modeling","","","","","","","19-23 June 2016","","IEEE","IEEE Conference Publications"
"A re-ranking model for accurate knowledge base completion with knowledge base schema and web statistic","S. J. Choi; H. J. Song; H. G. Yoon; S. B. Park; S. Y. Park","School of Computer Science and Engineering, Kyungpook National University, Daegu 702-701, South Korea","2016 IEEE Congress on Evolutionary Computation (CEC)","20161121","2016","","","4958","4964","Knowledge base completion aims to complete a knowledge base by filling up missing facts of the knowledge base. Neural knowledge base embeddings proposed to solve this task measure the plausibility of all candidate triples, and then select top-ranked triples by the plausibility as new facts for the knowledge base. The plausibility by neural embeddings allows true facts to be ranked at high positions, but not at top positions. This is because neural knowledge base embeddings are limited to using only the information within the knowledge base. Therefore, this paper proposes a re-ranking model for precise knowledge base completion. As a re-ranking model, a neural network which uses knowledge base schema and web statistic additionally is adopted. As a result, the proposed re-ranking model has an effect of using additional information for knowledge base completion. Thus, the candidate triples are first ranked by a neural knowledge base embedding, and then the result is re-ranked by the neural network. The experimental results show that the proposed re-ranking model improves the base neural embeddings up to 16% in Hits@1. This implies that the re-ranking model places true facts at top positions effectively.","","Electronic:978-1-5090-0623-6; POD:978-1-5090-0624-3; USB:978-1-5090-0622-9","10.1109/CEC.2016.7744426","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744426","","Computational modeling;Data mining;Information retrieval;Knowledge based systems;Knowledge engineering;Natural language processing;Neural networks","Web sites;knowledge based systems;neural nets","Hits@1;Web statistics;knowledge base completion;knowledge base schema;neural knowledge base embeddings;neural network;reranking model;top-ranked triples","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"Sentence Based Mathematical Problem Solving Approach via Ontology Modeling","D. T. K. Geeganage; D. N. Koggalahewa; J. L. Amararachchi; A. S. Karunananda","Dept. of Inf. Technol., Sri Lanka Inst. of Inf. Technol., Sri Lanka","2016 6th International Conference on IT Convergence and Security (ICITCS)","20161110","2016","","","1","5","Mathematics includes solving a variety of problems by applying theories and formulas. Thus mathematical problem solving requires performing arithmetical operations by using analytical and problem solving skills. Sentence based mathematical problems contains real world scenarios and requires to apply both mathematical and problem analyzing knowledge to solve problems. Human beings solve sentence based mathematical problems by applying different mathematical formulas and theorems to the comprehend questions. Understanding the sentence based questions requires an additional effort to grab the content and grasped content should be mapped with known concepts in terms of variables. Organizing the variables and formulas by understanding the relationships and properties would be important to formulate the answer. Thus the content can be easily modeled using an ontological approach and the problem solving can be accomplished by querying the ontology using a multi agent approach. Sentenced based mathematical problem solving approach demonstrates a system which can solve mathematical questions by acquiring the semantics of the question and applying learnt formulas. Information extraction from the question, ontology based knowledge representation, multi agent based ontology querying and answer generation with explanations can be defined as major functions. This approach can be used to introduce effective intelligent tutoring systems in any domain.","","Electronic:978-1-5090-3765-0; POD:978-1-5090-3766-7","10.1109/ICITCS.2016.7740368","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740368","","Information retrieval;Mathematical model;Ontologies;Problem-solving;Semantics","intelligent tutoring systems;mathematics computing;multi-agent systems;ontologies (artificial intelligence);problem solving;query processing","analytical skills;arithmetical operations;information extraction;intelligent tutoring systems;knowledge analysis;mathematical formulas;mathematical theorems;multiagent based answer generation;multiagent based ontology querying;ontology based knowledge representation;ontology modeling;problem solving skills;sentence based mathematical problem solving approach;sentence based questions","","","","","","","26-26 Sept. 2016","","IEEE","IEEE Conference Publications"
"Discovering News Frames: Exploring Text, Content, and Concepts in Online News Sources to Address Water Insecurity in the Southwest Region","L. H. Cheeks; T. L. Stepien; D. M. Wald","Dept. of Comput., Arizona State Univ., Tempe, AZ, USA","2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)","20161219","2016","","","454","462","The Internet is a major source of online news content. Current efforts to evaluate online news content, including text, story line and sources is limited by the use of small-scale manual techniques that are time consuming and dependent on human judgments. This article explores the use of machine learning algorithms and mathematical techniques for Internet-scale data mining and semantic discovery of news content that will enable researchers to mine, analyze and visualize large-scale datasets. This research has the potential to inform the integration and application of data mining to address real-world socio-environmental issues, including water insecurity in the Southwestern United States. This paper establishes a formal definition of framing and proposes an approach for the discovery of distinct patterns that characterize prominent frames. Our experimental evaluation shows that the proposed process is an effective and efficient semi-supervised machine learning method to inform data mining for inferring classification.","","Electronic:978-1-5090-3207-5; POD:978-1-5090-3208-2","10.1109/IRI.2016.67","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7785776","Clustering;Content Analysis;Data Mining;News Frames;Non-Negative Matrix Factorization;Text Mining","Feature extraction;Frequency measurement;Information retrieval;Semantics;Text mining;Water resources","data mining;learning (artificial intelligence);text analysis;water supply","Internet-scale data mining;mathematical techniques;news content semantic discovery;news frames;online news sources;semisupervised machine learning method;southwest region;water insecurity","","","","","","","28-30 July 2016","","IEEE","IEEE Conference Publications"
"Research on the privacy-preserving retrieval over ciphertext on cloud","Z. Xinyi; Z. Ru; W. Fangyu; L. Jianyi; Y. Yuangang","Beijing University of Posts and Telecommunications, Beijing 100876, China","2016 6th International Conference on Information Communication and Management (ICICM)","20161215","2016","","","100","104","With the development of cloud storage services, more and more users choose to store their information and data in the cloud and use Cloud Service Provider to manage their data. In order to ensure the security of the data in the cloud, users will encrypt their data before store it in the cloud. Although the encryption ensure the security of the data, it brings some problem to users when searching their data at the same time. At present, the research on the technology of the plain text has been relatively mature, how to study for a retrieval technology over cipher text in the cloud is the key to cipher text retrieval technology. In this paper, we analyze the existing search algorithm over cipher text, for the problem that most algorithm will disclosure user's access patterns, we propose a new method of private information retrieval supporting keyword search which combined with homomorphic encryption and private information retrieval. This method can achieve the query operation without disclose private information of both sides. In view of the efficiency problem of the private information retrieval scheme, we introduce the MapReduce into the search algorithm which can meet both the efficiency and privacy requirements.","","CD:978-1-5090-3493-2; Electronic:978-1-5090-3495-6; POD:978-1-5090-3496-3","10.1109/INFOCOMAN.2016.7784223","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784223","MapReduce (key words);cloud computing;homomorphic encryption;keyword retrieval;privacy protection","Ciphers;Cloud computing;Encryption;Indexes;Information retrieval;Servers","authorisation;cloud computing;data protection;parallel programming;query processing;text analysis","MapReduce;cipher text retrieval technology;cloud service provider;cloud storage services;data encryption;data management;data security;data store;homomorphic encryption;information store;keyword search;privacy-preserving retrieval;private information retrieval;query operation;user access pattern disclosure","","","","","","","29-31 Oct. 2016","","IEEE","IEEE Conference Publications"
"Learning text to model: A Bayesian network based L-system modeling strategy","C. Chen; G. Ji; B. Zhao","School of Computer Science and Technology, Nanjing Normal University","2016 International Conference on Behavioral, Economic and Socio-cultural Computing (BESC)","20170105","2016","","","1","2","L-system is a prevailing modeling method for generating fractals, especially self-similar patterns such as plants. However it's too hard to design an appropriate L-system to get the desired visual models of plants. In order to generate a favorable plant model, usually we need to deduce backwards or guess the production rules of the L-system and then try to modify some control parameters over and over again. Inspired by information extraction technology, we propose a new strategy to model visual plants. We use Bayesian Networks to extract structured information describing the plant characters from user given text first, then we use that information to automatically generate an L-system alphabet, axiom and production rules. Comprehensive experimental evaluation conducted on real botanic text corpora demonstrates that our proposal is very helpful in artistic plants modelling.","","Electronic:978-1-5090-6164-8; POD:978-1-5090-6165-5","10.1109/BESC.2016.7804496","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7804496","","Bayes methods;Biological system modeling;Computational modeling;Data mining;Information retrieval;Production;Visualization","Bayes methods;botany;learning (artificial intelligence);network theory (graphs);text analysis","Bayesian network based L-system modeling strategy;L-system alphabet;artistic plant modelling;botanic text corpora;information extraction technology;production rules;self-similar patterns;structured information extraction;text learning","","","","","","","11-13 Nov. 2016","","IEEE","IEEE Conference Publications"
"Impact of stemming on Arabic text summarization","N. Alami; M. Meknassi; S. A. Ouatik; N. Ennahnahi","Laboratoire Informatique et Mod&#x00E9;lisation, Faculty of Science Dhar EL Mahraz, University Sidi Mohamed Ben Abdellah (USMBA), Fez, Morocco","2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)","20170105","2016","","","338","343","Stemming is a process of reducing inflected words to their stem or root from a generally written word form. This process is used in many text mining application as a feature selection technique. Moreover, Arabic text summarization has increasingly become an important task in natural language processing area (NLP). Therefore, the aim of this paper is to evaluate the impact of three different Arabic stemmers (i.e. Khoja, Larekey and Alkhalil's stemmer) on the text summarization performance for Arabic language. The evaluation of the proposed system, with the three different stemmers and without stemming, on the dataset used shows that the best performance was achieved by Khoja stemmer in term of recall, precision and F1-measure. The evaluation also shows that the performances of the proposed system are significantly improved by applying the stemming process in the pre-processing stage.","","Electronic:978-1-5090-0751-6; POD:978-1-5090-0752-3","10.1109/CIST.2016.7805067","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805067","Arabic Text Summarization;Graph model;Light Stemming;Stemming","Algorithm design and analysis;Clustering algorithms;Feature extraction;Information retrieval;Natural language processing;Redundancy;Text mining","data mining;feature selection;graph theory;natural language processing;text analysis;word processing","Alkhalil stemmer;Arabic stemmers;Arabic text summarization;F1-measure;Khoja stemmer;Larekey stemmer;NLP;feature selection;inflected word reduction;natural language processing;precision;recall;stemming process;text mining","","","","","","","24-26 Oct. 2016","","IEEE","IEEE Conference Publications"
"Towards an argument-based method for answering why-question in Vietnamese language","C. T. Nguyen; D. T. Nguyen","Faculty of Computer Science, University of Information Technology, VNU-HCM, Ho Chi Minh city, Viet Nam","2016 3rd National Foundation for Science and Technology Development Conference on Information and Computer Science (NICS)","20161031","2016","","","130","134","In this paper, an argument-based method of answering why-questions in Vietnamese is presented. This method is developed in different way from many approaches which use cue phrases of causal relation to find the answers for why-questions. In this method, the arguments is extracted firstly, then the causal part and consequential part of every argument are split in order to index the consequential part. When a why-question is asked, the asking information is extracted and used to search for the reason, then the reason is used to identify the paragraph which can be used to answer the question. For evaluation, an experiment with keyword-based information retrieval and simple argument collecting process is conducted to show the applicability of the method.","","","10.1109/NICS.2016.7725637","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725637","Vietnamese why-question answering;argument analysis;argument-based answering","Computer science;Data mining;Indexing;Information retrieval;Power generation;Sugar industry","natural language processing;question answering (information retrieval)","Vietnamese language;argument-based method;causal part;consequential part;keyword-based information retrieval;why-questions","","","","","","","14-16 Sept. 2016","","IEEE","IEEE Conference Publications"
"Scalability analysis of distributed search in large peer-to-peer networks","W. Ke; J. Mostafa","College of Computing and Informatics, Drexel University, Philadelphia, PA 19104","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","909","914","We study decentralized searches in large-scale, self-organized peer-to-peer networks and investigate the influences of network size and degree distribution (neighborhood size) on search efficiency. Experimental results show that searches are efficient and scalable in large networks, especially with large neighborhood sizes (degrees). Analysis of the data supports a proposed scalability model, in which search path length L (efficiency) is proportional to a poly-logarithmic function of network size N, with degree d<sub>m</sub> (majority neighborhood size) as the log base. The model explains 90% (R<sup>2</sup>) of variances in search path lengths. Search time (search path length) predicted by the model shows great potential for efficient searches in real-scale networks of up to a billion distributed systems.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840686","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840686","Decentralization;Distributed search;Efficiency;Peer-to-peer networks;Scalability","Big data;Electronic mail;Information retrieval;Internet;Libraries;Peer-to-peer computing;Scalability","information retrieval;peer-to-peer computing","decentralized searches;degree distribution;distributed search;information retrieval;large-scale peer-to-peer networks;neighborhood size;network size;poly-logarithmic function;scalability analysis;search efficiency;search path length;self-organized peer-to-peer networks","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Sparse coding methods for music induced emotion recognition","J. Jakubik; H. Kwaśnicka","Wroc&#x0142;aw University of Science and Technology, Department of Computational Intelligence, Poland","2016 Federated Conference on Computer Science and Information Systems (FedCSIS)","20161107","2016","","","53","60","The paper concerns automatic recognition of emotion induced by music (MER, Music Emotion Recognition). Comparison of different sparse coding schemes in a task of MER is the main contribution of the paper. We consider a domain-specific categorization of emotions, called Geneva Emotional Music Scale (GEMS), which focuses on induced emotions rather than expressed emotions. We were able to find only one dataset, namely Emotify, in which data are annotated with GEMS categories, this set was used in our experiments. Our main goal was to compare different sparse coding approaches in a task of learning features useful for predicting musically induced emotions, taking into account categories present in the GEMS. We compared five sparse coding methods and concluded that sparse autoencoders outperform other approaches.","","","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733216","","Dictionaries;Emotion recognition;Encoding;Feature extraction;Music information retrieval;Neurons;Psychology","emotion recognition;information retrieval;music","GEMS;MER;geneva emotional music scale;music induced emotion recognition;music information retrieval;sparse coding methods","","","","","","","11-14 Sept. 2016","","IEEE","IEEE Conference Publications"
"A review of an information extraction technique approach for automatic short answer grading","U. Hasanah; A. E. Permanasari; S. S. Kusumawardani; F. S. Pribadi","Department of Electrical Engineering and Information Technology, Universitas Gadjah Mada, Yogyakarta, Indonesia","2016 1st International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE)","20170102","2016","","","192","196","The requirement for automatic short answer grading (ASAG) system brings researchers to discover more knowledge about this field. Many techniques have been developed to reach the highest accuration. It can be processed by following stages: creating data set, pre-processing, model building, grading, and model evaluation. One of the techniques which commonly used is information extraction technique. Information extraction is a technique that employing finding fact on the student answers as patterns and then matches these to the teacher answer. The accuration is pointed out in computer and human raters agreement. The goal of this paper is to present a review of several ASAG research which using information extraction technique. However, this paper does not conclude the best method which can be used for general cases.","","Electronic:978-1-5090-1567-2; POD:978-1-5090-1568-9; USB:978-1-5090-1566-5","10.1109/ICITISEE.2016.7803072","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803072","automatic grading;information extraction;natural language processing;pattern matching;short answer","Computational modeling;Electrical engineering;Information retrieval;Information technology;Natural language processing;Pattern matching;Semantics","educational administrative data processing;information retrieval;natural language processing;pattern matching","ASAG system;automatic short answer grading system;information extraction;natural language processing;pattern matching","","","","","","","23-24 Aug. 2016","","IEEE","IEEE Conference Publications"
"An Analysis of Main Solutions for the Automatic Construction of Ontologies from Text","R. Girardi","Comput. Sci. Dept., Fed. Univ. of Maranhao, Sao Luis, Brazil","2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)","20170116","2016","","","457","460","Ontologies are increasingly used by modern knowledge systems for representing and sharing knowledge. Supporting semantic processing, ontology-driven knowledge systems allow for more precise information interpretation, thus providing greater usability and effectiveness than traditional information systems. Manual construction of ontologies by domain experts and knowledge engineers is a costly task, therefore automatic and/or semi-automatic approaches to their development are needed, a field of research that is usually referred to as ontology learning and population. This is the main focus of this article which discusses main problems and corresponding solutions for the automated acquisition of each one of the components of an ontology (classes, properties, taxonomic and non-taxonomic relationships, axioms and instances).","","Electronic:978-1-5090-4470-2; POD:978-1-5090-4471-9","10.1109/WI.2016.0074","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817091","Machine Learning;Natural Language Processing;Ontology Learning;Ontology Population;Text Mining","Information retrieval;Knowledge based systems;Natural language processing;Ontologies;Sociology;Statistics;Syntactics","data mining;learning (artificial intelligence);ontologies (artificial intelligence);text analysis","knowledge representation;ontology learning;ontology population;ontology-driven knowledge systems;semantic processing;text mining","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"A graph-based approach to word sense disambiguation. An unsupervised method based on semantic relatedness","M. Arab; M. Z. Jahromi; S. M. Fakhrahmad","School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran","2016 24th Iranian Conference on Electrical Engineering (ICEE)","20161010","2016","","","250","255","Word Sense Disambiguation (WSD) is the task of automatically choosing the correct meaning of a word in a context. Due to the importance of this task, it is considered as one of the most important and challenging problems in the field of computational linguistics and plays a crucial role in various natural language processing (NLP) applications. In this paper, we present an improved version of a recent unsupervised graph-based word sense disambiguation method considered to be one of the states of the art techniques. Using WordNet as our knowledge-base, we introduce a new method of combining similarity metrics that uses higher order relations between words to assign appropriate weights to each edge in the graph. Furthermore, we propose a new approach for selecting the most appropriate sense of the target word that makes use of the in-degree centrality algorithm and senses of the neighbor words. Experimental results on benchmark datasets Senseval-2 and Senseval-3 shows that the proposed model outperforms all other graph-based methods presented in the literature.","","","10.1109/IranianCEE.2016.7585527","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585527","Natural Language Processing;Word sense disambiguation;WordNet;unsupervised graph-based","Computers;Context;Electrical engineering;Information retrieval;Knowledge based systems;Natural language processing;Semantics","computational linguistics;graph theory;natural language processing;word processing","NLP;WSD;WordNet;computational linguistics;in-degree centrality;natural language processing;semantic relatedness;similarity metrics;unsupervised graph-based word sense disambiguation method","","","","","","","10-12 May 2016","","IEEE","IEEE Conference Publications"
"Graph-Based Term Weighting Scheme for Topic Modeling","G. Bekoulis; F. Rousseau","LIX, Ecole Polytech., Palaiseau, France","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","1039","1044","LSI and LDA are widely used techniques to uncover the underlying topical structure of text. They traditionally rely on bag-of-words representation of documents and term frequency-based (TF) weighting schemes. In this paper, we represent documents as graph-of-words to capture the relationships between close words and propose the number of contexts of co-occurrences as alternative term weights (TW). Experiments with a downstream supervised task show that counting the importance of a node inside the graph results in statistically significant higher accuracy and macro-averaged F1score than with TF-based LSI and LDA.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836781","graph representation;text categorization;topic modeling","Context;Information retrieval;Large scale integration;Probabilistic logic;Semantics;Standards;Text categorization","graph theory;statistical analysis;text analysis","LDA;LSI;TW;close words;co-occurrences;downstream supervised task;graph node importance;graph-based term weighting scheme;graph-of-words;latent Dirichlet allocation;latent semantic indexing;macroaveraged F1-score;statistical analysis;topic modeling;topical text structure","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Survey of string similarity approaches and the challenging faced by the Arabic language","S. S. Aljameel; J. D. O'Shea; K. A. Crockett; A. Latham","Department of Computing, Math and Digital Technology, Manchester Metropolitan University, United Kingdom","2016 11th International Conference on Computer Engineering & Systems (ICCES)","20170119","2016","","","241","247","Measuring the similarity between strings plays an increasingly important role in many applications such as information retrieval, short answer grading, and conversational agent software. There has been much recent research interest in applying string similarity within Arabic language applications; however, the use of string similarity in Arabic poses a substantial challenge such as the complexity of the morphological system, ambiguity, and lack of resources. This survey discusses the existing research into string similarity approaches and the difficulties posed by the Arabic language by dividing them into three approaches; lexical-based similarity, semantic-based similarity, and hybrid similarity. The aim of this paper is to review these approaches and to identify suitable approaches with Arabic language.","","Electronic:978-1-5090-3267-9; POD:978-1-5090-3268-6; USB:978-1-5090-3266-2","10.1109/ICCES.2016.7822008","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7822008","Arabic Corpus;Arabic WordNet;Hybrid Similarity;Lexical-Based Similarity;Semantic-Based Similarity;String Similarity;The Arabic Language","Algorithm design and analysis;Context;Grammar;Information retrieval;Knowledge based systems;Natural language processing;Semantics","information retrieval;natural language processing","Arabic language applications;Arabic poses;conversational agent software;hybrid similarity;information retrieval;lexical-based similarity;morphological system;semantic-based similarity;short answer grading;string similarity","","","","","","","20-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"A New Fuzzy Ontology Development Methodology (FODM) Proposal","X. Li; J. F. Martínez; G. Rubio","Research Center on Software Technologies and Multimedia Systems for Sustainability, Technical University of Madrid, Madrid, Spain","IEEE Access","20161117","2016","4","","7111","7124","There is an upsurge in applying fuzzy ontologies to represent vague information in the knowledge representation field. Current research in the fuzzy ontologies paradigm mainly focuses on developing formalism languages to represent fuzzy ontologies, designing fuzzy ontology editors, and building fuzzy ontology applications in different domains. Less focus falls on establishing a formal methodological approach for building fuzzy ontologies. Existing fuzzy ontology development methodologies, such as the IKARUS-Onto methodology and fuzzy ontomethodology, provide formalized schedules for the conversion from crisp ontologies into fuzzy ones. However, a formal guidance on how to build fuzzy ontologies from scratch still lacks in this paper. Therefore, this paper presents the first methodology, named fuzzy ontology development methodology (FODM), for developing fuzzy ontologies from scratch. The proposed FODM can provide a very good guideline for formally constructing fuzzy ontologies in terms of completeness, comprehensiveness, generality, efficiency, and accuracy. To explain how the FODM works and demonstrate its usefulness, a fuzzy seabed characterization ontology is built based on the FODM and described step by step.","2169-3536;21693536","","10.1109/ACCESS.2016.2621756","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728006","Fuzzy ontologies;generality;knowledge representation;methodology;vagueness","Fuzzy logic;Information analysis;Information retrieval;Knowledge representation;Ontologies","formal languages;fuzzy set theory;information analysis;ontologies (artificial intelligence)","FODM;IKARUS-Onto methodology;formal methodological approach;formalism languages;fuzzy ontology applications;fuzzy ontology development methodology;fuzzy ontology editors;fuzzy ontomethodology;fuzzy seabed characterization ontology;information representation;knowledge representation","","","","","","20161101","2016","","IEEE","IEEE Journals & Magazines"
"Re-examination of training of semantic class disambiguation classifier for all words","T. V. H. Thien; K. Shirai","School of Advanced Science and Technology, Japan Advanced Institute of Science and Technology, 1-1, Asahidai, Nomi, 923-1292, Ishikawa, Japan","2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)","20161201","2016","","","187","192","This paper presents a method for semantic class disambiguation for all words. Unlike the ordinary word sense disambiguation, a set of semantic classes or coarse grained senses is defined as a common sense inventory, then universal classifiers to select an appropriate semantic class of a target word in a given context, which can be applicable to all words, are trained by supervised learning. In the previous method, imbalance of number of positive and negative samples in training data causes disambiguation errors. On the other hand, our new method can train the classifiers from well balanced training data. Results of the experiment showed that our method outperformed the previous method by 1.43% accuracy.","","Electronic:978-1-4673-8929-7; POD:978-1-4673-8930-3","10.1109/KSE.2016.7758051","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7758051","","Context;Information retrieval;Manuals;Semantics;Syntactics;Training;Training data","learning (artificial intelligence);pattern classification;word processing","coarse grained senses;common sense inventory;disambiguation errors;ordinary word sense disambiguation;semantic class disambiguation classifier;semantic classes;supervised learning;target word;training data;universal classifiers","","","","","","","6-8 Oct. 2016","","IEEE","IEEE Conference Publications"
"Future digital libraries: Research and responsibilities","M. Zemankova","National Science Foundation","2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)","20160905","2016","","","1","1","Summary form only given. In October 1991 the National Science Foundation (NSF) sponsored a workshop to examine the role of the Information Retrieval research community in the emerging environment of Internet, high performance text processing capabilities and ever-increasing volumes of digitized documents. Ed Fox, Michael Lesk and Michael McGill drafted a White Paper, calling for a National Electronic Science, Engineering, and Technology Library. The term “Digital Library” was adopted and for follow-up workshops with the goal to identify research directions, leading to National Science Foundation (NSF)/Defense Advanced Research Projects Agency (DARPA)/National Aeronautics and Space Administration (NASA) Research in Digital Libraries Initiative announced in late 1993. Now, in 2016, 25 years after the first workshop, 15 years after the Joint Conference on Digital Libraries has been established, and many initiatives and developments around the world, what is the state of Digital Libraries? What items should be in digital libraries, who should their custodians, how can the items be organized to support knowledge discovery, how can the contents be safeguarded and preserved? Ebla, Syria (2500 B.C.-2250 B.C.) constitutes the oldest organized library of tables yet discovered. What will the archeologists discover in year 4400 about the world, politics, economies, technologies, science, climate, species, health, food, culture, art, entertainment and everyday life through the ages? The talk will examine what we can do to support innovative research and design and implementation of lasting, informative Digital Libraries that will promote global goals of knowledge discovery and international understanding and personal needs to organize and selectively share important facts, creations, and memories.","","Electronic:978-1-4503-4229-2; POD:978-1-5090-5254-7","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559555","Digital library;digitized documents;knowledge discovery;preservation","Conferences;Information retrieval;Information systems;Knowledge discovery;Libraries;US Government;Visual databases","digital libraries;information retrieval","DARPA;Defense Advanced Research Projects Agency;Internet;NASA;NSF;National Aeronautics and Space Administration;National Science Foundation;digital libraries;high performance text processing capabilities;information retrieval research community","","","","","","","19-23 June 2016","","IEEE","IEEE Conference Publications"
"Proposal of statistical method of semantic indexing for multilingual documents","S. Mallat; M. Maraoui; E. Hkiri; M. Zrigui","LATICE Laboratory Research Department of Computer Science, Faculty of Sciences, University of Monastir, Tunisia","2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","20161110","2016","","","2417","2424","In this paper, we present a statistical approach to semantic indexing for multilingual text documents based on conceptual network formalism. We propose to use this formalism as an indexing language to represent the descriptive concepts and their weighting. These concepts represent the content of the document. Our contribution is based on two steps; we propose, in the first step, the extraction of index terms using the multilingual lexical resource EuroWordNet (EWN). In the second step, we pass from the representation of index terms to the representation of index concepts through conceptual network formalism. This latter is generated using the EWN resource and the association rules model (in attempt to discover the non taxonomic relations or contextual relations between the concepts of a document). These lasts are latent relations, buried in the text, and carried by the semantic context of the co-occurrence of concepts in the document. The proposed approach can be applied to several languages because it builds a linguistic and statistical process. This approach is validated by a set of experiments and comparison with other methods of indexing based on a corpus of TREC evaluation campaign 2001 and 2002 of the ad hoc task. We prove that the proposed indexing approach provides encouraging results.","","Electronic:978-1-5090-0626-7; POD:978-1-5090-0627-4; USB:978-1-5090-0625-0","10.1109/FUZZ-IEEE.2016.7737996","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7737996","association rules;concept extraction;conceptual network formalism;multilingual EuroWordNet;multilingual documents;semantic indexing;statistical measures","Computer science;Context;Context modeling;Indexing;Information retrieval;Semantics","data mining;indexing;natural language processing;statistical analysis;text analysis","EWN resource;EuroWordNet;TREC evaluation campaign;association rule model;conceptual network formalism;contextual relations;descriptive concepts;index term extraction;index terms representation;indexing language;multilingual lexical resource;multilingual text documents;nontaxonomic relations;semantic indexing;statistical method;statistical process","","","","","","","24-29 July 2016","","IEEE","IEEE Conference Publications"
"A Semantic-Based Approach to Building Auxiliary System for Screen-Based Reading","H. Zhong; S. Li; F. Zhao; H. Jin; Q. Zhang","","2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","20161013","2016","","","927","932","With an increasing amount of electronic documents,screen-based reading becomes popular, how toimprove the ability of reading deeply and sustaining aprolonged engagement in reading is one of challenges in a digital environment. In this paper, we describe a semantic analysis framework for proactively decreasing fragmented time while screen-based reading. The central idea is to utilize semantic analysis programs to extract an extensive set of information that describes keyword spotting. And the auxiliary knowledge can be used for deeply reading. We discuss the strengths of our semantic analysis programs, namely, text extraction, name recognition, feature matching and knowledge map analysis. We report our results of applying these programs to parse scientific literature, such as citation recognition, the author name recognition, reference extraction and etc. The experimental results show that proposed approach can greatly decrease the fragmented time while reading in the digital environment.","","","10.1109/DASC-PICom-DataCom-CyberSciTec.2016.167","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588959","electronic documents;screen-based reading","Algorithm design and analysis;Buildings;Data mining;Feature extraction;Information retrieval;Semantics;Text recognition","","","","","","","","","8-12 Aug. 2016","","IEEE","IEEE Conference Publications"
"Urban Safety Prediction Using Context and Object Information via Double-Column Convolutional Neural Network","H. W. Kang; H. B. Kang","Dept. of Digital Media, Catholic Univ. of Korea, Bucheon, South Korea","2016 13th Conference on Computer and Robot Vision (CRV)","20161229","2016","","","399","405","Recently, various studies are performed on the analysis of urban environment (e.g. safety, hygiene, economy and population). Particularly, the prediction of urban safety has received a lot of attention in society. In this paper, we propose safety prediction of urban place using Convolutional Neural Network. To predict more accurate safety score, we use the fusion of context and object information. To do this, we adopt Double-columns Convolutional Neural Network which consist of one column is context information extraction column, the other object information extraction column. Context information is extracted from re-sized whole image and object information is extracted from highest saliency score patch of object saliency map. We trained our prediction model using Place Pulse 1.0 dataset. To evaluate the performance of our prediction model, we compared with SVR model and Alexnet model using RMSE. Also, we analyze correlation ground truth safety scores and predicted safety scores. From our experimental results, our prediction model showed the best performance (RMSE of 0.7403 and pearson/spearman correlation coefficient of 0.903/0.900).","","Electronic:978-1-5090-2491-9; POD:978-1-5090-2492-6","10.1109/CRV.2016.64","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801549","Convolutional neural network;Deep learning;Saliency map;urban safety","Context;Data mining;Information retrieval;Neural networks;Predictive models;Safety;Urban areas","image segmentation;learning (artificial intelligence);neural nets;visual databases","Pearson correlation coefficient;Place Pulse 1.0 dataset;RMSE;Spearman correlation coefficient;context information extraction column;context object information fusion;correlation ground truth safety score analysis;double-column convolutional neural network;image resizing;object information extraction column;object saliency map;safety score;saliency score patch;urban place safety prediction","","","","","","","1-3 June 2016","","IEEE","IEEE Conference Publications"
"Big Data for Remote Sensing: Challenges and Opportunities","M. Chi; A. Plaza; J. A. Benediktsson; Z. Sun; J. Shen; Y. Zhu","School of Computer Science, Shanghai Key Laboratory of Data Science, Key Laboratory for Information Science of Electromagnetic Waves (MoE), State Key Laboratory of Satellite Ocean Environment Dynamics, Fudan University, Second Institute of Oceanography (SOA), Shanghai, Hangzhou, ChinaChina","Proceedings of the IEEE","20161019","2016","104","11","2207","2219","Every day a large number of Earth observation (EO) spaceborne and airborne sensors from many different countries provide a massive amount of remotely sensed data. Those data are used for different applications, such as natural hazard monitoring, global climate change, urban planning, etc. The applications are data driven and mostly interdisciplinary. Based on this it can truly be stated that we are now living in the age of big remote sensing data. Furthermore, these data are becoming an economic asset and a new important resource in many applications. In this paper, we specifically analyze the challenges and opportunities that big data bring in the context of remote sensing applications. Our focus is to analyze what exactly does big data mean in remote sensing applications and how can big data provide added value in this context. Furthermore, this paper describes the most challenging issues in managing, processing, and efficient exploitation of big data for remote sensing problems. In order to illustrate the aforementioned aspects, two case studies discussing the use of big data in remote sensing are demonstrated. In the first test case, big data are used to automatically detect marine oil spills using a large archive of remote sensing data. In the second test case, content-based information retrieval is performed using high-performance computing (HPC) to extract information from a large database of remote sensing images, collected after the terrorist attack to the World Trade Center in New York City. Both cases are used to illustrate the significant challenges and opportunities brought by the use of big data in remote sensing applications.","0018-9219;00189219","","10.1109/JPROC.2016.2598228","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565634","Big data;big data challenges;big data life cycle;big data opportunities;high-performance computing (HPC);remote sensing","Big data;Computer applications;Context modeling;High performance computing;Information retrieval;Remote sensing;Sensors;Terrorism","Big Data;information retrieval;parallel processing;remote sensing","Earth observation spaceborne sensors;HPC;airborne sensors;big data;content-based information retrieval;high-performance computing;marine oil spills;remote sensing applications;remote sensing data","","1","","","","20160913","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Bounds for batch codes with restricted query size","H. Zhang; V. Skachek","Department of Computer Science, Technion - Israel Institute of Technology, Haifa 32000, Israel","2016 IEEE International Symposium on Information Theory (ISIT)","20160811","2016","","","1192","1196","Batch codes are of potential use in load balancing, private information retrieval and distributed storage. In this article, we present new bounds on the parameters of linear batch codes with restricted query size. The derivation techniques are partly based on ideas in the literature for codes with locality, combined with additional ideas.","","Electronic:978-1-5090-1806-2; POD:978-1-5090-1807-9","10.1109/ISIT.2016.7541487","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7541487","","Computer science;Information retrieval;Information theory;Load management;Switches;Systematics;Upper bound","linear codes;query processing;storage management","distributed storage;linear batch codes;load balancing;private information retrieval;restricted query size","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Towards Accurate Binary Correspondence Using Runtime-Observed Values","U. Kargén; N. Shahmehri","Dept. of Comput. & Inf. Sci., Linkoping Univ., Linkoping, Sweden","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","438","442","Establishing binary correspondence is the process of finding corresponding pairs of program elements, e.g., functions or individual instructions, between two semantically equivalent (or nearly-equivalent) but syntactically different program binaries. The binary-correspondence problem has applications in many fields, e.g., plagiarism and clone detection, reverse engineering, and security, and has therefore received significant attention both in industry and academia. Most binary-correspondence methods used in practice today are based on static analysis of the control structure in binaries. Unfortunately, such methods are often highly sensitive to syntactic differences between binaries, and discrepancies in the control structure due to, for example, using different compilers or optimization levels often severely reduce their accuracy. Several recent works have therefore proposed using dynamic analysis and comparing runtime-observed results of computations to establish binary correspondence. In this paper, we study the discriminative power of runtime-values for matching instructions in binaries, and propose several ways to increase the accuracy of value-based analyses. By utilizing techniques from the field of information retrieval combined with dynamic data-flow analysis, we improve matching accuracy by up to 55% in our experiments.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.54","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816490","binary correspondence;binary matching;dynamic analysis;runtime values","Information retrieval;Libraries;Optimization;Plagiarism;Registers;Semantics;Syntactics","program diagnostics","binary correspondence;dynamic data-flow analysis;information retrieval;program binaries;runtime-observed values;static analysis","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"A new feature selection method for sentiment analysis of Turkish reviews","T. Parlar; S. A. Özel","Department of Mathematics, Mustafa Kemal University, Hatay, T&#x00FC;rkiye","2016 International Symposium on INnovations in Intelligent SysTems and Applications (INISTA)","20160922","2016","","","1","6","Sentiment analysis identifies people's opinions, sentiments about a product, a service, an organization, or an event. Because of huge review documents, researchers explore different feature selection methods that aim to eliminate non valuable features. However, not much work has been done on feature selection methods for sentiment analysis of Turkish reviews. In this study, we propose a new feature selection method called Query Expansion Ranking that is based on query expansion term weighting methods, which are used in Information Retrieval domain to determine the most valuable terms for query expansion. We compare Query Expansion Ranking with Chi Square method, which is a well-known and successful feature selector, and Document Frequency Difference which is a feature selection method proposed for sentiment analysis of English reviews. Experiments are conducted on four Turkish product review datasets that are book, DVDs, electronics, and kitchen appliances reviews by using a supervised machine learning classification method, namely Naïve Bayes Multinomial classifier. We show that our new proposed method improves sentiment analysis performance in terms of classification accuracy and time. In the experimental evaluation, we also show that our new feature selector improves classification accuracy better than Chi Square, and Document Frequency Difference methods.","","","10.1109/INISTA.2016.7571833","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7571833","Sentiment analysis;feature selection;text classification","Cameras;Entropy;Feature extraction;Information retrieval;Niobium;Sentiment analysis;Text categorization","Bayes methods;feature selection;learning (artificial intelligence);pattern classification;query processing;sentiment analysis","Naïve Bayes multinomial classifier;Turkish product review datasets;Turkish reviews;chi square method;document frequency difference;feature selection method;information retrieval domain;nonvaluable features elimination;query expansion ranking;query expansion term weighting methods;sentiment analysis;supervised machine learning classification method","","","","","","","2-5 Aug. 2016","","IEEE","IEEE Conference Publications"
"A semi-supervised approach for temporal information extraction from clinical text","G. Moharasar; T. B. Ho","Japan Advanced Institute of Science and Technology, 1-1 Asahidai, Nomi, Ishikawa 923-1292, Japan","2016 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future (RIVF)","20161229","2016","","","7","12","The implementation of electronic medical records (EMRs) produces a huge amount of unstructured clinical text. This domain-specific clinical text has opened a stage for temporal information extraction (TIE) due to its significance of exploitation in medical care and richness of temporality. Processing temporal information in clinical text is much more difficult in comparison to newswire text due to implicit expression of temporal information, domain-specific nature, lack of structure and writing quality. Despite of these limitations, the existing works established various methods to extract temporal information with the help of annotated corpora. But it is costly and time consuming to prepare the annotated corpora and thus their small size inevitably affect the processing quality. Motivated by this fact, in this work we propose a novel two-stage semi-supervised framework to exploit the abundant unannotated clinical text to automatically detect the temporal information and gradually increase the size of annotated corpora and then subsequently improve the temporal information extraction accuracy. In our pilot study of the proposed framework stage-one, we developed a conditional random fields (CRFs) model for the temporal event and expression extractions on the annotated data with the various features sets at phrase level. At first, we generated the possible features from the annotated corpora and significant features are selected. Finally we trained and evaluated our model with the selected features. Our model achieved F-measure of 81.34% for event recognition, and F-measure of 79.95% for temporal expression extraction.","","Electronic:978-1-5090-4134-3; POD:978-1-5090-4135-0; Paper:978-1-5090-4133-6","10.1109/RIVF.2016.7800261","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7800261","Clinical text;Electronic medical records;Information extraction;Machine learning;Natural Language Processing;Temporal events;Temporal expressions and Text mining","Data mining;Feature extraction;Information retrieval;Medical diagnostic imaging;Medical services;Natural language processing;Time-frequency analysis","electronic health records;feature selection;natural language processing;patient care;text analysis","CRF;EMR;TIE;abundant unannotated clinical text;conditional random fields;domain-specific clinical text;electronic medical records;feature selection;medical care;newswire text;temporal event;temporal expression extraction;temporal information extraction;temporal information processing;two-stage semisupervised framework;unstructured clinical text","","","","","","","7-9 Nov. 2016","","IEEE","IEEE Conference Publications"
"An uncertain diagnostic system of the constructional and technological preferences","A. Bryniarska","Institute of Control and Computer Engineering, Opole University of Technology, Opole, Poland","2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR)","20160926","2016","","","256","260","In this paper will be defined the diagnostic agent that finds information about the constructional and technological preferences of the customers for a reference technical object. Based on this agent can be implemented an uncertain system that can be used to define customers preferences. For imprecise information from users this system would give their preferences and exact data which can be used to improve production.","","","10.1109/MMAR.2016.7575143","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575143","agent;constructional-technological classifier;preference diagnostic;semantic network;similarity indicator;uncertain system","Information retrieval;Mathematical model;Ontologies;Semantics;Thesauri;Turbines;Uncertainty","data mining;information retrieval;program diagnostics;uncertain systems","customer constructional preferences;customer technological preferences;diagnostic agent;exact data;reference technical object;uncertain diagnostic system;user information","","","","","","","Aug. 29 2016-Sept. 1 2016","","IEEE","IEEE Conference Publications"
"Tailored Summary for automatic poster generator","K. Paramita; M. L. Khodra","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2016 International Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA)","20170102","2016","","","1","6","Information delivery with posters is more effective than textual because of human cognitive abilities to understand information is increasing with the help of the graphical representation of the visual system. However, for some people, making posters is more difficult than making paper because it requires the ability to deliver information visually and the ability to extract critical information. But with the development of methods of extracting information in a text, appears an opportunity to generate poster automatically. This paper's aim is to develop an automated generation of poster by implementing paper summarization methods Tailored Summary that extracts information on paper using the rhetorical classification results of sentences. This method allows the generation of a poster performed using the method of summarizing a paper by considering the information needs of the user. The classification model has a F-Measure value of 0.804 and average users score of the posters generated gained is 6.622. Maximal Marginal Relevance is used in the process of sentences selection.","","Electronic:978-1-5090-1636-5; POD:978-1-5090-1637-2","10.1109/ICAICTA.2016.7803112","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7803112","Tailored Summary;information extraction;scientific poster;sentence rhetorical classification","Buildings;Data mining;Generators;Informatics;Information retrieval;Layout;Maintenance engineering","information retrieval;pattern classification;text analysis","F-Measure value;automatic poster generator;critical information extraction;graphical representation;human cognitive abilities;information delivery;maximal marginal relevance;paper summarization;rhetorical classification;sentence selection;tailored summary;visual system","","","","","","","16-19 Aug. 2016","","IEEE","IEEE Conference Publications"
"CLRIC: Collecting Lane-Based Road Information Via Crowdsourcing","L. Tang; X. Yang; Z. Dong; Q. Li","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Intelligent Transportation Systems","20160826","2016","17","9","2552","2562","Lane-based road network information, such as the number and locations of traffic lanes on a road, has played an important role in intelligent transportation systems. In this paper, we propose a Collecting Lane-based Road Information via Crowdsourcing (CLRIC) method, which can automatically extract detailed lane structure of roads by using crowdsourcing data collected by vehicles. First, CLRIC filters the high-precision GPS data from the raw trajectories based on region growing clustering with prior knowledge. Second, CLRIC mines the number and locations of traffic lanes through optimized constrained Gaussian mixture model. Experiments are conducted with taxi GPS trajectories in Wuhan, China, and the results show that CLRIC is quantified and displays detailed road networks with the number and locations of traffic lanes comparing with the satellite image and human-interpreted situation.","1524-9050;15249050","","10.1109/TITS.2016.2521482","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442564","Lane-based road information;crowdsourcing data;high-precision GPS data filtering;spatiotemporal GPS trajectories","Crowdsourcing;Data mining;Global Positioning System;Information retrieval;Roads;Trajectory;Vehicles","Gaussian processes;Global Positioning System;information filtering;intelligent transportation systems;mixture models;pattern clustering;road traffic","CLRIC;China;Wuhan;collecting lane-based road information via crowdsourcing;crowdsourcing data;high-precision GPS data;intelligent transportation systems;lane structure extraction;optimized constrained Gaussian mixture model;raw trajectories;region growing clustering;road networks;taxi GPS trajectories;traffic lanes","","","","","","20160328","Sept. 2016","","IEEE","IEEE Journals & Magazines"
"Damaged road extraction from post-seismic remote sensing images based on gis and object-oriented method","Q. Li; J. Zhang; N. Wang","Key Laboratory of Earthquake Engineering and Engineering Vibration, Institute of Engineering Mechanics, China Earthquake Administration, Harbin, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","4247","4250","Earthquake is one of the most destructive natural disasters. Lushan Ya'an earthquake occurred in April 20, 2013 caused a wide range of road damage. The earthquake made Baoxing country become an island and rescue was difficult to carry out. The secondary disaster caused by earthquakes is one of the main causes of road damage. According to the present situation of damaged road extraction, a new method of remote sensing image extraction based on object-oriented and GIS spatial analysis is proposed in this paper. The method can extract damaged road caused by secondary disasters in a short period of time, then identify the location and extent of damage to roads as quickly as possible. Through a series of experiments, This method can quickly extract the location and range of the damage caused by the secondary disasters, and get good result. The overall extraction accuracy is 95%. The method can greatly reduce the time of post-earthquake imagery interpretation, improve the efficiency of earthquake emergency and provide decision support for earthquake emergency response eventually.","","","10.1109/IGARSS.2016.7730107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730107","Damaged roads;GIS;Object-oriented;Remote sensing;Secondary disaster","Data mining;Earthquakes;Image segmentation;Information retrieval;Remote sensing;Roads;Visualization","earthquakes;feature extraction;geographic information systems;geophysical image processing;roads;seismology;terrain mapping","AD 2013 04 20;Baoxing country;GIS spatial analysis;Lushan Yaan earthquake;damaged road extraction;earthquake emergency efficiency;earthquake emergency response;object-oriented analysis;object-oriented method;postearthquake imagery interpretation;postseismic remote sensing image;remote sensing image extraction;road damage;road damage extent","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Measuring Code Similarity in Large-Scaled Code Corpora","C. Ragkhitwetsagul","Dept. of Comput. Sci., Univ. Coll. London, London, UK","2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)","20170116","2016","","","626","630","Source code similarity measurement is a fundamental technique in software engineering research. Techniques to measure code similarity have been invented and applied to various research areas such as code clone detection, finding bug fixes, and software plagiarism detection. We perform an evaluation of 30 similarity analysers for source code. The results show that specialised tools including clone and plagiarism detectors, with proper parameter tuning, outperform general techniques such as string matching. Although these specialised tools can handle code similarity in local code bases, they fail to locate similar code artefacts from large-scaled corpora. This is increasingly important considering the rising amount of online code artefacts. We propose a scalable search system specifically designed for source code. It lays a foundation to discovering online code reuse, large-scale code clone detection, finding usage examples, detecting software plagiarism, and finding software licensing conflicts. Our proposed code search framework is a hybrid of information retrieval and code clone detection techniques. This framework will be able to locate similar code artefacts instantly. The search is not only based on textual similarity, but also syntactic and structural similarity. It is resilient to incomplete code fragments that are normally found on the Internet.","","Electronic:978-1-5090-3806-0; POD:978-1-5090-3807-7","10.1109/ICSME.2016.18","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816530","clone detection;code search;empirical study;software plagiarism detection;source code similarity","Cloning;Detectors;Indexes;Information retrieval;Java;Plagiarism;Software","software engineering;source code (software)","bug fix finding;code clone detection;information retrieval technique;large-scaled code corpora;scalable search system;similarity analysers;software engineering research;software plagiarism detection;source code similarity measurement;structural similarity;syntactic similarity","","","","","","","2-7 Oct. 2016","","IEEE","IEEE Conference Publications"
"Visualizing the First World War Using StreamGraphs and Information Extraction","A. A. Haidar; B. Yang; J. G. Ganascia","Lip6, Labex OBVIL, Paris, France","2016 20th International Conference Information Visualisation (IV)","20160901","2016","","","290","293","In this paper, we develop a visualization tool that applies unsupervised named entity recognition and streamgraphs in order to visualize massive amounts of unstructured textual stream data, namely, French newspapers (e.g. Le Figaro, La presse, L'humanité) from the first world war period. Such a visualization allows us to identify main characters, events and locations involved in or relevant to the first world war, according to the French press. Furthermore, our visualization technique can help visually identify correlations between major people (e.g. presidents, generals, public figures...), locations (e.g. countries, cities, towns...) and organizations and events (e.g. corporations, battles...) on multiple aligned streamgraphs. We discuss the streamgraph method and compare it to alternative methods while drawing some examples from the visualization of the first world war. Our method helps users identify named entities and visualise them highlighting significant ones at specific time periods. Furthermore, it can be applied to unstructured data streams of any domain or time period.","","Electronic:978-1-4673-8942-6; POD:978-1-4673-8943-3","10.1109/IV.2016.81","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557941","Named entity recognition;data visualization;infoVis;information extraction;information visualization;stacked graphs;stream data;stream graphs","Computer science;Data visualization;Information retrieval;Organizations;Presses;Rivers;Visualization","data visualisation;graph theory;unsupervised learning","entity recognition;first-world war visualization;information extraction;multiple aligned streamgraphs;streamgraph method;unstructured data streams;unstructured textual stream data;users identify;visualization technique;visualization tool","","","","","","","19-22 July 2016","","IEEE","IEEE Conference Publications"
"A survey of document clustering using semantic approach","N. Y. Saiyad; H. B. Prajapati; V. K. Dabhi","Department of Information Technology, Dharamsinh Desai University, Nadiad, India","2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)","20161124","2016","","","2555","2562","Document clustering is the application of cluster analysis to textual documents. It is commonly used technique in data mining, information retrieval, knowledge discovery from data, pattern recognition, etc. In traditional document clustering, a document is considered as a bag of words; where semantic meaning of word is not taken into consideration. However, to achieve accurate document clustering, feature such as meanings of the words is important. Document clustering can be done using semantic approach because it takes semantic relationship among words into account. This paper highlights the problems in traditional approach as well as semantic approach. This paper identifies four major areas under semantic clustering and presents a survey of 23 papers that are studied, covering major significant work. Moreover, this paper also provides a survey of tools specifically used for text processing, and clustering algorithms, that help in applying and evaluating document clustering. The presented survey is used in preparing the proposed work in the same direction. This proposed work uses the sense of a word for text clustering system. Lexical chains will be used as features that are to be developed using the identity/synonymy relation from WordNet ontology as background knowledge. Later, clustering will be done using the lexical chains.","","CD:978-1-4673-9936-4; DVD:978-1-4673-9937-1; Electronic:978-1-4673-9939-5; POD:978-1-4673-9940-1; USB:978-1-4673-9938-8","10.1109/ICEEOT.2016.7755154","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755154","Latent Semantic Indexing;WordNet;clustering;lexical chains;ontology;semantic clustering","Clustering algorithms;Information retrieval;Large scale integration;Matrix decomposition;Ontologies;Semantics;Text analysis","feature selection;ontologies (artificial intelligence);pattern clustering;text analysis","WordNet ontology;cluster analysis;clustering algorithms;document clustering;identity relation;lexical chains;semantic clustering;synonymy relation;text clustering system;text processing;textual documents","","","","","","","3-5 March 2016","","IEEE","IEEE Conference Publications"
"Paragraph vector based retrieval model for similar cases recommendation","Y. Zhao; J. Wang; F. Y. Wang; X. Shi; Y. Lv","State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China","2016 12th World Congress on Intelligent Control and Automation (WCICA)","20160929","2016","","","2220","2225","Internet inquiry is playing an increasingly important role as the complement of the traditional medical service system, especially the similar cases recommendation. It can not only save the patients' waiting time, but also make use of the historical resources, for many cases with the same purpose have been solved perfectly. However, because of the diversity and non-standard of the patients' descriptions, the inquiry platform cannot find the cases with similar semantic easily. Most traditional retrieval methods require the overlap of two sentences, and this is not suitable with the diversity and non-standard descriptions. In this paper, we try to utilize the sentences' semantic representation in a continuous space to understand the cases, and then recommend the similar cases. We also incorporate it into query likelihood language models, trying to get better results. Our experimental data are all collected from a real internet inquiry platform, and the results show that our methods significantly outperform the state-of-the-art translation based methods for similar cases recommendation.","","","10.1109/WCICA.2016.7578614","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7578614","","Context;Information retrieval;Internet;MONOS devices;Medical services;Optimization;Semantics","Internet;medical computing;query processing;recommender systems;vectors","Internet inquiry platform;medical service system;paragraph vector based retrieval model;query likelihood language models;sentences semantic representation;similar cases recommendation","","","","","","","12-15 June 2016","","IEEE","IEEE Conference Publications"
"Identification & verification of mutations in Human Genome","D. N. Kamalsooriya; R. D. T. W. Ranasinghe; A. R. Weerasinghe; M. W. A. C. R. Wijesinghe","University of Colombo School of Computing, 35, Reid Avenue, 07, Sri Lanka","2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer)","20170126","2016","","","147","154","The completion of Human Genome Project (HGP) catalyzed the development of Next Generation Sequencing (NGS) technologies. An important use of NGS techniques is to detect the full range of genomic variation which lead to diseases. However, the velocity at which such findings are published sometimes causes latency to exist between the NGS data available to researchers and the latest research that is published. Due to the prevalence of gaps and inconsistencies in the variant analysis process, researchers are facing issues in discriminating disease-associated variations (mutations) from a large number of un-harmful genomic variants. This paper presents a framework to discriminate variants with deleterious effects and to verify such effects using an automated literature search. The proposed regression model to identify mutations quantitatively assesses the harmful nature of variants by taking into account the annotated information about variants from online databases. In addition, a TF-IDF based document relevancy ranking algorithm is used to improve the efficiency of document retrieval for verifying the harmfulness of the variants. The regression model was successfully able to narrow down variants to a very small number of candidate variants achieving a value of 89.5% for the average accuracy. Spearman's rho and Kendall's tau values were measured to assess the performance of the ranking algorithm, yielding values of 85.5% and 68.8% respectively, confirming the validity of the framework in verification of deleterious impacts of identified mutations.","","CD:978-1-5090-6076-4; Electronic:978-1-5090-6078-8; POD:978-1-5090-6079-5; Paper:978-1-5090-6077-1","10.1109/ICTER.2016.7829912","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829912","bioinformatics;logisitc regression;mutations;prioritization;tf-idf","Bioinformatics;Databases;Diseases;Genomics;Information retrieval;Logistics;Sequential analysis","bioinformatics;document handling;genomics;regression analysis;relevance feedback","HGP;NGS data;NGS techniques;Next Generation Sequencing technologies;TF-IDF based document relevancy ranking algorithm;automated literature search;bioinformatics;disease-associated variations;document retrieval efficiency improvement;genomic variation;human genome project;mutation identification;mutation verification;online databases;regression model;variant analysis process","","","","","","","1-3 Sept. 2016","","IEEE","IEEE Conference Publications"
"A new stemmer for Nepali language","I. Shrestha; S. S. Dhakal","Department of Information Technology Engineering Nepal College of Information Technology Balkumari, Lalitpur, Nepal","2016 2nd International Conference on Advances in Computing, Communication, & Automation (ICACCA) (Fall)","20161121","2016","","","1","5","Nepali is an inflectionally rich language. The unavailability of a stemmer, that effectively eliminates inflections, makes the task of searching, text mining, information retrieval and natural language processing in Nepali challenging. Stemming helps in mapping all variants of inflected words to their respective stems. Stemmers are widely used in Text Mining, Information Retrieval and Natural Language Processing systems to improve the performance of such systems. This paper presents a new rule-based stemmer for Nepali language. We have composed 128 suffix rules, which are executed in step-by-step and iterative manner to eliminate inflections. The proposed stemmer was tested on 5000 Nepali words and the overall accuracy was around 88.78%.","","Electronic:978-1-5090-3480-2; POD:978-1-5090-3481-9","10.1109/ICACCAF.2016.7749008","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749008","Inflections;Nepali language;Stemmer;rule-based","Algorithm design and analysis;Gold;Information retrieval;Information technology;Morphology;Strips","data mining;information retrieval;iterative methods;natural language processing;word processing","Nepali language;inflection elimination;information retrieval;iterative manner;natural language processing systems;rule-based stemmer;text mining","","","","","","","Sept. 30 2016-Oct. 1 2016","","IEEE","IEEE Conference Publications"
"Toward multilingual system in e-orientation domain","K. J'Nini; F. Benabbou; N. Sael","Laboratoire Traitement de l'Information et Mod&#x00E9;lisation, Facult&#x00E9; des sciences Ben M'SIK Casablanca","2016 11th International Conference on Intelligent Systems: Theories and Applications (SITA)","20161208","2016","","","1","6","The orientation is occupying an increasingly important role in the process of determining the future of the students, which leads to the obligation to make it automatic and accessible, both in terms of immediate accessibility throughout the world or in the offering of several languages that correspond to the language used by the majority of students or persons in need of guidance. These problems can be avoided by setting up an e-orientation platform accessible from the web this will solve the problem of accessibility, and the use of approaches and targeted ontologies can handle multilingual aspects.","","Electronic:978-1-5090-5781-8; POD:978-1-5090-5782-5","10.1109/SITA.2016.7772307","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7772307","CLIR;document translation;multilingual;query translation","Computers;Data mining;Information retrieval;Intelligent systems;Internet;Ontologies;Training","Internet;language translation;linguistics;ontologies (artificial intelligence);query processing","document translation;e-orientation domain;multilingual system;query translation;targeted ontologies","","","","","","","19-20 Oct. 2016","","IEEE","IEEE Conference Publications"
"Forest land type precise classification based on SPOT5 and GF-1 images","C. Ren; H. Ju; H. Zhang; J. Huang","Research Institute of Forest Resource Information Techniques, Chinese Academy of Forestry, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","894","897","The objective of this paper is to develop a hierarchical classification scheme and propose forest land type precise classification method based on SPOT5, GF-1 images, and other multi-source data, focusing on fine classification of forest land using high resolution remote sensing image in complex mountainous terrain conditions. The experiments were carried out by multi-source data integration, multiple features analysis and multiple classifier combination. The proposed method in this paper have advantages in fine identification of forest land types with high accuracy and high reliability, and the detail degree of fine identification reaches dominant tree species, which could fully meet the needs of forestry applications such as forest resources investigation, forest land change monitoring and thematic map digital update.","","","10.1109/IGARSS.2016.7729226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729226","Forest Land Type;GF-1 Image;Hierarchical Information Extraction;Multi-Source Data;Multiple Classifier Combination;Precise Classification","Forestry;Image resolution;Indexes;Information retrieval;Remote sensing;Vegetation;Vegetation mapping","forestry;terrain mapping","GF-1 images;SPOT5;forest land type precise classification;forestry applications;mountainous terrain conditions;multi-source data integration;remote sensing image;thematic map digital update","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"A novel approach to sentence clustering","D. Sahoo; R. Balabantaray","Dept. of Computer Science & Engineering, IIIT-Bhubaneswar, Odisha, India-751003","2016 International Conference on Computing, Communication and Automation (ICCCA)","20170116","2016","","","1","6","Sentence clustering is often used as the first step in various information retrieval tasks like automatic text summarization, topic detection and tracking etc. Researchers face difficulty to cluster sentences because a single sentence is less informative compared to document. We present a sentence Feature Based Sentence Clustering, FBSC, which incorporates some sentence level relationship features like transition relationship, anaphoric relationship, and term (word) similarity relationship in association with Markov Clustering Algorithm (MCL) to generate sentence clusters. We observe that Purity of clusters generated by Feature Based Sentence Clustering (FBSC) compared to baseline k-means sentence clustering is better.","","Electronic:978-1-5090-1666-2; POD:978-1-5090-1667-9","10.1109/CCAA.2016.7813697","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813697","anaphoric relationship;clustering;term similarity;transition relationship","Automation;Clustering algorithms;Computational modeling;Information retrieval;Markov processes;Silicon;Text mining","Markov processes;information retrieval;pattern clustering;text analysis","FBSC;MCL;Markov clustering algorithm;anaphoric relationship;automatic text summarization;feature based sentence clustering;information retrieval tasks;term similarity relationship;topic detection;topic tracking;transition relationship;word similarity relationship","","","","","","","29-30 April 2016","","IEEE","IEEE Conference Publications"
"Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems","M. Wan; J. McAuley","Univ. of California, San Diego, La Jolla, CA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","489","498","Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single 'correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system, and (2) What makes a 'good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions-and over 3.1 million answers-and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837873","","Cameras;Data mining;Information retrieval;Knowledge discovery;Lenses;Predictive models;Standards","Web sites;question answering (information retrieval)","QA dataset;QA system;QA tasks;opinion question answering systems;personalization;product review Web sites","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Extraction of biomedical informtion related to breast cancer using text mining","L. Gong; R. Yan; Q. Liu; H. Yang; G. Yang; K. Jiang","Jiangsu High Technology Research Key Lab for Wireless Sensor Networks, College of Computer Science & Technology, Nanjing University of Posts and Telecommunications, Nanjing, 210003, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","20161024","2016","","","801","805","In this paper, we provide an approach to extract biomedical information related to breast cancer using text mining technology. We first extract entities related to breast cancer, then find these relationships, and visualize these biomedical information. Finally, these extracted biomedical information are annotated in the original experimental dataset which offer researchers as breast cancer biomedical information corpus. the approach provide an new way to obtain biomedical information of breast cancer. Moreover, it is promising for development of biomedical text mining.","","","10.1109/FSKD.2016.7603278","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603278","breast cancer;entity recognition;relationship extraction;visualization","Breast cancer;Information retrieval;Natural language processing;Proteins;Telecommunications;Text mining","cancer;data mining;data visualisation;feature extraction;medical computing;text analysis","biomedical information extraction;biomedical information visualization;breast cancer;text mining","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conference Publications"
"Semi-structured document extraction based on document element block model","T. Lv; J. Liu; F. Lu; P. Zhang; X. Wang; C. Wang","School of Software Engineering, Beijing University of Posts and Telecommunications, Beijing 100876, China","2016 4th International Conference on Cloud Computing and Intelligence Systems (CCIS)","20161219","2016","","","461","465","A large number of documents related to its specific business are produced continually by enterprises and institutions in their daily work. To get useful information from these semi-structured documents we have proposed document element block model(DEBM) and applied it in the semi-structured document extraction. The model makes full use of the information contains in the document, not only the structural information, but also the content. DEBM extracts document element block from template documents and target documents, and then generate corresponding regular expression rules based on the document element block characteristic of template document, after that process each type of document elements of a set of blocks extracted document elements according to the corresponding elements block position by regular expression matching. The experiments show that extraction based on DEBM achieved good results and compared to traditional regular expressions and template matching, the approach based on DEBM performs better. The result shows that we propose a simple, efficient model to extract semi-structured documents.","","CD:978-1-5090-1254-1; Electronic:978-1-5090-1256-5; POD:978-1-5090-1257-2","10.1109/CCIS.2016.7790302","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790302","Document extraction;Regular expression;Semi-structured document","Business;Hospitals;Information retrieval;Measurement;Telecommunications;Text analysis","data structures;document handling;feature extraction;pattern matching","DEBM;document element block characteristic;document element block extraction;document element block model;regular expression matching;regular expression rules;semistructured document extraction;target documents;template documents;template matching","","","","","","","17-19 Aug. 2016","","IEEE","IEEE Conference Publications"
"Secure Semantic Computing","H. Yamaguchi; R. Fujita; S. Tsujii","R&D Initiative, Chuo Univ., Tokyo, Japan","2016 IEEE Second International Conference on Multimedia Big Data (BigMM)","20160818","2016","","","390","393","Semantic computing is an emerging research field that has drawn much attention from both academia and industry. It addresses the derivation and matching of semantics of computational ""contents"" where ""contents"" may be anything including text, multimedia, hardware, network, etc. which can be mapped to many areas in computer science that involve analyzing and processing the intension of humans with computational content. This paper discusses some potential applications of Semantic Computing in Computer Science. We examine semantic analytical techniques as applied for primary sequence analysis for genes and proteins in biomedical area. We also introduce how privacy and confidentiality problems are resolved, while the query from a user are written in natural language.","","Electronic:978-1-5090-2179-6; POD:978-1-5090-2180-2","10.1109/BigMM.2016.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545055","Cryptosystem;Semantic computing;confidentiality;formal method;privacy","Encryption;Information retrieval;Natural languages;Privacy;Proteins;Semantics","bioinformatics;data privacy;natural language processing","biomedical area;computational contents;computer science;confidentiality problem resolution;gene analysis;humans intension analysis;natural language query;primary sequence;privacy problem resolution;protein analysis;secure semantic computing;semantic analytical techniques;semantics matching","","","","","","","20-22 April 2016","","IEEE","IEEE Conference Publications"
"Contextual Binding and Intelligent Targeting","J. Q. Chen","Nat. Defense Univ., Washington, DC, USA","2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)","20170116","2016","","","701","704","Rapid and accurate targeting is in great demand in cybersecurity, especially for automated targeting. Current automated approaches are not efficient and effective in selecting the most relevant hints within the first attempt or within the first few attempts in investigative search, not mentioning generating intelligent hints. In most case, human intervention is required. How can intelligent automation be built into searches to speed up the process? What structural relationship can be utilized in providing intelligence not only in capturing current status but also offering intelligent hints or guidance in getting closer to the correct target in order to find out truth? These are the questions that this paper addresses. It also explores the unique structural relationship in contextual binding, which is unfortunately not well studied.","","Electronic:978-1-5090-4470-2; POD:978-1-5090-4471-9","10.1109/WI.2016.0125","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7817142","Contexts;accurate attribution;contextual binding;intelligent hints;machine learning;structural relationship","Automation;Computer security;Context;Cyberspace;Information retrieval;Supervised learning;Unsupervised learning","learning (artificial intelligence);security of data","automated targeting;contextual binding;cybersecurity;intelligent automation;intelligent hints;intelligent targeting;investigative search;machine learning","","","","","","","13-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Geological disaster detection from remote sensing image based on experts' knowledge and image features","Y. Ren; Y. Liu","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","677","680","It is significant for geological disasters detection from remote sensing image in emergency rescue. However, the automatic detection methods for geological disasters, depending only on low-level imagery features, generally result in low recognition precision. In practice, the most current extraction approaches of geological disasters are manual visual interpretation with the aid of experts' knowledge. The means have been inefficient and expensive. Accordingly, the aim of this paper is improving the recognition accuracy and efficiency of geological disasters. On the basis of object-oriented image segmentation, the study proposes a method for geological disasters detection with experts' knowledge and image features comprehensively. The results show that this method is more accurate than pixel-based classification and object-oriented image classification only with image features and more efficiency than manual visual interpretation. The study will promote the researches of disaster information extraction from remote sensing images automatically and intelligently.","","","10.1109/IGARSS.2016.7729170","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729170","expert's knowledge;geological disaster;image feature;object-oriented;remote sensing","Feature extraction;Geology;Image segmentation;Information retrieval;Remote sensing;Terrain factors;Visualization","disasters;feature extraction;geophysical image processing;image classification;image segmentation;remote sensing","disaster information extraction;expert knowledge;feature extraction;geological disaster detection;image feature;image recognition accuracy;object-oriented image classification;object-oriented image segmentation;pixel-based classification;remote sensing image;visual interpretation","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Semantic similarity based assessment of descriptive type answers","K. Meena; R. Lawrance","Research & Development Centre, Bharathiar University, Coimbatore, India","2016 International Conference on Computing Technologies and Intelligent Data Engineering (ICCTIDE'16)","20161031","2016","","","1","7","The knowledge assessment and competence is an important division in an education procedure. An automatic evaluation system would be used to make sure the confidentiality and neutrality of evaluation. In this paper, proposed an algorithm called Assessment algorithm that uses semantic similarity for evaluation of detailed type answers. It will eradicate the inconsistency in the manual assessment. Also, for preprocessing proposed an algorithm for pruning and stemming which is used to reduce the size of the descriptive type answers. Stemmed words are converted to vectors using the semantic method, Latent Semantic Analysis (LSA). The vectors obtained from the semantic method are clustered using the Self-organizing map. Cosine similarity is used to measure the similarity between two vectors. Based on the value returned by the similarity measure, marks will be awarded. The proposed system will be experienced with detailed type answers written by the student. This method resulted in good precision, improved dependability of results, reduced the time and effort taken by the staff. The time complexity of the proposed algorithm is O (n).","","","10.1109/ICCTIDE.2016.7725366","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7725366","Descriptive type answers;Latent Semantic Analysis;Self-Organizing Map;clustering","Context;Context modeling;Data mining;Information retrieval;Matrix decomposition;Self-organizing feature maps;Semantics","computational complexity;educational computing;self-organising feature maps;text analysis","LSA;answers size reduction;assessment algorithm;automatic evaluation system;cosine similarity;descriptive type answers;education procedure;evaluation confidentiality;evaluation neutrality;knowledge assessment;knowledge competence;latent semantic analysis;pruning method;self-organizing map;semantic similarity based assessment;stemmed words;time complexity;vectors","","","","","","","7-9 Jan. 2016","","IEEE","IEEE Conference Publications"
"BioProber: Software System for Biomedical Relation Discovery from PubMed","H. Jang; J. Lim; J. H. Lim; S. J. Park; K. C. Lee","Bioinformatics Team, Electronics and Telecommunication Research Institute, Gajeong-Dong, Yuseon-Gu, Daejeon, 305-700, Korea","2006 International Conference of the IEEE Engineering in Medicine and Biology Society","20161215","2006","","","5779","5782","The numbers of articles and journals that are published are increasing at a considerable rate, and the published information is growing continuously and fast. Because of this, researches to acquire knowledge automatically have been carried out in the areas of information retrieval, information extraction and text mining. Information retrieval approaches are good for specific topics that the number of related articles is small. But, if the number is bigger, searching skill and knowledge acquisition ability are useless. Though many efforts have been made to extract information from literature, many approaches have concentrated on specific entities, such as proteins, genes and their interactions, and much information is still remained in unstructured text. So, we have developed a system that discovers relations between various categories of biomedical entities. Our system collects abstracts from PubMed by queries representing a topic and visualizes relationship from the collection by automatic information extraction","1557-170X;1557170X","CD:1-4244-003303; Paper:1-4244-0032-5","10.1109/IEMBS.2006.259838","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463120","","Abstracts;Cities and towns;Data mining;Databases;Erbium;Information retrieval;Natural language processing;Proteins;Software systems;USA Councils","data mining;medical information systems;natural language processing;query formulation","BioProber;PubMed;biomedical entities;biomedical relation discovery;information extraction;information retrieval;knowledge acquisition ability;natural language processing techniques;query information;software system;text mining","Algorithms;Alzheimer Disease;Computational Biology;Databases, Bibliographic;Humans;Information Storage and Retrieval;Natural Language Processing;Pattern Recognition, Automated;Programming Languages;PubMed;Software;Subject Headings;User-Computer Interface;Vocabulary, Controlled","","","","","","Aug. 30 2006-Sept. 3 2006","","IEEE","IEEE Conference Publications"
"Object oriented land cover classification combining scale parameter preestimation and mean-shift segmentation","Y. Qiu; D. Ming; X. Zhang","School of Information Engineering, China University of Geosciences (Beijing), 29 Xueyuan Road, Haidian, Beijing, 100083, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","20161103","2016","","","6332","6335","Object-based image analysis (OBIA) provides a better solution for information extraction from high spatial resolution remote sensing image. Currently, selection of scale parameters is often dependent on subjective trial-and-error methods or post-evaluation of multi-segmentation, which directly reduces efficiency of land cover classification. This paper proposes a OBIA classification method combining spatial statistics based adaptive scale parameter pre-estimation and Mean-shift segmentation. Series of object based classification were employed to verify the validity of this method. Experimental results show that the pre-estimated scale parameter can guarantee a classification result with both high classification accuracy and good completeness for land cover classification. This presented method avoids the time-consuming trial-and-error practice so that it speeds up the object-oriented classification procedure.","","","10.1109/IGARSS.2016.7730655","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730655","Object-based image analysis;image classification;mean-shift segmentation;scale pre-estimation","Bandwidth;Feature extraction;Histograms;Image segmentation;Information retrieval;Remote sensing;Spatial resolution","geophysical techniques;image classification;image segmentation;land cover;remote sensing","OBIA classification method;adaptive scale parameter pre-estimation;high spatial resolution remote sensing image;information extraction solution;mean-shift segmentation;multisegmentation post-evaluation;object based classification;object oriented land cover classification;object-based image analysis;scale parameter preestimation;spatial statistics;subjective trial-and-error methods;time-consuming trial-and-error practice","","","","","","","10-15 July 2016","","IEEE","IEEE Conference Publications"
"Multitemporal Level- $1beta$ Products: Definitions, Interpretation, and Applications","D. Amitrano; F. Cecinati; G. D. Martino; A. Iodice; P. P. Mathieu; D. Riccio; G. Ruello","Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy","IEEE Transactions on Geoscience and Remote Sensing","20160927","2016","54","11","6545","6562","In this paper, we present a new framework for the fusion, representation, and analysis of multitemporal synthetic aperture radar (SAR) data. It leads to the definition of a new class of products representing an intermediate level between the classic Level-1 and Level-2 products. The proposed Level-1β products are particularly oriented toward nonexpert users. In fact, their principal characteristics are the interpretability and the suitability to be processed with standard algorithms. The main innovation of this paper is the design of a suitable RGB representation of data aiming to enhance the information content of the time-series. The physical rationale of the products is presented through examples, in which we show their robustness with respect to sensor, acquisition mode, and geographic area. A discussion about the suitability of the proposed products with Sentinel-1 imagery is also provided, showing the full compatibility with data acquired by the new European Space Agency sensor. Finally, we propose two applications based on the use of Kohonen's self-organizing maps dealing with classification problems.","0196-2892;01962892","","10.1109/TGRS.2016.2586189","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523302","High-level processing;Level- $1beta$ products;Sentinel-1;human–machine interaction;image enhancement;multitemporal synthetic aperture radar (SAR);self-organizing maps (SOMs)","Context;Data analysis;Data mining;Data models;Information retrieval;Standards;Synthetic aperture radar","radar imaging;self-organising feature maps;synthetic aperture radar;time series","European Space Agency sensor;Kohonen self-organizing maps;RGB representation;SAR data;acquisition mode;classic level-1 products;classic level-2 products;fusion framework;geographic area;intermediate level;multitemporal level-1β products;multitemporal synthetic aperture radar data;representation framework;sensor mode;sentinel-1 imagery;time series","","","","","","20160727","Nov. 2016","","IEEE","IEEE Journals & Magazines"
"Improvement of complementary pedagogical resources indexing based on pedagogical warehouse for recommendation system CEHL","R. M. Kamal; K. Mohamed; D. Ali","LIROSA Laboratory, Abdelmalek Essaadi University, Faculty of sciences, Tetouan, Morocco","2016 Third International Conference on Systems of Collaboration (SysCo)","20170126","2016","","","1","5","In recent years, the fast growth of Web pages and the constant evolution of Internet technologies have lead to a significant increase in the number of pedagogical resources. Thus, the indexing and search problems have become crucial. To overcome this problem, it was proposed to use information coming from the norms and standards of educational metadata. However, this solution does not solve completely the problem. Previously, traditional information retrieval systems rely on indexing by keywords for representing pedagogical resources and queries content. This process, based on lexical matching, allows selecting pedagogical resources based on keywords shared with the query, which can reduce the accuracy of search results if the meaning of common words in the query and in the pedagogical resources is different. To overcome this issue and provide a more sophisticated search, adding semantics becomes necessary. Semantic Indexing offers a representation by the meaning of words in order to find pedagogical resources semantically relevant to the user request. We present in this paper the choice of the indexing approach used for the complementary educational resources. The main objective is to integrate this approach in the warehouse model that aims on one hand to store the complementary pedagogical resources, and on the other hand to help the user fill in the description fields of these resources.","","Electronic:978-1-5090-4926-4; POD:978-1-5090-4927-1","10.1109/SYSCO.2016.7831339","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831339","Indexing based ontology;e-Learning;information search;pedagogical warehouse;recommendation;semantic indexing;wordnet","Context;Indexing;Information retrieval;Metadata;Ontologies;Pragmatics;Semantics","computer aided instruction;data warehouses;database indexing;ontologies (artificial intelligence);query processing;recommender systems","CEHL recommendation system;Web pages;complementary educational resources;complementary pedagogical resource indexing improvement;educational metadata;lexical matching;pedagogical warehouse;query processing;search accuracy reduction;semantic indexing","","","","","","","28-29 Nov. 2016","","IEEE","IEEE Conference Publications"
"NLP-driven ontology modeling for handling event semantics in NL constraints","M. Malik; M. Saleem","Department of Computer Science, NCBA&E, Lahore","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","485","490","To assist decision makers, there is need of providing an insight on current of scenario of market, considering really sensitive news involving economic events like acquisitions, stock splits, or dividend announcements. Similar to the work discussed above, to machine process natural language constraints, there is need of a mechanism that can automate events related information extraction and knowledge acquisition processes to facilitate software developers in fulfilling their cumbersome tasks, for quicker and accurate of software modeling. However, extraction of events and their related information from natural language constraints is a complex task considering the ambiguous nature of natural languages such as English. However, similar problems have been solved previously by other researchers with the help of using event semantic ontologies. However, there is need to plan a mechanism for developing an event semantic ontology for natural language constraints. It is discussed in the previous sections that the Ontology can provide assistance in identification of relations existing in various annotations. It is discussed in this thesis that it is significant to use ontology for identifying events from text. With respect to the scope of this thesis, a framework is developed to generate an event semantic ontology.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845085","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845085","Event Extraction;Natural Language Processing;Ontology Engineering;Semantic Technology","Data mining;Information retrieval;Natural languages;Ontologies;Semantics;Software","constraint handling;knowledge acquisition;natural language processing;ontologies (artificial intelligence)","English language;NL constraints;NLP-driven ontology modeling;event semantic ontology;event semantics handling;events related information extraction;knowledge acquisition;natural language constraints;natural language processing;software modeling","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
