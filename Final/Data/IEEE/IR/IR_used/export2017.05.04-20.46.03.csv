"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6427744,6701510,6589140,6576807,6547637,6468199,6415918,6400557,6400492,6400549,6400488,6240502,6051424,5680902,5708146,5661768,5871572,5702097,5678590,5676123,5652896,5652890,5633972,5620890,5551108,5447529,5396304,5427377,5226632,5342377,5333437,5255211,5255213,4653482,4693708,4604659,4641911,4564462,4731234,4620107,4797938,4509437,4731229,4569847,4515870,4677369,4677362,4497195,4665915,4629725,4497193,4527251,4538228,4623219,4604500,4445672,4585330,4580548,4547955,4445668,4488443,4488251,4384483,4358974,4358969,4358952,4358941,4358944,4160221,4118562,1556536,6400518,7916770,7914947,7905698,7905587,7900156,7897282,7892454,7891819,7892635,7890880,7889546,7888729,7889462,7888315,7865976,7870570,7883434,7884609,7883382,7883763,7883681,7881504,7880229,7877977,7879334,7877284,7875945,7872753",2017/05/04 20:46:03
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Performance Analysis of Local Database Management Systems for Mobile Applications","M. Kandekar; R. Ingle","Comput. Eng. Dept., Pune Inst. of Comput. Technol., Pune, India","2013 International Conference on Cloud & Ubiquitous Computing & Emerging Technologies","20140109","2013","","","236","239","Nowadays mobile applications are being widely used for daily need. Among those applications most of the applications include data management. Database Management System (DBMS) plays a vital role for an application to be efficient and reliable. This in turn requires the DBMS to be high performing and reliable. Recently, immense growth of research has occurred in the field of database management system. There is a great need and scope for evaluating the performance of various DBMSs in order to measure its efficiency & reliability. In this paper we focus on the performance analysis of local DBMSs for mobile applications.","","Electronic:978-1-4799-2235-2; POD:978-1-4799-2236-9","10.1109/CUBE.2013.51","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6701510","Database Management;Information storage and retrieval;Performance Analysis","Database systems;Memory management;Mobile communication;Performance analysis;Performance evaluation;Time factors","database management systems;mobile computing","DBMS performance analysis;local database management system;mobile applications","","0","","13","","","15-16 Nov. 2013","","IEEE","IEEE Conference Publications"
"Toward an effective automated tracing process","A. Mahmoud","Dept. of Computer Science and Engineering, Mississippi State University, Mississippi State, MS","2012 20th IEEE International Conference on Program Comprehension (ICPC)","20120716","2012","","","269","272","The research on automated tracing has noticeably advanced in the past few years. Various methodologies and tools have been proposed in the literature to provide automatic support for establishing and maintaining traceability information in software systems. This movement is motivated by the increasing attention traceability has been receiving as a de jure standard in software quality assurance. Following that effort, in this research proposal we describe several research directions related to enhancing the effectiveness of automated tracing tools and techniques. Our main research objective is to advance the state of the art in this filed. We present our suggested contributions through a set of incremental enhancements over the conventional automated tracing process, and briefly describe a set of strategies for assessing these contributions impact on the process.","1092-8138;10928138","Electronic:978-1-4673-1216-5; POD:978-1-4673-1213-4; USB:978-1-4673-1215-8","10.1109/ICPC.2012.6240502","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6240502","automated tracing;experimentation;information search and retrieval","Accuracy;Human factors;Humans;Indexing;Semantics;Software;Strontium","quality assurance;software maintenance;software quality","automated tracing process;software quality assurance;software system traceability information establishment;software system traceability information maintenance","","0","","29","","","11-13 June 2012","","IEEE","IEEE Conference Publications"
"Roundtable: What's Next in Software Analytics","A. E. Hassan; A. Hindle; P. Runeson; M. Shepperd; P. Devanbu; S. Kim","Queen's University, Canada","IEEE Software","20130626","2013","30","4","53","56","For this special issue, the guest editors asked a panel of six established experts in software analytics to highlight what they thought were the most important, or overlooked, aspect of this field. They all pleaded for a much broader view of analytics than seen in current practice: software analytics should go beyond developers (Ahmed Hassan) and numbers (Per Runeson). Analytics should also prove its relevance to practitioners (Abram Hindle, Martin Shepperd). There are now opportunities for ""natural"" software analytics based on statistical natural language processing (Prem Devanbu). Lastly, software analytics needs information analysts and field agents like Chloe O'Brian and Jack Bauer in the TV show 24 (Sung Kim).","0740-7459;07407459","","10.1109/MS.2013.85","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547637","decision support;information search and retrieval;management;metrics;software/program verification;statistical methods;testing and debugging","Analytical models;Data mining;Natural language processing;Software development;Software engineering","natural language processing;software engineering;statistical analysis","natural software analytics;statistical natural language processing","","4","","7","","","July-Aug. 2013","","IEEE","IEEE Journals & Magazines"
"A Parameterized Approach to Spam-Resilient Link Analysis of the Web","J. Caverlee; S. Webb; L. Liu; W. B. Rouse","Texas A&M University, College Station","IEEE Transactions on Parallel and Distributed Systems","20090828","2009","20","10","1422","1438","Link-based analysis of the Web provides the basis for many important applications-like Web search, Web-based data mining, and Web page categorization-that bring order to the massive amount of distributed Web content. Due to the overwhelming reliance on these important applications, there is a rise in efforts to manipulate (or spam) the link structure of the Web. In this manuscript, we present a parameterized framework for link analysis of the Web that promotes spam resilience through a source-centric view of the Web. We provide a rigorous study of the set of critical parameters that can impact source-centric link analysis and propose the novel notion of influence throttling for countering the influence of link-based manipulation. Through formal analysis and a large-scale experimental study, we show how different parameter settings may impact the time complexity, stability, and spam resilience of Web link analysis. Concretely, we find that the source-centric model supports more effective and robust rankings in comparison with existing Web algorithms such as PageRank.","1045-9219;10459219","","10.1109/TPDS.2008.227","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4653482","Distributed systems;Internet search;Web Search;Web search;Web-based services;distributed systems;general;information search and retrieval;information storage and retrieval;information technology and systems;online information services.;systems and software","","Internet;Web sites;data mining;stability;unsolicited e-mail","PageRank;Web algorithms;Web link analysis;Web page categorization;Web search;Web-based data mining;World Wide Web;distributed Web content;formal analysis;link-based manipulation;source-centric link analysis;spam resilience;spam-resilient link analysis;stability;time complexity","","3","","41","","20081017","Oct. 2009","","IEEE","IEEE Journals & Magazines"
"On the Reduction of Verbose Queries in Text Retrieval Based Software Maintenance","O. Chaparro; A. Marcus","Univ. of Texas at Dallas, Richardson, TX, USA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","716","718","We argue that verbose queries used for software retrieval contain many terms that follow specific discourse rules, yet hinder retrieval. We report the results of an empirical study on the effect of removing such terms from verbose queries in the context of Text Retrieval-based concept location. In the study, we remove terms from 424 queries, generated from bug reports of nine open source systems. Removing the terms leads to substantial improvement in retrieval: 73% of the queries are improved, leading to 21.8% and 13.4% gain in terms of MRR and MAP, respectively. Such improvement is larger than that of many more sophisticated state-of-the-art approaches. The results show promise and the future challenge lies with automatically identifying the terms to be removed from the verbose queries.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883382","Query Reduction;Software Maintenance;Text Retrieval","Computer bugs;Conferences;Context;Information retrieval;Software engineering;Software maintenance","public domain software;query processing;software maintenance;text analysis","MAP;MRR;open source systems;software retrieval;text retrieval based software maintenance;text retrieval-based concept location;verbose query reduction","","","","","","","14-22 May 2016","","IEEE","IEEE Conference Publications"
"Nature-inspired classification for mining social space information: National security intelligence and big data perspective","K. Dhanasekaran; B. Surendiran","Computer Science and Engineering, Info Institute of Engineering, Coimbatore, India","2016 Online International Conference on Green Engineering and Technologies (IC-GET)","20170504","2016","","","1","6","Due to the increasing amount of information in web-based environment, analysts nowadays need information extracted from different sources. Extracting this information to guide decision making in a national security perspective remains a challenging task. The major issue arises due to a large amount of irrelevant information or complexity of unstructured data which makes information extraction and classification a very tedious task while analyzing the textual content for finding various aspects of entities or groups. In this paper, we describe our efforts towards introducing a context-based information extraction using National Security Information Sources (NSIS) which employs different types of knowledge inspired by natural activities of living things. In order to improve classification performance by utilizing relevant information from the sources related to national security and to make better decision, we assume that entities and relationships from these sources can be used to contextualize information from the records. This paper presents a new method for nature-inspired classification after extracting various features from dataset created using social posts and records. Then, analysis of classes of information to concepts available on knowledge sources is carried out to ensure quality of information based on the user needs. The simulation results demonstrated the ability of the proposed method inspired by cultural algorithm to extract group-centric information and improve classification performance using proposed Social Info Finder (SIF). This paper highlights the challenges and application of proposed SIF method to improve information extraction and performance of text classification focusing on national security and related threats.","","Electronic:978-1-5090-4556-3; POD:978-1-5090-4557-0","10.1109/GET.2016.7916770","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916770","Big data;Information extraction;National security;Nature-inspired computing;Social problems","Algorithm design and analysis;Classification algorithms;Data mining;Feature extraction;Information retrieval;National security;Proteins","","","","","","","","","19-19 Nov. 2016","","IEEE","IEEE Conference Publications"
"Optimal Storage Placement for Tree-Structured Networks with Heterogeneous Channel Costs","G. M. Chiu; L. H. Yen; T. L. Chin","National Taiwan University of Science and Technology, Taipei","IEEE Transactions on Computers","20110901","2011","60","10","1431","1444","This work considers data query applications in tree-structured networks, where a given set of source nodes generate (or collect) data and forward the data to some halfway storage nodes for satisfying queries that call for data generated by all source nodes. The goal is to determine an optimal set of storage nodes that minimizes overall communication cost. Prior work toward this problem assumed homogeneous channel cost, which may not be the case in many network environments. We generalize the optimal storage problem for a tree-structured network by considering heterogeneous channel costs. The necessary and sufficient conditions for the optimal solution are identified, and an algorithm that incurs a linear time cost is proposed. We have also conducted extensive simulations to validate the algorithm and to evaluate its performance.","0018-9340;00189340","","10.1109/TC.2010.231","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5661768","Algorithm/protocol design and analysis;information storage and retrieval;network problems.","Algorithm design and analysis;Data communication;Data models;Database systems;Memory;Peer to peer computing;Wireless sensor networks","query processing;storage management;trees (mathematics)","data query applications;heterogeneous channel costs;optimal storage placement;tree-structured networks","","2","","36","","20101210","Oct. 2011","","IEEE","IEEE Journals & Magazines"
"Label Transfer by Measuring Compactness","R. Varga; S. Nedevschi","Computer Science Department, Technical University of Cluj-Napoca, Cluj Napoca, Romania","IEEE Transactions on Image Processing","20130926","2013","22","12","4711","4723","This paper presents a new automatic image annotation algorithm. First, we introduce a new similarity measure between images: compactness. This uses low level visual descriptors for determining the similarity between two images. Compactness shows how close test image features lie to training image feature cluster centers. The measure provides the core for a k-nearest neighbor type image annotation method. Afterward, a formalism for defining different transfer techniques is devised and several label transfer techniques are provided. The method as whole is evaluated on four image annotation benchmarks. The results on these sets validate the accuracy of the approach, which outperforms many state-of-the-art annotation methods. The method presented here requires a simple training process, efficiently combines different feature types and performs better than complex learning algorithms, even in this incipient form. The main contributions of this paper are the usage of compactness as a similarity measure that enables efficient low level feature comparison and an annotation algorithm based on label transfer.","1057-7149;10577149","","10.1109/TIP.2013.2277818","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576807","Information search and retrieval;automatic image annotation;object recognition;scene analysis","Data models;Feature extraction;Hidden Markov models;Histograms;Training;Vectors;Visualization","feature extraction;image processing","automatic image annotation;compactness measurement;image feature cluster centers;incipient form;k-nearest neighbor;label transfer;simple training process;visual descriptors","","1","","40","","20130808","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Detecting, Assessing and Monitoring Relevant Topics in Virtual Information Environments","J. Ontrup; H. Ritter; S. W. Scholz; R. Wagner","Bielefeld University, Bielefeld","IEEE Transactions on Knowledge and Data Engineering","20090123","2009","21","3","415","427","The ability to assess the relevance of topics and related sources in information-rich environments is a key to success when scanning business environments. This paper introduces a hybrid system to support managerial information gathering. The system is made up of three components: 1) a hierarchical hyperbolic SOM for structuring the information environment and visualizing the intensity of news activity with respect to identified topics, 2) a spreading activation network for the selection of the most relevant information sources with respect to an already existing knowledge infrastructure, and 3) measures of interestingness for association rules as well as statistical testing facilitates the monitoring of already identified topics. Embedding the system by a framework describing three modes of human information seeking behavior endorses an active organization, exploration and selection of information that matches the needs of decision makers in all stages of the information gathering process. By applying our system in the domain of the hotel industry we demonstrate how typical information gathering tasks are supported. Moreover, we present an empirical study investigating the effectiveness and efficiency of the visualization framework of our system.","1041-4347;10414347","","10.1109/TKDE.2008.149","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4569847","Clustering;Data and knowledge visualization;Database Applications;Database Management;Graphical user interfaces;Human information processing;Information Search and Retrieval;Information Storage and Retrieval;Information Technology;Information Technology and Systems;Search process;Text mining","","data mining;data visualisation;decision making;hotel industry;information management;information needs;knowledge management;monitoring;self-organising feature maps;statistical testing;virtual reality","activation network;association rule;business environment;decision making;hierarchical hyperbolic SOM;hotel industry;human information seeking behavior;information gathering management;knowledge infrastructure;statistical testing;topic assessment;topic detection;topic monitoring;virtual information environment scanning","","4","","37","","20080718","March 2009","","IEEE","IEEE Journals & Magazines"
"Salient local visual features for shape-based 3D model retrieval","Ryutarou Ohbuchi; Kunio Osada; Takahiko Furuya; Tomohisa Banno","University of Yamanashi, Japan","2008 IEEE International Conference on Shape Modeling and Applications","20080620","2008","","","93","102","In this paper, we describe a shape-based 3D model retrieval method based on multi-scale local visual features. The features are extracted from 2D range images of the model viewed from uniformly sampled locations on a view sphere. The method is appearance-based, and accepts all the models that can be rendered as a range image. For each range image, a set of 2D multi-scale local visual features is computed by using the scale invariant feature transform [22] algorithm. To reduce cost of distance computation and feature storage, a set of local features describing a 3D model is integrated into a histogram using the bag-of-features approach. Our experiments using two standard benchmarks, one for articulated shapes and the other for rigid shapes, showed that the methods achieved the performance comparable or superior to some of the most powerful 3D shape retrieval methods.","","CD-ROM:978-1-4244-2261-6; POD:978-1-4244-2260-9","10.1109/SMI.2008.4547955","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547955","Content-based retrieval;H.3.3 [Information Search and Retrieval]: Information filtering;I.3.5 [Computational Geometry and Object Modeling]: Surface based 3D shape models;I.4.8 [Scene Analysis]: Object recognition;Scale Invariant Feature Transform;bag-of-features;multi-scale feature","Computational efficiency;Costs;Feature extraction;Histograms;Humans;Noise shaping;Rendering (computer graphics);Shape;Solid modeling;Spatial databases","feature extraction;image retrieval;rendering (computer graphics);solid modelling;transforms","bag-of-feature approach;feature extraction;rendering;salient local visual feature;scale invariant feature transform algorithm;shape-based 3D model retrieval method","","47","","41","","","4-6 June 2008","","IEEE","IEEE Conference Publications"
"Document Understanding Using Improved Sqrt-Cosine Similarity","S. Sohangir; D. Wang","Dept. of Comput. & Electr. Eng. & Comput. Sci., Florida Atlantic Univ., Boca Raton, FL, USA","2017 IEEE 11th International Conference on Semantic Computing (ICSC)","20170330","2017","","","278","279","Text similarity measurement aims to find the commonality existing among text documents, which is fundamental to most information extraction, information retrieval, and text mining problems. In this paper, we propose a new similarity measure. We conduct comprehensive experiments to evaluate our new similarity measure with existing ones. The experimental results show the effectiveness of our proposed method.","","Electronic:978-1-5090-4284-5; POD:978-1-5090-4285-2","10.1109/ICSC.2017.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889546","","Computer science;Computers;Conferences;Electrical engineering;Euclidean distance;Information retrieval","text analysis","document understanding;improved sqrt-cosine similarity;information extraction;information retrieval;text documents;text mining problems;text similarity measurement","","","","","","","Jan. 30 2017-Feb. 1 2017","","IEEE","IEEE Conference Publications"
"Place-Aware Content Selection from Dynamic Web Sources for Public Displays","F. R. Ribeiro; R. Jose","Inf. Dept., Polytech. Inst. of Castelo Branco, Castelo Branco, Portugal","2009 Fifth International Conference on Signal Image Technology and Internet Based Systems","20101111","2009","","","302","309","Public digital displays could greatly benefit from the ability to dynamically select from the Internet content items that would be strongly related with the place where each display is installed. Generically, this is similar to the type of problem addressed by recommender systems. However, the usage context of a public display raises specific challenges that may limit the applicability of existing recommender systems. In this paper, we explore the creation of a recommender system for public situated displays that is able to autonomously select relevant content from Internet sources using keywords as input. This type of recommender system should enable public displays to become devices for Internet information delivery in public spaces, while also making them more situated in the social settings in which they are installed. We have created a recommender system based on these principles and we have conducted two studies to evaluate the perceived performance of the system. The results have shown that keywords can be very effective in driving user-generated content, but they often need to be complemented with contextual information that disambiguates their semantics.","","Electronic:978-1-4244-5741-0; POD:978-1-4244-5740-3; USB:978-0-7695-3959-1","10.1109/SITIS.2009.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633972","Dynamic Sources;Information Integration;Public Displays;Situated Displays;Social Information Systems;Web 2.0;Web Information Filtering and Retrieval","Accuracy;Adaptive scheduling;Context;Feeds;History;Internet;Recommender systems","Internet;computer displays;performance evaluation;public administration;public information systems;recommender systems","Internet content items;Internet information delivery;Internet sources;contextual information;dynamic Web sources;performance evaluation;place-aware content selection;public digital displays;public displays;public situated displays;recommender systems;social settings;user-generated content","","1","","11","","","Nov. 29 2009-Dec. 4 2009","","IEEE","IEEE Conference Publications"
"Graph-Based Topic-Focused Retrieval in Distributed Camera Network","J. Xu; V. Jagadeesh; Z. Ni; S. Sunderrajan; B. S. Manjunath","Comput. Sci. Dept., Univ. of California, Santa Barbara, Santa Barbara, CA, USA","IEEE Transactions on Multimedia","20131113","2013","15","8","2046","2057","Wide-area wireless camera networks are being increasingly deployed in many urban scenarios. The large amount of data generated from these cameras pose significant information processing challenges. In this work, we focus on representation, search and retrieval of moving objects in the scene, with emphasis on local camera node video analysis. We develop a graph model that captures the relationships among objects without the need to identify global trajectories. Specifically, two types of edges are defined in the graph: object edges linking the same object across the whole network and context edges linking different objects within a spatial-temporal proximity. We propose a manifold ranking method with a greedy diversification step to order the relevant items based on similarity as well as diversity within the database. Detailed experimental results using video data from a 10-camera network covering bike paths are presented.","1520-9210;15209210","","10.1109/TMM.2013.2281019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6589140","Distributed camera network;diverse and relevant ranking;graph-based modeling;information search and retrieval","Cameras;Feature extraction;Histograms;Indexes;Optical imaging;Tracking;Visualization","edge detection;graph theory;greedy algorithms;image representation;image retrieval;image sensors;video signal processing","distributed camera network;global trajectory identification;graph-based topic-focused retrieval;greedy diversification step;information processing challenges;local camera node video analysis;manifold ranking method;moving object representation;moving object retrieval;moving object search;spatial-temporal proximity;wide-area wireless camera networks","","5","","39","","20130905","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Top-Down Parameter-Free Clustering of High-Dimensional Categorical Data","E. Cesario; G. Manco; R. Ortale","ICAR-CNR, Rende","IEEE Transactions on Knowledge and Data Engineering","20071022","2007","19","12","1607","1624","A parameter-free, fully-automatic approach to clustering high-dimensional categorical data is proposed. The technique is based on a two-phase iterative procedure, which attempts to improve the overall quality of the whole partition. In the first phase, cluster assignments are given, and a new cluster is added to the partition by identifying and splitting a low-quality cluster. In the second phase, the number of clusters is fixed, and an attempt to optimize cluster assignments is done. On the basis of such features, the algorithm attempts to improve the overall quality of the whole partition and finds clusters in the data, whose number is naturally established on the basis of the inherent features of the underlying data set rather than being previously specified. Furthermore, the approach is parametric to the notion of cluster quality: Here, a cluster is defined as a set of tuples exhibiting a sort of homogeneity. We show how a suitable notion of cluster homogeneity can be defined in the context of high-dimensional categorical data, from which an effective instance of the proposed clustering scheme immediately follows. Experiments on both synthetic and real data prove that the devised algorithm scales linearly and achieves nearly optimal results in terms of compactness and separation.","1041-4347;10414347","","10.1109/TKDE.2007.190649","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358941","Clustering;Database Applications - Clustering;Information Search and Retrieval - Clustering","","data mining;pattern classification;pattern clustering;unsupervised learning","high-dimensional categorical data;top-down parameter-free clustering;two-phase iterative procedure;unsupervised classification technique","","29","","42","","","Dec. 2007","","IEEE","IEEE Journals & Magazines"
"Learning a Maximum Margin Subspace for Image Retrieval","X. He; D. Cai; J. Han","Yahoo! Inc., Burbank","IEEE Transactions on Knowledge and Data Engineering","20071226","2008","20","2","189","201","One of the fundamental problems in Content-Based Image Retrieval (CBIR) has been the gap between low-level visual features and high-level semantic concepts. To narrow down this gap, relevance feedback is introduced into image retrieval. With the user-provided information, a classifier can be learned to distinguish between positive and negative examples. However, in real-world applications, the number of user feedbacks is usually too small compared to the dimensionality of the image space. In order to cope with the high dimensionality, we propose a novel semisupervised method for dimensionality reduction called Maximum Margin Projection (MMP). MMP aims at maximizing the margin between positive and negative examples at each local neighborhood. Different from traditional dimensionality reduction algorithms such as Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA), which effectively see only the global euclidean structure, MMP is designed for discovering the local manifold structure. Therefore, MMP is likely to be more suitable for image retrieval, where nearest neighbor search is usually involved. After projecting the images into a lower dimensional subspace, the relevant images get closer to the query image; thus, the retrieval performance can be enhanced. The experimental results on Corel image database demonstrate the effectiveness of our proposed algorithm.","1041-4347;10414347","","10.1109/TKDE.2007.190692","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358969","Image databases;Image/video retrieval;Information Search and Retrieval","","content-based retrieval;image retrieval;principal component analysis;relevance feedback;visual databases","Corel image database;content-based image retrieval;dimensionality reduction algorithm;global euclidean structure;linear discriminant analysis;maximum margin projection;maximum margin subspace;novel semisupervised method;principal component analysis;relevance feedback","","99","","44","","","Feb. 2008","","IEEE","IEEE Journals & Magazines"
"Distributed View Divergence Control of Data Freshness in Replicated Database Systems","T. Yamashita","Nippon Telegraph and Telephone Corporation, Tokyo","IEEE Transactions on Knowledge and Data Engineering","20090821","2009","21","10","1403","1417","In this paper, we propose a distributed method to control the view divergence of data freshness for clients in replicated database systems whose facilitating or administrative roles are equal. Our method provides data with statistically defined freshness to clients when updates are initially accepted by any of the replicas, and then, asynchronously propagated among the replicas that are connected in a tree structure. To provide data with freshness specified by clients, our method selects multiple replicas using a distributed algorithm so that they statistically receive all updates issued up to a specified time before the present time. We evaluated by simulation the distributed algorithm to select replicas for the view divergence control in terms of controlled data freshness, time, message, and computation complexity. The simulation showed that our method achieves more than 36.9 percent improvement in data freshness compared with epidemic-style update propagation.","1041-4347;10414347","","10.1109/TKDE.2008.230","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4693708","Data replication;Data sharing;Distributed databases;Distributed systems;Information Search and Retrieval;Information Storage and Retrieval;Information Technology and Systems;asynchronous update.;delay;freshness;weak consistency","Centralized control;Computational modeling;Control systems;Data processing;Database systems;Distributed algorithms;Distributed control;Large-scale systems;Peer to peer computing;Scalability","data handling;distributed algorithms;replicated databases","data freshness;distributed algorithm;distributed view divergence control;replicated database systems","","1","","38","","20081202","Oct. 2009","","IEEE","IEEE Journals & Magazines"
"XML Document Parsing: Operational and Performance Characteristics","T. C. Lam; J. J. Ding; J. C. Liu","Cisco","Computer","20080909","2008","41","9","30","37","Parsing is an expensive operation that can degrade XML processing performance. A survey of four representative XML parsing models-DOM, SAX, StAX, and VTD-reveals their suitability for different types of applications.","0018-9162;00189162","","10.1109/MC.2008.403","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4623219","XML processing;information storage and retrieval;information technology and systems;online information services","Application software;Computer aided instruction;Computer languages;Costs;Data analysis;Databases;Degradation;Java;Network servers;XML","XML;document handling;grammars","DOM model;SAX model;StAX model;VTD model;XML document parsing;XML processing;operational characteristics;performance characteristics","","20","1","10","","","Sept. 2008","","IEEE","IEEE Journals & Magazines"
"Knowledge Discovery in the Blogosphere: Approaches and Challenges","G. Lakshmanan; M. Oberhofer","IBM T. J. Watson Research Center, Cambridge","IEEE Internet Computing","20100311","2010","14","2","24","32","Knowledge discovery in blogs is different from knowledge discovery in areas such as databases or Web documents due to blogs' unique characteristics, which introduce additional mining challenges. Although researchers have investigated several techniques to address different aspects of blog discovery, no comparisons among key knowledge discovery techniques for blogs exist. This article examines three prominent techniques that are frequently applied to discovery in blogs - clustering, matrix decomposition, and ranking. The authors compare them in terms of effectiveness in combating present challenges and their ability to accomplish challenging tasks required for effective blog mining.","1089-7801;10897801","","10.1109/MIC.2010.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5396304","artificial intelligence;blogs;clustering;collective wisdom;computing methodologies;database applications;database management;information search and retrieval;information storage and retrieval;information technology and systems;interactive data exploration and discovery;knowledge management;knowledge retrieval;mining methods and algorithms;topic detection;trend analysis","Blogs;Databases;Matrix decomposition","Web sites;data mining;matrix decomposition;pattern clustering","Web documents;blog mining;blogosphere;knowledge discovery;matrix decomposition","","6","1","20","","20100122","March-April 2010","","IEEE","IEEE Journals & Magazines"
"The Use of Text Retrieval and Natural Language Processing in Software Engineering","S. Haiduc; V. Arnaoudova; A. Marcus; G. Antoniol","Florida State Univ., Tallahassee, FL, USA","2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)","20170323","2016","","","898","899","This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.","","Electronic:978-1-4503-4205-6; POD:978-1-5090-2245-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883434","Text retrieval;natural language processing","Computer bugs;Conferences;Information retrieval;Natural language processing;Software;Software engineering;Tutorials","information retrieval;natural language processing;software engineering;text analysis","natural language processing;software engineering;text retrieval","","","","","","","14-22 May 2016","","IEEE","IEEE Conference Publications"
"Production of origin destination matrix by extracting information from unstructured textual data","M. Mejri; S. Y. Turki; S. Faiz","Laboratoire de T&#x00E9;l&#x00E9;d&#x00E9;tection et Syst&#x00E8;mes, d'Information &#x00E0; R&#x00E9;f&#x00E9;rence Spatiale, &#x00C9;cole Nationale d'Ing&#x00E9;nieurs de Tunis, University of Tunis El Manar","2014 Information and Communication Technologies Innovation and Application (ICTIA)","20170323","2014","","","1","4","In this paper, we present an approach for the production of origin destination matrices by extracting information from unstructured textual data of Web sites. This approach, which we called “Origin Destination Matrix Extractor” is based on three main modules of Information Extraction: an extraction of events module with which we tried to extract any travel events contained in a given text, a named entity recognition module for the recovery and detection of named entities that correspond to the different target information and finally a dependency syntactic analysis module to check the existence of interdependencies between extracted entities and detected travel events. The experiments carried out on a set of real data show that the proposed method gives satisfactory results with a precision of over 90%.","","Electronic:978-1-4799-6950-0; POD:978-1-4799-6951-7","10.1109/ICTIA.2014.7883763","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883763","","Data mining;Electronic mail;Estimation;Information retrieval;Predictive models;Production;Syntactics","data mining;text analysis;transportation","Web sites;dependency syntactic analysis module;events module extraction;information extraction;information extraction module;named entity detection;named entity recognition module;named entity recovery;origin destination matrix production;origin-destination matrix extractor;travel event module extraction;unstructured textual data","","","","","","","6-8 March 2014","","IEEE","IEEE Conference Publications"
"Coordinated En-Route Web Caching in Multiserver Networks","H. Shen; S. Xu","University of Adelaide, Adelaide","IEEE Transactions on Computers","20090324","2009","58","5","605","619","With the emergence of various advanced networks that comprise a group of geographically distributed servers, such as content delivery networks (CDNs) and peer-to-peer (P2P) systems, coordinated en-route Web caching in multiserver networks becomes increasingly attractive but remains of great challenge as solutions for single-server networks become invalid here. In this paper, we first establish mathematical formulation for this problem that takes into account all requests (to any server) that pass through the intermediate nodes on a response path and caches the requested object optimally among these nodes so that system's total gain is maximized. Then, we derive efficient dynamic programming-based methods for finding optimal solutions to the problem for the unconstrained case and two QoS-constrained cases, respectively. For each case, we present a caching scheme to illustrate application of the corresponding method. Finally, we evaluate the proposed schemes on different performance metrics through extensive simulation experiments. The experiment results show that our proposed schemes can yield a steady performance improvement and achieve desired QoS in a multiserver network. To the best of our knowledge, these are the first results for solving the problem of coordinated en-route Web caching in multiserver networks.","0018-9340;00189340","","10.1109/TC.2008.162","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4620107","Artificial Intelligence;Computing Met;Control Methods and Search;Distributed Systems;Dynamic programming;En-route Web caching;Information Storage and Retrieval;Optimization;Problem Solving;Storage Management;Web object placement;cache content replacement;dynamic programming;multiserver network;quality of service.","Australia;Bandwidth;Computer science;Delay;Dynamic programming;HTML;Measurement;Network servers;Peer to peer computing;Quality of service;Scalability;Web services","Internet;cache storage;dynamic programming;network servers;peer-to-peer computing;quality of service","P2P systems;QoS-constrained cases;content delivery networks;coordinated en-route Web caching;dynamic programming-based methods;geographically distributed servers;multiserver networks;peer-to-peer systems","","7","","41","","20080905","May 2009","","IEEE","IEEE Journals & Magazines"
"HDSKG: Harvesting domain specific knowledge graph from content of webpages","X. Zhao; Z. Xing; M. A. Kabir; N. Sawada; J. Li; S. W. Lin","Rolls-Royce@NTU Corporate Lab, Nanyang Technological University (NTU), Singapore","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)","20170323","2017","","","56","67","Knowledge graph is useful for many different domains like search result ranking, recommendation, exploratory search, etc. It integrates structural information of concepts across multiple information sources, and links these concepts together. The extraction of domain specific relation triples (subject, verb phrase, object) is one of the important techniques for domain specific knowledge graph construction. In this research, an automatic method named HDSKG is proposed to discover domain specific concepts and their relation triples from the content of webpages. We incorporate the dependency parser with rule-based method to chunk the relations triple candidates, then we extract advanced features of these candidate relation triples to estimate the domain relevance by a machine learning algorithm. For the evaluation of our method, we apply HDSKG to Stack Overflow (a Q&A website about computer programming). As a result, we construct a knowledge graph of software engineering domain with 35279 relation triples, 44800 concepts, and 9660 unique verb phrases. The experimental results show that both the precision and recall of HDSKG (0.78 and 0.7 respectively) is much higher than the openIE (0.11 and 0.6 respectively). The performance is particularly efficient in the case of complex sentences. Further more, with the self-training technique we used in the classifier, HDSKG can be applied to other domain easily with less training data.","","Electronic:978-1-5090-5501-2; POD:978-1-5090-5502-9","10.1109/SANER.2017.7884609","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884609","Dependency Parse;Knowledge Graph;Stack Overflow;Structural Information Extraction;openIE","Data mining;Feature extraction;Information retrieval;Knowledge engineering;Libraries;Software engineering;Support vector machines","Internet;estimation theory;grammars;graph theory;information retrieval;knowledge based systems;learning (artificial intelligence);software engineering","HDSKG;Stack Overflow;Web page content;dependency parser;domain relevance estimation;domain specific relation triple extraction;harvesting domain specific knowledge graph;machine learning algorithm;rule-based method;software engineering","","","","","","","20-24 Feb. 2017","","IEEE","IEEE Conference Publications"
"Identifying Features in Opinion Mining via Intrinsic and Extrinsic Domain Relevance","Z. Hai; K. Chang; J. J. Kim; C. C. Yang","N4-B3C-14 DISCO Lab., Nanyang Technol. Univ., Singapore, Singapore","IEEE Transactions on Knowledge and Data Engineering","20140204","2014","26","3","623","634","The vast majority of existing approaches to opinion feature extraction rely on mining patterns only from a single review corpus, ignoring the nontrivial disparities in word distributional characteristics of opinion features across different corpora. In this paper, we propose a novel method to identify opinion features from online reviews by exploiting the difference in opinion feature statistics across two corpora, one domain-specific corpus (i.e., the given review corpus) and one domain-independent corpus (i.e., the contrasting corpus). We capture this disparity via a measure called domain relevance (DR), which characterizes the relevance of a term to a text collection. We first extract a list of candidate opinion features from the domain review corpus by defining a set of syntactic dependence rules. For each extracted candidate feature, we then estimate its intrinsic-domain relevance (IDR) and extrinsic-domain relevance (EDR) scores on the domain-dependent and domain-independent corpora, respectively. Candidate features that are less generic (EDR score less than a threshold) and more domain-specific (IDR score greater than another threshold) are then confirmed as opinion features. We call this interval thresholding approach the intrinsic and extrinsic domain relevance (IEDR) criterion. Experimental results on two real-world review domains show the proposed IEDR approach to outperform several other well-established methods in identifying opinion features.","1041-4347;10414347","","10.1109/TKDE.2013.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6427744","Chinese;Information search and retrieval;natural language processing;opinion feature;opinion mining","Batteries;Data mining;Dispersion;Educational institutions;Feature extraction;Hidden Markov models;Syntactics","data mining;feature extraction;statistical analysis;text analysis","EDR score estimation;IDR;IEDR criterion;contrasting corpus;domain review corpus;domain-dependent corpora;domain-independent corpora;extrinsic-domain relevance score estimation;feature identification;interval thresholding approach;intrinsic and extrinsic domain relevance criterion;intrinsic-domain relevance score estimation;one domain-independent corpus;one domain-specific corpus;opinion feature extraction;opinion feature statistics;opinion mining;pattern mining;syntactic dependence rules;text collection;word distributional characteristics","","17","","35","","20130201","March 2014","","IEEE","IEEE Journals & Magazines"
"The Globus Replica Location Service: Design and Experience","A. L. Chervenak; R. Schuler; M. Ripeanu; M. A. Amer; S. Bharathi; I. Foster; A. Iamnitchi; C. Kesselman","University of Southern California, Marina del Rey","IEEE Transactions on Parallel and Distributed Systems","20090728","2009","20","9","1260","1272","Distributed computing systems employ replication to improve overall system robustness, scalability, and performance. A replica location service (RLS) offers a mechanism to maintain and provide information about physical locations of replicas. This paper defines a design framework for RLSs that supports a variety of deployment options. We describe the RLS implementation that is distributed with the Globus toolkit and is in production use in several grid deployments. Features of our modular implementation include the use of soft-state protocols to populate a distributed index and Bloom filter compression to reduce overheads for distribution of index information. Our performance evaluation demonstrates that the RLS implementation scales well for individual servers with millions of entries and up to 100 clients. We describe the characteristics of existing RLS deployments and discuss how RLS has been integrated with higher-level data management services.","1045-9219;10459219","","10.1109/TPDS.2008.151","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604659","Distributed systems;Information Storage and Retrieval;Metadata;Replica Location Service;data management.;grids;replica management","","data handling;grid computing;protocols","Bloom filter compression;Globus toolkit;data management services;distributed computing systems;distributed index;grid deployments;replica location service;soft-state protocols","","27","1","56","","20080822","Sept. 2009","","IEEE","IEEE Journals & Magazines"
"Efficient Approaches for Retrieving Protein Tertiary Structures","G. Mirceva; I. Cingovska; Z. Dimov; D. Davcev","Faculty of Electrical Engineering and Information Technologies, Skopje","IEEE/ACM Transactions on Computational Biology and Bioinformatics","20120518","2012","9","4","1166","1179","The 3D conformation of a protein in the space is the main factor which determines its function in living organisms. Due to the huge amount of newly discovered proteins, there is a need for fast and accurate computational methods for retrieving protein structures. Their purpose is to speed up the process of understanding the structure-to-function relationship which is crucial in the development of new drugs. There are many algorithms addressing the problem of protein structure retrieval. In this paper, we present several novel approaches for retrieving protein tertiary structures. We present our voxel-based descriptor. Then we present our protein ray-based descriptors which are applied on the interpolated protein backbone. We introduce five novel wavelet descriptors which perform wavelet transforms on the protein distance matrix. We also propose an efficient algorithm for distance matrix alignment named Matrix Alignment by Sequence Alignment within Sliding Window (MASASW), which has shown as much faster than DALI, CE, and MatAlign. We compared our approaches between themselves and with several existing algorithms, and they generally prove to be fast and accurate. MASASW achieves the highest accuracy. The ray and wavelet-based descriptors as well as MASASW are more accurate than CE.","1545-5963;15455963","","10.1109/TCBB.2011.138","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6051424","Information search and retrieval;feature extraction.;protein databases","Amino acids;Feature extraction;Interpolation;Matrix decomposition;Proteins;Three dimensional displays;Wavelet transforms","bioinformatics;biological techniques;matrix algebra;molecular biophysics;molecular configurations;proteins;wavelet transforms","MASASW;Matrix Alignment by Sequence Alignment within Sliding Window;computational methods;distance matrix alignment;drug development;interpolated protein backbone;protein 3D conformations;protein distance matrix;protein function;protein ray based descriptors;protein tertiary structure retrieval;structure-function relationship;voxel based descriptor;wavelet descriptors;wavelet transforms","Algorithms;Computational Biology;Databases, Protein;Imaging, Three-Dimensional;Protein Structure, Tertiary;Proteins;Sequence Alignment;Sequence Analysis, Protein;Wavelet Analysis","1","","51","","20111018","July-Aug. 2012","","IEEE","IEEE Journals & Magazines"
"Performance Evolution of Mobile Web-Based Services","C. Canali; M. Colajanni; R. Lancellotti","University of Modena and Reggio Emilia","IEEE Internet Computing","20090304","2009","13","2","60","68","The mobile Web's widespread diffusion opens many interesting design and management issues about server infrastructures that must satisfy present and future client demand. Future mobile Web-based services will have growing computational costs. Even requests for the same Web resource will require services to dynamically generate content that takes into account specific devices, user profiles, and contexts. The authors consider the evolution of the mobile Web workload and trends in server and client devices with the goal of anticipating future bottlenecks and developing management strategies.","1089-7801;10897801","","10.1109/MIC.2009.43","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797938","Web services;information storage and retrieval;location-dependent and sensitive;mobile applications;multimedia applications;multimedia signal processing;online information services;performance evaluation;systems and software","Context awareness;Context-aware services;Costs;Mobile computing;Navigation;Personal communication networks;Personal digital assistants;Portable computers;Smart phones;Video sharing","Web services;client-server systems;mobile computing","mobile Web-based service;performance evolution;server-client device","","16","","12","","","March-April 2009","","IEEE","IEEE Journals & Magazines"
"Publishing Search Logs—A Comparative Study of Privacy Guarantees","M. Gotz; A. Machanavajjhala; G. Wang; X. Xiao; J. Gehrke","Cornell University, Ithaca","IEEE Transactions on Knowledge and Data Engineering","20120126","2012","24","3","520","532","Search engine companies collect the “database of intentions,” the histories of their users' search queries. These search logs are a gold mine for researchers. Search engine companies, however, are wary of publishing search logs in order not to disclose sensitive information. In this paper, we analyze algorithms for publishing frequent keywords, queries, and clicks of a search log. We first show how methods that achieve variants of k-anonymity are vulnerable to active attacks. We then demonstrate that the stronger guarantee ensured by ε-differential privacy unfortunately does not provide any utility for this problem. We then propose an algorithm ZEALOUS and show how to set its parameters to achieve (ε, δ)-probabilistic privacy. We also contrast our analysis of ZEALOUS with an analysis by Korolova et al. [17] that achieves (ε',δ')-indistinguishability. Our paper concludes with a large experimental study using real applications where we compare ZEALOUS and previous work that achieves k-anonymity in search log publishing. Our results show that ZEALOUS yields comparable utility to k-anonymity while at the same time achieving much stronger privacy guarantees.","1041-4347;10414347","","10.1109/TKDE.2011.26","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5708146","Security;and protection;database management;general;information storage and retrieval;information technology and systems;information technology and systems.;integrity;web search","Histograms;History;Indexes;Privacy;Probabilistic logic;Publishing;Search engines","data privacy;search engines","ZEALOUS algorithm;k-anonymity;privacy guarantees;probabilistic privacy;search engine;search log click;search log keyword;search log publishing;search log query;user search query","","17","","23","","20110204","March 2012","","IEEE","IEEE Journals & Magazines"
"Annotation schemes for constructing Uyghur named entity relation corpus","K. Abiderexiti; M. Maimaiti; T. Yibulayin; A. Wumaier","School of Information Science and Engineering, Xinjiang University, Urumqi, China","2016 International Conference on Asian Language Processing (IALP)","20170313","2016","","","103","107","The Uyghur language is a minority language in China, and it is one of the official languages in Xinjiang Uyghur Autonomous Region of China. Approximately 10 million people use Uyghur in their daily lives and regular use is even found on the internet. However, the lack of an Uyghur named-entity relation corpus constrains Uyghur language extraction applications. In this paper, we will propose such an Uyghur named-entity and Uyghur named-entity relation annotation specifications based on existing guidelines and experiences in other languages for Uyghur corpus construction. By sampling raw text from Uyghur language websites, a small experiment has been conducted concerning the practicality of our annotation schemes using an annotation tool. After review, we conclude that this method has practical future applications for other resource-poor minority languages of the world. This schemes will provide a basis for further studies of entity relation corpus construction.","","Electronic:978-1-5090-0922-0; POD:978-1-5090-0923-7; USB:978-1-5090-0921-3","10.1109/IALP.2016.7875945","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875945","Uyghur;annotation;corpus;named entity;relation","China;Information filters;Information retrieval;Natural languages;Text mining","Internet;Web sites;natural language processing;text analysis","China;Internet;Uyghur corpus construction;Uyghur language Web sites;Uyghur language extraction applications;Uyghur named entity relation corpus;Uyghur named-entity relation annotation specifications;Xinjiang Uyghur Autonomous Region of China;entity relation corpus construction;minority language;official language","","","","","","","21-23 Nov. 2016","","IEEE","IEEE Conference Publications"
"Measuring Data Management Practice Maturity: A Community's Self-Assessment","P. Aiken; M. D. Allen; B. Parker; A. Mattia","Virginia Commonwealth University/Institute for Data Research","Computer","20070423","2007","40","4","42","50","As increasing amounts of data flow within and between organizations, the problems that can result from poor data management practices are becoming more apparent. Studies have shown that such poor practices are widespread. All organizations have data architectures, whether explicitly documented or implicitly assumed. An important data management process is to document the architecture's capabilities, making it more useful to the organization. Increasing data management practice maturity levels can positively impact the coordination of data flow among organizations, individuals, and systems. Results from a self-assessment provide a roadmap for improving organizational data management practices","0018-9162;00189162","","10.1109/MC.2007.139","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4160221","data management;information storage and retrieval;metadata","Appropriate technology;Asset management;Data models;Databases;Feedback;Process design;Software packages;Standards development;Standards organizations;Technology management","business data processing;database management systems","data architecture;organizational data flow;organizational data management","","12","","16","","","April 2007","","IEEE","IEEE Journals & Magazines"
"Bounded Approximation: A New Criterion for Dimensionality Reduction Approximation in Similarity Search","K. Vu; K. A. Hua; H. Cheng; S. D. Lang","Sch. of Electr. Eng. & Comput. Sci., Univ. of Central Florida, Orlando, FL","IEEE Transactions on Knowledge and Data Engineering","20080422","2008","20","6","768","783","We examine the problem of efficient distance-based similarity search over high-dimensional data. We show that a promising approach to this problem is to reduce dimensions and allow fast approximation. Conventional reduction approaches, however, entail a significant shortcoming: The approximation volume extends across the dataspace, which causes overestimation of retrieval sets and impairs performance. This paper focuses on a new criterion for dimensionality reduction methods: bounded approximation. We show that this requirement can be accomplished by a novel nonlinear transformation scheme that extracts two important parameters from the data. We devise two approximation formulations, namely, rectangular and spherical range search, each corresponding to a closed volume around the original search sphere. We discuss in detail how we can derive tight bounds for the parameters and prove further results, as well as highlight insights into the problems and our proposed solutions. To demonstrate the benefits of the new criterion, we study the effects of (un)boundedness on approximation performance, including selectivity, error toleration, and efficiency. Extensive experiments confirm the superiority of this technique over recent state-of-the-art schemes.","1041-4347;10414347","","10.1109/TKDE.2008.30","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445668","Information Search and Retrieval;Information Storage and Retrieval;Search process","","approximation theory;query processing","bounded approximation;dimensionality reduction approximation;nonlinear transformation scheme;similarity search","","4","","41","","","June 2008","","IEEE","IEEE Journals & Magazines"
"Recording Process Documentation for Provenance","P. Groth; L. Moreau","University of Southern California, Marina del Rey","IEEE Transactions on Parallel and Distributed Systems","20090728","2009","20","9","1246","1259","Scientific and business communities are adopting large-scale distributed systems as a means to solve a wide range of resource-intensive tasks. These communities also have requirements in terms of provenance. We define the provenance of a result produced by a distributed system as the process that led to that result. This paper describes a protocol for recording documentation of a distributed system's execution. The distributed protocol guarantees that documentation with characteristics suitable for accurately determining the provenance of results is recorded. These characteristics are confirmed through a number of proofs based on an abstract state machine formalization.","1045-9219;10459219","","10.1109/TPDS.2008.215","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4641911","Distributed debugging;Distributed systems;Information Storage and Retrieval;Information Technology and grids;Provenance;Systems and Software;data protocols;data protocols.;distributed systems;grids;lineage;provenance","","distributed processing;program compilers;protocols;system documentation","abstract state machine formalization;distributed protocol;large-scale distributed systems;provenance;recording process documentation;resource-intensive tasks","","19","","24","","20081010","Sept. 2009","","IEEE","IEEE Journals & Magazines"
"The challenges in developing digital collections of phonograph records","I. Fujinaga; C. A. Leive; C. Lai","McGill University, Montreal, QC","Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '05)","20070305","2005","","","332","333","To facilitate long-term preservation and sustain the utility of phonograph records, an efficient and economical workflow management system for digitization is necessary. We describe in this paper the digitization process for building an online digital collection of phonograph records and our procedure for creating the ground-truth data, which is essential for developing an efficient metadata and content capturing system. We also discuss the challenges of defining metadata for phonograph records and their packaging to enhance access and use across traditional boundaries","","POD:1-58113-876-8","10.1145/1065385.1065460","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118562","analog sound recordings;digital library collections;digitization;metadata;music information acquisition and retrieval;phonograph records;preservation;use and access","Audio systems;Costs;Data mining;Digital recording;Disk recording;Humans;Image converters;Packaging;Software libraries;Workflow management software","audio databases;digital libraries;meta data;music;workflow management software","content capturing system;digital collections;ground-truth data;long-term preservation;metadata system;music information acquisition;online digital collection;phonograph records;workflow management system","","0","","2","","","7-11 June 2005","","IEEE","IEEE Conference Publications"
"Sundanese ancient manuscript retrieval system comparison of two probability approaches","M. Suryani; S. Hadi; A. M. I. Nasuha","Computer Science Department, University of Padjadjaran, Indonesia","2016 International Conference on Informatics and Computing (ICIC)","20170424","2016","","","105","110","Nowadays, the development of information technology both in terms of hardware and software has been growing rapidly. Many people used technology to search many types of information. On the other hand, there are information on Sundanese ancient manuscript as cultural heritage with many valuable information still accessed manually, such as the manuscript which located in Situs Kabuyutan Ciburuy, Garut, Jawa Barat. Many people with many purposes (i.e. Research, education, etc.) came to access directly. Its becoming a problem because due to lack of maintainability, the manuscripts became fragile. Furthermore, the information will be lost. So, the emergency solution should be proposed to preserve this valuable heritage. We proposed a retrieval system using the probability approach that can be effective for search the information in ancient Sundanese language. The two probability approaches: Bayesian Networks and Divergence form Randomness have been compared. The 50 queries for three different types of structured query have been run and evaluated by Mean Average Precision (MAP). The results give that the effective methods come from Bayesian Network. There is another finding during the building process of retrieval system. The interactive visualization can be delivered by the retrieval system according to the given query.","","Electronic:978-1-5090-1648-8; POD:978-1-5090-1649-5","10.1109/IAC.2016.7905698","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905698","Sundanese;ancient manuscript;bayesian network;divergence from randomness;retrieval system","Adaptation models;Bayes methods;Cultural differences;Informatics;Information retrieval;Probabilistic logic;Writing","","","","","","","","","28-29 Oct. 2016","","IEEE","IEEE Conference Publications"
"nanoHUB.org: Advancing Education and Research in Nanotechnology","G. Klimeck; M. McLennan; S. P. Brophy; G. B. Adams III; M. S. Lundstrom","Purdue University","Computing in Science & Engineering","20080822","2008","10","5","17","23","In 2002, the National Science Foundation established the Network for Computational Nanotechnology (NCN), a network of universities supporting the National Nanotechnology Initiative by bringing computational tools online, making the tools easy to use, and supporting the tools with educational materials. Along the way, NCN created a unique cyberinfrastructure to support its Web site, nanoHUB.org, where researchers, educators, and professionals collaborate, share resources, and solve real nanotechnology problems. In 2007, nanoHUB.org served more than 56,000 users from 172 countries. In this article, the authors share their experiences in developing this cyberinfrastructure and using it, particularly in an educational context.","1521-9615;15219615","","10.1109/MCSE.2008.120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4604500","HPC;Web-based services;and enhancement;and visualization;computer applications;computer graphics;computer-aided design;computer-aided engineering;computing methodologies;distribution;education;environments;evolving Internet applications;graphics systems;high-performance computing;human factors in software design;image processing and computer vision;image representation;information storage and retrieval;information technology and systems;maintenance;modeling;online information services;pattern recognition;physical sciences and engineering;remote systems;sciences;simulation;simulation support systems;software engineering;user interfaces;volume visualization;volumetric","Animation;Collaboration;Computational modeling;Computer networks;Educational institutions;Grid computing;Licenses;Nanobioscience;Nanoelectronics;Nanotechnology","Web sites;computer science education;nanotechnology","National Nanotechnology Initiative;National Science Foundation;Network for Computational Nanotechnology;cyberinfrastructure;educational materials;nanoHUB.org","","86","","18","","","Sept.-Oct. 2008","","IEEE","IEEE Journals & Magazines"
"Entity search techniques for expediting entitlement resolution in technology support services","S. Sarkar; B. Tak","","IBM Journal of Research and Development","20170314","2017","61","1","8:86","8:97","“Entity search” addresses the problem of finding data objects most accurately identified by multidimensional attributes, where individual attributes span multiple data sources with duplicate or inconsistent values. We have applied efficient entity search techniques to expedite entitlement validation in IBM's Technology Support Services (TSS) division. Entitlement validation enables a customer entity to obtain services—e.g., repair or replacement of a product entity—under warranty or contract. Expedited verification ensures customer satisfaction but is challenging when a single critical customer-provided attribute error results in a validation failure, and resolution requires exploring a large search space to find alternate entitlements that are most similar to the original request to be viable alternatives. We have built an entity search tool (consisting of data field interpretation, approximate matching, and weighted scoring) that has accelerated alternate entitlement search in TSS from minutes to seconds. Core concepts include the use of error correction/exploration techniques (where different entity identity attributes are varied based on nearness measures), entitlement repositories searched using the variations, and attribute-level fuzzy matching used to compute similarity scores between the original request and alternate entitlements and return the highest scoring results. Our experiences with deploying these tools in a large worldwide organization are described.","0018-8646;00188646","","10.1147/JRD.2016.2631399","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877284","","Error analysis;Hardware;Information retrieval;Maintenance engineering;Search problems;Software engineering","","","","","","","","","Jan.-Feb. 1 2017","","IBM","IBM Journals & Magazines"
"Novel Application of MapReduce and Conceptual Graphs","S. Velampalli; W. Eberle","Dept. of Comput. Sci. & Eng., Jawaharlal Nehru Technol. Univ., Kakinada, India","2016 International Conference on Computational Science and Computational Intelligence (CSCI)","20170320","2016","","","1107","1112","Knowledge discovery is the process of extracting useful or hidden patterns in data. With the growth of data in a structural form, such as social networks, extracting knowledge from data represented in the form of graphs is an emerging technique. In this paper, we demonstrate how ""skills"" data from resumes (i.e., what skills an applicant possesses) can be modelled into a type of graph data structure called a conceptual graph using the MapReduce programming model. Initial storage and pre-processing is done in a big data framework using the well known Hadoop Distributed File System (HDFS) and MapReduce, and skill-set discovery is accomplished using well-established graph mining techniques. We empirically evaluate our approach in the domain of skill-set analytics, where common skill-sets are extracted from a dataset of resumes. Common skill-set extraction is useful for course curriculum designers as well as job seekers.","","Electronic:978-1-5090-5510-4; POD:978-1-5090-5511-1","10.1109/CSCI.2016.0211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881504","Conceptual Graph;Graph mining;HDFS;MapReduce;Resumes;Skill","Data mining;Databases;Hidden Markov models;Information retrieval;Knowledge discovery;Natural language processing;Resumes","data handling;data mining;distributed databases;graph theory","HDFS;Hadoop Distributed File System;MapReduce programming model;conceptual graphs;graph data structure;graph mining techniques;knowledge discovery;knowledge extraction;pattern extraction;skill-set analytics;skill-set discovery;structural form","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Latent Semantic Analysis (LSA) for syslog correlation","G. Slomovitz","Universidad de la Rep&#x00FA;blica, Montevideo, Uruguay","2017 International Conference on Electronics, Communications and Computers (CONIELECOMP)","20170406","2017","","","1","4","Latent Semantic Analysis is a novel method to extract the principal components of a text corpus which has been initially used for categorization and information search. However, due to the significant results obtained, similar to human processing, LSA has become much more than a simple method to analyze text. In this work, we propose to use LSA in order to infer similarity degree of syslog messages by discovering hidden relations between them. Using real syslog message samples, we show that LSA is able to highlight the most correlated messages by topic. This method can be used to avoid complex event correlation systems which usually need signatures or rule set definitions and high expertise for its configuration.","","Electronic:978-1-5090-3621-9; POD:978-1-5090-3622-6","10.1109/CONIELECOMP.2017.7891819","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7891819","event detection;latent semantic analysis (LSA);syslog correlation","Correlation;Data mining;Event detection;Information retrieval;Matrix decomposition;Protocols;Semantics","natural language processing;protocols;text analysis","LSA;complex event correlation systems;human processing;latent semantic analysis;rule set definitions;syslog correlation;syslog message samples;text corpus principal component extraction","","","","","","","22-24 Feb. 2017","","IEEE","IEEE Conference Publications"
"3-D Reconstruction of the Human Ribcage Based on Chest X-Ray Images and Geometric Template Models","C. Koehler; T. Wischgoll; F. Golshani","Wright State University, Dayton","IEEE MultiMedia","","2015","PP","99","1","1","In order to avoid later stages of lung diseases, preventative methods such as regular preemptive screenings are an absolute necessity. For these screenings, costly methods, such as CT scans, are often times cost prohibitive. X-ray imagery is significantly more cost effective. However, it does not provide a three-dimensional view of the patient&#x2019;s torso. Hence, this paper describes a technique that, based on a series of geometric template models and two X-ray images of a subject&#x2019;s chest, creates a patient specific 3-D reconstruction of the ribcage. The 3-D reconstruction helps improve visualization of the chest without needing to use expensive 3-D scanners and can be combined with existing computer-aided disease detection techniques to help with diagnosis.","1070-986X;1070986X","","10.1109/MMUL.2009.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255213","H.2 Database Management;H.3 Information Storage and Retrieval;I.3 Computer Graphics;I.4 Image Processing and Computer Vision;I.4.0.b Image processing software;I.5 Pattern Recognition;I.6 Simulation;I.6.9 Visualization;I.7 Document and Text Processing;J.3 Life and Medical Sciences;J.3.c Medical information systems;J.8.b Databases;J.8.c Database connectivity;Modeling;and Visualization;multimedia","Computed tomography;Costs;Diseases;Humans;Lungs;Solid modeling;Three dimensional displays;Torso;Visualization;X-ray imaging","","","","1","","11","","20090922","","","IEEE","IEEE Early Access Articles"
"Near-duplicate Video Retrieval: Current Research and Future Trends - Withdrawn","H. T. Shen; J. Liu; Z. Huang; C. W. Ngo; W. Wang","The University of Queensland, Brisbane","IEEE MultiMedia","","2015","PP","99","1","1","Withdrawn.","1070-986X;1070986X","","10.1109/MMUL.2011.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871572","H.2.0.b Database design;H.2.0.c Query design and implementation language;H.2.2.c Indexing methods;H.2.4.d Distributed databases;H.2.4.e Multimedia databases;H.2.4.h Query processing;H.2.8.e Feature extraction or construction;H.2.8.g Image databases;H.3.1 Content Analysis and Indexing;H.3.1 Content Analysis and Indexing H.3 Information Storage and Retrieval HInformation Technology and Systems;H.3.3 Information Search and Retrieval;H.3.3 Information Search and Retrieval H.3 Information Storage and Retrieval HInformation Technology and Systems;H.3.7 Digital Libraries;H.5.1.f Image/video retrieval;H.5.1.g Video;I.2.1.b Computer vision;I.2.10 Vision and Scene Understanding;I.5.4.b Computer vision;K.2.c Software;modeling and management","Copyright protection;Databases;Feature extraction;Image color analysis;Semantics;Streaming media;Visualization","","","","1","","","","20110609","","","IEEE","IEEE Early Access Articles"
"HIVEBEAT — A highly interactive visualization environment for broad-scale exploratory analysis and tracing: VAST 2012 Mini Challenge 1 award: Honorable mention for comprehensive visualization suite","R. Krüger; H. Bosch; S. Koch; C. Müller; G. Reina; D. Thom; T. Ertl","Institute for Visualization and Interactive Systems (VIS), Universit&#x00E4;t Stuttgart, Germany Visualization Research Center Universit&#x00E4;t Stuttgart (VISUS), Germany","2012 IEEE Conference on Visual Analytics Science and Technology (VAST)","20130103","2012","","","277","278","The VAST Challenge 2012 deals with large data network analysis. The challenge scenario is centered around the computer network of the fictitious Bank of Money (BoM). BoM operates 888,977 computers which are geographically distributed on a fictitious landmass. The provided data has a hierarchical structure grouping machines in business units, facilities, machine classes and machine types. For each machine, up to 192 status logs in a two day time period were provided. These logs report on three status attributes: policy, activity and number of connections. The main task of Mini-Challenge 1 was to highlight up to five anomalies in the massive data set. We address this challenge by presenting HIVEBEAT, a highly interactive visualization environment for broad-scale exploratory analysis and tracing. It offers eight interactive views which visualize different aspects of the data, support brushing and linking and are complemented with time-dependent and sequence-based filtering. Some of these components build on ideas from earlier works [2, 3, 4]. With HIVEBEAT, we were able to identify five anomalies and justify them with plausible hypotheses.","","Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5","10.1109/VAST.2012.6400518","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400518","H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval — Search process;H.5.2 [Information Interfaces and Presentation]: User Interfaces — GUI","","","","","0","","4","","","14-19 Oct. 2012","","IEEE","IEEE Conference Publications"
"Technology Independent UI Framework for Service Providers","V. S. Subramaniam; A. M. Asadullah; D. N. Hoshing","Infosys Ltd., Bangalore, India","2012 Third International Conference on Services in Emerging Markets","20130225","2012","","","189","194","Business agility is the key driver for successful enterprises. This is very common in the service industry where service provider has to develop the service and keep it up to date with the technology and industry trends. We present a framework which we created to focus on business feature development at the client side (browser), isolating developers from rapid changes in UI technology. This allowed us for a quick movement to a new UI platform with minimal changes in the existing logic. We demonstrate the importance of such a framework and the challenges faced during the framework development. We also reference some of the open platforms which have developed a framework similar to us and how they won over their competitors. Finally, we compare the cost of developing the framework with the cost of business agility.","","Electronic:978-0-7695-4937-8; POD:978-1-4673-5729-6","10.1109/ICSEM.2012.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468199","Information storage and retrieval;Models Transformations;Rich Internet Applications;Software Modernization;Software Reengineering;Web engineering","Browsers;Business;Java;Layout;Navigation;Servers;XML","business data processing;user interfaces","business agility;business feature development;framework development;open platforms;service industry;service providers;technology independent UI framework;user interface","","0","","10","","","12-15 Dec. 2012","","IEEE","IEEE Conference Publications"
"Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition","J. Chae; D. Thom; H. Bosch; Y. Jang; R. Maciejewski; D. S. Ebert; T. Ertl","","2012 IEEE Conference on Visual Analytics Science and Technology (VAST)","20130103","2012","","","143","152","Recent advances in technology have enabled social media services to support space-time indexed data, and internet users from all over the world have created a large volume of time-stamped, geo-located data. Such spatiotemporal data has immense value for increasing situational awareness of local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing social media data, researchers have mainly focused on finding temporal trends according to volume-based importance. Hence, a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach that provides users with scalable and interactive social media data analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and YouTube. In order to find and understand abnormal events, the analyst can first extract major topics from a set of selected messages and rank them probabilistically using Latent Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks and outliers within topic time series. Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive visual analysis process.","","Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5","10.1109/VAST.2012.6400557","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400557","H.3.3 [Information Storage and Retrieval];H.5.2 [Information Interfaces and Presentation]: User Interfaces — GUI;Information Search and Retrieval — Information filtering;relevance feedback","Data mining;Earthquakes;Educational institutions;Media;Spatiotemporal phenomena;Time series analysis;Twitter","Internet;data visualisation;graphical user interfaces;interactive systems;social networking (online);time series","Flickr;Internet user;Twitter;YouTube;abnormal event detection;abnormal topics examination;abnormal topics exploration;control chart method;interactive social media data analysis;interactive visual analysis process;latent Dirichlet allocation;local event;seasonal trend decomposition;seasonal-trend decomposition;situational awareness;social media service;space-time indexed data;spatiotemporal data;spatiotemporal social media analytics;time-stamped geo-located data;topic time series;visual analytics approach;visualization;volume-based importance","","32","","39","","","14-19 Oct. 2012","","IEEE","IEEE Conference Publications"
"Entity-based collaboration tools for intelligence analysis","E. A. Bier; S. K. Card; J. W. Bodnar","Palo Alto Research Center, Inc., 3333 Coyote Hill Road, California, 94304, USA","2008 IEEE Symposium on Visual Analytics Science and Technology","20081118","2008","","","99","106","Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts.","","POD:978-1-4244-2935-6","10.1109/VAST.2008.4677362","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677362","H.3.3 [Information Search and Retrieval]: Information filtering;H.4 [Information Systems Applications]: H.4.m Miscellaneous;H.5.2 [User Interfaces]: Graphical user interfaces (GUI);H.5.3 [Group and Organization Interfaces]: Collaborative computing, Computer-supported cooperative work, Web-based interaction;argumentation marshalling;collaboration;collective intelligence;entity-based;exploratory search;information foraging;information workspace;intelligence analysis;semantic notebook;sensemaking;visual analytics;visualization","Collaborative software;Collaborative tools;Collaborative work;Design optimization;Information analysis;Laboratories;Organizing;Software tools;System testing;Text analysis","entity-relationship modelling;groupware;software tools","entity workspace system;entity-based collaboration tools;information structures;intelligence analysis;software tools","","16","","17","","","19-24 Oct. 2008","","IEEE","IEEE Conference Publications"
"Analysis and Synthesis of Music using Semiotic Intelligent System","Y. Molchanyuk","Odessa Mechnikov National University, Dvoryanskaya str. 2, Odessa, Ukraine 65000, molchanyuk@onu.edu.ua","2007 4th IEEE Workshop on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications","20080415","2007","","","379","382","This paper presents one of the possible ways of computer music perception and synthesis using intelligent systems. Music pieces express emotions which are defined by certain musical features. The author suggests using the semiotic intelligent system to analyze the emotions in music, tag music pieces according to their emotional content, and also create new music pieces that express certain defined emotions.","","CD-ROM:978-1-4244-1348-5; POD:978-1-4244-1347-8","10.1109/IDAACS.2007.4488443","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488443","computer music perception;computer perception;data analysis;intelligent information and retrieval systems","Algorithm design and analysis;Application software;Frequency;Information analysis;Intelligent systems;Mood;Music;Rhythm;Signal analysis;Timbre","audio signal processing;emotion recognition;music;signal synthesis","computer music perception;computer music synthesis;emotion analysis;music pieces tagging;semiotic intelligent system","","0","","8","","","6-8 Sept. 2007","","IEEE","IEEE Conference Publications"
"Principles and Tools for Collaborative Entity-Based Intelligence Analysis","E. A. Bier; S. K. Card; J. W. Bodnar","Palo Alto Research Center, Inc., Palo Alto","IEEE Transactions on Visualization and Computer Graphics","20100115","2010","16","2","178","191","Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts. Long-term tests suggest that this approach can support both top-down and bottom-up styles of analysis.","1077-2626;10772626","","10.1109/TVCG.2009.104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226632","User interfaces;Web-based interaction.;collaborative computing;computer-supported cooperative work;graphical user interfaces (GUI);group and organization interfaces;information filtering;information search and retrieval;information systems applications;miscellaneous","","groupware;software tools;user interfaces","bottom-up analysis style;collaborative entity;entity workspace system;evidence notebook;evidence visualization;information structure;intelligence analysis;notification system;software tools;top-down analysis style","Algorithms;Artificial Intelligence;Computer Graphics;Computer Simulation;Cooperative Behavior;Information Storage and Retrieval;Models, Theoretical;Software;User-Computer Interface","7","2","18","","20090828","March-April 2010","","IEEE","IEEE Journals & Magazines"
"Efficient Similarity Search over Future Stream Time Series","X. Lian; L. Chen","Hong Kong Univ. of Sci. & Technol., Hong Kong","IEEE Transactions on Knowledge and Data Engineering","20071127","2008","20","1","40","54","With the advance of hardware and communication technologies, stream time series is gaining ever-increasing attention due to its importance in many applications such as financial data processing, network monitoring, Web click-stream analysis, sensor data mining, and anomaly detection. For all of these applications, an efficient and effective similarity search over stream data is essential. Because of the unique characteristics of the stream, for example, data are frequently updated and real-time response is required, the previous approaches proposed for searching through archived data may not work in the stream scenarios. Especially, in the cases where data often arrive periodically for various reasons (for example, the communication congestion or batch processing), queries on such incomplete time series or even future time series may result in inaccuracy using traditional approaches. Therefore, in this paper, we propose three approaches,<i> polynomial, Discrete Fourier Transform (DFT), and probabilistic</i>, to predict the unknown values that have not arrived at the system and answer similarity queries based on the predicted data. We also apply efficient indexes, that is, a multidimensional hash index and a B<sup>+</sup>-tree, to facilitate the prediction and similarity search on future time series, respectively. Extensive experiments demonstrate the efficiency and effectiveness of our methods for prediction and answering queries.","1041-4347;10414347","","10.1109/TKDE.2007.190666","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358952","Information Search and Retrieval;Multimedia databases;Query processing;Search process","","discrete Fourier transforms;polynomials;probability;query processing;time series","Web click-stream analysis;anomaly detection;discrete Fourier transform;financial data processing;future stream time series;multidimensional hash index;network monitoring;polynomial;probabilistic;sensor data mining;similarity search","","11","","51","","","Jan. 2008","","IEEE","IEEE Journals & Magazines"
"A semantic content based recommendation system for cross-lingual news","S. N. Ferdous; M. M. Ali","Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh","2017 IEEE International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","20170403","2017","","","1","6","News articles in web narrate important events happening worldwide. These articles are not only written in English, but also in different languages for different native people. In this paper, we propose an approach for an automated Bengali-English semantic recommender system based on ontology by analyzing news domain. News ontology is designed automatically by using information extraction techniques. Both the news title and news body are considered separately in the ontology creation process. First, important information from news is extracted and ontology is created from the source language document. Then, ontology is created from target language document following similar technique. Next, ontology matching is performed between the translated source ontology and target English Ontology. Matching can also be done with synonymous documents. A matching factor is calculated which can be taken as the semantic similarity measure between the cross-lingual documents. Recommendation of news items is done based on this matching factor. The experiment study verifies the proposed method adopted by us.","","Electronic:978-1-5090-6004-7; POD:978-1-5090-6005-4","10.1109/ICIVPR.2017.7890880","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890880","Bilingual News;News Ontology;Recommendation System;Semantic Recommendation;Semantic Web","Collaboration;Information retrieval;Manuals;Ontologies;Organizations;Semantics","document handling;information retrieval;natural language processing;ontologies (artificial intelligence);pattern matching;recommender systems","English langauge;English ontology;automated Bengali-English semantic recommender system;cross-lingual documents;cross-lingual news;information extraction;matching factor;news articles;news body;news domain;news items recommendation;news ontology;news title;ontology creation process;ontology matching;semantic content based recommendation system;semantic similarity measure;source language document;synonymous documents","","","","","","","13-14 Feb. 2017","","IEEE","IEEE Conference Publications"
"Analysis and Semantic Querying in Large Biomedical Image Datasets","V. S. Kumar; S. Narayanan; T. Kurc; J. Kong; M. N. Gurcan; J. H. Saltz","Ohio State University","Computer","20080415","2008","41","4","52","59","Biomedical image analysis plays an important role in diagnosing, prognosing, and treating complex diseases. The authors describe a set of techniques for analyzing, processing, and querying large image datasets using semantic and spatial information.","0018-9162;00189162","","10.1109/MC.2008.108","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4488251","artificial intelligence;computer vision;computer-assisted prognosis;data-intensive computing;information storage and retrieval;parallel architectures;processor architectures","Adaptive algorithm;Algorithm design and analysis;Biomedical imaging;Image analysis;Image generation;Image resolution;Image sequence analysis;Partitioning algorithms;Spatial resolution","medical image processing;query processing","biomedical image analysis;large biomedical image datasets;querying large image datasets;semantic information;semantic querying;spatial information","","8","","3","","","April 2008","","IEEE","IEEE Journals & Magazines"
"A novel text structure feature extractor for Chinese scene text detection and recognition","X. Ren; K. Chen; X. Yang; Y. Zhou; J. He; J. Sun","Institute of Image Communication and Information Processing, Shanghai Jiao Tong University, China","2016 23rd International Conference on Pattern Recognition (ICPR)","20170424","2016","","","3380","3385","Scene text information extraction plays an important role in many computer vision applications. Unlike most existing text extraction algorithms for English texts, in this paper, we focus on Chinese texts, which are more complex in stroke and structure. To tackle this challenging problem, we propose a novel convolutional neural network (CNN) based text structure feature extractor for Chinese texts. Each Chinese character contains its specific types and combination of text structure components, which is rarely seen in backgrounds. Thus, different from the features only applicable to one text extraction stage (text detection or text recognition), the text structure component feature is suitable for both Chinese text detection and recognition. A text structure component detector (TSCD) layer is designed to detect the large amount of component types, which is the most challenging part of extracting text structure component features. Through statistical classification various types of text structure component are detected by their specially designed convolutional units in the TSCD layer. With the TSCD layer, the CNN has improvements in the accuracy and uniqueness of text feature description. In the evaluation, both text detection and recognition algorithms based on the proposed text structure feature extractor achieve state-of-the-art results in two datasets.","","Electronic:978-1-5090-4847-2; POD:978-1-5090-4848-9","10.1109/ICPR.2016.7900156","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7900156","","Algorithm design and analysis;Data mining;Detectors;Feature extraction;Information retrieval;Periodic structures;Text recognition","","","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Novel 2D Urban Map Search Framework Based on Attributed Graph Matching","M. Strintzis; A. Mademlis; K. Kostopoulos; K. Moustakas; D. Tzovaras","Aristotle University of Thessaloniki, Thessaloniki","IEEE MultiMedia","","2015","PP","99","1","1","This paper presents a novel framework for urban map search. The search capabilities of the existing GIS systems are restricted to text-based search, neglecting significant topological and semantic information. We propose a framework that aims to extend the search capabilities by offering sketch-based search. Initially, the urban maps are processed in an offline step in order to extract the topological information in the form of an attributed graph. In the online step, the user queries the system by sketching the desired network structure. The search algorithm is based on attributed graph matching of the query graph and the attributed graphs of the urban maps and allows both partial and global matching of the query. Experimental results illustrate the excellent performance of the system for intuitive search in maps.","1070-986X;1070986X","","10.1109/MMUL.2009.59","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5255211","Content-Based Retrieval;Databases;H.1.2 User/Machine Systems;H.2.8.e Feature extraction or construction;H.3 Information Storage and Retrieval;H.3.1 Content Analysis and Indexing;H.3.3 Information Search and Retrieval;H.5 Information Interfaces and Representation (HCI);H.5.1.f Image/video retrieval;H.5.2.g Haptic I/O;I.;I.2.1.b Computer vision;I.2.10 Vision and Scene Understanding;I.3 Computer Graphics;I.3.5 Computational Geometry and Object Modeling;I.3.5.d Geometric algorithms;I.3.5.e Hierarchy and geometric transformations;I.3.7 Three-Dimensional Graphics and Realism;I.3.7.g Virtual reality;I.4 Image Processing and Computer Vision;I.4.10.a Hierarchical;I.4.10.c Multidimensional;I.4.2.e Video coding;I.4.3.a Filtering;I.5 Pattern Recognition;I.5.4.b Computer vision;I.5.4.m Signal processing;I.6.5.a Modeling methodologies;I.6.8.a Animation;Information Storage and Retrieval;Information Technology and Systems;Multimedia;and systems;languages","Cities and towns;Data mining;Digital filters;Geographic Information Systems;Image databases;Information filtering;Information filters;Matched filters;Roads;Satellite navigation systems","","","","0","","19","","20090922","","","IEEE","IEEE Early Access Articles"
"Automatic Website Summarization by Image Content: A Case Study with Logo and Trademark Images","E. Baratis; E. G. M. Petrakis; E. E. Milios","Dept. of Electron. & Comput. Eng., Tech. Univ. of Crete (TUC), Chania","IEEE Transactions on Knowledge and Data Engineering","20080822","2008","20","9","1195","1204","Image-based abstraction (or summarization) of a Web site is the process of extracting the most characteristic (or important) images from it. The criteria for measuring the importance of images in Web sites are based on their frequency of occurrence, characteristics of their content and Web link information. As a case study, this work focuses on logo and trademark images. These are important characteristic signs of corporate Web sites or of products presented there. The proposed method incorporates machine learning for distinguishing logo and trademarks from images of other categories (e.g., landscapes, faces). Because the same logo or trademark may appear many times in various forms within the same Web site, duplicates are detected and only unique logo and trademark images are extracted. These images are then ranked by importance taking frequency of occurrence, image content and Web link information into account. The most important logos and trademarks are finally selected to form the image-based summary of a Web site. Evaluation results of the method on real Web sites are also presented. The method has been implemented and integrated into a fully automated image-based summarization system which is accessible on the Web (www.intelligence.tuc.gr/websummarization)","1041-4347;10414347","","10.1109/TKDE.2008.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4445672","Abstracting methods;Applications;Content Analysis and Indexing;Indexing Methods;Information Storage and Retrieval","","Web sites;abstracting;document image processing;feature extraction;learning (artificial intelligence)","Web link information;automatic Website summarization;corporate Web sites;feature extraction;image content;image extraction;image ranking;image-based abstraction;importance ranking;logo image;machine learning;occurrence frequency;trademark image","","4","1","19","","20080202","Sept. 2008","","IEEE","IEEE Journals & Magazines"
"Runtime Verification of Web Service Interface Contracts","S. Hallé; T. Bultan; G. Hughes; M. Alkhalaf; R. Villemaire","University of California, Santa Barbara","Computer","20100311","2010","43","3","59","66","Asynchronous JavaScript and XML (Ajax) is a collection of technologies used to develop rich and interactive Web applications. A typical Ajax client runs locally in the user's Web browser and refreshes its interface on the fly in response to user input. Using this method with the AWS-ECS let us automatically generate test sequences and detect two deviations of their service implementation with respect to the online documentation provided, in less than three minutes of testing. We also provided a framework that allows the runtime monitoring of both client and server contract constraints with minimal modification to an existing Ajax application code. Experiments with the Amazon E-Commerce Service demonstrate the advantages of using a model-based approach for the runtime testing and monitoring of Web applications.","0018-9162;00189162","","10.1109/MC.2010.76","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5427377","Information storage and retrieval;Software engineering;Testing and debugging;Web-based services","Automatic testing;Contracts;Documentation;Java;Monitoring;Runtime;Web services;XML","Java;Web services;XML;electronic commerce;online front-ends;program verification","AWS-ECS;Ajax;Amazon E-Commerce Service;Web applications;XML;asynchronous JavaScript;online documentation;runtime monitoring;runtime testing;service implementation;test sequence generation","","20","","13","","","March 2010","","IEEE","IEEE Journals & Magazines"
"A Similarity Metric for Retrieval of Compressed Objects: Application for Mining Satellite Image Time Series","L. Gueguen; M. Datcu","GET-Telecom Paris, Paris","IEEE Transactions on Knowledge and Data Engineering","20080226","2008","20","4","562","575","This paper addresses the problem of building an index of compressed object databases. We introduce an informational similarity measure based on the coding length of two part codes. Then, we present a methodology for compressing the database by taking into account interobject redundancies and by using the informational similarity measure. The method produces an index included in the code of the data volume. This index is built such that it contains the minimal sufficient information to discriminate the data-volume objects. Then, we present an optimal two-part coder for compressing spatio-temporal events contained in satellite image time series (SITS). The two-part coder allows us to measure similarity and then to derive an optimal index of SITS spatio-temporal events. The resulting index is representative of the SITS information content and enables queries based on information content.","1041-4347;10414347","","10.1109/TKDE.2007.190718","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4384483","Clustering;Compression (Coding);Content Analysis and Indexing;IIndex Generation;Information Search and Retrieval;Information theory;Model classification;Model-based coding;Models;Pattern Recognition;Remote sensing;Statistical;Texture;Time-varying imagery","","image retrieval;indexing;spatiotemporal phenomena;time series;visual databases","compressed object databases;compressed objects retrieval;information content;information queries;informational similarity measure;interobject redundancies;satellite image time series;spatiotemporal events","","16","","28","","","April 2008","","IEEE","IEEE Journals & Magazines"
"Visual mining of multimedia data for social and behavioral studies","Chen Yu; Yiwen Zhong; T. Smith; I. Park; Weixia Huang","Indiana University, USA","2008 IEEE Symposium on Visual Analytics Science and Technology","20081118","2008","","","155","162","With advances in computing techniques, a large amount of high-resolution high-quality multimedia data (video and audio, etc.) has been collected in research laboratories in various scientific disciplines, particularly in social and behavioral studies. How to automatically and effectively discover new knowledge from rich multimedia data poses a compelling challenge since state-of-the-art data mining techniques can most often only search and extract pre-defined patterns or knowledge from complex heterogeneous data. In light of this, our approach is to take advantages of both the power of human perception system and the power of computational algorithms. More specifically, we propose an approach that allows scientists to use data mining as a first pass, and then forms a closed loop of visual analysis of current results followed by more data mining work inspired by visualization, the results of which can be in turn visualized and lead to the next round of visual exploration and analysis. In this way, new insights and hypotheses gleaned from the raw data and the current level of analysis can contribute to further analysis. As a first step toward this goal, we implement a visualization system with three critical components: (1) A smooth interface between visualization and data mining. The new analysis results can be automatically loaded into our visualization tool. (2) A flexible tool to explore and query temporal data derived from raw multimedia data. We represent temporal data into two forms - continuous variables and event variables. We have developed various ways to visualize both temporal correlations and statistics of multiple variables with the same type, and conditional and high-order statistics between continuous and event variables. (3) A seamless interface between raw multimedia data and derived data. Our visualization tool allows users to explore, compare, and analyze multi-stream derived variables and simultaneously switch to access raw multimedia data. We de- - monstrate various functions in our visualization program using a set of multimedia data including video, audio and motion tracking data.","","POD:978-1-4244-2935-6","10.1109/VAST.2008.4677369","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4677369","H.1.2 [Information Systems]: Models and principles - user/machine systems;H.2.8 [Database Management]: Database Applications —Data mining;H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval;H.5.1 [Information Interface and Presentation]: Multimedia Information Systems;H.5.2 [Information Interface and Presentation]: User Interfaces — Graphical user interfaces (GUI);multimedia data;visual data mining","Data mining;Data visualization;Humans;Laboratories;Management information systems;Multimedia computing;Multimedia databases;Multimedia systems;Statistics;Switches","behavioural sciences computing;data mining;multimedia computing;social sciences computing;visual programming","behavioral studies;data mining;high-quality multimedia data;social studies;visual mining;visualization program","","0","1","12","","","19-24 Oct. 2008","","IEEE","IEEE Conference Publications"
"Integrating chemistry scholarship with web architectures, grid computing and semantic web","S. K. Challa; M. Pierce; S. Marru","Community Grids Laboratory, Pervasive Technology Institute, Indiana University, Bloomington, 2719 East 10th Street, 47408, USA","2010 Gateway Computing Environments Workshop (GCE)","20101223","2010","","","1","8","A chemist given a compound would be interested in knowing the experiments performed using the compound, journals containing the compound and also molecular properties of the compound. If there is a way to integrate this data, it would enhance the chemist's knowledge about a given compound. The Object Reuse and Exchange (ORE) specification may provide a solution to this problem. ORE is a model proposed by the digital libraries community to aggregate resources on the web. OREChem is a research project funded by Microsoft External Research that aims to apply and extend ORE to enable the integration of experimental, bibliographical and molecular properties data. OREChem targets crystallography as its primary application domain. This effort will design a prototypical, semantic-based eScience infrastructure for chemistry and chemical informatics. In this paper we describe how we have used REST as well as SOAP web services, TeraGrid cyberinfrastructure and Semantic Web technologies, such as RDF and triple stores, to facilitate the metadata integration.","2152-1085;21521085","Electronic:978-1-4244-9752-2; POD:978-1-4244-9751-5","10.1109/GCE.2010.5676123","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676123","Information Storage and Retrieval;Online Information Services - Web-based services;REST services;Rule-based databases","Arrays;Crystals;Feeds;Resource description framework;Simple object access protocol","Web services;chemistry computing;digital libraries;formal specification;grid computing;semantic Web;software architecture","Microsoft external research;OREChem;SOAP web services;TeraGrid cyberinfrastructure;Web architectures;chemical informatics;chemistry scholarship integration;crystallography;digital libraries community;grid computing;metadata integration;object reuse and exchange specification;semantic Web;semantic based eScience infrastructure","","1","","31","","","14-14 Nov. 2010","","IEEE","IEEE Conference Publications"
"Semantic-Aware and QoS-Aware Image Caching in Ad Hoc Networks","B. Yang; A. R. Hurson","Bowie State Univ., Bowie","IEEE Transactions on Knowledge and Data Engineering","20071022","2007","19","12","1694","1707","Semantic caching was originally used for structural data objects such as 2D location data and cannot be directly applied to mobile image data access. First, traditional semantic caching relies on exact match and therefore is not suitable for approximate and similarity-based queries. Second, the semantic description of cached data is defined on query context instead of data content, which leads to inefficient use of cache storage. Third, the semantic description of cached data does not reflect the popularity of the data, making it difficult to conduct popularity-driven content analysis and prediction. To facilitate content-based image retrieval in wireless ad hoc networks, we propose a semantic-aware image caching (SAIC) scheme in this paper. The proposed scheme can efficiently utilize the cache space and significantly reduce the cost of image retrieval. The proposed SAIC scheme is based on several innovative ideas: 1) multilevel representation of the semantic contents, 2) association-based and Bayesian probability-based content prediction, 3) constraint-based representation method showing the semantic similarity between images, 4) nonflooding query processing, and 5) adaptive cache consistency maintenance. The proposed model is introduced, and through extensive simulation, its behavior has been compared against two state-of-the-art ad hoc caching schemes as advanced in the literature.","1041-4347;10414347","","10.1109/TKDE.2007.190652","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358944","Content Analysis and Indexing;Database semantics;Image/video retrieval;Information Search and Retrieval;Mobile Computing","","Bayes methods;ad hoc networks;cache storage;content-based retrieval;image retrieval;mobile computing;probability;quality of service;query processing","Bayesian probability;QoS-aware image caching;association-based content prediction;content-based image retrieval;nonflooding query processing;semantic-aware image caching;structural data object;wireless ad hoc network","","6","","21","","","Dec. 2007","","IEEE","IEEE Journals & Magazines"
"Term weighting based class indexes using space density for Al-Qur'an relevant meaning ranking","Kurniawati; A. Syauqi","Faculty of Science and Technology, Universitas Islam Negeri Maulana Malik Ibrahim Malang, Malang, East Java, Indonesia","2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)","20170309","2016","","","460","463","Nowadays information retrieval based on specific queries is already used in computer system. One of the popular methods is document ranking using Vector Space Model (SVM) based on TF.IDF term-weighting. In this paper TF.IDF.ICS<sub>δ</sub>F term-weighting based class-indexing is proposed, afterward comparing its effectiveness to TF.IDF and TF.IDF.ICF term weighting. Each method is investigated through Al-Qur'an dataset. Al-Qur'an consist many verses, each verse of the Al-Qur'an is a single document which is ranked based on user query. The experimental show that the proposed method can be implemented on document ranking and the performance is better than previous methods with accurate value 93%.","","Electronic:978-1-5090-4629-4; POD:978-1-5090-4630-0; USB:978-1-5090-4628-7","10.1109/ICACSIS.2016.7872753","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872753","ICF;ICS<inf>δ</inf>F;TF.IDF;class indexing;document ranking;term weighting","Computers;Indexing;Information retrieval;Integrated circuits;Mathematical model;Testing","document handling;indexing;query processing;vectors","Al-Qur'an relevant meaning ranking;SVM;TF.IDF.ICSδF term-weighting based class-indexing;computer system queries;document ranking;information retrieval;space density;term weighting based class indexes;user query;vector space model","","","","","","","15-16 Oct. 2016","","IEEE","IEEE Conference Publications"
"Timely and Keyword-Based Dynamic Content Selection for Public Displays","F. R. S. G. Ribeiro; R. J. P. José","Inf. Dept., Polytech. Inst. of Castelo Branco, Castelo Branco, Portugal","2010 International Conference on Complex, Intelligent and Software Intensive Systems","20100415","2010","","","655","660","In public display systems determine what to present and when is a central feature. Although several adaptive scheduling alternatives have been explored, which introduce sensibility of the display to some type of external variable, they are still very dependent on the user in their behavior, content specific in their nature and very rigid in their adaptation to their social environment, not providing visitors of the place with appropriate, rich and personalized information according to their interests and expectations. There is a need for solutions that successfully integrate the wealth of dynamic web sources as providers for situated and updated content with social and contextual environment around the display so as to present the most appropriate content at every moment, and thus improving the utility of the system. In this paper, we present a recommender system for public situated displays that is able to autonomously select relevant content from Internet sources using a keyword-based place model as input. Based on external relevance criteria the system finds and pre-selects only those sources that are more relevant, and an adaptive scheduling algorithm continuously select content that are relevant, timely, in accordance with the place model, sensitive to immediate indications of interest and balanced to serve the broad range of interests of the target population. To evaluate this system we have carried out two partial experiments. The results showed that keyword-based shared place models jointly with content specific relevance models are a simple and valid approach to user-generated content for public displays.","","Electronic:978-1-4244-5918-6; POD:978-1-4244-5917-9","10.1109/CISIS.2010.114","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5447529","Information Integration;Intelligent Environments;Public Displays;Situated Displays;Social Information Systems;Ubicomp;Web Information Filtering and Retrieval","Adaptive scheduling;Blogs;Competitive intelligence;Content management;Displays;Informatics;Information systems;Internet;Recommender systems;Software systems","Internet;content management;recommender systems;scheduling","adaptive scheduling algorithm;content selection;contextual environment;dynamic Web sources;keyword-based place model;public display systems;recommender system;social environment;user-generated content","","1","","14","","","15-18 Feb. 2010","","IEEE","IEEE Conference Publications"
"Automatic extraction of malicious behaviors","K. H. T. Dam; T. Touili","IRIF, University Paris Diderot and CNRS","2016 11th International Conference on Malicious and Unwanted Software (MALWARE)","20170330","2016","","","1","10","The number of new malwares is increasing everyday. Thus malware detection is nowadays a big challenge. The existing techniques for malware detection require a huge effort of engineering to manually extract the malicious behaviors. To avoid this tedious task, we propose in this paper an approach to automatically extract the malicious behaviors. We model a program using an API call graph, and we represent the malicious behaviors using a malicious API graph. We then reduce the malicious behavior extraction problem to the problem of retrieving from the benign and malicious API call graphs the set of subgraphs that are relevant for malicious behaviors. We solve this issue by applying and adapting well-known efficient Information Retrieval techniques based on the TFIDF scheme. We use our automatically extracted malicious behavior specification for malware detection using a kind of product between graphs. We obtained interesting experimental results, as we get 99.04% of detection rate. Moreover, we were able to detect several malwares that well-known and widely used antiviruses such as Panda, Avira, Kaspersky, Avast, Qihoo- 360, McAfee, AVG, BitDefender, ESET-NOD32, F-Secure, and Symantec could not detect.","","Electronic:978-1-5090-4542-6; POD:978-1-5090-4543-3","10.1109/MALWARE.2016.7888729","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888729","","Binary codes;Data mining;Databases;Image edge detection;Information retrieval;Malware;Robustness","application program interfaces;graph theory;invasive software","API;automatic extraction;information retrieval techniques;malicious API call graphs;malicious behavior extraction;malware detection","","","","","","","18-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"Partial Hash Update via Hamming Subspace Learning","C. Ma; I. W. Tsang; F. Peng; C. Liu","College of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Image Processing","20170329","2017","26","4","1939","1951","Hashing technique has become an effective method for information retrieval due to the fast calculation of the Hamming distance. However, with the continuous growth of data coming from the Internet, the online update of hashing on the massive social data becomes very time-consuming. To alleviate this issue, in this paper, we propose a novel updating technique for hashing methods, namely Hamming Subspace Learning (HSL). The motivation of HSL is to generate a low-dimensional Hamming subspace from a high-dimensional Hamming space by selecting representative hash functions. Through HSL, we aim to improve the speed of updating binary codes for all samples. We present a method for Hamming subspace learning based on greedy selection strategy and the Distribution Preserving Hamming Subspace learning (DHSL) algorithm by designing a novel loss function. The experimental results demonstrate that the HSL is effective to improve the speed of online updating and the performance of hashing algorithm.","1057-7149;10577149","","10.1109/TIP.2017.2675342","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865976","Hamming subspace;binary codes;hash update","Binary codes;Electronic mail;Image retrieval;Information retrieval;Streaming media;Training","file organisation;learning (artificial intelligence);visual databases","DHSL algorithm;HSL;Hamming distance;binary code update;distribution preserving Hamming subspace learning;greedy selection strategy;hash function selection;high-dimensional Hamming subspace;information retrieval;loss function;low-dimensional Hamming subspace;partial hash update","","","","","","20170301","April 2017","","IEEE","IEEE Journals & Magazines"
"Annotating Images by Mining Image Search Results","X. J. Wang; L. Zhang; X. Li; W. Y. Ma","Microsoft Research Asia, Beijing","IEEE Transactions on Pattern Analysis and Machine Intelligence","20080919","2008","30","11","1919","1932","Although it has been studied for years by the computer vision and machine learning communities, image annotation is still far from practical. In this paper, we propose a novel attempt at model-free image annotation, which is a data-driven approach that annotates images by mining their search results. Some 2.4 million images with their surrounding text are collected from a few photo forums to support this approach. The entire process is formulated in a divide-and-conquer framework where a query keyword is provided along with the uncaptioned image to improve both the effectiveness and efficiency. This is helpful when the collected data set is not dense everywhere. In this sense, our approach contains three steps: 1) the search process to discover visually and semantically similar search results, 2) the mining process to identify salient terms from textual descriptions of the search results, and 3) the annotation rejection process to filter out noisy terms yielded by Step 2. To ensure real-time annotation, two key techniques are leveraged - one is to map the high-dimensional image visual features into hash codes, the other is to implement it as a distributed system, of which the search and mining processes are provided as Web services. As a typical result, the entire process finishes in less than 1 second. Since no training data set is required, our approach enables annotating with unlimited vocabulary and is highly scalable and robust to outliers. Experimental results on both real Web images and a benchmark image data set show the effectiveness and efficiency of the proposed algorithm. It is also worth noting that, although the entire approach is illustrated within the divide-and- conquer framework, a query keyword is not crucial to our current implementation. We provide experimental results to prove this.","0162-8828;01628828","","10.1109/TPAMI.2008.127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4527251","Applications;Clustering;Computer vision;Computing Methodologies;Information Search and Retrieval;Information Storage and Retrieval;Information Technology a;Pattern Recognition;Retrieval models","","Web services;computer vision;content-based retrieval;data mining;divide and conquer methods;feature extraction;image retrieval;information filtering;learning (artificial intelligence)","Web services;annotation rejection process;computer vision;data-driven approach;divide-and-conquer framework;hash codes;high-dimensional image visual features;image annotation;image search result mining;machine learning;photo forum;query keyword;salient term identification;semantically similar search results;textual description;visually similar search results","Artificial Intelligence;Database Management Systems;Databases, Factual;Documentation;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Radiology Information Systems","82","9","56","","20080523","Nov. 2008","","IEEE","IEEE Journals & Magazines"
"A Novel Text Structure Feature Extractor for Chinese Scene Text Detection and Recognition","X. Ren; Y. Zhou; Z. Huang; J. Sun; X. Yang; K. Chen","Department of Electronic Engineering, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","20170327","2017","5","","3193","3204","Scene text information extraction plays an important role in many computer vision applications. Most features in existing text extraction algorithms are only applicable to one text extraction stage (text detection or recognition), which significantly weakens the consistency in an end-to-end system, especially for the complex Chinese texts. To tackle this challenging problem, we propose a novel text structure feature extractor based on a text structure component detector (TSCD) layer and residual network for Chinese texts. Inspired by the three-layer Chinese text cognition model of a human, we combine the TSCD layer and the residual network to extract features suitable for both text extraction stages. The specialized modeling for Chinese characters in the TSCD layer simulates the key structure component cognition layer in the psychological model. And the residual mechanism in the residual network simulates the key bidirectional connection among the layers in the psychological model. Through the organic combination of the TSCD layer and the residual network, the extracted features are applicable to both text detection and recognition, as humans do. In evaluation, both text detection and recognition models based on our proposed text structure feature extractor achieve great improvements over baseline CNN models. And an end-to-end Chinese text information extraction system is experimentally designed and evaluated, showing the advantage of the proposed feature extractor as a unified feature extractor.","2169-3536;21693536","","10.1109/ACCESS.2017.2676158","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7870570","Chinese text;Text structure feature;deep learning;residual network;unified model","Character recognition;Cognition;Data mining;Engines;Feature extraction;Information retrieval;Text recognition","cognition;computer vision;feature extraction;natural language processing;text detection","Chinese characters;Chinese scene text detection;Chinese scene text recognition;TSCD layer;computer vision;end-to-end Chinese text information extraction system;key bidirectional connection;key structure component cognition layer;psychological model;residual network;scene text information extraction;text structure component detector;text structure feature extractor;three-layer Chinese text cognition model","","","","","","20170303","2017","","IEEE","IEEE Journals & Magazines"
"Describing story evolution from dynamic information streams","S. Rose; S. Butner; W. Cowley; M. Gregory; J. Walker","Pacific Northwest National Laboratory, USA","2009 IEEE Symposium on Visual Analytics Science and Technology","20091113","2009","","","99","106","Sources of streaming information, such as news syndicates, publish information continuously. Information portals and news aggregators list the latest information from around the world enabling information consumers to easily identify events in the past 24 hours. The volume and velocity of these streams causes information from prior days to quickly vanish despite its utility in providing an informative context for interpreting new information. Few capabilities exist to support an individual attempting to identify or understand trends and changes from streaming information over time. The burden of retaining prior information and integrating with the new is left to the skills, determination, and discipline of each individual. In this paper we present a visual analytics system for linking essential content from information streams over time into dynamic stories that develop and change over multiple days. We describe particular challenges to the analysis of streaming information and present a fundamental visual representation for showing story change and evolution over time.","","POD:978-1-4244-5283-5","10.1109/VAST.2009.5333437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5333437","H.3.1 [Content Analysis and Indexing]: Abstracting methods—;H.3.3 [Information Search and Retrieval]: Information Filtering—;H.3.7 [Digital Libraries]: User Issues—;I.3.3 [Computer Graphics]: Picture/Image Generation—","Data models;Graphics;Image analysis;Indexing;Information analysis;Joining processes;Laboratories;Portals;Streaming media;Visual analytics","data visualisation;information analysis;information filtering;portals","dynamic information streaming;fundamental visual representation;information analysis;information portals;visual analytics system","","10","","18","","","12-13 Oct. 2009","","IEEE","IEEE Conference Publications"
"Automated nanopublications generation from biomedical literature","P. Sernadela; J. L. Oliveira","DETI/IEETA, University of Aveiro, 3810-193 Aveiro, Portugal","2017 IEEE 5th Portuguese Meeting on Bioengineering (ENBENG)","20170330","2017","","","1","4","The continuous growth of unstructured information resulting from biomedical research is a trending challenge for the scientific community. In this way, novel methods for information management are emerging to improve knowledge distribution and access. The concept of nanopublications illustrates one of these recent strategies to implement machine-readable knowledge assertions. It tries to overcome inconsistency, ambiguity and redundancy of traditional publications. The purpose is that they are more suited than traditional papers to represent relationships that exist between research data, providing an efficient mechanism for knowledge exchange. Although the evident benefits of these RDF-based snippets, its applicability stills challenging due to the inexistence of extraction and publications methods. To solve that issue, we propose an automated workflow for nanopublications generation from biomedical literature. The proposed method consists of exploring an automated information extraction tool for relevant information detection from published documents and respective standardization of the mined information through semantic web recommendations, for further exploration.","","Electronic:978-1-5090-4801-4; POD:978-1-5090-4802-1","10.1109/ENBENG.2017.7889462","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889462","","Data mining;Information retrieval;Knowledge engineering;Pipelines;Resource description framework;Semantics","data mining;electronic publishing;medical information systems;relevance feedback;semantic Web;text analysis","automated information extraction tool;automated nanopublication generation;biomedical literature;information management;information mining;knowledge distribution;machine-readable knowledge assertions;published documents;relevant information detection;semantic Web recommendations","","","","","","","16-18 Feb. 2017","","IEEE","IEEE Conference Publications"
"An Integrated Framework for Avatar Data Collection from the Virtual World: A Case Study in Second Life","","","IEEE Intelligent Systems","","2010","PP","99","1","1","Hundreds of thousands of people from different physical locations can join virtual worlds whenever they want to interact with each other and create objects in a computer-based simulated environment, typically in an interactive 3D format. The rich social media data generated in virtual worlds has important implications for business, education, social science, and society at large. However, how to collect such social media data is a relatively unexplored topic and remains an issue. In this study, we developed an integrated framework combining both bot-based and spider-based approaches to collect avatar behavioral and profile data. Following the proposed framework, we conducted a case study to examine avatar action differences between the two genders as well as between young-aged and old-aged avatars. The results of the case study indicated that in general male avatars tended to perform more active physical actions than female avatars, and young-aged avatars tended to perform more active physical actions than old-aged avatars.","1541-1672;15411672","","10.1109/MIS.2010.106","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5551108","Avatars;B.1.2.c Simulation;B.1.3.b Error-checking;B.3.3.b Simulation;Business;C.3.h Ubiquitous computing;H.2.8.b Clustering;H.2.8.c Data and knowledge visualization;H.2.8.d Data mining;H.2.8.f Knowledge management applications;H.2.8.l Text mining;H.2.8.m Web mining;H.3.3 Information Search and Retrieval;H.4 Information Technology and Systems Applications;H.5 Information Interfaces and Representation (HCI);H.5.1.b Artificial;I.2.13 Knowledge Management;I.2.7 Natural Language Processing;Intelligent Control;Intelligent systems;Internet;Media;Second Life;Social Computing;Three dimensional displays;and association rules;and virtual realities < H.5.1 Multimedia Information Systems < H.5 Information Interfaces and Rep;augmented;classification","Avatars;Business;Intelligent systems;Internet;Media;Second Life;Three dimensional displays","","","","1","","15","","20100819","0","","IEEE","IEEE Early Access Articles"
"Exploiting User Tags to Build a Semantic Cultural Heritage Portal","K. v. d. Sluijs; G. J. Houben","Eindhoven University of Technology","IEEE Intelligent Systems","20101230","2010","25","6","84","92","Based on a metadata structure that intelligently builds a semantically linked data set, the Chi Explorer Web application discloses cultural heritage collections to the general public.","1541-1672;15411672","","10.1109/MIS.2010.143","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5678590","information repositories;information search and retrieval;intelligent systems;libraries;multimedia information systems;publishing;relevance feedback;video retrieval","Cultural differences;Data visualization;Databases;Navigation;Ontologies;Resource description framework;Semantics","history;meta data;portals;public information systems;semantic Web","Chi explorer;Web application;cultural heritage;heritage portal;metadata structure;user tag","","0","","12","","","Nov.-Dec. 2010","","IEEE","IEEE Journals & Magazines"
"Long-Term Cross-Session Relevance Feedback Using Virtual Features","P. Y. Yin; B. Bhanu; K. C. Chang; A. Dong","Nat. Chi Nan Univ., Nantou","IEEE Transactions on Knowledge and Data Engineering","20080131","2008","20","3","352","368","Relevance feedback (RF) is an iterative process, which refines the retrievals by utilizing the user's feedback on previously retrieved results. Traditional RF techniques solely use the short-term learning experience and do not exploit the knowledge created during cross sessions with multiple users. In this paper, we propose a novel RF framework, which facilitates the combination of short-term and long-term learning processes by integrating the traditional methods with a new technique called the virtual feature. The feedback history with all the users is digested by the system and is represented in a very efficient form as a virtual feature of the images. As such, the dissimilarity measure can dynamically be adapted, depending on the estimate of the semantic relevance derived from the virtual features. In addition, with a dynamic database, the user's subject concepts may transit from one to another. By monitoring the changes in retrieval performance, the proposed system can automatically adapt the concepts according to the new subject concepts. The experiments are conducted on a real image database. The results manifest that the proposed framework outperforms the traditional within-session and log-based long-term RF techniques.","1041-4347;10414347","","10.1109/TKDE.2007.190697","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4358974","Information Search and Retrieval;Multimedia databases;Query formulation;Relevance feedback","","image retrieval;iterative methods;relevance feedback","iterative process;log-based long-term RF techniques;long-term cross-session relevance feedback;semantic relevance;short-term learning experience;virtual features;within-session technique","","17","","27","","","March 2008","","IEEE","IEEE Journals & Magazines"
"Interorganizational Knowledge Exchanges","D. Apostolou; G. Mentzas; B. Klein; A. Abecker; W. Maass","University of Piraeus","IEEE Intelligent Systems","20080729","2008","23","4","65","74","The INKASS (intelligent knowledge assets sharing and trading) knowledge exchange system couples case-based reasoning with ontologies to assist match-making between knowledge offers and knowledge demands in an interorganizational context. A tool suite lets system administrators maintain and improve the knowledge exchange.","1541-1672;15411672","","10.1109/MIS.2008.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4580548","information search and retrieval;knowledge management","","case-based reasoning;knowledge management;ontologies (artificial intelligence)","INKASS;case-based reasoning;interorganizational knowledge exchange system;ontology","","2","","5","","","July-Aug. 2008","","IEEE","IEEE Journals & Magazines"
"E-Discovery: Identifying and Mitigating Security Risks during Litigation","F. M. Heikkila","Pivot Group","IT Professional","20080801","2008","10","4","20","25","When producing electronically stored information (ESI) in response to lawsuits, businesses face several security risks as well as legal requirements they must satisfy. Customized document management programs and e-discovery policies are key tools in protecting against inadvertent disclosure as well as meeting business and legal needs.","1520-9202;15209202","","10.1109/MITP.2008.67","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4585330","ESI;IT;electronically stored information;information security;information storage and retrieval;information technology;legal issues;unauthorized access","Companies;Data security;Information security;Law;Legal factors;Personnel;Privacy;Protection;Risk management;World Wide Web","business data processing;document handling;information storage","businesses;document management programs;e-discovery policies;electronically stored information;lawsuits;security risks","","1","","8","","","July-Aug. 2008","","IEEE","IEEE Journals & Magazines"
"Real-time aggregation of Wikipedia data for visual analytics","N. Boukhelifa; F. Chevalier; J. D. Fekete","Microsoft Research - INRIA Joint Centre","2010 IEEE Symposium on Visual Analytics Science and Technology","20101210","2010","","","147","154","Wikipedia has been built to gather encyclopedic knowledge using a collaborative social process that has proved its effectiveness. However, the workload required for raising the quality and increasing the coverage of Wikipedia is exhausting the community. Based on several participatory design sessions with active Wikipedia contributors (a.k.a. Wikipedians), we have collected a set of measures related to Wikipedia activity that, if available and visualized effectively, could spare a lot of monitoring time to these Wikipedians, allowing them to focus on quality and coverage of Wikipedia instead of spending their time navigating heavily to track vandals and copyright infringements. However, most of these measures cannot be computed on the fly using the available Wikipedia API. Therefore, we have designed an open architecture called WikiReactive to compute incrementally and maintain several aggregated measures on the French Wikipedia. This aggregated data is available as a Web Service and can be used to overlay information on Wikipedia articles through Wikipedia Skins or for new services for Wikipedians or people studying Wikipedia. This article describes the architecture, its performance and some of its uses.","","Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0","10.1109/VAST.2010.5652896","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652896","Database Management [H.2.1]: Logical Design-Schema and subschema;Database Management [H.2.4]: System-Query processing;Information Storage and Retrieval [H.3.5]: Online Information Services-Web-based services","Data visualization;Databases;Electronic publishing;Encyclopedias;Internet;Measurement","Web services;application program interfaces;data visualisation;query processing","French Wikipedia;Web Service;WikiReactive;Wikipedia API;Wikipedia articles;Wikipedia data;Wikipedia skins;collaborative social process;copyright infringements;encyclopedic knowledge;participatory design sessions;real-time aggregation;visual analytics","","2","","23","","","25-26 Oct. 2010","","IEEE","IEEE Conference Publications"
"Hidden Chinese Restaurant Game: Grand Information Extraction for Stochastic Network Learning","C. Y. Wang; Y. Chen; K. J. R. Liu","Research Center for Information Technology Innovation, Academia Sinica, Taiwan.","IEEE Transactions on Signal and Information Processing over Networks","","2017","PP","99","1","1","Agents in networks often encounter circumstances requiring them to make decisions. Nevertheless, the effectiveness of the decisions may be uncertain due to the unknown system state and the uncontrollable externality. The uncertainty can be eliminated through learning from information sources, such as user-generated contents or revealed actions. Nevertheless, the user-generated contents could be untrustworthy since other agents may maliciously create misleading contents for their selfish interests. The passively-revealed actions are potentially more trustworthy and also easier to be gathered through simple observations. In this paper, we propose a new stochastic gametheoretic framework, Hidden Chinese Restaurant Game (HCRG), to utilize the passively-revealed actions in stochastic social learning process. We propose Grand Information Extraction, a novel Bayesian belief extraction process, to extract the belief on the hidden information directly from the observed actions. We utilize the coupling relation between belief and policy to transform the original continuous belief state Markov Decision Process (MDP) into a discrete-state MDP. The optimal policy is then analyzed in both centralized and game-theoretic approaches. We demonstrate how the proposed H-CRG can be applied to the channel access problem in cognitive radio networks. We then conduct data-driven simulations using the CRAWDAD Dartmouth campus WLAN trace. The simulation results show that the equilibrium strategy derived in H-CRG provides higher expected utilities for new users and maintains a reasonable high social welfare comparing with other candidate strategies.","2373-776X;2373776X","","10.1109/TSIPN.2017.2682799","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879334","","Cognitive radio;Data mining;Games;Information processing;Information retrieval;Sensors;User-generated content","","","","","","","","20170315","","","IEEE","IEEE Early Access Articles"
"Measuring documents similarity in large corpus using MapReduce algorithm","M. Birjali; A. Beni-Hssane; M. Erritali","Department of Computer Sciences, University of Chouaib Doukkali, Faculty of Sciences, Eljadida, Morocco","2016 5th International Conference on Multimedia Computing and Systems (ICMCS)","20170424","2016","","","24","28","Document similarity measures between documents and queries has been extensively studied in information retrieval. Measuring the similarity of documents are crucial components of many text-analysis tasks, including information retrieval, document classification, and document clustering. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. There exist a large number of composed documents in a large amount of corpus. Most of them are required to compute the similarity for validation. In this paper, we propose our approach of measuring similarity between documents in large amount of corpus. For evaluation, we compare the proposed approach with other approaches previously presented by using our new MapReduce algorithm. Simulation results, on Hadoop framework, show that our new MapReduce algorithm outperforms the classical ones in term of running time performance and increases the value of the similarity.","","CD:978-1-5090-5145-8; Electronic:978-1-5090-5146-5; POD:978-1-5090-5147-2","10.1109/ICMCS.2016.7905587","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7905587","Hadoop cluster;MapReduce programming model;document similarity;similarity measure","Algorithm design and analysis;Frequency measurement;Indexing;Information retrieval;Ontologies;Semantics;Time measurement","","","","","","","","","Sept. 29 2016-Oct. 1 2016","","IEEE","IEEE Conference Publications"
"Multiple delayed position of echo hiding algorithm research and development","Y. Kang; K. Yang; J. Wang; Y. Liu","University of International Relations, Beijing, China","2016 IEEE International Conference on Signal and Image Processing (ICSIP)","20170330","2016","","","514","518","Echo hiding algorithm is a type of Information hiding technology which has been widely used. This paper introduces the principle and the mathematical model of echo hiding algorithm. Generally, the traditional echo hiding algorithm just allows to hide 180 bits of information in 1 second, in order to increase the hiding capacity, we proposed multiple delayed position of echo hiding algorithm. It permits each segment hiding 2 bits of data and introducing 4 delay positions, we expanded echo hiding space to a double capacity. The result was tested by experiments, extracted data which was encrypted by the algorithm has a stable effect showing that this algorithm has practical value in this field.","","CD:978-1-5090-2375-2; Electronic:978-1-5090-2377-6; POD:978-1-5090-2378-3","10.1109/SIPROCESS.2016.7888315","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888315","audio signal hiding;echo hiding algorithm;multiple delayed position","Algorithm design and analysis;Attenuation;Cepstrum;Data mining;Delays;Information retrieval;Watermarking","audio coding;cryptography;data encapsulation;echo;research and development","audio signal hiding;data encryption;echo hiding algorithm;echo hiding space;hiding capacity;information hiding technology;mathematical model;multiple delayed position;research and development","","","","","","","13-15 Aug. 2016","","IEEE","IEEE Conference Publications"
"A Study on Music Distribution Framework Using Music Representation Model","A. Kim; J. Lee; M. Kim","Smart Content Res. Sect., Electron. & Telecommun. Res. Inst., Daejeon, South Korea","2017 International Conference on Platform Technology and Service (PlatCon)","20170323","2017","","","1","3","In this paper, we propose a music management framework to manage the distribution of large volume music contents at home and abroad. In this paper, we define the music contents as an expression model that can be distributed internationally, and distribute the sound sources, analyze all the transaction information and related tasks of the distributed music, process them into various types of data. By defining standardized music expression model specific to music contents and managing music by using big data technology using proposed model, it is possible to automate all transaction information and related tasks online in the music market to provide statistical, analysis, and visualization information and proposed a music management framework to provide the sound source.","","Electronic:978-1-5090-5140-3; POD:978-1-5090-5141-0","10.1109/PlatCon.2017.7883681","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883681","","Analytical models;Classification algorithms;Data models;Feature extraction;Music;Music information retrieval;Recommender systems","Big Data;content management;music;statistical analysis","Big Data;music content distribution;music market;music representation model;sound sources;standardized music expression model;statistical analysis;visualization information","","","","","","","13-15 Feb. 2017","","IEEE","IEEE Conference Publications"
"A Distributed System for Consuming Web Services and Caching Their Responses in MANETs","H. Artail; S. Saab","American University of Beirut, Beirut","IEEE Transactions on Services Computing","20090403","2009","2","1","17","33","Due to the widespread of different types of hand-held computing devices whose bandwidth, memory size, and processing capacity are becoming comparable to those of desktop computers, mobile ad-hoc network (MANET) applications can now make use of proven distributed computing paradigms, in particular, web services. Indeed, in MANET environments, mobile computing devices can invoke web services methods to gain access to needed data, like stock quotes, currency exchange rates, etc. However, in addition to the communication costs, device mobility can cause temporary loss of connectivity to the server, thus rendering needed services inaccessible during those periods. For such reasons, caching of web service responses within the MANET can become beneficial for increasing data availability and reducing delays. This paper describes a distributed system designed for MANET environments to cache proxies of consumed web services and responses of their invoked methods. Furthermore, it analyzes the analytical and experimental performance of the system, and demonstrates the savings that could be realized after implementing it.","1939-1374;19391374","","10.1109/TSC.2008.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731234","Information Search and Retrieval;Pervasive computing","Ad hoc networks;Application software;Bandwidth;Computer networks;Distributed computing;Handheld computers;Mobile ad hoc networks;Mobile computing;Performance analysis;Web services","Web services;ad hoc networks;cache storage;mobile radio","MANET;Web services;distributed system;memory size;mobile ad hoc network;processing capacity","","3","","16","","20090227","Jan.-March 2009","","IEEE","IEEE Journals & Magazines"
"A literature review of question answering system using Named Entity Recognition","R. Wongso; Meiliana; D. Suhartono","School of Computer Science, Bina Nusantara University, Jakarta, Indonesia","2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)","20170406","2016","","","274","277","Named Entity Recognition (NER) is well-known as the core component of question answering system. NER has traditionally been developed as a component for information extraction systems and current techniques are focused on this end use. This paper describes some approaches that are used by some research in this decade. We also do some analysis regarding the approach, resource (like corpora, language and so on), and result of the research. At the end of this paper, we conclude several issues regarding NER and find out that research in question answering system is still an interesting field to work on. Some resources are still able to be utilized for machine learning tools. Building NER for the origin country of the authors will be beneficial as well.","","DVD:978-1-5090-1433-0; Electronic:978-1-5090-0890-2; POD:978-1-5090-0891-9; Paper:978-1-5090-1434-7","10.1109/ICITACEE.2016.7892454","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892454","NER;Named Entity Recognition;Question Answering System","Bibliographies;Buildings;Data mining;Engines;Information retrieval;Knowledge discovery;Organizations","question answering (information retrieval);reviews","NER;author origin country;literature review;machine learning tool;named entity recognition;question answering system","","","","","","","19-20 Oct. 2016","","IEEE","IEEE Conference Publications"
"Handwritten-Word Spotting Using Biologically Inspired Features","T. van der Zant; L. Schomaker; K. Haak","University of Groningen, Groningen","IEEE Transactions on Pattern Analysis and Machine Intelligence","20080919","2008","30","11","1945","1957","For quick access to new handwritten collections, current handwriting recognition methods are too cumbersome. They cannot deal with the lack of labeled data and would require extensive laboratory training for each individual script, style, language, and collection. We propose a biologically inspired whole-word recognition method that is used to incrementally elicit word labels in a live Web-based annotation system, named Monk. Since human labor should be minimized given the massive amount of image data, it becomes important to rely on robust perceptual mechanisms in the machine. Recent computational models of the neurophysiology of vision are applied to isolated word classification. A primate cortex-like mechanism allows us to classify text images that have a low frequency of occurrence. Typically, these images are the most difficult to retrieve and often contain named entities and are regarded as the most important to people. Usually, standard pattern-recognition technology cannot deal with these text images if there are not enough labeled instances. The results of this retrieval system are compared to normalized word-image matching and appear to be very promising.","0162-8828;01628828","","10.1109/TPAMI.2008.144","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4538228","Computational neuroscience;Computer vision;Computing Methodologies;Digital Libraries;Feature Measurement;Handwriting analysis;Image Processing and Computer Vision;Image/video retrieval;Information Storage and Retrieval;Information Technology and Systems;Interactive systems;Invariants","","Internet;handwriting recognition;image classification;image retrieval;text analysis","Monk;Web-based annotation system;biologically inspired features;handwriting recognition;handwritten-word spotting;retrieval system;text images classification;whole-word recognition;word classification","Artificial Intelligence;Biomimetics;Handwriting;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Pattern Recognition, Automated;Pattern Recognition, Visual","39","","40","","20080606","Nov. 2008","","IEEE","IEEE Journals & Magazines"
"IPPTGen-intelligent PPT generator","P. Ganguly; P. M. Joshi","Department of Computer Engineering, MIT College of Engineering, Pune University, India","2016 International Conference on Computing, Analytics and Security Trends (CAST)","20170501","2016","","","96","99","Presentation slides have always been the most effective tool to express a topic or theme. To present a particular topic, there is necessity for an individual to appropriately identify the relevant and important aspects that contribute to the related theme or topic. At present the tools for PowerPoint presentation generator like MSOffice, OpenOffice etc. carry out the task of generation of presentation slides but from representation perspective. They lack the aspect of content analysis that would be of interest to the user. This work aims to alleviate the problem of this content based PowerPoint presentation generator where a draft slides would be made available to the user based on content of the documents and the theme. In this work, an automated technique to generate presentation slides from a text document is proposed such that the original concepts in the inputted document are conveyed in the output slides.","","Electronic:978-1-5090-1338-8; POD:978-1-5090-1339-5","10.1109/CAST.2016.7914947","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7914947","Extractive summary;Information extraction;Text Summarization","Clustering algorithms;Computers;Feature extraction;Generators;Information retrieval;Text mining","","","","","","","","","19-21 Dec. 2016","","IEEE","IEEE Conference Publications"
"Interactive querying of temporal data using a comic strip metaphor","J. Jin; P. Szekely","Information Sciences Institute University of Southern California","2010 IEEE Symposium on Visual Analytics Science and Technology","20101210","2010","","","163","170","Finding patterns in temporal data is an important data analysis task in many domains. Static visualizations can help users easily see certain instances of patterns, but are not specially designed to support systematic analysis tasks, such as finding all instances of a pattern automatically. VizPattern is an interactive visual query environment that uses a comic strip metaphor to enable users to easily and quickly define and locate complex temporal patterns. Evaluations provide evidence that VizPattern is applicable in many domains, and that it enables a wide variety of users to answer questions about temporal data faster and with fewer errors than existing state-of-the-art visual analysis systems.","","Electronic:978-1-4244-9487-3; POD:978-1-4244-9488-0","10.1109/VAST.2010.5652890","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5652890","H.3.3 [Information Systems]: Information Search and Retrieval-Query formulation;H.5.2 [Information Interfaces and Presentation]: User Interface-User-centered design","Data analysis;Data visualization;Pain;Stomach;Strips;Time series analysis;Visualization","data analysis;data visualisation;pattern recognition;query processing","VizPattern;comic strip metaphor;data analysis;interactive querying;static visualization;temporal data;temporal pattern;visual analysis","","5","","23","","","25-26 Oct. 2010","","IEEE","IEEE Conference Publications"
"A framework for countering denial-of-information attacks","G. Conti; Mustaque Ahamad","Georgia Inst. of Technol., Atlanta, GA, USA","IEEE Security & Privacy","20051212","2005","3","6","50","56","Denial-of-information (DoI) attacks degrade a given user's ability to seek, assimilate, and process information, and are becoming more prevalent due to the Internet's rapid growth. To counter such attacks, the authors' taxonomy provides structure to this area and proposes a model for describing the information space.","1540-7993;15407993","","10.1109/MSP.2005.140","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1556536","DOI;H.3.3 Information Search and Retrieval;H.3.4 Systems and Software;denial-of-service;security","Communication channels;Computer security;Humans;Information filtering;Information filters;Information security;Signal design;Signal to noise ratio;Space technology;Terminology","Internet;security of data;unsolicited e-mail","Internet;denial-of-information attack","","3","","5","","","Nov.-Dec. 2005","","IEEE","IEEE Journals & Magazines"
"Subspace search and visualization to make sense of alternative clusterings in high-dimensional data","A. Tatu; F. Maaß; I. Färber; E. Bertini; T. Schreck; T. Seidl; D. Keim","Univ. of Konstanz, Konstanz, Germany","2012 IEEE Conference on Visual Analytics Science and Technology (VAST)","20130103","2012","","","63","72","In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.","","Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5","10.1109/VAST.2012.6400488","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400488","Display algorithms;H.2.8 [Database Applications]: Data mining;H.3.3 [Information Search and Retrieval]: Selection process;I.3.3 [Picture/Image Generation]","Algorithm design and analysis;Clustering algorithms;Data visualization;Educational institutions;High definition video;Topology;Visualization","data analysis;data reduction;data visualisation;pattern clustering;search problems","HD data space;HD input data space;cluster analysis;dimensionality reduction;explorative data analysis;high-dimensional data clusterings;high-dimensional data space;interestingness-guided subspace search algorithm;navigation facilities;subspace search;subspace similarity functions;subspace visualization;visual analysis;visual mappings;visual-interactive methods","","17","","35","","","14-19 Oct. 2012","","IEEE","IEEE Conference Publications"
"CDNs Content Outsourcing via Generalized Communities","D. Katsaros; G. Pallis; K. Stamos; A. Vakali; A. Sidiropoulos; Y. Manolopoulos","University of Thessaly, Volos, Aristotle University of Thessaloniki, Thessaloniki","IEEE Transactions on Knowledge and Data Engineering","20081125","2009","21","1","137","151","Content distribution networks (CDNs) balance costs and quality in services related to content delivery. Devising an efficient content outsourcing policy is crucial since, based on such policies, CDN providers can provide client-tailored content, improve performance, and result in significant economical gains. Earlier content outsourcing approaches may often prove ineffective since they drive prefetching decisions by assuming knowledge of content popularity statistics, which are not always available and are extremely volatile. This work addresses this issue, by proposing a novel self-adaptive technique under a CDN framework on which outsourced content is identified with no a-priori knowledge of (earlier) request statistics. This is employed by using a structure-based approach identifying coherent clusters of ""correlated"" Web server content objects, the so-called Web page communities. These communities are the core outsourcing unit and in this paper a detailed simulation experimentation has shown that the proposed technique is robust and effective in reducing user-perceived latency as compared with competing approaches, i.e., two communities-based approaches, Web caching, and non-CDN.","1041-4347;10414347","","10.1109/TKDE.2008.92","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4515870","Communication/Networking and Information Technology;Information Search and Retrieval;Information Storage;Systems and Software","","Internet;cache storage;content management;outsourcing;pattern clustering","CDN content outsourcing;Web caching;Web server content objects;client-tailored contents;communities-based approaches;content distribution networks;content outsourcing policy;content popularity statistics;core outsourcing unit;generalized communities;self-adaptive technique;structure-based approach;user-perceived latency","","12","","37","","20080512","Jan. 2009","","IEEE","IEEE Journals & Magazines"
"Supervised and Traditional Term Weighting Methods for Automatic Text Categorization","M. Lan; C. L. Tan; J. Su; Y. Lu","East China Normal University, Shanghai","IEEE Transactions on Pattern Analysis and Machine Intelligence","20090220","2009","31","4","721","735","In vector space model (VSM), text representation is the task of transforming the content of a textual document into a vector in the term space so that the document could be recognized and classified by a computer or a classifier. Different terms (i.e. words, phrases, or any other indexing units used to identify the contents of a text) have different importance in a text. The term weighting methods assign appropriate weights to the terms to improve the performance of text categorization. In this study, we investigate several widely-used unsupervised (traditional) and supervised term weighting methods on benchmark data collections in combination with SVM and kNN algorithms. In consideration of the distribution of relevant documents in the collection, we propose a new simple supervised term weighting method, i.e. tf.rf, to improve the terms' discriminating power for text categorization task. From the controlled experimental results, these supervised term weighting methods have mixed performance. Specifically, our proposed supervised term weighting method, tf.rf, has a consistently better performance than other term weighting methods while other supervised term weighting methods based on information theory or statistical metric perform the worst in all experiments. On the other hand, the popularly used tf.idf method has not shown a uniformly good performance in terms of different data sets.","0162-8828;01628828","","10.1109/TPAMI.2008.110","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4509437","Artificial Intelligence;Clustering;Computing Methodologies;Content Analysis and Indexing;Database Applications;Database Management;Indexing methods;Information Storage and Retrieval;Information Technolog;Information Technology and Systems;Knowledge and data engineering tools and techniques;Natural Language Processing;Text analysis;Text mining;and association rules;classification","","natural language processing;text analysis","automatic text categorization;supervised term weighting methods;text representation;traditional term weighting methods;vector space model","","142","15","28","","20080502","April 2009","","IEEE","IEEE Journals & Magazines"
"Weighing Stars: Aggregating Online Product Reviews for Intelligent E-commerce Applications","Z. Zhang","University of Arizona","IEEE Intelligent Systems","20080923","2008","23","5","42","49","A new task in text-sentiment analysis adds usefulness scoring to polarity/ opinion extraction to improve product- review ranking services, helping shoppers and vendors leverage information from multiple sources. Human language is a medium not only for exchanging information but also for conveying subjective opinions and emotion. Recently, interest in text-subjectivity and sentiment analysis has increased as part of the larger research effort in affective computing, which aims to make computers understand and generate human-like emotions through language and other expressive activities such as gesture.","1541-1672;15411672","","10.1109/MIS.2008.95","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4629725","Information Search and Retrieval;Machine learning;Text mining;Web mining;Web text analysis","","computational linguistics;electronic commerce;emotion recognition;text analysis","gesture recognition;human language;human-like emotion;intelligent e-commerce application;online product review;text-sentiment analysis","","20","1","15","","","Sept.-Oct. 2008","","IEEE","IEEE Journals & Magazines"
"Information extraction from unstructured data using RDF","K. Gandhi; N. Madia","Department of Computer Engineering, Silver Oak College of Engineering & Technology, Ahmedabad, Gujarat, India","2016 International Conference on ICT in Business Industry & Government (ICTBIG)","20170406","2016","","","1","6","The Internet exhibits a gigantic measure of helpful data which is generally designed for its users, which makes it hard to extract applicable information from different sources. Accordingly, the accessibility of strong, adaptable Information Extraction framework that consequently concentrate structured data such as, entities, relationships between entities, and attributes from unstructured or semi-structured sources. But somewhere during extraction of information may lead to the loss of its meaning, which is absolutely not feasible. Semantic Web adds solution to this problem. It is about providing meaning to the data and allow the machine to understand and recognize these augmented data more accurately. The proposed system is about extracting information from research data of IT domain like journals of IEEE, Springer, etc., which aid researchers and the organizations to get the data of journals in an optimized manner so the time and hard work of surfing and reading the entire journal's papers or articles reduces. Also the accuracy of the system is taken care of using RDF, the data extracted has a specific declarative semantics so that the meaning of the research papers or articles during extraction remains unchanged. In addition, the same approach shall be applied on multiple documents, so that time factor can get saved.","","Electronic:978-1-5090-5515-9; POD:978-1-5090-5516-6","10.1109/ICTBIG.2016.7892635","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892635","Heuristic;Information Extraction;RDF;SPO;Semantic Web;Unstructured Data","Data mining;Feature extraction;Information retrieval;Resource description framework;Semantics;XML","Internet;data structures;semantic Web","IEEE;IT domain;Internet;RDF;Springer;adaptable information extraction framework;declarative semantics;semantic Web;structured data;time factor;unstructured data","","","","","","","18-19 Nov. 2016","","IEEE","IEEE Conference Publications"
"Automated histologic grading from free-text pathology reports using graph-of-words features and machine learning","H. J. Yoon; L. Roberts; G. Tourassi","Health Data Science Institute, Oak Ridge National Laboratory, Oak Ridge, TN 37831 USA","2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)","20170413","2017","","","369","372","Traditional n-gram feature representation of freetext documents often fails to capture word ordering and semantics, thus compromising text comprehension. Graph-of-words, a new text representation approach based on graph analytics, is a superior method overcoming the limitations by modeling word co-occurrence. In this study, we present a novel application of graph-of-words text description for automated extraction of histologic grade from unstructured pathology reports. Using 10-fold cross-validation tests, the proposed approach resulted in substantially higher macro and micro-F1 scores with undirected graph-of-words features, compared to traditional bi-gram text features. Our feasibility study demonstrated that graph-of-words is a highly efficient method of text comprehension for information extraction from free-text clinical documents.","","Electronic:978-1-5090-4179-4; POD:978-1-5090-4180-0","10.1109/BHI.2017.7897282","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7897282","","Breast;Cancer;Feature extraction;Information retrieval;Natural languages;Pathology;Unified modeling language","","","","","","","","","16-19 Feb. 2017","","IEEE","IEEE Conference Publications"
"A Unified Probabilistic Framework for Name Disambiguation in Digital Library","J. Tang; A. C. M. Fong; B. Wang; J. Zhang","Tsinghua University, Beijing","IEEE Transactions on Knowledge and Data Engineering","20120420","2012","24","6","975","987","Despite years of research, the name ambiguity problem remains largely unresolved. Outstanding issues include how to capture all information for name disambiguation in a unified approach, and how to determine the number of people K in the disambiguation process. In this paper, we formalize the problem in a unified probabilistic framework, which incorporates both attributes and relationships. Specifically, we define a disambiguation objective function for the problem and propose a two-step parameter estimation algorithm. We also investigate a dynamic approach for estimating the number of people K. Experiments show that our proposed framework significantly outperforms four baseline methods of using clustering algorithms and two other previous methods. Experiments also indicate that the number K automatically found by our method is close to the actual number.","1041-4347;10414347","","10.1109/TKDE.2011.13","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5680902","Digital libraries;database applications;heterogeneous databases.;information search and retrieval","Clustering algorithms;Databases;Heuristic algorithms;Hidden Markov models;Marine vehicles;Partitioning algorithms;Probabilistic logic","digital libraries;parameter estimation;pattern clustering;probability","clustering algorithms;digital library;name disambiguation;parameter estimation algorithm;unified probabilistic framework","","38","","52","","20110106","June 2012","","IEEE","IEEE Journals & Magazines"
"Indexing Patterns within a Distributed System","C. Mouline; C. Lai","Heudiasyc CNRS, UTC Univ. of Technol., Compie&#x0300;gne, France","2010 International Conference on Intelligent Networking and Collaborative Systems","20110128","2010","","","206","213","This paper is concerning issues regarding the semantic indexing of resources in a peer to peer network. An important requirement is a good quality of the discovery, i.e. the selection of results very related to a specific request. Keys used for indexing and the corresponding urls of indexed resources are stored in a distributed hash table scattered among the different peers of a community. A key is a semantic description of resources and refers to concepts and properties belonging to ontologies. We propose a system based on Indexing Patterns in order to generate keys. It is used to guide users for producing assertions about their resources. It induces a navigation inside ontologies presented in graphical user interfaces in a friendly and easy to use way. With different examples, we present the main cases of key generation that define the indexing context of a resource.","","Electronic:978-1-4244-4278-2; POD:978-1-4244-8828-5","10.1109/INCOS.2010.21","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5702097","information search and retrieval;query formulation;retrieval models;search process","","distributed processing;file organisation;graphical user interfaces;indexing;ontologies (artificial intelligence);peer-to-peer computing","distributed hash table scattered;distributed system;graphical user interfaces;indexed resources;indexing pattern;ontologies;peer to peer network;semantic indexing","","0","","20","","","24-26 Nov. 2010","","IEEE","IEEE Conference Publications"
"Inter-active learning of ad-hoc classifiers for video visual analytics","B. Höferlin; R. Netzel; M. Höferlin; D. Weiskopf; G. Heidemann","","2012 IEEE Conference on Visual Analytics Science and Technology (VAST)","20130103","2012","","","23","32","Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users' trust in the classifier trained. We approach the challenges of ad-hoc training by inter-active learning, which extends active learning by integrating human experts' background knowledge to greater extent. In contrast to active learning, not only does inter-active learning include the users' expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of inter-active learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets.","","Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5","10.1109/VAST.2012.6400492","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400492","H.3.3 [Information Systems]: Information Storage and Retrieval — Information Search and Retrieval;I.2.6 [Computing Methodologies]: Artificial Intelligence — Learning","Analytical models;Data models;Data visualization;Humans;Labeling;Training;Visual analytics","cognition;data visualisation;inference mechanisms;information filtering;information filters;learning (artificial intelligence);pattern classification;training;video retrieval","ad-hoc classifier training process visual feedback;analytical reasoning process;classifier learning;data annotation quality;data labeling queries;data visualization;human expert background knowledge;information filters;inter-active learning;time constraints;user expertise;user mental model;video visual analytics","","6","","43","","","14-19 Oct. 2012","","IEEE","IEEE Conference Publications"
"NNexus: An Automatic Linker for Collaborative Web-Based Corpora","J. Gardner; A. Krowne; L. Xiong","Emory University, Atlanta","IEEE Transactions on Knowledge and Data Engineering","20090424","2009","21","6","829","839","In this paper, we introduce Noosphere Networked Entry eXtension and Unification System (NNexus), a generalization of the automatic linking engine of Noosphere (at PlanetMath.org) and the first system that automates the process of linking disparate ""encyclopediardquo entries into a fully connected conceptual network. The main challenges of this problem space include: 1) linking quality (correctly identifying which terms to link and which entry to link to with minimal effort on the part of users), 2) efficiency and scalability, and 3) generalization to multiple knowledge bases and web-based information environment. We present the NNexus approach that utilizes subject classification and other metadata to address these challenges. We also present evaluation results demonstrating the effectiveness and efficiency of the approach and discuss ongoing and future directions of research.","1041-4347;10414347","","10.1109/TKDE.2008.136","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4564462","Digital Libraries;E-learning;Information Storage and Retrieval;Semantic Web.;automatic linking;wiki","","groupware;knowledge based systems;meta data;search engines","NNexus;Noosphere networked entry extension and unification system;Web-based information environment;automatic linker;automatic linking engine;collaborative Web-based corpora;metadata;subject classification","","2","","18","","20080715","June 2009","","IEEE","IEEE Journals & Magazines"
"Searching the Long Tail of Social Media Streams on the Web","","","IEEE Intelligent Systems","","2010","PP","99","1","1","Information is increasingly being distributed in the form of dynamic streams instead of static web pages. It began with news RSS feeds, but with the emergence of social media services such as twitter and facebook, now encompasses instant status updates as well as shared links to various types of web content. While one of the challenging tasks in using such stream based services is to search quality streams of interests, existing work has mainly focused on the retrieval models for individual posts or classification frameworks for blogs, leaving the problems arising in building a dedicated stream search engine in real-world settings largely unexplored. This paper presents a novel stream search engine, named FeedMil, that can satisfy the need for retrieving quality streams of topical relevance for the purpose of subscription. Through addressing the issues unique to the stream search problem, FeedMil is able to give a new search experience that is focused on quality and topic relevance beyond just a sim-ple query matching, enabling users to quickly discover high quality but less popular streams located in the long tail of millions of streams.","1541-1672;15411672","","10.1109/MIS.2010.115","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5620890","B.3.3.b Simulation;Blogs;C.3.h Ubiquitous computing;Feeds;H.3 Information Storage and Retrieval;H.3.0.a Web Search;H.3.3 Information Search and Retrieval;H.3.3.b Information filtering;H.3.5 Online Information Services;Indexing;Intelligent Control;Intelligent systems;Internet Services;Media;Mobile Services;Online Game;Portals;Search engines;Social Computing;Text processing;Web services","Blogs;Feeds;Indexing;Intelligent systems;Media;Search engines;Text processing","","","","0","","14","","20101109","0","","IEEE","IEEE Early Access Articles"
"Prajna: Adding Automated Reasoning to the Visual Analysis Process","","","IEEE Computer Graphics and Applications","","2009","PP","99","1","1","Applications and systems can represent knowledge in a variety of ways. A graphic display might allow a knowledge analyst to infer new information through interactive visualizations. Knowledge can be represented as a collection of facts, which can then be used for automatic inference. Knowledge can also be represented or stored in various archives, such as databases or formatted files. Those developers challenged with creating applications for knowledge representation frequently have to contend not only with data challenges, but also with challenges caused by a wide variety of software toolkits, architectures, and standards for knowledge representation. To meet these obstacles, we developed the Prajna Project. The Prajna Project is a Java toolkit designed to provide various capabilities for visualization, knowledge representation, geographic displays, semantic reasoning, and data fusion. Within this paper, we present both the capabilities of the Prajna project, and use it to illustrate techniques that address these challenges.","0272-1716;02721716","","10.1109/MCG.2009.152","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5342377","Bridges;Cognition;D.1 Programming Techniques;D.2.11 Software Architectures;D.2.11.b Domain-specific architectures;D.2.13 Reusable Software;D.2.14 Human Factors in Software Design;Data visualization;Databases;H.2.8 Database Applications;H.3 Information Storage and Retrieval;H.5 Information Interfaces and Representation (HCI);I.2.12 Intelligent Web Services and Semantic Web;I.2.4 Knowledge Representation Formalisms and Methods;I.2.4.j Representations (procedural and rule-based);I.3 Computer Graphics;I.3.6 Methodology and Techniques;I.3.6.d Interaction techniques;I.3.8 Applications;Ontologies;Software;Visualization;graphics;interface","Application software;Computer architecture;Data visualization;Displays;Graphics;Information analysis;Knowledge representation;Software standards;Software tools;Visual databases","","","","0","","9","","20091201","0","","IEEE","IEEE Early Access Articles"
"Browsing within Lecture Videos Based on the Chain Index of Speech Transcription","S. Repp; A. Grob; C. Meinel","Hasso-Plattner-Institut f&#x0FC;r Softwaresystemtechnik GmbH, Potsdam","IEEE Transactions on Learning Technologies","20090206","2008","1","3","145","156","The number of digital lecture video recordings has increased dramatically since recording technology became easier to use. The accessibility and ability to search within this large archive are limited and difficult. Additionally, detailed browsing in videos is not supported due to the lack of an explicit annotation. Manual annotation and segmentation is time-consuming and therefore useless. A promising approach is based on using the audio layer of a lecture recording to obtain information about the lecture's contents. In this paper we're going to present an indexing method for computer science courses based on their existing recorded videos. The transcriptions from a speech-recognition engine (SRE) are sufficient to create a chain index for detailed browsing inside a lecture video. The index structure and the evaluation of the supplied keywords are presented. The user interface for dynamic browsing of the e-learning contents concludes this paper.","1939-1382;19391382","","10.1109/TLT.2008.22","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4731229","Content Analysis and Indexing;Digital Libraries;Distance learning;Image/video retrieval;Information Search and Retrieval;Multimedia Information Systems;User Interfaces;Web Search","IP networks;Indexes;Indexing;Speech;Synchronization;Tagging;Videos","computer aided instruction;speech recognition;user interfaces;video recording;video retrieval","computer science courses;detailed browsing;digital lecture video recordings;e-learning;speech transcription;speech-recognition engine;user interface","","15","","53","","20081231","July-Sept. 2008","","IEEE","IEEE Journals & Magazines"
"Matrix-based visual correlation analysis on large timeseries data","M. Behrisch; J. Davey; T. Schreck; D. Keim; J. Kohlhammer","","2012 IEEE Conference on Visual Analytics Science and Technology (VAST)","20130103","2012","","","209","210","In recent years, the quantity of time series data generated in a wide variety of domains grown consistently. Thus, it is difficult for analysts to process and understand this overwhelming amount of data. In the specific case of time series data another problem arises: time series can be highly interrelated. This problem becomes even more challenging when a set of parameters influences the progression of a time series. However, while most visual analysis techniques support the analysis of short time periods, e.g. one day or one week, they fail to visualize large-scale time series, ranging over one year or more. In our approach we present a time series matrix visualization that tackles this problem. Its primary advantages are that it scales to a large number of time series with different start and end points and allows for the visual comparison / correlation analysis of a set of influencing factors. To evaluate our approach, we applied our technique to a real-world data set, showing the impact of local weather conditions on the efficiency of photovoltaic power plants.","","Electronic:978-1-4673-4753-2; POD:978-1-4673-4752-5","10.1109/VAST.2012.6400549","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6400549","H.3.3 [Information Search and Retrieval Design Tools and Techniques]: Information filtering —","Correlation;Meteorology;Power generation;Substations;Temperature measurement;Time series analysis;Visualization","data visualisation;matrix algebra;time series","local weather conditions;matrix-based visual correlation analysis;photovoltaic power plant efficiency;time series data generation quantity;time series matrix data visualization;visual analysis techniques;visual comparison analysis;visual correlation analysis","","0","","7","","","14-19 Oct. 2012","","IEEE","IEEE Conference Publications"
"Using a Probabilistic Neural Network for a Large Multi-label Problem","E. Oliveira; P. M. Ciarelli; A. F. D. Souza; C. Badue","Dept. of Inf. Sci., Univ. Fed. do Espirito Santo, Vitoria","2008 10th Brazilian Symposium on Neural Networks","20081105","2008","","","195","200","The automation of the categorization of economic activities from business descriptions in free text format is a huge challenge for the Brazilian governmental administration in the present day. When this problem is tackled by humans, the subjectivity on their classification brings another problem: different human classifiers can give different results when working on a set of the same business descriptions. This can cause a serious distortion on the information for the planning and taxation of the governmental administrations on the three levels: County, State and Federal. Furthermore, the number of possible categories considered is very large, more than 1000 in the Brazilian scenario. The large number of categories makes the problem even harder to be solved, as this is also a multi-labeled problem. In this work we compared the multi-label lazy learning technique, ML-kNN, to our probabilistic neural network approach. Our implementation overcome the ML-kNN algorithm in four metrics typically used in the literature for multi-label categorization problems.","1522-4899;15224899","CD-ROM:978-0-7695-3361-2; POD:978-1-4244-3219-6","10.1109/SBRN.2008.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4665915","Categorization multi-labeled problem;Categorization of economic activities;Content Analysis and Indexing;Information Search and Retrieval;Probabilistic Neural Network","Companies;Computer science;Contracts;Humans;Law;Legal factors;Neural networks;Software libraries;US Government;US local government","commerce;government data processing;learning (artificial intelligence);neural nets;pattern classification;probability;public administration;taxation","Brazilian governmental administration;business descriptions;government planning;human classification;large multilabel categorization problem;multilabel lazy learning;probabilistic neural network;taxation","","1","","16","","","26-30 Oct. 2008","","IEEE","IEEE Conference Publications"
"Framework to extract context vectors from unstructured data using big data analytics","T. Ahmad; R. Ahmad; S. Masud; F. Nilofer","Dept. of Computer Engg., Jamia Millia Islamia, New Delhi, India","2016 Ninth International Conference on Contemporary Computing (IC3)","20170320","2016","","","1","6","When multiple terms in the query point to a single concept, the solution is easy to map. But, when many morphologically similar terms refer to separate concepts (showing fuzzy behavior), then arriving at a solution becomes difficult. Before applying any knowledge generation or representation techniques to such polysemic words, word sense disambiguation becomes imperative. Unfortunately, with an exponential increase in data, the process of information extraction becomes difficult. For text data this information is represented in form of context vectors. But, the generation of context vectors is limited by the memory heap and RAM of traditional systems. The aim of this study is to examine and propose a framework for computing context vectors of large dimensions over Big Data, trying to overcome the bottleneck of traditional systems. The proposed framework is based on set of mappers and reducers, implemented on Apache Hadoop. With increase in the size of the input dataset, the dimensions of the related concepts (in form of resultant matrix) increases beyond the capacity of a single system. This bottleneck of handling large dimensions is resolved by clustering. As observed from the study, transition from a single system to a distributed system ensures that the process of information extraction runs smoothly, even with an increase in data.","","CD:978-1-5090-3249-5; Electronic:978-1-5090-3251-8; POD:978-1-5090-3252-5","10.1109/IC3.2016.7880229","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880229","Big Data;Big Data Analytics;Hadoop;Text Mining","Big Data;Context;Information retrieval;Random access memory;Text mining","Big Data;text analysis;word processing","Apache Hadoop;Big Data analytics;context vectors;distributed system;information extraction;knowledge generation;polysemic words;unstructured data;word sense disambiguation","","","","","","","11-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Music retrieval based on rhythm content and dynamic time warping method","Z. Ren; C. Fan; Y. Ming","Beijing Key Laboratory of Work Safety Intelligent Monitoring, School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, P.R. China","2016 IEEE 13th International Conference on Signal Processing (ICSP)","20170316","2016","","","989","992","Rhythm information, which plays an important role in music features, still has a long way to go. Most current researches on this field are based on single feature, which is unstable. In this paper, we proposed a novel method to change this by fusing rhythm feature with gammatone frequency cepstral coefficients(GFCC) feature. After the pre-processing including detecting the beginning of songs, removing the silence part using Energy and Zero crossing rate, our training and testing features are generated by fusing rhythm feature including pitch, tempo etc. with GFCC feature. Furthermore, we present several ways to measure the rhythmic similarity between two or more songs. This allows similar songs to be retrieved from a large collection. For recognition, we choose the Dynamic Time Warping(DTW) algorithm calculating the distance between test music and music database, and then we get a ranking list based on distance. It is demonstrated that we can improve the recognition rates by 21.3% on average based on our music database by using rhythm features fused with GFCC features comparing with Predominant melody, MFCC and GFCC features. Our music database has 500 songs and we choose 100 songs as testing music.","2164-5221;21645221","CD:978-1-5090-1343-2; Electronic:978-1-5090-1345-6; POD:978-1-5090-1346-3; Paper:978-1-5090-1344-9","10.1109/ICSP.2016.7877977","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7877977","DTW;GFCC;Rhythm feature;music retrieval","Databases;Feature extraction;Mel frequency cepstral coefficient;Music information retrieval;Rhythm;Testing","information retrieval;music;sensor fusion","DTW;GFCC feature;MFCC;dynamic time warping method;energy crossing rate;feature fusion;gammatone frequency cepstral coefficients feature;music retrieval;recognition rates;rhythm content;zero crossing rate","","","","","","","6-10 Nov. 2016","","IEEE","IEEE Conference Publications"
"Capturing Social Data Evolution Using Graph Clustering","M. Giatsoglou; A. Vakali","Aristotle University","IEEE Internet Computing","20130121","2013","17","1","74","79","The fast and unpredictable evolution of social data poses challenges for capturing user activities and complex associations. Evolving social graph clustering promises to uncover the dynamics of latent user and content patterns. This Web extra overviews evolving data clustering approaches.","1089-7801;10897801","","10.1109/MIC.2012.141","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6415918","Web mining;clustering;data structures;database applications;database management;graphs and networks;information search and retrieval;mining methods and algorithms;pattern recognition","Adaptation models;Clustering algorithms;Complexity theory;Data models;Internet","data handling;graph theory;pattern clustering;social networking (online)","capturing social data evolution;complex associations;data clustering;social graph clustering","","7","","15","","","Jan.-Feb. 2013","","IEEE","IEEE Journals & Magazines"
"Web People Search via Connection Analysis","D. V. Kalashnikov; Z. Chen; S. Mehrotra; R. Nuray-Turan","University of California, Irvine, Irvine","IEEE Transactions on Knowledge and Data Engineering","20080923","2008","20","11","1550","1565","Nowadays, searches for Webpages of a person with a given name constitute a notable fraction of queries to web search engines. Such a query would normally return Webpages related to several namesakes, who happened to have the queried name, leaving the burden of disambiguating and collecting pages relevant to a particular person (from among the namesakes) on the user. In this article we develop a Web People Search approach that clusters Webpages based on their association to different people. Our method exploits a variety of semantic information extracted from Web pages, such as named entities and hyperlinks, to disambiguate among namesakes referred to on the Web pages. We demonstrate the effectiveness of our approach by testing the efficacy of the disambiguation algorithms and its impact on person search.","1041-4347;10414347","","10.1109/TKDE.2008.78","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497193","Clustering;Database Management;Information Storage and Retrieval;Internet search;Search process;Web Search;Web mining;and association rules;classification","Clustering algorithms;Data mining;Internet;Machine learning;Psychology;Search engines;Social network services;Testing;Web pages;Web search","Internet;search engines","Web people search;Webpage;connection analysis;search engine;semantic information extraction","","15","6","41","","20080425","Nov. 2008","","IEEE","IEEE Journals & Magazines"
"Toward the Optimal Itinerary-Based KNN Query Processing in Mobile Sensor Networks","S. H. Wu; K. T. Chuang; C. M. Chen; M. S. Chen","National Taiwan University, Taipei, Taiwan","IEEE Transactions on Knowledge and Data Engineering","20081111","2008","20","12","1655","1668","The K-nearest neighbors (KNN) query has been of significant interest in many studies and has become one of the most important spatial queries in mobile sensor networks. Applications of KNN queries may include vehicle navigation, wildlife social discovery, and squad/platoon searching on the battlefields. Current approaches to KNN search in mobile sensor networks require a certain kind of indexing support. This index could be either a centralized spatial index or an in-network data structure that is distributed over the sensor nodes. Creation and maintenance of these index structures, to reflect the network dynamics due to sensor node mobility, may result in long query response time and low battery efficiency, thus limiting their practical use. In this paper, we propose a maintenance-free itinerary-based approach called density-aware itinerary KNN query processing (DIKNN). The DIKNN divides the search area into multiple cone-shape areas centered at the query point. It then performs a query dissemination and response collection itinerary in each of the cone-shape areas in parallel. The design of the DIKNN scheme takes into account several challenging issues such as the trade-off between degree of parallelism and network interference on query response time, and the dynamic adjustment of the search radius (in terms of number of hops) according to spatial irregularity or mobility of sensor nodes. To optimize the performance of DIKNN, a detailed analytical model is derived that automatically determines the most suitable degree of parallelism under various network conditions. This model is validated by extensive simulations. The simulation results show that DIKNN yields substantially better performance and scalability over previous work, both as kappa increases and as the sensor node mobility increases. It outperforms the second runner with up to a 50 percent saving in energy consumption and up to a 40 percent reduction in query response time, while rendering the same level- - of query result accuracy.","1041-4347;10414347","","10.1109/TKDE.2008.80","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4497195","Database Management;Information Storage and Retrieval;Query processing","","database indexing;distributed databases;mobile radio;query processing;search problems;spatial data structures;wireless sensor networks","centralized spatial index;density-aware itinerary KNN query processing;in-network data structure;long query response time;low battery efficiency;mobile sensor network;optimal itinerary-based k-nearest neighbor query processing;query dissemination;squad-platoon searching;vehicle navigation;wildlife social discovery","","11","1","35","","20080425","Dec. 2008","","IEEE","IEEE Journals & Magazines"
