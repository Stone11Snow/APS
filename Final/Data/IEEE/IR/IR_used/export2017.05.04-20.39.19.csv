"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=6732578,6732565,6726755,6714400,6720596,6724344,6724259,6720777,6724324,6724175,6719096,6720000,6719936,6718248,6716703,6716693,6716603,6714676,6716437,6710489,6710543,6705583,6703391,6707698,6693487,6693417,6693543,6693491,6697992,6693984,6693509,6690713,6690727,6688888,6685687,6681357,6680033,6679967,6680995,6680503,6679814,6676901,6676229,6676225,6475942,6361392,6671311,6671054,6661801,6664972,6661829,6664919,6664243,6662020,6664734,6588600,6657286,6657148,6646118,6587770,6583960,6636703,6637519,6637645,6637748,6637598,6636705,6637741,6639336,6637597,6637745,6638351,6637406,6637507,6637539,6631638,6634195,6630381,6630311,6628636,6629211,6624069,6624026,6625510,6624025,6620159,6616506,6616456,6616510,6617453,6616154,6613840,6614043,6607823,6606670,6612228,6606723,6606604,6607869,6612402",2017/05/04 20:39:19
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Semantic smells and errors in access control models: A case study in PHP","F. Gauthier; E. Merlo","Polytechnique Montr&#x00E9;al, Canada","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1169","1172","Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606670","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606670","access control models;code smells;information retrieval;security;static analysis","Access control;Analytical models;Context;Information retrieval;Logistics;Semantics","authorisation;information retrieval;program diagnostics","PHP;access control models;access control smells;information retrieval techniques;privilege checking;semantic smells;sensitive data;static analysis;unprivileged users","","0","","16","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"SmartMart: IoT-based In-store mapping for mobile devices","D. Hicks; K. Mannix; H. M. Bowles; B. J. Gao","","9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20131212","2013","","","616","621","Quite often, when shopping in a supermarket (e.g. Walmart), shoppers are frustrated at locating the items on the shopping list and no assistance is available. On the other hand, retailers also lose a large volume of sales as a result. In this paper, we present a feasibility study that leverages the Internet of Things (IoT) technology to make store items “smart” so that they can automatically register and update their location information in an information retrieval system, allowing shoppers to search, locate, and map them on the store floor plan using mobile devices. A free-accessible Android-based mobile app SmartMart has been developed to demonstrate the promise of this preliminary work. Continuous development of this research could lead to a complete change in our day-to-day shopping experience.","","Electronic:978-1-936968-92-3; POD:978-1-4799-2754-8; USB:978-1-936968-91-6","10.4108/icst.collaboratecom.2013.254116","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680033","Internet of things;in-store mapping;information retrieval;mobile devices","Androids;Databases;Humanoid robots;Layout;Mobile communication;Servers;XML","Internet of Things;information retrieval;mobile computing;retail data processing","Internet of Things technology;IoT-based in-store mapping;SmartMart;day-to-day shopping experience;free-accessible Android-based mobile app;information retrieval system;location information;mobile devices;store floor plan;supermarket","","1","","14","","","20-23 Oct. 2013","","IEEE","IEEE Conference Publications"
"Online Library Content Generation Using Focused Crawling Based Upon Meta Tags and Tf-Idf","M. Kumar; R. Vig","Inst. of Eng. & Technol., Panjab Univ., Chandigarh, India","2013 International Symposium on Computational and Business Intelligence","20140127","2013","","","158","161","Electronic library is the collection of digital information related to an individual domain and in turn to all domains. A focused crawler traverses the Web looking for the pages most relevant to a domain and at the same time discarding the irrelevant pages and hence is helpful for generating the-e contents for digital library related to a particular domain. In this paper a focused crawling technique to generate online contents for e-library is proposed. The applicability of the proposed approach is shown by retrieving the documents which are highly related to a single domain. The quality of the pages included into the library is derived from the relevancy measure of the page with the content of domain related pages.","","Electronic:978-0-7695-5066-4; POD:978-1-4799-0998-8","10.1109/ISCBI.2013.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724344","Focused Web crawler;Tf-Idf;indexing.;information retrieval;search engine;semantics","Crawlers;Indexes;Libraries;Marine animals;Search engines;Semantics;Web sites","Internet;digital libraries;information retrieval;search engines","Tf-Idf;World Wide Web;digital information;digital library;document retrieval;domain related pages;e-content generation;e-library;electronic library;focused crawling;meta tags;online content generation;online library content generation","","0","","15","","","24-26 Aug. 2013","","IEEE","IEEE Conference Publications"
"EPlag: A two layer source code plagiarism detection system","O. Ajmal; M. M. Saad Missen; T. Hashmat; M. Moosa; T. Ali","Dept. of Comput. Sci. & IT, Islamia Univ. of Bahawalpur, Bahawalpur, Pakistan","Eighth International Conference on Digital Information Management (ICDIM 2013)","20140102","2013","","","256","261","In academic environments where students are partly evaluated on the assignments, it is necessary to discourage the practice of copying assignments of other students. The detection of plagiarism in code from large source code repositories, manual detection is fairly complex, if not impossible. Therefore, for fair evaluation there must be a fast, efficient and automatedlsemi-automated way to detect the assignments copied. Source Code metrics can be used to detect the source code plagiarism in programming assignments submitted by university students. In this paper we have developed a source code plagiarism detection system and tried to improve the existing techniques by separating the suspected files and the non-plagiarized files, thus reducing the dataset for further comparison. A number of source code metrics have been calculated, combined using similarity detection formula to give an aggregate view of the source code metrics. After that the suspected files are separated and then performed string-matching to detect the level of similarity.","","Electronic:978-1-4799-0615-4; POD:978-1-4799-0614-7","10.1109/ICDIM.2013.6693984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693984","Greedy String Tiling;Java;information retrieval;plagirism detection","Algorithm design and analysis;Complexity theory;Educational institutions;Java;Measurement;Plagiarism","computer aided instruction;computer science education;source code (software);string matching","EPlag;academic environments;assignment copying;automated way;nonplagiarized files;semiautomated way;source code metrics;source code repositories;string-matching;suspected files;two layer source code plagiarism detection system;university students","","1","","15","","","10-12 Sept. 2013","","IEEE","IEEE Conference Publications"
"Automatic Chord Estimation from Audio: A Review of the State of the Art","M. McVicar; R. Santos-Rodríguez; Y. Ni; T. D. Bie","Dept. of Eng. Math., Univ. of Bristol, Bristol, UK","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20140114","2014","22","2","556","575","In this overview article, we review research on the task of Automatic Chord Estimation (ACE). The major contributions from the last 14 years of research are summarized, with detailed discussions of the following topics: feature extraction, modeling strategies, model training and datasets, and evaluation strategies. Results from the annual benchmarking evaluation Music Information Retrieval Evaluation eXchange (MIREX) are also discussed as well as developments in software implementations and the impact of ACE within MIR. We conclude with possible directions for future research.","2329-9290;23299290","","10.1109/TASLP.2013.2294580","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705583","Music information retrieval;expert systems;knowledge based systems;machine learning;supervised learning","Accuracy;Feature extraction;Harmonic analysis;Spectrogram;Time-frequency analysis;Tuning;Vectors","feature extraction;information retrieval;learning (artificial intelligence);music","ACE;MIR;MIREX;audio signal;automatic chord estimation;feature extraction;model training;music information retrieval evaluation exchange","","7","","105","","20140109","Feb. 2014","","IEEE","IEEE Journals & Magazines"
"An effective, simple tempo estimation method based on self-similarity and regularity","G. Tzanetakis; G. Percival","Department of Computer Science, University of Victoria, Canada","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","241","245","Tempo estimation is a fundamental problem in music information retrieval. It also forms the basis of other types of rhythmic analysis such as beat tracking and pattern detection. There is a large body of work in tempo estimation using a variety of different approaches that differ in their accuracy as well as their complexity. Fundamentally they take advantage of two properties of musical rhythm: 1) the music signal tends to be self-similar at periodicities related to the underlying rhythmic structure, 2) rhythmic events tend to be spaced regularly in time. We propose an algorithm for tempo estimation that is based on these two properties. We have tried to reduce the number of steps, parameters and modeling assumptions while retaining good performance and causality. The proposed approach outperforms a large number of existing tempo estimation methods and has similar performance to the best-performing ones. We believe that we have conducted the most comprehensive evaluation to date of tempo induction algorithms in terms of number of datasets and tracks.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637645","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637645","audio signal processing;music information retrieval;rhythm analysis;tempo induction","Accuracy;Algorithm design and analysis;Correlation;Estimation;Histograms;Speech;Vectors","audio signal processing;information retrieval;music","audio signal processing;beat tracking;effective simple tempo estimation method;music information retrieval;music signal;musical rhythmic analysis;pattern detection;rhythmic events;rhythmic structure;tempo induction algorithms","","3","","30","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Supporting requirements traceability through refactoring","A. Mahmoud; N. Niu","Computer Science and Engineering Mississippi State University Mississippi State, MS, USA","2013 21st IEEE International Requirements Engineering Conference (RE)","20131021","2013","","","32","41","Modern traceability tools employ information retrieval (IR) methods to generate candidate traceability links. These methods track textual signs embedded in the system to establish relationships between software artifacts. However, as software systems evolve, new and inconsistent terminology finds its way into the system's taxonomy, thus corrupting its lexical structure and distorting its traceability tracks. In this paper, we argue that the distorted lexical tracks of the system can be systematically re-established through refactoring, a set of behavior-preserving transformations for keeping the system quality under control during evolution. To test this novel hypothesis, we investigate the effect of integrating various types of refactoring on the performance of requirements-to-code automated tracing methods. In particular, we identify the problems of missing, misplaced, and duplicated signs in software artifacts, and then examine to what extent refactorings that restore, move, and remove textual information can overcome these problems respectively. We conduct our experimental analysis using three datasets from different application domains. Results show that restoring textual information in the system has a positive impact on tracing. In contrast, refactorings that remove redundant information impact tracing negatively. Refactorings that move information among the system modules are found to have no significant effect. Our findings address several issues related to code and requirements evolution, as well as refactoring as a mechanism to enhance the practicality of automated tracing tools.","1090-705X;1090705X","Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0","10.1109/RE.2013.6636703","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636703","Refactoring;information retrieval;traceability","Animals;Educational institutions;Manuals;Measurement;Medical services;Software systems","information retrieval;program diagnostics;program verification;software maintenance;software quality","behavior-preserving transformations;distorted lexical tracks;information retrieval;refactoring;requirements traceability tool;requirements-to-code automated tracing method performance;software artifacts;system quality control;textual information restoration","","2","","58","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"Understanding Effects of Subjectivity in Measuring Chord Estimation Accuracy","Y. Ni; M. McVicar; R. Santos-Rodríguez; T. De Bie","Intelligent Systems Laboratory, Department of Engineering Mathematics, University of Bristol, U.K.","IEEE Transactions on Audio, Speech, and Language Processing","20131023","2013","21","12","2607","2615","To assess the performance of an automatic chord estimation system, reference annotations are indispensable. However, owing to the complexity of music and the sometimes ambiguous harmonic structure of polyphonic music, chord annotations are inherently subjective, and as a result any derived accuracy estimates will be subjective as well. In this paper, we investigate the extent of the confounding effect of subjectivity in reference annotations. Our results show that this effect is important, and they affect different types of automatic chord estimation systems in different ways. Our results have implications for research on automatic chord estimation, but also on other fields that evaluate performance by comparing against human provided annotations that are confounded by subjectivity.","1558-7916;15587916","","10.1109/TASL.2013.2280218","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6587770","Subjectivity evaluation;automatic chord estimation;music information retrieval;sequence crowd learning","Complexity theory;Hamming distance;Information retrieval;Learning systems;Music;Music information retrieval","audio signal processing;information retrieval;music","ambiguous harmonic structure;automatic chord estimation system;chord annotations;chord estimation accuracy;human provided annotations;polyphonic music;reference annotations;subjectivity evaluation","","4","","23","","20130829","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Research on query results Cache based on log analysis in web search engines","H. Ma","Nat. Comput. network Emergency Response Tech. Team Coordination Center of China, Beijing, China","2013 3rd International Conference on Consumer Electronics, Communications and Networks","20140109","2013","","","551","554","Query results cache is an effective technology to improve the performance of Web search engines. In this paper, we conducted an analysis of Web search engine query logs from SOGOU Inc. to explore some issues of query results cache for Web search engines including the locality of Web search engine workload and the impact of cache replace policy. We also conducted a series of experiments on query results cache with a variety of cache capacity and cache replacement policy settings. Experimental results showed that when there are only a small number of historical query logs, it would be better to choose a dynamic policy, and hybrid cache approach had significant improvement than any other policies in smaller cache capacity.","","DVD:978-1-4799-2858-3; Electronic:978-1-4799-2860-6; POD:978-1-4799-2861-3","10.1109/CECNet.2013.6703391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703391","Log analysis;Web search engines;cache system;information retrieval;query results cache","Computers;Engines;Internet;Search engines;Solids;Time factors;Web search","cache storage;query processing;search engines","Web search engines;cache capacity;cache replace policy;historical query log;log analysis;query result cache;workload locality","","0","","23","","","20-22 Nov. 2013","","IEEE","IEEE Conference Publications"
"Re-ranking for personalization using concept hierarchy in DL environment","M. A. Potey; S. P. Pawar; P. K. Sinha","Coll. of Eng., Pune, India","2013 15th International Conference on Advanced Computing Technologies (ICACT)","20140116","2013","","","1","6","In Digital Library (DL) system, users interact with the system to search for books or research papers. Users can search through metadata or search for information in the pages by querying using keywords. In both cases, a huge amount of results are returned; however, the relevant ones to the user are not often amongst the top few. Re-ranking of the search results based on the user's interest has received wide attention in information retrieval. This work presents extending conventional search engine to searching digital library data of user's interests. The proposed system improve information access by building knowledge about a user, acquired using the user's interaction with the system, in order to customize information access. User profiling is done using a hybrid approach by taking into consideration login details and click-through data. This system mapping framework automatically maps Dmoz Open Directory Project (ODP) topics to users' interests and takes advantage of manually edited data available in ODP, to categorize and personalize search results, according to user interests. Reranking of the results is done based on user interests. This makes it easy to find relevant DL pages faster than normal search engines. Performance has been evaluated for online DL systems. System's performance improves by 16.55% if the average per query is calculated and approximately 10% if per user average is calculated over baseline (Google CSE) after re-ranking.","","CD-ROM:978-1-4673-2816-6; Electronic:978-1-4673-2818-0; POD:978-1-4673-2817-3","10.1109/ICACT.2013.6710489","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710489","Concept Hierarchy;Digital Library;Information Retrieval;Personalization;Personalized Information Retrieval;Ranking;Re-ranking;User Profiling","Collaboration;Google;Libraries;Navigation;Search engines;Vectors","digital libraries;human computer interaction;query processing;search engines","Dmoz Open Directory Project;ODP;click-through data;concept hierarchy;digital library system;information access customization;information retrieval;login details;online DL systems;personalization;query;search engine;search results re-ranking;user profiling","","1","","20","","","21-22 Sept. 2013","","IEEE","IEEE Conference Publications"
"Spatially Aware Term Selection for Geotagging","O. Van Laere; J. Quinn; S. Schockaert; B. Dhoedt","Dept. of Inf. Technol., Ghent Univ., Ghent, Belgium","IEEE Transactions on Knowledge and Data Engineering","20131125","2014","26","1","221","234","The task of assigning geographic coordinates to textual resources plays an increasingly central role in geographic information retrieval. The ability to select those terms from a given collection that are most indicative of geographic location is of key importance in successfully addressing this task. However, this process of selecting spatially relevant terms is at present not well understood, and the majority of current systems are based on standard term selection techniques, such as x<sup>2</sup> or information gain, and thus fail to exploit the spatial nature of the domain. In this paper, we propose two classes of term selection techniques based on standard geostatistical methods. First, to implement the idea of spatial smoothing of term occurrences, we investigate the use of kernel density estimation (KDE) to model each term as a two-dimensional probability distribution over the surface of the Earth. The second class of term selection methods we consider is based on Ripley's K statistic, which measures the deviation of a point set from spatial homogeneity. We provide experimental results which compare these classes of methods against existing baseline techniques on the tasks of assigning coordinates to Flickr photos and to Wikipedia articles, revealing marked improvements in cases where only a relatively small number of terms can be selected.","1041-4347;10414347","","10.1109/TKDE.2013.42","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475942","Information search and retrieval;artificial intelligence;classification;feature extraction;geographic information retrieval;knowledge management;metadata;semi-structured data;text mining","Context;Electronic publishing;Encyclopedias;Estimation;Internet;Standards","geography;information retrieval;statistical distributions","Earth surface;Flickr photos;KDE;Ripley K statistics;Wikipedia articles;geographic coordinates assignment;geographic information retrieval;geographic location;geotagging;kernel density estimation;spatial homogeneity;spatially aware term selection;standard geostatistical methods;standard term selection techniques;term occurrence spatial smoothing;textual resources;two-dimensional probability distribution","","2","","32","","20130307","Jan. 2014","","IEEE","IEEE Journals & Magazines"
"Genre-Based Music Language Modeling with Latent Hierarchical Pitman-Yor Process Allocation","S. A. Raczyński; E. Vincent","Gda&#x0144;sk Universit of Technology, Faculty of Electronics, Telecommunications and Informatics, Gda&#x0144;sk","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20140128","2014","22","3","672","681","In this work we present a new Bayesian topic model: latent hierarchical Pitman-Yor process allocation (LHPYA), which uses hierarchical Pitman-Yor process priors for both word and topic distributions, and generalizes a few of the existing topic models, including the latent Dirichlet allocation (LDA), the bigram topic model and the hierarchical Pitman-Yor topic model. Using such priors allows for integration of n-grams with a topic model, while smoothing them with the state-of-the-art method. Our model is evaluated by measuring its perplexity on a dataset of musical genre and harmony annotations 3 Genre Database (3GDB) and by measuring its ability to predict musical genre from chord sequences. In terms of perplexity, for a 262-chord dictionary we achieve a value of 2.74, compared to 18.05 for trigrams and 7.73 for a unigram topic model. In terms of genre prediction accuracy with 9 genres, the proposed approach performs about 33% better in relative terms than genre-dependent n-grams, achieving 60.4% of accuracy.","2329-9290;23299290","","10.1109/TASLP.2014.2300344","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714400","Chinese restaurant process;chord model;genre model;hierarchical Pitman-Yor process;music information retrieval;musical genre recognition;topic models","Context;Context modeling;IEEE transactions;Resource management;Smoothing methods;Speech;Speech processing","Bayes methods;music;natural language processing","3 Genre Database;Bayesian topic model;bigram topic model;genre based music language modeling;harmony annotations;hierarchical Pitman-Yor topic model;latent Dirichlet allocation;latent hierarchical Pitman-Yor process allocation;musical genre;topic distribution;word distribution","","0","","46","","20140116","March 2014","","IEEE","IEEE Journals & Magazines"
"Weighted PCA for improving Document Image Retrieval System based on keyword spotting accuracy","R. Tavoli; E. Kozegar; M. Shojafar; H. Soleimani; Z. Pooranian","Dept. of Math., Islamic Azad Univ., Chalous, Iran","2013 36th International Conference on Telecommunications and Signal Processing (TSP)","20130930","2013","","","773","777","Feature weighting is a technique used to approximate the optimal degree of influence of individual features. This paper presents a feature weighting method for Document Image Retrieval System (DIRS) based on keyword spotting. In this method, we weight the features using Weighted Principal Component Analysis (PCA). The purpose of PCA is to reduce the dimensionality of the data space to the smaller intrinsic dimensionality of feature space (independent variables), which are needed to describe the data economically. This is the case when there is a strong correlation between variables. The aim of this paper is to show feature weighting effect on increasing the performance of DIRS. After applying the feature weighting method to DIRS the average precision is 92.1% and average recall become 97.7% respectively.","","Electronic:978-1-4799-0404-4; POD:978-1-4799-0401-3","10.1109/TSP.2013.6614043","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614043","Document Image;Feature weighting;Indexing;Information Retrieval;Principal Component Analysis","Feature extraction;Image retrieval;Indexing;Loading;Principal component analysis;Shape","image retrieval;information retrieval;principal component analysis","DIRS;data space;dimensionality;document image retrieval system;feature space;feature weighting method;independent variables;keyword spotting accuracy;weighted PCA;weighted principal component analysis","","3","","21","","","2-4 July 2013","","IEEE","IEEE Conference Publications"
"Experiments on company name disambiguation with supervised classification techniques","N. Polat","Dept. of Electr. & Comput. Eng., Istanbul Sehir Univ., Istanbul, Turkey","2013 International Conference on Electronics, Computer and Computation (ICECCO)","20140123","2013","","","139","142","Entity disambiguation is the task of identifying the real world entity was referred to in a context. Ambiguous references to entities can occur due to variations of how entity was referenced (BT, British Telecom) or inherit ambiguities of the names used for entities (Orange Telecom vs. fruit orange) and misspellings (Best Buy vs. BestBuy). Ambiguities in company names however come with a price, when it comes to finding information about the company on the Web. Recently, tracking social media for brand management has become a very important part of the process in marketing, public relations, and product marketing. Therefore, resolving references to the real world objects has become an important part of the social media analytics systems. In this paper, we study different machine learning techniques for entity disambiguation in micro-blogging posts. Our experiments show that using supervised algorithms with carefully selected features, one can improve the disambiguation quality significantly.","","Electronic:978-1-4799-3343-3; POD:978-1-4799-3344-0","10.1109/ICECCO.2013.6718248","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718248","Information Retrieval;Machine Learning;Twitter;name ambiguity;online reputation management","Accuracy;Companies;Encyclopedias;Feature extraction;Twitter","Internet;information retrieval;learning (artificial intelligence);marketing data processing;pattern classification;social networking (online)","World Wide Web;brand management;company name disambiguation;entity disambiguation;entity identification;information retrieval;machine learning techniques;microblogging posts;product marketing;public relations;social media analytics systems;supervised classification techniques","","0","","14","","","7-9 Nov. 2013","","IEEE","IEEE Conference Publications"
"Visual Document Retrieval: Supporting Text Search and Analysis with Visual Analytics","S. Koch; F. Heimerl; T. Ertl","Univ. of Stuttgart, Stuttgart, Germany","Computing in Science & Engineering","20131114","2013","15","4","66","74","An interactive method combines visualization and machine learning to improve complex document retrieval tasks.","1521-9615;15219615","","10.1109/MCSE.2013.93","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664972","binary classifier;document search;information retrieval;interactive visualization;machine learning;scientific computing;visual analytics","Data visualization;Document handling;Information retrieval;Search methods","data visualisation;information retrieval;learning (artificial intelligence);text analysis","data visualization;interactive method;machine learning;text analysis;text search;visual analytics;visual document retrieval task","","0","","8","","","July-Aug. 2013","","IEEE","IEEE Journals & Magazines"
"Quantifying the value of pronunciation lexicons for keyword search in lowresource languages","G. Chen; S. Khudanpur; D. Povey; J. Trmal; D. Yarowsky; O. Yilmaz","Center for Language and Speech Processing, and Human Language Technology Center of Excellence, Johns Hopkins University, Baltimore, MD 21218, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","8560","8564","This paper quantifies the value of pronunciation lexicons in large vocabulary continuous speech recognition (LVCSR) systems that support keyword search (KWS) in low resource languages. State-of-the-art LVCSR and KWS systems are developed for conversational telephone speech in Tagalog, and the baseline lexicon is augmented via three different grapheme-to-phoneme models that yield increasing coverage of a large Tagalog word-list. It is demonstrated that while the increased lexical coverage - or reduced out-of-vocabulary (OOV) rate - leads to only modest (ca 1%-4%) improvements in word error rate, the concomitant improvements in actual term weighted value are as much as 60%. It is also shown that incorporating the augmented lexicons into the LVCSR system before indexing speech is superior to using them post facto, e.g., for approximate phonetic matching of OOV keywords in pre-indexed lattices. These results underscore the disproportionate importance of automatic lexicon augmentation for KWS in morphologically rich languages, and advocate for using them early in the LVCSR stage.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6639336","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6639336","Information Retrieval;Keyword Search;Morphology;Speech Recognition;Speech Synthesis","Acoustics;Hidden Markov models;Keyword search;Lattices;Speech;Speech recognition;Training","indexing;speech recognition","KWS systems;LVCSR systems;Tagalog word-list;approximate phonetic matching;baseline lexicon;concomitant improvements;conversational telephone speech;grapheme-to-phoneme models;indexing speech;keyword search;large vocabulary continuous speech recognition;lexical coverage;low resource languages;out-of-vocabulary rate;preindexed lattices;pronunciation lexicons value;word error rate","","4","1","21","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Social media communications networks and pharmacovigilance: SequelAE-2.0","E. W. Burger; H. J. Federoff; M. S. Fiandaca; O. Frieder; N. Goharian; A. Yates","Georgetown Univ., Georgetown, GA, USA","2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013)","20140127","2013","","","1","3","We discuss a system for tracking a patient's response during a clinical trial or post-marketing survey that leverages social media and other Internet-based sharing technologies. The system has many modalities. This work-in-progress paper describes a modality where an informational item is posted to a social media platform, inducing patients to post trial-related responses. The system aggregates the patient's responses and other environmental information to obtain deeper knowledge related to the trial or post-marketing survey, sooner than would be possible using traditional methods.","","Electronic:978-1-4673-5801-9; POD:978-1-4673-5799-9","10.1109/HealthCom.2013.6720777","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720777","Information retrieval;Prognostics and health management;Public healthcare;Social network services","Drugs;Educational institutions;Electronic mail;Market research;Media;Pain;Pattern matching","Internet;health care;marketing;social networking (online)","Internet-based sharing technologies;SequelAE-2.0;clinical trial;pharmacovigilance;post-marketing survey;social media communications networks","","1","","10","","","9-12 Oct. 2013","","IEEE","IEEE Conference Publications"
"G-Bean: An ontology-graph based web tool for biomedical literature retrieval","Y. Zhang; L. Dong; L. Li; P. K. Srimani; P. S. Yu; J. Z. Wang","Sch. of Comput., Clemson Univ., Clemson, SC, USA","2013 IEEE International Conference on Bioinformatics and Biomedicine","20140206","2013","","","623","623","To address the need for an efficient and effective biomedical information retrieval system, this paper presents an ontology-graph based web tool, G-Bean (graph based biomedical search engine), for searching biomedical documents from the MEDLINE database. This search engine uses an ontology-graph based query expansion technique to expand the initial query with additional query terms and/or more specific query concepts, resulting in the retrieval of more accurate and relevant information from the database. A multithreading parallel process is used to automatically generate the document index to address the inefficiency of the PubMed's manual indexing process. This search engine can also discover user's true search intention and retrieve additional relevant articles based on articles that he/she has already shown interest in.","","Electronic:978-1-4799-1309-1; POD:978-1-4799-1311-4","10.1109/BIBM.2013.6732578","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732578","biomedical information retrieval;query expansion","Bioinformatics;Educational institutions;Indexing;Ontologies;Search engines","Internet;information retrieval systems;medical information systems;ontologies (artificial intelligence);parallel processing;query processing;question answering (information retrieval);search engines","G-Bean;MEDLINE database;PubMed manual indexing process;biomedical document searching;biomedical information retrieval system;graph based biomedical search engine;multithreading parallel process;ontology-graph based Web tool;ontology-graph based query expansion technique","","0","","2","","","18-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"A discriminative approach to polyphonic piano note transcription using supervised non-negative matrix factorization","F. Weninger; C. Kirst; B. Schuller; H. J. Bungartz","Machine Intelligence & Signal Processing Group, MMK, Technische Universit&#x00E4;t M&#x00FC;nchen, Germany","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","6","10","We introduce a novel method for the transcription of polyphonic piano music by discriminative training of support vector machines (SVMs). As features, we use pitch activations computed by supervised non-negative matrix factorization from low-level spectral features. Different approaches to low-level feature extraction, NMF dictionary learning and activation feature extraction are analyzed in a large-scale evaluation on eight hours of piano music including synthesized and real recordings. We conclude that the proposed method delivers state-of-the-art results and clearly outperforms SVMs using simple spectral features.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637598","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637598","Transcription;music information retrieval;non-negative matrix factorization;sparse coding","Accuracy;Databases;Dictionaries;Feature extraction;Instruments;Support vector machines;Training","acoustic signal processing;feature extraction;information retrieval;learning (artificial intelligence);matrix decomposition;music;support vector machines","NMF dictionary learning;SVMs;activation feature extraction;discriminative training approach;large-scale evaluation;low-level spectral feature extraction;piano music;pitch activations;polyphonic piano note transcription;supervised nonnegative matrix factorization;support vector machines","","4","","25","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Towards a new approach of an automatic and contextual detection of meaning in text: Based on lexico-semantic relations and the concept of the context","H. Fadili","Laboratoire CEDRIC du Conservatoire National des Arts et M&#x00E9;tiers de Paris, 192, rue Saint Martin, 75141, Paris cedex 3, France","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","4","The goal of the research described here is to present an approach for automating the detection and the extraction of meaning from text using a range of linguistic and ontological techniques, concepts such as the lexico-semantic functions proposed in Meaning-Text Theory by Mel'cuk and the concept of the context. This is motivated, by the fact that, on one hand, these functions enable a better modeling and formalization of meaning. On the other hand, the concept of the context is not well supported in many important and essential domains such as in Natural Language Processing (NLP), semantic data analysis and knowledge management. Our work consists here to integrate these elements in an automatic and context-based approach we called PROSEM (SEMantic PROjection), based on an extended model of the context, that improves the detection of meaning in a text. This approach can be used in many areas related to the meaning and the context such as semantic data indexing, linguistic or semantic search, contextual information extraction, multilingualism, etc. In this paper, we present the main elements of the approach and three typical applications processes enhanced by PROSEM.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616456","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616456","Context;Indexing;Information Extraction (IE);Information Retrieval (IR);NLP;Ontologies;PROSEM;SDM&KM;Semantics","Context;Indexing;Information retrieval;Ontologies;Pragmatics;Semantics","data analysis;indexing;information retrieval;knowledge management;natural language processing;ontologies (artificial intelligence);semantic networks;text analysis","NLP;PROSEM;automatic text meaning detection;context concept;contextual information extraction;knowledge management;lexico-semantic relations;linguistic search;linguistic techniques;meaning extraction;multilingualism;natural language processing;ontological techniques;semantic data analysis;semantic data indexing;semantic projection;semantic search;text meaning contextual detection","","0","","4","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"The limitation of the SVD for latent semantic indexing","A. Mirzal","Fac. of Comput., Univ. Teknol. Malaysia, Skudai, Malaysia","2013 IEEE International Conference on Control System, Computing and Engineering","20140123","2013","","","413","416","Latent semantic indexing (LSI) is an indexing method for improving retrieval performance of an information retrieval system by grouping related documents to the same clusters so that each of these documents indexes the same (or almost the same) words, and unrelated documents index (relatively) different words. The de facto standard method for LSI is the truncated singular value decomposition (SVD). In this paper, we show that the LSI capability of the truncated SVD is not as conclusive as previously reported; rather it is a conditional aspect and when the condition is not met, then the truncated SVD can fail in recognizing the related documents resulting in a poor retrieval performance.","","Electronic:978-1-4799-1508-8; POD:978-1-4799-1507-1","10.1109/ICCSCE.2013.6720000","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720000","document analysis;indexing;information retrieval;latent semantic;singular value decomposition","Approximation methods;Indexing;Large scale integration;Rivers;Semantics;Vectors","document handling;indexing;information retrieval;information retrieval systems;singular value decomposition","LSI capability;de facto standard method;information retrieval system;latent semantic indexing;related documents;retrieval performance;truncated SVD;truncated singular value decomposition","","0","","16","","","Nov. 29 2013-Dec. 1 2013","","IEEE","IEEE Conference Publications"
"Information seeking behaviour factors: A measurement model","M. M. Nadzir; J. Salim","Sch. of Comput., Univ. Utara Malaysia, Sintok, Malaysia","2013 International Conference on Research and Innovation in Information Systems (ICRIIS)","20140123","2013","","","168","173","Graduate students are active information seekers. Their experiences with information seeking range from novices to experts. Graduate students' understanding of the needed information and the different types of information sources accessible are essential for them to search for the required information effectively and efficiently. Previous studies concluded that insufficient knowledge about information seeking behaviour is one of the factors that contributed to failure in searching for useful information. This paper discuss about a study conducted to evaluate a measurement model of information seeking behaviour factors, which is a part of the methods used to develop Research Information Seeking Behaviour Model. The data was collected through survey method by using web-based questionnaire. The measurement model was validated using Structural Equation Modeling with AMOS 16.0. Eight factors were extracted from exploratory factor analysis and labeled as preliminary research needs, data research needs, printed information sources, electronic information sources, media information sources, library information search, electronic information search and information use. Results of the assessment of model fit showed that all the relevant measurements fit a value that exceeds the value recommended by previous researchers. In conclusion, the measurement model provides a reasonably good fit and suitable for the development of Research Information Seeking Behaviour Model.","2324-8149;23248149","Electronic:978-1-4799-2487-5; POD:978-1-4799-2488-2","10.1109/ICRIIS.2013.6716703","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716703","graduate student;information retrieval;information seeking behaviour;model;research","Information systems;Libraries;Load modeling;Mathematical model;Media;Reliability;Technological innovation","academic libraries;information retrieval;statistical analysis","AMOS 16.0;Web-based questionnaire;data research needs;electronic information search;electronic information sources;exploratory factor analysis;information seeking behaviour factor;information use;library information search;measurement model;media information sources;preliminary research needs;printed information sources;research information seeking behaviour model;structural equation modeling","","0","","34","","","27-28 Nov. 2013","","IEEE","IEEE Conference Publications"
"IR in Software Traceability: From a Bird's Eye View","M. Borg; P. Runeson","Dept. of Comput. Sci., Lund Univ., Lund, Sweden","2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","20131212","2013","","","243","246","Background. Several researchers have proposed creating after-the-fact structure among software artifacts using trace recovery based on Information Retrieval (IR). Due to significant variation points in previous studies, results are not easily aggregated. Aim. We aim at an overview picture of the outcome of previous evaluations. Method. Based on a systematic mapping study, we perform a synthesis of published research. Results. Our synthesis shows that there are no empirical evidence that any IR model outperforms another model consistently. We also display a strong dependency between the Precision and Recall (P-R) values and the input datasets. Finally, our mapping of P-R values on the possible output space highlights the difficulty of recovering accurate trace links using naïve cut-off strategies. Conclusion. Based on our findings, we stress the need for empirical evaluations beyond the basic P-R 'race'.","1949-3770;19493770","Electronic:978-0-7695-5056-5; POD:978-1-4799-1144-8","10.1109/ESEM.2013.39","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681357","empirical software engineering;information retrieval;secondary study;software traceability","Accuracy;Information retrieval;Large scale integration;Software;Software engineering;Standards;Systematics","information retrieval;software engineering","IR model;information retrieval;precision and recall value;software artifacts;software engineering;software traceability;trace recovery","","3","","15","","","10-11 Oct. 2013","","IEEE","IEEE Conference Publications"
"A Comprehensive Review of Unstructured Data Management Approaches in Data Warehouse","V. Gupta; A. Gosain","Sch. of Inf. & Commun. Technol., GGS Indraprastha Univ., New Delhi, India","2013 International Symposium on Computational and Business Intelligence","20140127","2013","","","64","67","The amount of business data is large & keeps on evolving leading to heterogeneous information base. The challenge is to access, analyze & integrate various data sources for making intelligent decisions. Business data can be structured or unstructured. Structured data attains the row-column format easily while unstructured data (USD) is the one that poses problem in such kind of tabular storage. Owing to the fact that USD is more than three times of structured data, and that it is more resourceful business wise and helps in charting out strategies and making decisions, it becomes important to devise methods for handling USD in data warehouse. Since the importance of USD has been realized, various authors have discussed different ways to manage it and extract useful information from it. In this paper, we have first comprehensively reviewed & surveyed the representative research works of various authors that have demonstrated how unstructured data can be handled in the warehouse. Finally, we have manifested & sorted them on various parameters & provided the same in tabular form.","","Electronic:978-0-7695-5066-4; POD:978-1-4799-0998-8","10.1109/ISCBI.2013.20","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724324","Dimensions;Document Warehouse;Facts;Information Retrieval;Measures;Metadata;Multi-Dimensional Modelling;OLAP;Unstructured Data (USD);XML","Business;Context;Data mining;Data models;Data warehouses;Ontologies;XML","data warehouses","USD;business data;comprehensive review;data sources;data warehouse;heterogeneous information base;row-column format;structured data;tabular storage;unstructured data management approach","","1","","15","","","24-26 Aug. 2013","","IEEE","IEEE Conference Publications"
"Research of intelligent search engine based on computer vision","M. Wenpeng; A. Minazuki; H. Hayashi","Department of Technology and Information Education, Naruto University of Education, Tokushima, Japan","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","20130926","2013","","","95","100","The existing search engine system almost based on keywords from users inputting. In a network environment, people can make use of the pc or mobile device for information retrieval. The way of inputting keywords has lasted for more than 20 years until Siri appeared in 2011. Siri can do information retrieval and process by voice actions. Compared to the traditional search mode, this is a huge step forward. Siri solved some limitations of the current search engines. But people need a new system, at any time and place, and fast search and process information. In this paper, we described new search engine system based on computer vision. Containing the information search and problem solve two basic parts. And for the needs of the people in daily lives, designed a number of convenience functions. System can use web camera to read and identify information. With the aid of network, maintain a real-time search and feedback. It is noteworthy that mobile devices include the existing smart phones and tablet PCs for now. These mobile devices are an appropriate choice. Like Google Glass. However, the best choice is new concept of mobile devices in the near future. After all, the existing equipment are still many restrictions.","","Electronic:978-1-4799-0174-6; POD:978-1-4799-0173-9","10.1109/ICIS.2013.6607823","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607823","computer vision;information retrieval;mobile device;reality search;search engine","Cameras;Computer vision;Engines;Intelligent systems;Internet;Mobile handsets;Search engines","computer vision;information retrieval;mobile computing;search engines;smart phones","Google Glass;computer vision;information retrieval;information search;intelligent search engine;mobile devices;network environment;smart phones;tablet PC;web camera","","0","","5","","","16-20 June 2013","","IEEE","IEEE Conference Publications"
"Platform Zero: A Context-Based Computing Platform for Collaboration","A. Muhlbauer; T. Zelinsky; S. Kanhere","Univ. of New South Wales, Sydney, NSW, Australia","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications","20131212","2013","","","1420","1427","We present a real-time, context aware platform that promotes information-centric, natural interactions between many different personal computing devices. Our system allows devices to intelligently interpret the context they operate in to better serve the user. Moreover, our platform supports a rapidly emerging trend in computing, wherein, a single user simultaneously uses multiple devices in unison (computer, smartphone, tablet) to complete a particular task. Our platform can use current device context to sense collaboration between devices and users, automatically annotating information to improve future retrieval. Through removing the perceived borders between devices, our system aims to create a seamless information sharing experience in a multi-device, multi-user environment. Our reference implementation supports Apple iOS devices and OSX computers. Moreover, we present extensive evaluations to demonstrate its efficacy.","2324-898X;2324898X","Electronic:978-0-7695-5022-0; POD:978-1-4799-1444-9","10.1109/TrustCom.2013.172","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680995","context-aware;information retrieval;mobile;pervasive computing;sensing;ubiquitous computing","Collaboration;Computer architecture;Context;Market research;Real-time systems;Semantics;Sensors","ubiquitous computing","Apple iOS devices;OSX computers;computer;context aware platform;context based computing platform;information centric;natural interactions;personal computing devices;platform zero;smartphone;tablet","","0","","20","","","16-18 July 2013","","IEEE","IEEE Conference Publications"
"National Security and Social Media Monitoring: A Presentation of the EMOTIVE and Related Systems","M. D. Sykora; T. W. Jackson; A. OBrien; S. Elayan","Inf. Sci. Dept., Loughborough Univ., Loughborough, UK","2013 European Intelligence and Security Informatics Conference","20131107","2013","","","172","175","Today social media streams, such as Twitter, represent vast amounts of 'real-time' daily streaming data. Topics on these streams cover every range of human communication, ranging from banal banter, to serious reactions to events and information sharing regarding any imaginable product, item or entity. It has now become the norm for publicly visible events to break news over social media streams first, and only then followed by main stream media picking up on the news. It has been suggested in literature that social-media are a valid, valuable and effective real-time tool for gauging public subjective reactions to events and entities. Due to the vast big-data that is generated on a daily basis on social media streams, monitoring and gauging public reactions has to be automated and most of all scalable - i.e. human, expert monitoring is generally unfeasible. In this paper the EMOTIVE system, a project funded jointly by the DSTL (Defence Science and Technology Laboratory) and EPSRC, which focuses on monitoring fine-grained emotional responses relating to events of national security importance, will be presented. Similar systems for monitoring national security events are also presented and the primary traits of such national security social media monitoring systems are introduced and discussed.","","Electronic:978-0-7695-5062-6; POD:978-1-4799-0775-5","10.1109/EISIC.2013.38","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657148","Twitter;information retrieval;national security;natural language processing;social media monitoring","Media;Monitoring;National security;Ontologies;Twitter","emotion recognition;national security;real-time systems;social networking (online)","DSTL;EMOTIVE systems;Twitter;defence science and technology laboratory;fine-grained emotional response monitoring;human communication;information sharing;national security event monitoring;national security social media monitoring systems;public reaction gauging;public reaction monitoring;publicly visible events;real-time daily streaming data;real-time tool;social media streams","","2","","31","","","12-14 Aug. 2013","","IEEE","IEEE Conference Publications"
"Preparing Text Reports from Web Pages Employing Similarity Tests","J. G. Ramos; J. C. Solorio; L. Campoy; S. Ruiz; N. Jasso","Inst. Tecnol. de La Piedad, La Piedad, Mexico","2013 Mexican International Conference on Computer Science","20131212","2013","","","13","19","The World Wide Web is the main source of information for many organizations and common users. However, the analysis and selection of the web content is still an arduous manual task in many cases. When a web query is sent towards a web search engine, a list of URLs is received, frequently ordered by popularity (such as Google's PageRank algorithm). Then, the user must read and analyze each URL in order to find out the convenient information. In this work a method that automatically constructs a text report induced by a web query from a set of URLs is presented. The method extracts text slices (excerpts) from web pages considering the most similar text w.r.t. a web query as slicing criterion. A slice is composed by document object model (DOM) nodes, whereas similarity is calculated using standard techniques employed in natural language processing.","1550-4069;15504069","Electronic:978-0-7695-5087-9; POD:978-1-4799-1145-5","10.1109/ENC.2013.8","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679814","information retrieval;similarity;slicing;summarization","Google;Natural language processing;Software tools;Standards;Vectors;Web pages","Internet;natural language processing;query processing;search engines;text analysis","DOM nodes;URL;Web content analysis;Web content selection;Web pages;Web query;Web search engine;World Wide Web;document object model nodes;excerpts extraction;natural language processing;similarity tests;slicing criterion;text reports preparation;text slices extraction","","1","","18","","","Oct. 30 2013-Nov. 1 2013","","IEEE","IEEE Conference Publications"
"Similarity-based matrix completion algorithm for latent semantic indexing","A. Mirzal","Fac. of Comput., Univ. Teknol. Malaysia, Skudai, Malaysia","2013 IEEE International Conference on Control System, Computing and Engineering","20140123","2013","","","79","84","Latent semantic indexing (LSI) is an indexing method to improve performance of an information retrieval system by indexing terms that appear in related documents and weakening influences of terms that appear in unrelated documents. LSI usually is conducted by using the truncated singular value decomposition (SVD). The main difficulty in using this technique is its retrieval performance depends strongly on the choosing of an appropriate decomposition rank. In this paper, by observing the fact that the truncated SVD makes the related documents more connected, we devise a matrix completion algorithm that can mimick this capability. The proposed algorithm is nonparametric, has convergence guarantee, and produces a unique solution for each input. Thus it is more practical and easier to use than the truncated SVD. Experimental results using four standard datasets in LSI research show that the retrieval performances of the proposed algorithm are comparable to the best results offered by the truncated SVD over some decomposition ranks.","","Electronic:978-1-4799-1508-8; POD:978-1-4799-1507-1","10.1109/ICCSCE.2013.6719936","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719936","information retrieval;latent semantic indexing;matrix completion algorithm;nonparametric learning;singular value decomposition","Approximation methods;Convergence;Indexing;Large scale integration;Matrix decomposition;Rivers;Vectors","indexing;information retrieval systems;singular value decomposition","LSI;decomposition rank;information retrieval system;latent semantic indexing;retrieval performance;similarity-based matrix completion algorithm;truncated SVD;truncated singular value decomposition","","0","","16","","","Nov. 29 2013-Dec. 1 2013","","IEEE","IEEE Conference Publications"
"Automatic detection of gender on the blogs","F. Belbachir; K. Henni; L. Zaoui","LSSD Laboratory, USTO-MB","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","4","In this paper, we are interested in defining the gender of blogger while using only texts written from bloggers. For that purpose, we offer a number of features based on specific words, which were categorized into classes. For each blog, a score is calculated based on these characteristics, thereby determining the gender of its author. The evaluation was made on a corpus of 681,288 Blogs (140 million words) tagged as men or women. In our work, this collection will be taken as a reference. The obtained results show gender detection over 82% compared to the referenced collection.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616510","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616510","Information retrieval;blogs;gender detection;social network","Blogs;Dictionaries;Games;Grammar;Internet;Laboratories","Web sites;gender issues","automatic gender detection;blogs;specific words","","0","","13","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"Towards improving Khoja rule-based Arabic stemmer","M. N. Al-Kabi","Fac. of Sci., IT Zarqa Univ., Zarqa, Jordan","2013 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)","20140120","2013","","","1","6","Stemming algorithms are used to remove irrelevant morphological variations from different words, and extract the stem or the root from which the inputted word is derived. Stemming can then help to standardize terms referring to the same concept. These algorithms are widely used in information retrieval systems and Web search engines, in addition to other systems such as: Machine translation, text clustering, text summarization, question answering, indexing, text mining, text classification... etc. Khoja stemmer is a standard Arabic stemmer, which has a number of flaws. Previous studies and this one show that Khoja stemmer is better than other two competitive ones evaluated in this study. The Khoja stemmer and the other two evaluated Arabic stemmers depend mainly in their work on (Patterns, Forms). Therefore the identification of the flaws leads to identification of missing Patterns not used by Khoja stemmer. So the enhancement to Khoja stemmer is restricted to adding missing patterns, and this leads to around 5% improvement to the accuracy of Khoja stemmer.","","Electronic:978-1-4799-2303-8; POD:978-1-4799-3676-2","10.1109/AEECT.2013.6716437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716437","Arabic;Information Retrieval;Root-Based Stemming;Stemming;Tokenization","Accuracy;Algorithm design and analysis;Computers;Conferences;Electrical engineering;Fault diagnosis;Standards","information retrieval;natural language processing","Khoja rule-based Arabic stemmer;Web search engines;flaws identification;information retrieval systems;missing pattern identification;stemming algorithms","","4","","16","","","3-5 Dec. 2013","","IEEE","IEEE Conference Publications"
"Query-Document-Dependent Fusion: A Case Study of Multimodal Music Retrieval","Z. Li; B. Zhang; Y. Yu; J. Shen; Y. Wang","School of Computing, National University of Singapore","IEEE Transactions on Multimedia","20131113","2013","15","8","1830","1842","In recent years, multimodal fusion has emerged as a promising technology for effective multimedia retrieval. Developing the optimal fusion strategy for different modalities (e.g., content, metadata) has been the subject of intensive research. Given a query, existing methods derive a unified fusion strategy for all documents with the underlying assumption that the relative significance of a modality remains the same across all documents. However, this assumption is often invalid. We thus propose a general multimodal fusion framework, query-document-dependent fusion (QDDF), which derives the optimal fusion strategy for each query-document pair via intelligent content analysis of both queries and documents. By investigating multimodal fusion strategies adaptive to both queries and documents, we demonstrate that existing multimodal fusion approaches are special cases of QDDF and propose two QDDF approaches to derive fusion strategies. The dual-phase QDDF explicitly derives and fuses query- and document-dependent weights, and the regression-based QDDF determines the fusion weight for a query-document pair via a regression model derived from training data. To evaluate the proposed approaches, comprehensive experiments have been conducted using a multimedia data set with around 17 K full songs and over 236 K social queries. Results indicate that the regression-based QDDF is superior in handling single-dimension queries. In comparison, the dual-phase QDDF outperforms existing approaches for most query types. We found that document-dependent weights are instrumental in enhancing multimedia fusion performance. In addition, efficiency analysis demonstrates the scalability of QDDF over large data sets.","1520-9210;15209210","","10.1109/TMM.2013.2280437","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6588600","Information retrieval;multimodal;query-document-dependent fusion","Accuracy;Feature extraction;Fuses;Information retrieval;Multimedia communication;Streaming media;Videos","information retrieval;multimedia systems;regression analysis;sensor fusion","QDDF approaches;document-dependent weights;dual-phase QDDF;intelligent content analysis;large data sets;metadata;multimedia data set;multimedia fusion performance;multimedia retrieval;multimodal fusion approaches;multimodal fusion framework;multimodal fusion strategies;multimodal music retrieval;optimal fusion strategy;query-dependent weights;query-document-dependent fusion;regression model;regression-based QDDF;social queries;training data","","1","","45","","20130905","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Aggregate discovery and retrieval of contents from authenticated interface","G. Varshneya; D. Kumar; R. Kumar; D. Singh","Information Technology, HST Mathura, Mathura, India","2013 Sixth International Conference on Contemporary Computing (IC3)","20130926","2013","","","399","403","Retrieval of privatized information through deep web signifies that data extraction from the private email accounts of the users such as Gmail, Yahoo, rediff etc. Now a days most of the people keep their private information in their accounts and every user have more than one account of different service provider. All these service providers do not have adequate internal searching facility so it is tough for the individual to search its privatized contents on their own different accounts in mess-up scenario. We have analyzed through literature survey there is no proper architecture available to solve this problem. The functioning of proposed framework is divided into two parts, the first part of the architecture is based on retrieving of privatized information from the different private domains after authenticating the user on host domains and the second part is based on user authentication to domain mapper. In order to solve the particular problem we introduced the method of privatized information retrieval in deep web scenario.","","Electronic:978-1-4799-0192-0; POD:978-1-4799-0191-3","10.1109/IC3.2013.6612228","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612228","Deep web;Email Search Architecture;private credentials;privatized information retrieval","Authentication;Crawlers;Databases;Search engines;Servers;Web sites","Internet;content-based retrieval;electronic mail;message authentication","aggregate discovery;authenticated interface;content retrieval;data extraction;deep Web;domain mapper;private email account;privatized information retrieval;user authentication","","0","","18","","","8-10 Aug. 2013","","IEEE","IEEE Conference Publications"
"ScatterBlogs2: Real-Time Monitoring of Microblog Messages through User-Guided Filtering","H. Bosch; D. Thom; F. Heimerl; E. Püttmann; S. Koch; R. Krüger; M. Wörner; T. Ertl","Visualization and Interactive Systems, University of Stuttgart","IEEE Transactions on Visualization and Computer Graphics","20131016","2013","19","12","2022","2031","The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.","1077-2626;10772626","","10.1109/TVCG.2013.186","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6634195","Blogs;Information retrieval;Labeling;Microblog analysis;Real-time systems;Social network services;Spatiotemporal phenomena;Twitter;filter construction;information visualization;live monitoring;query construction;social media monitoring;text analytics;text classification;visual analytics","Blogs;Information retrieval;Labeling;Real-time systems;Social network services;Spatiotemporal phenomena;Twitter","decision making;emergency management;information filtering;information filters;meta data;pattern classification;query processing;social networking (online);statistical distributions","ScatterBlogs2;Twitter;co-occurrence statistical distribution;decision making;emergency management scenarios;message retrieval;metadata restrictions;microblog feeds;microblog message real-time monitoring;microblog posts;query creation;situational awareness;supervised classification;task-tailored message filters;term statistical distribution;time-critical tasks;user-defined keyword queries;user-guided filtering","Algorithms;Blogging;Computer Graphics;Computer Systems;Information Storage and Retrieval;Reproducibility of Results;Sensitivity and Specificity;Social Media;Software;User-Computer Interface","30","","35","","","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"Leveraging historical co-change information for requirements traceability","N. Ali; F. Jaafar; A. E. Hassan","Sch. of Comput., Queen's Univ., Kingston, ON, Canada","2013 20th Working Conference on Reverse Engineering (WCRE)","20131121","2013","","","361","370","Requirements traceability (RT) links requirements to the corresponding source code entities, which implement them. Information Retrieval (IR) based RT links recovery approaches are often used to automatically recover RT links. However, such approaches exhibit low accuracy, in terms of precision, recall, and ranking. This paper presents an approach (CoChaIR), complementary to existing IR-based RT links recovery approaches. CoChaIR leverages historical co-change information of files to improve the accuracy of IR-based RT links recovery approaches. We evaluated the effectiveness of CoChaIR on three datasets, i.e., iTrust, Pooka, and SIP Communicator. We compared CoChaIR with two different IR-based RT links recovery approaches, i.e., vector space model and Jensen-Shannon divergence model. Our study results show that CoChaIR significantly improves precision and recall by up to 12.38% and 5.67% respectively; while decreasing the rank of true positive links by up to 48% and reducing false positive links by up to 44%.","1095-1350;10951350","Electronic:978-1-4799-2931-3; POD:978-1-4799-2932-0","10.1109/WCRE.2013.6671311","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671311","Traceability;co-change;information retrieval;requirements;source code","Accuracy;Calculators;Engines;History;Joining processes;Probability distribution;Vectors","information retrieval;software engineering","CoChaIR approach;IR based RT links recovery approaches;Jensen-Shannon divergence model;Pooka dataset;SIP Communicator dataset;false positive links;historical co-change information;iTrust dataset;information retrieval;precision;recall;requirements traceability;true positive links;vector space model","","1","","24","","","14-17 Oct. 2013","","IEEE","IEEE Conference Publications"
"Dataset Retrieval","S. R. Kunze; S. Auer","Dept. of Comput. Sci., Chemnitz Univ. of Technol., Chemnitz, Germany","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","1","8","Recently, a large number of dataset repositories, catalogs and portals are emerging in the science and government realms. Once a large number of datasets are published on such data portals, the question arises how to retrieve datasets satisfying an information need. In this paper, we present an approach for retrieving datasets according to user queries. We define dataset retrieval as a specialization of information retrieval. Instead of retrieving documents that are relevant to a certain information need, dataset retrieval describes the process of returning relevant RDF datasets. As with information retrieval, the term relevance cannot be clearly defined when using traditional methods like stemming. The inherent usage of RDF in these datasets enables a better way of retrieving relevant ones. We therefore propose an additional retrieval mechanism, which is inspired by facet search: dataset filtering. When querying, the entire set of available datasets is processed by a set of semantic filters each of which can unambiguously decide whether or not a given dataset is relevant to the query. The resulting set is then given back to the requester. We implemented and evaluated our approach in CKAN, which fuels publicdata.eu and is the most popular data portal worldwide.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693487","RDF;VoID;dataset filtering;dataset repository;dataset retrieval;information retrieval;similarity relation;vocabulary","Catalogs;Companies;Portals;Resource description framework;Semantics;Vocabulary","information filtering;information needs;query processing;relevance feedback","CKAN;RDF datasets;catalogs;data portals;dataset filtering;dataset repositories;dataset retrieval;facet search;information need;information retrieval;semantic filters;term relevance;user queries","","2","","16","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Contradiction detection between opinions: From a big data perspective","B. Vancea; A. Marchis; M. Dinsoreanu; R. Potolea","Department of Computer Science, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","2013 IEEE 9th International Conference on Intelligent Computer Communication and Processing (ICCP)","20131024","2013","","","259","266","This paper offers a solution to the problem of detecting contradictions among opinions on the same topic. The opinions are extracted from a large number of unstructured documents and stored in a structured format. Due to the increase in data available for analysis, we focus on providing a storage/retrieval and analysis solution suitable for managing large quantities of data while maintaining the speed and reliability present in smaller scale systems. Our approach consists in building a distributed system able to scale horizontally with the increase in input data without any significant performance decay. We represent opinions in a tuple based structured model, more suitable for retrieval and analysis. This approach allows us to formalize an algorithm for detecting contradictions between opinion tuples. Furthermore, we present a method for improving the recall of the system by using synonyms for the opinion target to expand the set of possible contradicting opinions. Our main focus is to optimize the structure of the opinion tuple to provide the best retrieval time and to allow for a simple, structured approach for detecting contradictions.","","Electronic:978-1-4799-1494-4; POD:978-1-4799-1492-0","10.1109/ICCP.2013.6646118","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646118","big data;contradiction detection;information retrieval;opinion;scalability","Bicycles;Data handling;Distributed databases;File systems;Indexes;Information management;Information retrieval","document handling;information retrieval","big data perspective;contradiction detection;distributed system;information retrieval;structured format;unstructured documents","","0","","12","","","5-7 Sept. 2013","","IEEE","IEEE Conference Publications"
"Exploiting structural relationships in audio music signals using Markov Logic Networks","H. Papadopoulos; G. Tzanetakis","Laboratoire des Signaux et Syst&#x00E8;mes, UMR 8506, CNRS-SUPELEC-Univ. Paris-Sud, France","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","1","5","We propose an innovative approach for music description at several time-scales in a single unified formalism. More specifically, chord information at the analysis-frame level and global semantic structure are integrated in an elegant and flexible model. Using Markov Logic Networks (MLNs) low-level signal features are encoded with high-level information expressed by logical rules, without the need of a transcription step. Our results demonstrate the potential of MLNs for music analysis as they can express both structured relational knowledge through logic as well as uncertainty through probabilities.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637597","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637597","Chord Detection;Markov Logic Networks;Music Information Retrieval;Structure Analysis","Estimation;Hidden Markov models;Markov processes;Multiple signal classification;Music;Probabilistic logic;Semantics","Markov processes;audio coding;music","MLNs;Markov logic networks;analysis-frame level;audio music signals;chord information;global semantic structure;high-level information;logical rules;low-level signal features;music analysis;music description;probability;single unified formalism;structural relationships;structured relational knowledge","","2","","42","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Why do students prefer search engines over academic databases","A. S. Öğrenci","Fac. of Eng. & Natural Sci., Kadir Has Univ., Istanbul, Turkey","2013 12th International Conference on Information Technology Based Higher Education and Training (ITHET)","20131121","2013","","","1","3","Students' preference of search engines in online information retrieval has been a problem for several reasons. This work tries to share our experience about the reasons why they do not utilize academic databases. Past experiences, seeking convenience, and the availability of automatic translation in search engines seem to be the main reasons. Several suggestions are offered that aim to increase the level of competency of students in utilizing academic databases. Furthermore, students should also be informed adequately about the potential threats in using search engines as their only online information source.","","Electronic:978-1-4799-0086-2; POD:978-1-4799-0084-8; USB:978-1-4799-0085-5","10.1109/ITHET.2013.6671054","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671054","academic databases;information retrieval;online information seeking behavior;search engines","Databases;Educational institutions;Google;Libraries;Materials;Search engines","database management systems;information retrieval;language translation;search engines","academic database;automatic translation;online information retrieval;online information source;search engine","","0","","8","","","10-12 Oct. 2013","","IEEE","IEEE Conference Publications"
"Dynamic ontology management for bioinformatics domain","A. A. Shah; F. Maqbool; M. U. G. Khan","Dept. of Comput. Sci. & Eng., Univ. of Eng. & Technol., Lahore, Pakistan","2013 International Conference on Open Source Systems and Technologies","20140127","2013","","","1","7","Bioinformatics is an emerging field as an interdisciplinary field of computer science and molecular biology. The growth rate of data and information of this domain is exponential, and the techniques and strategies adopted few years earlier to handle data of this domain are considered obsolete now. Therefore, management of data of bioinformatics domain is facing serious problems of data management due to huge volume and high rate of growth of web resources of biological data. One of these problems is to store the data and information on semantic web resources of biological data, and then retrieve only the relevant data and information from these resources. Therefore, to achieve this objective, we need new technique to design and maintain ontologies of the web recourses in different way. In our opinion, we need a technique that can maintain dynamically ontologies. For this purpose, we introduce the concept of dynamic ontology which can control irrelevancy in retrieval of data stores by adding new index terms of the domain.","","CD-ROM:978-1-4799-2047-1; Electronic:978-1-4799-2046-4; POD:978-1-4799-2048-8","10.1109/ICOSST.2013.6720596","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720596","bioinformatics;directory;dynamic;information retrieval;ontology","Biology;Computer architecture;Indexes;Ontologies;Resource description framework;Service-oriented architecture","bioinformatics;data handling;indexing;information retrieval;ontologies (artificial intelligence);semantic Web","bioinformatics domain;biological data;computer science;data growth rate;data handling;data management;data storage retrieval;dynamic ontology management;index terms;information growth rate;molecular biology;semantic Web resources","","0","","20","","","16-18 Dec. 2013","","IEEE","IEEE Conference Publications"
"Context filtering process for mobile web search","S. Missaoui; R. Faiz","LARODEC, IHEC University of Carthage, Carthage Presidency, Tunisia","2013 ACS International Conference on Computer Systems and Applications (AICCSA)","20131003","2013","","","1","8","The increase of mobile computing systems and the growth of mobile Web highly influence the needs for personalized mobile browsers. To this aim, mobile applications take benefits from context-aware computing by adapting search process through user's context information (parameters). However, Contextualized Mobile Information Retrieval still remains a challenging problem. This last is to identify contextual parameters that improve search effectiveness and should therefore be in the user's focus. We investigate in this paper the problem of filtering mobile user's context and we propose a Language model for relevant context fields recognition. The proposed model interprets a context field relevance as a metric measure and estimates it using different features. In particular, we propose to filter as much as possible the mobile user's context to emanate efficient information that help in a personalization access to information.person- alized mobile browsers.","2161-5322;21615322","Electronic:978-1-4799-0792-2; POD:978-1-4799-0791-5; USB:978-1-4799-0790-8","10.1109/AICCSA.2013.6616506","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616506","Context awareness;Context filtering process;Mobile information retrieval;Relevance metric measure","Computational modeling;Context;Context modeling;Filtering;Measurement;Mobile communication;Web search","information filtering;mobile computing;natural language processing;online front-ends;relevance feedback","context field recognition;context field relevance;context-aware computing;contextualized mobile information retrieval;feature estimation;language model;metric measure;mobile Web search;mobile computing systems;mobile user context filtering process;personalized information access;personalized mobile browsers;search effectiveness improvement;user context information;user context parameters","","0","","27","","","27-30 May 2013","","IEEE","IEEE Conference Publications"
"Towards an intelligent information research system based on the human behavior: Recognition of user emotional state","M. Neji; M. Ben Ammar; A. Wali; A. M. Alimi","Research Groups on Intelligent Machines, University of Sfax, National Engineering School of Sfax (ENIS), BP1173, Sfax, 3038, Tunisia","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","20130926","2013","","","371","376","Our works deals with the problem of Information Retrieval System (IRS) that integrates the human behaviour. This system must be able to recognize the degree of satisfaction of the user of the result found through its facial expression, its physiological state, its gestures and its voice. For this, we propose in this paper an algorithm for recognizing the emotional state of a user during a search session in order to issue the relevant documents that he needs. We present also, the architecture agent of the envisaged system.","","Electronic:978-1-4799-0174-6; POD:978-1-4799-0173-9","10.1109/ICIS.2013.6607869","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607869","Emotional agent;Information Retrieval System;human behaviour","Erbium;Hip","emotion recognition;human factors;information retrieval systems","IRS;architecture agent;facial expression;human behavior;information retrieval system;intelligent information research system;physiological state;search session;user emotional state recognition","","1","","20","","","16-20 June 2013","","IEEE","IEEE Conference Publications"
"Using code ownership to improve IR-based Traceability Link Recovery","D. Diaz; G. Bavota; A. Marcus; R. Oliveto; S. Takahashi; A. De Lucia","Univ. de Los Andes, Bogota, Colombia","2013 21st International Conference on Program Comprehension (ICPC)","20130930","2013","","","123","132","Information Retrieval (IR) techniques have gained wide-spread acceptance as a method for automating traceability recovery. These techniques recover links between software artifacts based on their textual similarity, i.e., the higher the similarity, the higher the likelihood that there is a link between the two artifacts. A common problem with all IR-based techniques is filtering out noise from the list of candidate links, in order to improve the recovery accuracy. Indeed, software artifacts may be related in many ways and the textual information captures only one aspect of their relationships. In this paper we propose to leverage code ownership information to capture relationships between source code artifacts for improving the recovery of traceability links between documentation and source code. Specifically, we extract the author of each source code component and for each author we identify the “context” she worked on. Thus, for a given query from the external documentation we compute the similarity between it and the context of the authors. When retrieving classes that relate to a specific query using a standard IR-based approach we reward all the classes developed by the authors having their context most similar to the query, by boosting their similarity to the query. The proposed approach, named TYRION (TraceabilitY link Recovery using Information retrieval and code OwNership), has been instantiated for the recovery of traceability links between use cases and Java classes of two software systems. The results indicate that code ownership information can be used to improve the accuracy of an IR-based traceability link recovery technique.","1092-8138;10928138","Electronic:978-1-4673-3092-3; POD:978-1-4673-3090-9","10.1109/ICPC.2013.6613840","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613840","Code Ownership;Empirical Studies;Information Retrieval;Traceability Link Recovery","Accuracy;Context;Java;Software systems;Standards;Vocabulary","Java;information retrieval;software maintenance;system documentation;text analysis","IR technique;IR-based traceability link recovery improvement;Java classes;TYRION;TraceabilitY link Recovery using Information retrieval and code OwNership;class retrieval;code ownership information;documentation;noise filtering;software artifacts;software system;source code component;textual information;textual similarity","","2","","40","","","20-21 May 2013","","IEEE","IEEE Conference Publications"
"A tracelab-based solution for identifying traceability links using LSI","N. Alhindawi; O. Meqdadi; B. Bartman; J. I. Maletic","Department of Computer Science, Kent State University, Ohio 44242 USA","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","20131007","2013","","","79","82","An information retrieval technique, latent semantic indexing (LSI), is used to automatically identify traceability links from system documentation to program source code. The experiment is performed in the TraceLab framework. The solution provides templates and components for building and querying LSI space and datasets (corpora) that can be used as inputs for these components. The proposed solution is evaluated on traceability links already discovered by mining adaptive commits of the open source system KDE/Koffice. The results show that the approach can identify of traceability links with high precision using TraceLab components.","2157-2186;21572186","Electronic:978-1-4799-0495-2; POD:978-1-4799-0496-9","10.1109/TEFSE.2013.6620159","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620159","Adaptive Changes;Information Retrieval;TraceLab;Traceability links","Adaptive systems;Data mining;Dictionaries;Documentation;Educational institutions;Large scale integration;Vocabulary","data mining;indexing;information retrieval;program diagnostics;public domain software;source coding;system documentation","KDE;Koffice;LSI;TraceLab-based solution;adaptive commit mining;automatic traceability link identification;information retrieval technique;latent semantic indexing;open source system;program source code;system documentation","","2","","12","","","19-19 May 2013","","IEEE","IEEE Conference Publications"
"Effectual extraction of Data Relations from unstructured data","S. Geetha; G. S. Anandha Mala","JNTU Hyderabad, Hyderabad, India","IET Chennai 3rd International on Sustainable Energy and Intelligent Systems (SEISCON 2012)","20140123","2012","","","1","4","Unstructured data intend to any data that has no distinctive structure. This paper aims to extract the structured data from unstructured data using Parts Of Speech (POS), analyzing this data syntactically, organize the data into entities, rules, associations and facts. Prepare the data into structured way in the form of data tables. The textual data in documents to be transformed into text file which can be transferred into database. Parts of Speech categorize the data into entities, actions and build the relations among these entities and actions. Due to complexity involved in extracting, mining and structuring the data, research is considered for textual data either in form of documents or web pages. The structured information can be used in decision support systems or serve the purpose intended for the process. This approach is to extract the key information from scattered unstructured data exist across database. In this paper, an application “News retrieval system” has been proposed as model which takes out the news from various web pages and processes them on the basis of page ranking and display on a single web page. The use of regular expressions is to realize the required patterns of the data and to convert the web pages into plain text. This plain text analyzed for entities, facts, relationships, synonyms and verb phrases. Data dictionary is used to realize English words. Extracted data is stored in database in the form of tables. Database models can be constructed using constructive information by inference rules or actionable intelligence. The structured information can be used for the purposes signifi ed in order to achieve improved, effective information retrieval system with this approach.","","Electronic:978-1-84919-797-7","10.1049/cp.2012.2190","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6719096","Extracted data model;Information retrieval;Unstructured data","","Internet;data analysis;data mining;database management systems;electronic publishing;inference mechanisms;information retrieval;text analysis","English words;POS;Web pages;actionable intelligence;constructive information;data analysis;data dictionary;data mining;data relation extraction;data structuring;data tables;database model;decision support systems;inference rules;information retrieval system;news retrieval system;page ranking;parts of speech;structured data extraction;structured information;synonyms;text file;textual data;unstructured data;verb phrases","","1","","","","","27-29 Dec. 2012","","IET","IET Conference Publications"
"Proposal of a Matching System for Companies and Researchers Using Patents and Scientific Papers","Y. Yamada; T. Tansho; S. Hirokawa","Interdiscipl. Grad. Sch. of Sci. & Eng., Shimane Univ., Matsue, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","397","398","For open innovation and industry-university cooperation, it is important for companies and university researchers to find suitable partners to ensure collaborative development. However, it is difficult to find partners via traditional information retrieval systems since they only output a list of patent and scientific paper related to an input query. This paper proposes a system that matches companies with university researchers, and vice versa, by using patents and scientific papers.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.55","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630381","industry-university cooperation;information retrieval;open innovation;text mining","Collaboration;Companies;Databases;Educational institutions;Information retrieval;Patents;Technological innovation","groupware;innovation management;query processing","collaborative development;industry-university cooperation;information retrieval systems;input query;matching system;open innovation;patents papers;scientific papers;university researchers","","1","","2","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Research on engineering change knowledge representation and retrieval technology based on ontology","Z. Wang; Y. Wan","Key Lab. of High Efficiency & Clean Manuf., Shandong Univ., Jinan, China","2013 19th International Conference on Automation and Computing","20131114","2013","","","1","5","It is an effective approach to achieve solutions for new engineering changes by retrieving and reusing existing cases. An ontology-based retrieving method for engineering change was proposed in manufacturing enterprises. Three steps of change case retrieving were proposed on the basis of eigenvector representation of change case and extension of retrieval requirements. Then, retrieving requirements and similarity calculation method was studied. Finally, the algorithm was validated with a change case of tensile testing machine components.","","Electronic:978-1-908549-08-2; POD:978-1-4799-1236-0","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662020","engineering change;information retrieval;ontology","Automobiles;Manufacturing;Ontologies;Organizations;Standards organizations;Testing","computer integrated manufacturing;design engineering;eigenvalues and eigenfunctions;manufacturing data processing;ontologies (artificial intelligence);product design","change case retrieving;eigenvector representation;engineering change knowledge representation;manufacturing enterprise;ontology-based retrieving method;retrieval technology;tensile testing machine component","","0","","15","","","13-14 Sept. 2013","","IEEE","IEEE Conference Publications"
"An Empirical Performance Evaluation of Relational Keyword Search Techniques","J. Coffman; A. C. Weaver","Johns Hopkins Univ. Appl. Phys. Lab., Laurel, MD, USA","IEEE Transactions on Knowledge and Data Engineering","20131125","2014","26","1","30","42","Extending the keyword search paradigm to relational data has been an active area of research within the database and IR community during the past decade. Many approaches have been proposed, but despite numerous publications, there remains a severe lack of standardization for the evaluation of proposed search techniques. Lack of standardization has resulted in contradictory results from different evaluations, and the numerous discrepancies muddle what advantages are proffered by different approaches. In this paper, we present the most extensive empirical performance evaluation of relational keyword search techniques to appear to date in the literature. Our results indicate that many existing search techniques do not provide acceptable performance for realistic retrieval tasks. In particular, memory consumption precludes many search techniques from scaling beyond small data sets with tens of thousands of vertices. We also explore the relationship between execution time and factors varied in previous evaluations; our analysis indicates that most of these factors have relatively little impact on performance. In summary, our work confirms previous claims regarding the unacceptable performance of these search techniques and underscores the need for standardization in evaluations--standardization exemplified by the IR community.","1041-4347;10414347","","10.1109/TKDE.2012.228","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6361392","Keyword search;empirical evaluation;information retrieval;relational database","Benchmark testing;Databases;Electronic publishing;Encyclopedias;Internet;Keyword search","information retrieval;relational databases","IR community;empirical performance evaluation;information retrieval;keyword search paradigm;memory consumption;relational data;relational keyword search techniques;standardization","","11","","42","","20121126","Jan. 2014","","IEEE","IEEE Journals & Magazines"
"Joint search in text and concept spaces for EMR-based cohort identification","D. Zhu; B. Carterette","Dept. of Comput. & Inf. Sci., Univ. of Delaware, Newark, DE, USA","2013 IEEE International Conference on Bioinformatics and Biomedicine","20140206","2013","","","597","601","In this paper, we present a novel electronic health records (EMR) retrieval system that helps to reduce manual in work in cohort identification. Our system effectively uses medical domain knowledge to enhance retrieval performance. In particular, our system first maps raw text of EMR and EMR queries from the text space to concept unique identifiers in the medical concept space, and then searches in the two spaces independently, and finally weights and combines search results adaptively based on several heuristics. Our cross-validation results show that the proposed adaptive weighting mechanism is significantly better than the fixed-weighting mechanism, and also that the joint search is more effective than text-based search or concept-based search. In addition, we introduce an effective approach for external expansion in the concept space.","","Electronic:978-1-4799-1309-1; POD:978-1-4799-1311-4","10.1109/BIBM.2013.6732565","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6732565","cohort identification;electronic medical records;information retrieval;retrieval models","Auditory system;Bioinformatics;Genomics;Indexes;Joints;Merging;Unified modeling language","bioinformatics;electronic health records;full-text databases;identification;information retrieval system evaluation;search engines","EMR-based cohort identification;adaptive weighting mechanism;concept-based search;cross-validation;electronic health records;fixed-weighting mechanism;information retrieval system;medical concept space;medical domain knowledge;text-based search","","0","","9","","","18-21 Dec. 2013","","IEEE","IEEE Conference Publications"
"GeoEvents - An Interactive Tool to Analyze and Visualize Spatial Information from the Social Web","C. Sabty; M. Memmel; S. Abdennadher","German Univ. in Cairo, Cairo, Egypt","2013 International Conference on Social Computing","20140102","2013","","","803","808","A growing number of social media services like Flickr and Twitter allows for the association of locations with digital resources such as photos or text messages. This work tackles the problem of exploiting the abundance of such kind of data, by designing and implementing an application that analyzes and visualizes spatial information from different social media services. The application provides two different data retrieval modes, the scenarios and the exploring mode. The scenarios mode accesses information that was already harvested into a local storage, while the exploring mode retrieves information on the fly using the respective APIs. In both modes, information about the returned resources is aggregated on a map and displayed using different visualization means. In addition, the retrieved information is analyzed using three different analysis techniques.","","Electronic:978-0-7695-5137-1; POD:978-1-4799-1519-4","10.1109/SocialCom.2013.120","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693417","Social media;geospatial data;information extraction;information retrieval","Data mining;Databases;Media;Reliability;Standards;Twitter","data visualisation;geographic information systems;information retrieval;interactive systems;social networking (online)","API;Flickr;GeoEvents;Twitter;data retrieval modes;digital resources;exploring mode;information access;information retrieval;interactive tool;local storage;locations;photos;scenarios mode;social Web;social media services;spatial information visualization;text messages","","0","","14","","","8-14 Sept. 2013","","IEEE","IEEE Conference Publications"
"Mining A change history to quickly identify bug locations : A case study of the Eclipse project","C. Tantithamthavorn; R. Teekavanich; A. Ihara; K. i. Matsumoto","Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol., Nara, Japan","2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","20131219","2013","","","108","113","In this study, we proposed an approach to mine a change history to improve the bug localization performance. The key idea is that a recently fixed file may be fixed in the near future. We used a combination of textual feature and mining the change history to recommend source code files that are likely to be fixed for a given bug report. First, we adopted the Vector Space Model (VSM) to find relevant source code files that are textually similar to the bug report. Second, we analyzed the change history to identify previously fixed files. We then estimated the fault proneness of these files. Finally, we combined the two scores, from textual similarity and fault proneness, for every source code file. We then recommend developers examine source code files with higher scores. We evaluated our approach based on 1,212 bug reports from the Eclipse Platform and Eclipse JDT. The experimental results show that our proposed approach can improve the bug localization performance and effectively identify buggy files.","","Electronic:978-1-4799-2552-0; POD:978-1-4799-2553-7","10.1109/ISSREW.2013.6688888","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688888","Bug Localization;Information Retrieval;Mining Change History;Software Debugging","Accuracy;Computer bugs;Equations;History;Indexes;Mathematical model;Vectors","data mining;information retrieval;program debugging;project management;source code (software)","Eclipse JDT;Eclipse project;VSM;bug localization performance improvement;bug location identification;bug report;change history mining;file fault proneness estimation;fixed-files;source code file recommendation;textual similarity feature;vector space model","","2","","13","","","4-7 Nov. 2013","","IEEE","IEEE Conference Publications"
"Case-Based Support System for Collaborative Tourism Planning","M. Samejima","Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan","2013 Second IIAI International Conference on Advanced Applied Informatics","20131015","2013","","","21","25","Recently, the local community frequently addresses the tourism planning through their collaboration. The process of the collaborative tourism planning consists of learning and consensus building. Without the knowledge of the tourism planning, it is difficult to begin the collaborative tourism planning. The practical cases of tourism planning are available online and include useful knowledge for other tourism planning. So, I propose the support system for the collaborative tourism planning by using the cases that are collected via Internet. The case-based support system provides two functions: a case retrieval function and a key point extraction function. First the cases are retrieved by the related words to the tourism resources and the words to indicate the situation of the tourist site. Next the cases are clustered and the labels of the clusters are extracted as the key points. Through the discussion of the case-based support system, I describe the issues to realize the support system.","","Electronic:978-0-7695-5071-8; POD:978-1-4799-2136-2","10.1109/IIAI-AAI.2013.24","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630311","Case-based support;Collaborative tourism planning;Information retrieval","Buildings;Collaboration;Communities;Data mining;Internet;Local government;Planning","information retrieval;pattern clustering;planning;travel industry","Internet;case retrieval function;case-based support system;collaborative tourism planning;consensus building;key point extraction function","","0","","13","","","Aug. 31 2013-Sept. 4 2013","","IEEE","IEEE Conference Publications"
"Singing voice timbre classification of Chinese popular music","C. Y. Sha; Y. H. Yang; Y. C. Lin; H. H. Chen","National Taiwan University, Taiwan","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","734","738","Singing voice plays an important role in the listening experience of music. In this paper, we propose to classify popular music by the timbre quality of the singing voice. Specifically, we adopt six singing voice timbre classes as the taxonomy and build a new data set, KKTIC, that contains the expert annotations of 387 Chinese popular songs. To build an automatic classifier, we resort to signal processing and machine learning techniques and extract a number of singing voice-related features such as vibrato and harmonic-to-noise ratio. We also propose the use of vocal segment detection and singing voice separation as preprocessing steps. Our evaluation identifies the relevant acoustic features and validates the importance of these preprocessing steps. The accuracy in timbre classification reaches 79.84% in a five-fold stratified cross validation.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637745","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637745","Singing voice timbre;music information retrieval;singing voice separation;vocal segment detection","Accuracy;Feature extraction;Instruments;Signal processing;Timbre","music;signal classification;speech synthesis","Chinese popular music;KKTIC;automatic classifier;five-fold stratified cross validation;harmonic-to-noise ratio;machine learning techniques;signal processing;singing voice separation;singing voice timbre classification;singing voice-related features;timbre quality;vibrato;vocal segment detection","","3","","23","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Effective pseudo-relevance feedback for language modeling in speech recognition","B. Chen; Y. W. Chen; K. Y. Chen; E. E. Jan","Nat. Taiwan Normal Univ., Taipei, Taiwan","2013 IEEE Workshop on Automatic Speech Recognition and Understanding","20140109","2013","","","13","18","A part and parcel of any automatic speech recognition (ASR) system is language modeling (LM), which helps to constrain the acoustic analysis, guide the search through multiple candidate word strings, and quantify the acceptability of the final output hypothesis given an input utterance. Despite the fact that the n-gram model remains the predominant one, a number of novel and ingenious LM methods have been developed to complement or be used in place of the n-gram model. A more recent line of research is to leverage information cues gleaned from pseudo-relevance feedback (PRF) to derive an utterance-regularized language model for complementing the n-gram model. This paper presents a continuation of this general line of research and its main contribution is two-fold. First, we explore an alternative and more efficient formulation to construct such an utterance-regularized language model for ASR. Second, the utilities of various utterance-regularized language models are analyzed and compared extensively. Empirical experiments on a large vocabulary continuous speech recognition (LVCSR) task demonstrate that our proposed language models can offer substantial improvements over the baseline n-gram system, and achieve performance competitive to, or better than, some state-of-the-art language models.","","Electronic:978-1-4799-2756-2; POD:978-1-4799-2757-9; USB:978-1-4799-2755-5","10.1109/ASRU.2013.6707698","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6707698","Speech recognition;information retrieval;language modeling;pseudo-relevance feedback;relevance","Adaptation models;Analytical models;DH-HEMTs;History;Mathematical model;Speech recognition;Training","relevance feedback;speech recognition","ASR system;LM;LVCSR;PRF;acoustic analysis;automatic speech recognition;language modeling;large vocabulary continuous speech recognition;n-gram model;pseudo-relevance feedback;utterance-regularized language model","","0","","33","","","8-12 Dec. 2013","","IEEE","IEEE Conference Publications"
"Semantic Entity Search Diversification","T. Ruotsalo; M. Frosterus","Helsinki Inst. for Inf. Technol. HIIT, Aalto Univ., Espoo, Finland","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","32","39","We present an approach to diversify entity search by utilizing semantics present and inferred from the initial entity search results. Our approach makes use of ontologies and independent component analysis of the entity descriptions to reveal direct and latent semantic connections between the entities present in the initial search results. The semantic connections are then used to sample a set of diverse entities. We empirically demonstrate the performance of our approach through retrieval experiments that use a real-world dataset composed from four entity databases. The results indicate that our approach significantly improves both diversity and effectiveness of entity search.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693491","Semantic search;diversification;information retrieval","Cognition;Eigenvalues and eigenfunctions;Indexing;Information retrieval;Semantics;Standards;Vectors","database management systems;ontologies (artificial intelligence);query formulation","component analysis;direct semantic connections;diverse entities;entity databases;entity descriptions;latent semantic connections;ontologies;real-world dataset;retrieval experiments;semantic entity search diversification","","0","","26","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Using fuzzy logic to estimate user interests in multiscreen viewing situation","Y. Maki; M. Motegi; Y. Takashima; T. Kobayashi; T. Yamada","NTT Service Evolution Laboratories, Nippon Telegraph and Telephone Corporation, Yokosuka, Japan","2013 IEEE Third International Conference on Consumer Electronics ¿¿ Berlin (ICCE-Berlin)","20140102","2013","","","1","2","This paper describes a method to extract user interest keywords for user profiling applicable to various recommendation systems from browsed web pages during multiscreen viewing, i.e. watching TV while browsing web pages related to the TV program on smartphones or Tablet PCs. Our method extracts keywords from browsed web pages and computes interest weights, which is based on the user's natural behaviors of “browsed”, “bookmarked; like social bookmarking” or “no reaction; user did not browse it although it was shown”. Ranking the keywords in order of interest weights allows us to estimate the interest keywords. Experiments show that our method has better potential in filtering out non-useful keywords and thus is superior to the basic TFIDF approach.","2166-6814;21666814","Electronic:978-1-4799-1412-8; POD:978-1-4799-1410-4","10.1109/ICCE-Berlin.2013.6697992","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6697992","Fuzzy logic;Information Retrieval;Multiscreen;User profiling","Feature extraction;Fuzzy logic;Smart phones;TV;Tablet computers;Vectors;Web pages","fuzzy logic;information retrieval;mobile television;online front-ends;smart phones","TV program;Tablet PC;browsed Web pages;fuzzy logic;multiscreen viewing situation;non-useful keyword;smartphones;social bookmarking;user interest keyword","","0","","5","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Supervised Method to Enhance Vocabulary with the Creation of Domain Specific Lexica","P. Fernandes; L. O. C. Furquim; L. Lopes","Comput. Sci. Dept. - FACIN, PUCRS Univ., Porto Alegre, Brazil","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","3","","139","142","This paper proposes a method to enhance lexica by processing domain specific corpora. The proposed method relies on the identification of the more relevant unknown terms in each domain corpus. The innovative points of the proposed approach is to automatically detect unknown terms using MTMDD technology to handle lexical structures, and to automatically rank and identify domain specific terms using gini and tf-dcf indices. The proposed method is experimented in six corpora in order to illustrate its benefits.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.168","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690713","information retrieval;natural language processing;term extraction","Computer science;Context;Dictionaries;Indexes;Ontologies;Standards;Vocabulary","computational linguistics;indexing;vocabulary","MTMDD technology;domain corpus;domain specific corpora;domain specific lexica;domain specific terms;enhance vocabulary;gini indix;lexical structures;supervised method;tf-dcf index","","1","","15","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"BirdFinder: Web search and audio search extension","C. Restrepo-Arango; C. Jiménez-Guarín","Department of Systems and Computing Engineering, Universidad de los Andes, Bogot&#x00E1;, Colombia","2013 8th Computing Colombian Conference (8CCC)","20131021","2013","","","1","6","BirdFinder is a bird search engine that applies concepts of information retrieval and Web 2.0 to allow nonexpert users to access information using text and audio search mechanisms. It integrates audio, images and identification characteristics related to the ornithology domain. Content is stored in a highly scalable NoSQL repository. The main contribution of this work is a software architecture and two complementary search mechanisms to access non-structured content. This paper presents an evaluation of the application using information retrieval relevance measures to evaluate the search performance. Evaluation shows a good performance for both search engines providing ground for further research.","","Electronic:978-1-4799-1056-4; POD:978-1-4799-1055-7","10.1109/ColombianCC.2013.6637519","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637519","Audio Search;Bird Information;HBase;Information Retrieval;NoSQL;Non-Structured Content;Web Search;Web2.0","Birds;Data models;Feature extraction;Indexes;Libraries;Search engines","Internet;information retrieval;search engines;software architecture","BirdFinder;NoSQL repository;Web 2.0;Web search extension;audio search extension;bird search engine;complementary search mechanisms;identification characteristics;images characteristics;information retrieval relevance measures;nonstructured content;ornithology domain;search performance evaluation;software architecture","","0","","23","","","21-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Local alignment for query by humming","Q. Wang; Z. Guo; G. Liu; C. Li; J. Guo","Pattern Recognition and Intelligent System Laboratory, Beijing University of Posts and Telecommunications, China","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","3711","3715","Query by humming (QBH) allows users to retrieve songs by humming a clip. In the previous work, the query has been regarded as a fragment of the music, so the task of QBH is considered to find a subsequence, which is most similar to the whole query, from the database. Taking into account humming errors, especially at the beginning or ending of the query, we assume that only part of the query is a subsequence of the music. Based on this assumption, we propose a local alignment framework which searches for the best match common subsequence between the query and database music. To verify the effectiveness of local alignment, two popular match algorithms, i.e. Linear Scaling and Dynamic Time Warping, are extended to identify the common subsequence. Experimental results on the 2010 MIREX-QBH corpus show that the new algorithms improve the retrieval accuracy significantly.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6638351","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638351","Query by humming;dynamic time warping;linear scaling;local alignment;music information retrieval","Accuracy;Acoustics;Conferences;Heuristic algorithms;Indexes;Multimedia communication","audio signal processing;music;query formulation","2010 MIREX-QBH corpus;dynamic time warping match algorithm;humming error;linear scaling match algorithm;local alignment framework;query by humming;query local alignment","","3","","21","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Context Dependent Bag of words generation","S. A. Jadhav; D. V. L. N. Somayajulu; S. Nagesh Bhattu; R. B. V. Subramanyam; P. Suresh","Department of Computer Science and Engineering, NIT Warangal-506004, India","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20131021","2013","","","1526","1531","Query spelling correction is a crucial component in modern text mining systems such as Question-answering systems and Sentiment Analysis systems where noise can affect the query matching score. In many existing query matching systems Bag of Words (BoW) generation method is used to generate candidates for noisy words. But in these systems candidate generation do not depend upon context of a query sentence. BoW count for each noisy word may vary and selecting correct candidates from such list is not easy and may result in wrong selection. With our context dependent BoW generation method very few but highly probable candidates are generated which are easy for look up and process of query spelling correction would be easier and efficient.","","Electronic:978-1-4673-6217-7; POD:978-1-4799-1664-1","10.1109/ICACCI.2013.6637406","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637406","faq retrieval;information retrieval;nlp;noise removal;question answering;sentiment analysis;spelling correction;text mining","Accuracy;Context;Dictionaries;Hidden Markov models;Mobile communication;Noise;Noise measurement","data mining;query processing;text analysis","BoW generation method;context dependent bag-of-words generation;noisy words candidate generation;query matching score;query sentence;query spelling correction;question-answering systems;sentiment analysis systems;text mining systems","","0","","36","","","22-25 Aug. 2013","","IEEE","IEEE Conference Publications"
"The Architecture of ProMe Instant Question Answering System","G. Zhang; T. Jiang; R. Bie; X. Liu; Z. Wang; J. Rao","Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China","2013 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery","20131219","2013","","","237","242","An instant question answering system ProMe is proposed in this paper which sets automated question answering system as prototype and combines with the search engines and instant messenger. For any question queried by a user, ProMe looks for the answers in the flow of three methods. The first one is to match questions in FAQ database with TF-IDF (Term Frequency-Inverse Document Frequency) algorithm. If still not found, submit it to some search engines and get the answers by extracting information from the web pages returned by the search engine. If the answer is not found or not satisfied after evaluation, it will be sent to other users who are probably interested in it through instant messengers, and get the final answers. Experimental results show that ProMe greatly improves the effectiveness of the automated question answering systems. It can be applied for information searching to provide real-time and personalized information search services for mobile Internet users.","","Electronic:978-0-7695-5106-7; POD:978-1-4799-1327-5","10.1109/CyberC.2013.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685687","FAQ;TF-IDF;information retrieval;question answering system;similarity calculation","Algorithm design and analysis;Data mining;HTML;Knowledge discovery;Search engines;Web pages","Internet;document handling;electronic messaging;mobile computing;question answering (information retrieval);search engines","FAQ database;ProMe instant question answering system;TF-IDF algorithm;Web pages;automated question answering system;information extraction;instant messenger;mobile Internet users;personalized information search services;question matching;search engines;term frequency-inverse document frequency algorithm","","0","","13","","","10-12 Oct. 2013","","IEEE","IEEE Conference Publications"
"A Replicated Comparative Study of Source Code Authorship Attribution","M. F. Tennyson","Dept. of Comput. Sci. & Inf. Syst., Bradley Univ., Peoria, IL, USA","2013 3rd International Workshop on Replication in Empirical Software Engineering Research","20131114","2013","","","76","83","Source code authorship attribution is, simply, the task of deciding who wrote a piece of software given its source code. Applications include software forensics, plagiarism detection, and determining software ownership. Several methods of source code authorship attribution have been proposed in the past. Based on the only known controlled, comprehensive comparative study of these methods, the two most effective methods are the Burrows method and the SCAP method. This paper presents a partial replication of that comparative study. Specifically, it only compares the two most effective methods (Burrows and SCAP). This paper also includes a slight extension of that study: the original comparative study only considered anonymized data, while the replicated study considers both anonymized and non-anonymized data. The original comparative study indicated that the Burrows method outperformed all other methods - including the SCAP method - by a considerable margin. However, the results of the replicated study indicate that the SCAP method outperforms the Burrows method by a small margin when using anonymized data and by a large margin when using non-anonymized data.","","Electronic:978-0-7695-5121-0; POD:978-1-4799-2799-9","10.1109/RESER.2013.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664734","authorship attribution;information retrieval;plagiarism detection;software forensics","Forensics;Java;Open source software;Plagiarism;Programming","authoring systems;digital forensics","Burrows method;SCAP method;anonymized data;nonanonymized data;partial replicated comparative study;plagiarism detection;software forensics;software ownership;source code author profile;source code authorship attribution","","0","","17","","","9-9 Oct. 2013","","IEEE","IEEE Conference Publications"
"Generalized and lightweight algorithms for automated web forum content extraction","W. Y. Lim; V. Raja; V. L. L. Thing","Cybercrime & Security Intell. Dept., Inst. for Infocomm Res., Singapore, Singapore","2013 IEEE International Conference on Computational Intelligence and Computing Research","20140127","2013","","","1","8","As online forums contain a vast amount of information that can aid in the early detection of fraud and extremist activities, accurate and efficient information extraction from forum sites is very important. In this paper, we discuss the limitations of existing works in the extraction of information from generic web sites and forum sites. We also identify the need for better suited, generalized and lightweight algorithms to carry out a more accurate and efficient information extraction while eliminating noisy data from forum sites. In this paper, we propose three generalized and lightweight algorithms to carry out accurate thread and post content extraction from web forums. We evaluate our algorithms based on two strict criteria and to the granularity of the (DOM tree) node level correctness. We consider a thread or post as successfully extracted by our algorithms only if (i) all the contents in its text and anchor nodes are extracted correctly, and (ii) each content node is grouped correctly according to its respective thread or post. Our experiments on ten different forum sites show that our proposed thread extraction algorithm achieves an average recall and precision rate of 100% and 98.66%, respectively, while our core post extraction algorithm achieves an average recall and precision rate of 99.74% and 99.79%, respectively.","","Electronic:978-1-4799-1597-2; POD:978-1-4799-1596-5","10.1109/ICCIC.2013.6724259","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724259","Online forums;content extraction;information retrieval;web intelligence","Containers;Context;Data mining;Feature extraction;Message systems;Web pages","Web sites;content management;content-based retrieval;fraud","DOM tree node level correctness;anchor nodes;automated Web forum content extraction;forum sites;fraud;generalized algorithms;generic Web sites;information extraction;information retrieval;lightweight algorithms;noisy data elimination;online forums;post content extraction;text nodes;thread extraction","","0","","27","","","26-28 Dec. 2013","","IEEE","IEEE Conference Publications"
"Improving multi-term topics focused crawling by introducing term Frequency-Information Content (TF-IC) measure","A. Pesaranghader; A. Pesaranghader; N. Mustapha; N. M. Sharef","Fac. of Comput. Sci. & Inf. Technol., Univ. Putra Malaysia, Serdang, Malaysia","2013 International Conference on Research and Innovation in Information Systems (ICRIIS)","20140123","2013","","","102","106","By rapid growth of the Internet, finding desirable information would be a challenging and time consuming task. In order to tackle this issue, focused crawlers, as the ideal solution, through mining of the Web, help us to find web pages closely relevant to the desired information. For this purpose, a variety of methods are devised and implemented. Nonetheless, the majority of these methods do not favor more informative terms in a given multi-term topic. In this paper, we propose a new measure called Term Frequency-Information Content (TF-IC) to prioritize terms in a multi-term topic accordingly. Through conducted experiments, we compare our measure against both Term Frequency-Inverse Document Frequency (TF-IDF) and Latent Semantic Indexing (LSI) measures applied in focused crawlers. Experimental results indicate superiority of our measure over TF-IDF and LSI for collecting more relevant web pages of both general and specialized multi-term topics.","2324-8149;23248149","Electronic:978-1-4799-2487-5; POD:978-1-4799-2488-2","10.1109/ICRIIS.2013.6716693","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716693","Focused Crawling;Information Content;Information Retrieval;Relevant Page Prediction;Web Data Mining","Crawlers;Frequency measurement;Heart rate;Large scale integration;Vectors;Web pages","Internet;data mining;information retrieval","Internet;LSI measure;TF-IC measure;TF-IDF measure;Web mining;Web pages;latent semantic indexing;multi-term topics focused crawling;term frequency-information content;term frequency-inverse document frequency","","2","","13","","","27-28 Nov. 2013","","IEEE","IEEE Conference Publications"
"A Bag of Systems Representation for Music Auto-Tagging","K. Ellis; E. Coviello; A. B. Chan; G. Lanckriet","Department of Electrical and Computer Engineering, University of California at San Diego, La Jolla, CA, USA","IEEE Transactions on Audio, Speech, and Language Processing","20131023","2013","21","12","2554","2569","We present a content-based automatic tagging system for music that relies on a high-level, concise “Bag of Systems” (BoS) representation of the characteristics of a musical piece. The BoS representation leverages a rich dictionary of musical codewords, where each codeword is a generative model that captures timbral and temporal characteristics of music. Songs are represented as a BoS histogram over codewords, which allows for the use of traditional algorithms for text document retrieval to perform auto-tagging. Compared to estimating a single generative model to directly capture the musical characteristics of songs associated with a tag, the BoS approach offers the flexibility to combine different generative models at various time resolutions through the selection of the BoS codewords. Additionally, decoupling the modeling of audio characteristics from the modeling of tag-specific patterns makes BoS a more robust and rich representation of music. Experiments show that this leads to superior auto-tagging performance.","1558-7916;15587916","","10.1109/TASL.2013.2279318","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6583960","Audio annotation and retrieval;bag of systems;content-based music processing;dynamic texture model;music information retrieval","Computational modeling;Data models;Feature extraction;Hidden Markov models;Histograms;Music;Tagging","content-based retrieval;music","BoS codewords;BoS histogram;BoS representation;audio characteristics;bag of systems representation;content-based automatic tagging system;generative model;music auto-tagging;musical characteristics;musical codewords;musical piece;tag-specific patterns;temporal characteristics;text document retrieval;timbral characteristics;time resolutions","","4","","44","","20130821","Dec. 2013","","IEEE","IEEE Journals & Magazines"
"GA enabled ontology for platform free dynamic semantic web","P. Marikkannu; P. P","Anna Univ. Regional Centre, Coimbatore, India","2013 Science and Information Conference","20131114","2013","","","774","781","Internet plays a vital role in every aspect of our lives. The urge for speed has become a thriving faction when it comes to downloading and the information to be retrieved is not efficient as the web contents are not understandable by computers.GA enabled ontology for platform free dynamic semantic web aims at enhancing the web content with metadata to make it more meaningful. The analysis of the extracted token is carried out similar to Bootstrapping Ontology using TF/IDF for word ranking, Genetic Algorithm for finding the promise able region of identifying relevant words and fuzzy logic for ontology mapping. Using fuzzy logic, page with maximum optimization or best fit is ranked and grouped in index order, based on the priority background knowledge to the reference word and other possible links are displayed in the adjacent frame of the splitter window so that the user can toggle between the splitter windows. Thus an ordinary web page is converted into semantic web in the space specified for the user dynamically and similarly as and when the user clicks on the other references these grouped contents are also converted into semantic structure dynamically.","","Electronic:978-0-9893193-0-0; POD:978-1-4799-1272-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661829","Bootstrapping Ontology;Genetic Algorithm;Information retrieval;Inverse document frequency;Ontology;Semantic web;Term frequency","Biological cells;Genetic algorithms;Java;Ontologies;Semantic Web;Semantics;Sociology","Internet;fuzzy logic;genetic algorithms;information retrieval;ontologies (artificial intelligence);semantic Web","GA enabled ontology;Internet;Web content;Web page;bootstrapping ontology;fuzzy logic;genetic algorithm;information retrieval;metadata;ontology mapping;platform free dynamic semantic Web;splitter windows","","0","","12","","","7-9 Oct. 2013","","IEEE","IEEE Conference Publications"
"Likelihood calculation classification for Indonesian language news documents","A. Rachmania; J. Jaafar; N. Zamin","Dept. of Comput. & Inf. Sci., Univ. Teknol. PETRONAS, Tronoh, Malaysia","2013 International Conference on Information Technology and Electrical Engineering (ICITEE)","20131202","2013","","","149","154","Text categorization has been an important research area that seeks to classify textual documents into a group of predetermined categories. Unfortunately, the interest towards Indonesian news classification has been very little. In this paper, we propose a text categorization algorithm based on Bracewell method that uses the likelihood calculation between the article and the category's keywords. Through experiments, the algorithm succeeded in classifying Indonesian news corpus with accuracy as high as 93,84% in offline environment, 93,82% in online environment, and 80% benchmarking against human evaluation.","","Electronic:978-1-4799-0425-9; POD:978-1-4799-0422-8","10.1109/ICITEED.2013.6676229","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676229","Indonesian documents;information retrieval;likelihood calculation;news domain;text categorization","","classification;information retrieval;natural language processing;text analysis","Bracewell method;Indonesian language news document;Indonesian news classification;article keywords;category keywords;likelihood calculation classification;text categorization;textual document","","1","","22","","","7-8 Oct. 2013","","IEEE","IEEE Conference Publications"
"Mining multi-class industrial data with evolutionary fuzzy rules","P. Krömer; J. Platoš; V. Snášel","IT4Innovations & Department of Computer Science V&#x0160;B-Technical University of Ostrava 17. listopadu 12, Ostrava-Poruba, Czech Republic","2013 IEEE International Conference on Cybernetics (CYBCO)","20131003","2013","","","191","196","Methods based on fuzzy sets and fuzzy logic have proved to be efficient data classifiers and value estimators. This study presents an application of evolutionary evolved fuzzy rules based on the concept of extended Boolean queries to a multi-class data mining problem. Fuzzy rules are used as symbolic classifiers machine-learned from the data and used to label data samples and predict the value of an output variable. The output variable can be both a label (category) and a continuous value. This study presents an application of evolutionary fuzzy rules to the prediction of multi-class quality attributes in an industrial data set and compares the prediction obtained by fuzzy rules to the prediction achieved by support vector machines.","","Electronic:978-1-4673-6469-0; POD:978-1-4673-6468-3","10.1109/CYBConf.2013.6617453","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617453","Fuzzy Information Retrieval;Fuzzy Rules;Genetic Programming;Industrial Applications;Multi-class Data Mining","Biological cells;Coils;Fuzzy sets;Genetic programming;Sociology;Statistics;Support vector machines","data mining;fuzzy logic;fuzzy set theory;learning (artificial intelligence);pattern classification;support vector machines","data classifiers;evolutionary evolved fuzzy rules;extended Boolean queries;fuzzy logic;fuzzy sets;machine-learned symbolic classifiers;multiclass data mining problem;multiclass industrial data mining;multiclass quality attributes;support vector machines;value estimators","","1","","20","","","13-15 June 2013","","IEEE","IEEE Conference Publications"
"Characterizing the Performance and Behaviors of Runners Using Twitter","Q. He; E. Agu; D. Strong; B. Tulu; P. Pedersen","Dept. of Comput. Sci., Worcester Polytech. Inst., Worcester, MA, USA","2013 IEEE International Conference on Healthcare Informatics","20131212","2013","","","406","414","Running is a popular physical activity that improves physical and mental well being. Unfortunately, up-to-date information about runners' performance and psychological well being is limited. Many questions remain unanswered, such as how far and how fast runners typically run, their preferred running times and frequencies, how long new runners persist before dropping out, and what factors cause runners to quit. Without hard data, establishing patterns of runner behavior and mitigating challenges they face are difficult. Collecting data manually from large numbers of runners for research studies is costly and time consuming. Emerging Social Networking Services (SNS) and fitness tracking devices make tracking and sharing personal physical activity information easier than before. By monitoring the tweets of a runner group on Twitter (SNS) over a 3-month period, we collected 929,825 messages (tweets), in which runners used Nike+ fitness trackers while running. We found that (1) fitness trackers were most popular in North America (2) one third of runners dropped out after one run (3) Over 95% of runners ran for at least 10 minutes per session (4) less than 2% of runners consistently ran for at least 150 minutes a week, which is the level of physical activity recommended by the CDC (5) 5K was the most popular distance.","","Electronic:978-0-7695-5089-3; POD:978-1-4799-0974-2","10.1109/ICHI.2013.56","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680503","Twitter;data analysis;information retrieval;physical activity;running;social network services","Cities and towns;Global Positioning System;Monitoring;Portable media players;Sensors;Twitter","information retrieval;medical computing;psychology;social networking (online);sport","CDC;Nike+ fitness trackers;North America;SNS;Twitter;data collection;fitness tracking devices;mental wellbeing improvement;personal physical activity information sharing;personal physical activity information tracking;physical activity;physical wellbeing improvement;runner behavior characterization;runner group;runner performance characterization;runner performance information;runner psychological wellbeing information;running frequencies;running times;social networking services;tweets messages;tweets monitoring","","0","","23","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"SOLFABOT: Low-cost support tool for solfeo training","A. M. Barbancho; A. Ortiz; I. Barbancho; A. Rosa-Pujazón; L. J. Tardón","Dpt. Ingenier&#x00ED;a de Comunicaciones, E.T.S.I. Telecomunicaci&#x00F3;n, Universidad de M&#x00E1;laga, Campus de Teatinos s/n, 29071 M&#x00E1;laga, Spain","2013 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","20131010","2013","","","400","404","This paper presents a low-cost add-on sensor that has been developed to provide other systems with music listening and note detection capabilities. This sensor has been integrated as part of an intelligent system for solfeo learning. The system is capable of listening the performance of a user as well as assessing the correctness of both the rhythmic pattern and intonation in the performance, and presenting practice exercises. Concretely, the system is based on the Lego Mindstorms NXT 2.0 robot. The system designed implements a training application with three training modalities, depending on whether the user wishes to improve his musical reading (rhythm and intonation) or listening abilities.","1555-5798;15555798","Electronic:978-1-4799-1501-9; POD:978-1-4799-1500-2; USB:978-1-4799-1499-9","10.1109/PACRIM.2013.6625510","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625510","Human-Computer Interaction;Music Information Retrieval;Music Interaction;Note detection;Signal Processing","Databases;Educational institutions;Intelligent sensors;Music;Robot sensing systems;Training","computer based training;human computer interaction;information retrieval;learning (artificial intelligence);signal detection","Lego Mindstorms NXT 2.0 robot;SOLFABOT;intelligent system;intonation;listening abilities;low-cost add-on sensor;music listening capabilities;musical reading;note detection capabilities;practice exercises;rhythmic pattern;solfeo learning;training application","","0","","10","","","27-29 Aug. 2013","","IEEE","IEEE Conference Publications"
"Using Twitter's Mentions for Efficient Emergency Message Propagation","K. Y. Itakura; N. Sonehara","Res. Organ. of Inf. & Syst., Nat. Inst. of Inf., Tokyo, Japan","2013 International Conference on Availability, Reliability and Security","20131107","2013","","","530","537","Using social media such as Twitter for emergency message propagation in times of crisis is widely thought to be a good addition to other traditional emergency population warning systems such as televisions. At the same time, most studies on Twitter influence propagation focus on retweetability of tweets. In this paper, we propose the importance of Twitter's mention function as another method of message propagation. Specifically, we show that graphs constructed from Twitter's retweet, mention, and reply functions show structural differences suggesting that using the mention function is the most efficient method of reaching the mass audience. Moreover, we show that influencers are the most prominent on the mention graph. From these analysis we conclude that we need further research in the direction of non-traditional methods of population warning systems. Further, this is the first paper that characterizes the structural differences of the retweet/mention/reply graphs in Twitter.","","Electronic:978-0-7695-5008-4; POD:978-1-4799-1097-7","10.1109/ARES.2013.70","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657286","Twitter;information retrieval;information security;social network services","Alarm systems;Media;Sociology;Statistics;Tsunami;Twitter","graph theory;information retrieval;social networking (online)","Twitter mention function;Twitter reply function;Twitter retweet function;emergency message propagation;mass audience;mention graph;reply graph;retweet graph;social media;tweet retweetability","","0","","20","","","2-6 Sept. 2013","","IEEE","IEEE Conference Publications"
"A Two-Stage Approach for Word Spotting in Graphical Documents","A. Tarafdar; U. Pal; P. P. Roy; N. Ragot; J. Y. Ramel","CVPR Unit, Indian Stat. Inst., Kolkata, India","2013 12th International Conference on Document Analysis and Recognition","20131015","2013","","","319","323","Presence of multi-oriented characters, connected characters with graphical lines, intersection of text and symbols with graphical lines/curves etc. are very common in graphical documents. As a result word spotting in graphical documents is still a challenging task that we try to solve (partially) in this paper. The proposed approach proceeds in two stages. In the first stage, recognition of isolated components is done using rotation invariant features and an SVM classifier. The characters having good recognition score and match in the query string are first selected for initial spotting. Because of structural complexity of graphical documents as well as of touching components, we may miss some of the query characters during initial spotting in some documents. In that case, based on the position, size and orientation of the recognized characters in the input document image, regions where missing characters may be located (candidate regions) are defined. In the second stage, Scale Invariant Feature Transform (SIFT) is used to find those missing characters in the candidate regions for possible spotting. Finally, using the position, size, orientation as well as intercharacter gap information of the recognized components, spotting is validated. Experimental results demonstrate that the method is efficient to locate a query word in multi-oriented and/or touching graphical documents.","1520-5363;15205363","Electronic:978-0-7695-4999-6; POD:978-1-4799-0193-7","10.1109/ICDAR.2013.71","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628636","Document Image Analysis;Graphical documents;Information Retrieval;SIFT;Word Spotting","Character recognition;Feature extraction;Shape;Support vector machines;Text analysis;Text recognition","document image processing;image classification;image retrieval;support vector machines","SIFT;SVM classifier;graphical documents;graphical lines;input document image;multi-oriented characters;query characters;query string;rotation invariant features;scale invariant feature transform;structural complexity;two-stage approach;word spotting","","2","","18","","","25-28 Aug. 2013","","IEEE","IEEE Conference Publications"
"Application of reinforcement learning to requirements engineering: requirements tracing","H. Sultanov; J. H. Hayes","University of Kentucky Lexington, KY, USA","2013 21st IEEE International Requirements Engineering Conference (RE)","20131021","2013","","","52","61","We posit that machine learning can be applied to effectively address requirements engineering problems. Specifically, we present a requirements traceability method based on the machine learning technique Reinforcement Learning (RL). The RL method demonstrates a rather targeted generation of candidate links between textual requirements artifacts (high level requirements traced to low level requirements, for example). The technique has been validated using two real-world datasets from two problem domains. Our technique demonstrated statistically significant better results than the Information Retrieval technique.","1090-705X;1090705X","Electronic:978-1-4673-5765-4; POD:978-1-4673-5763-0","10.1109/RE.2013.6636705","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636705","Research Project 2 of Grand Challenges of Traceability;Ubiquitous Grand Challenge;information retrieval;machine learning;reinforcement learning;requirements traceability;software engineering","Educational institutions;Joining processes;Learning (artificial intelligence);Navigation;Software;Vocabulary","formal verification;learning (artificial intelligence);program diagnostics","RL method;machine learning technique;reinforcement learning;requirements engineering;requirements traceability method;textual requirements artifacts","","4","","19","","","15-19 July 2013","","IEEE","IEEE Conference Publications"
"New Semantic Indexing and Search System  Based on Ontology","F. Salam","Fac. of Inf. Technol., AL-Ahliyya Amman Univ., Amman, Jordan","2013 Fourth International Conference on Emerging Intelligent Data and Web Technologies","20131017","2013","","","313","318","Information retrieval becomes a very complex process for search engines on the Web, this is due to, first, the staggering growth speed of the number of web site and, in the other hand, the search algorithms by keywords (terms) used currently are not suitable to better exploit this huge information quantity. These elements make the information retrieval by the current search engines very difficult and does not meet the users' needs. In this paper, we propose a new method of semantic information retrieval based on ontology. Our method allows the indexing and searching engine to take in consideration the documents semantic level, which significantly improves the quality of search results. We have implemented our approach on an indexing and searching engine based on ontology (called MIRO Moteur d'Indexation et de Recherche base sur les Ontologies). MIRO offers a multilingual semantic search of documents using concept instead of term. Additionally, MIRO offers a guided search tool, and a tool for an automatic enrichment of ontology. Moreover a comparison of results between MIRO and PhpDig (an open source search engine) is presented.","","Electronic:978-1-4799-2141-6; POD:978-1-4799-2142-3","10.1109/EIDWT.2013.60","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6631638","and evolution; Semantic Query Processing.;component; Ontology; Semantic Indexing System; Searching Engine; Semantic Web Information retrieval System; Ontology modeling; reuse;extraction","Engines;Indexing;Ontologies;Search engines;Semantic Web;Semantics","indexing;information retrieval;natural languages;ontologies (artificial intelligence);public domain software;search engines;text analysis","MIRO Moteur d'Indexation et de Recherche base sur les Ontologies;PhpDig;Web search engine;Web site;automatic ontology enrichment;concept;document semantic level;guided search tool;information quantity;keywords;multilingual semantic search;open source search engine;search algorithm;search result quality;semantic indexing system;semantic information retrieval;semantic search system","","0","","11","","","9-11 Sept. 2013","","IEEE","IEEE Conference Publications"
"Framework for web content mining using semantic search and natural language queries","A. J. Shaikh; V. L. Kolhe","Dept. of Comput. Eng., D.Y. Patil Coll. of Eng., Pune, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","20140127","2013","","","1","5","Current keyword based search engines are not able to search semantics in web pages. In this paper we present framework for semantic based web content mining system using semantic ontology and SPARQL. A major challenge in the work involves building ontology database from natural language web pages. Second challenge is automated translation of natural language queries into more precise SPARQL queries. Proposed approach is tested on cricket websites and shows outstanding improvements over traditional keyword based search results.","","Electronic:978-1-4799-1597-2; POD:978-1-4799-1596-5","10.1109/ICCIC.2013.6724175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724175","Information Retrieval;NLP;OWL;SPARQL;Semantic Web;Web Mining;ontology;search engine","Natural languages;Ontologies;Semantic Web;Semantics;Web mining;Web pages","SQL;data mining;database management systems;natural language processing;ontologies (artificial intelligence);query processing;semantic Web","SPARQL queries;cricket Websites;keyword based search results;natural language Web pages;natural language queries;ontology database;semantic based Web content mining system;semantic ontology;semantic search","","0","","15","","","26-28 Dec. 2013","","IEEE","IEEE Conference Publications"
"Ontology Based Framework to Represent Relationships between Biomedical Spatial Data","M. J. Somodevilla; C. P. d. C. Herrero; J. A. Hernández; I. H. P. Torres","Fac. de Cienc. de la Comput., Benemerita Univ. Autonoma de Puebla Puebla, Puebla, Mexico","2013 12th Mexican International Conference on Artificial Intelligence","20140123","2013","","","257","261","Geographic Information Systems (GIS) and services describe formalized knowledge from spatial domain. GIS use spatial data referring to places names (toponyms) in a geographical space, which could be ambiguous. Ontologies allow support for storing information in this context, providing a structure which defines the data integration and also supports toponyms disambiguation. This article presents an ontology based framework including the interpretation of the geographical biomedical domain concepts and their spatial relationships. The work is focused on solving queries involving ambiguous toponyms, illness and health services. Emphasis has been placed on the topological relationships, which will be used later in the spatial axioms definition. The framework is complemented with Google Maps and Jena APIs.","","Electronic:978-1-4799-2605-3; POD:978-1-4799-2606-0","10.1109/MICAI.2013.37","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714676","Ontology;biomedical data;geo-referencing;geographic information retrieval;spatial relationships","Artificial intelligence","data structures;geographic information systems;ontologies (artificial intelligence);query processing","GIS;Google Maps;Jena API;ambiguous toponyms;application program interfaces;formalized knowledge;geographic information systems;geographical biomedical domain concepts;geographical space;health services;illness;information storage;ontology based framework;relationships representation;spatial data;toponyms disambiguation","","1","","10","","","24-30 Nov. 2013","","IEEE","IEEE Conference Publications"
"Search-based duplicate defect detection: An industrial experience","M. Amoui; N. Kaushik; A. Al-Dabbagh; L. Tahvildari; S. Li; W. Liu","University of Waterloo Waterloo, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","173","182","Duplicate defects put extra overheads on software organizations, as the cost and effort of managing duplicate defects are mainly redundant. Due to the use of natural language and various ways to describe a defect, it is usually hard to investigate duplicate defects automatically. This problem is more severe in large software organizations with huge defect repositories and massive number of defect reporters. Ideally, an efficient tool should prevent duplicate reports from reaching developers by automatically detecting and/or filtering duplicates. It also should be able to offer defect triagers a list of top-N similar bug reports and allow them to compare the similarity of incoming bug reports with the suggested duplicates. This demand has motivated us to design and develop a search-based duplicate bug detection framework at BlackBerry. The approach follows a generalized process model to evaluate and tune the performance of the system in a systematic way. We have applied the framework on software projects at BlackBerry, in addition to the Mozilla defect repository. The experimental results exhibit the performance of the developed framework and highlight the high impact of parameter tuning on its performance.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624025","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624025","Duplicate Defect Detection;Information Retrieval;Parameter Tuning;Search-based Software Engineering","Electronic mail;Indexing;Noise;Search problems;Software;Tuning","program debugging;project management","BlackBerry;Mozilla defect repository;bug reports;duplicate detection;duplicate filtering;generalized process model;natural language;search-based duplicate bug detection framework;search-based duplicate defect detection;software organizations;software projects","","2","","24","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Forum Summarization Using Topic Models and Content-Metadata Sensitive Clustering","J. Krishnamani; Y. Zhao; R. Sunderraman","Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","20131223","2013","3","","195","198","The advent of the Internet and improvements in data sharing and storage, have resulted in an explosion of textual data. But, complete assimilation of such massive amounts of data in its raw form is a daunting task. Automated text mining methods such as text summarization present the user with a condensed version of data containing only key information. This is especially useful in the case of online user forums that contain a large number of posts spread out across several threads. Document summarization methods have been extensively studied and several methods have been developed in the recent past. This paper aims at developing a new method for automatic summarization of online forums by using topic models and content/metadata sensitive clustering.","","Electronic:978-0-7695-5145-6; POD:978-1-4799-3932-9","10.1109/WI-IAT.2013.182","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690727","document summarization;information retrieval;text mining;topic models","Abstracts;Clustering algorithms;Conferences;Educational institutions;Joints;Mathematical model;Message systems","Web sites;data mining;pattern clustering;statistical analysis;text analysis","content-metadata sensitive clustering;data assimilation;data sharing;data storage;document summarization methods;forum summarization;online user forums;text mining methods;text summarization;textual data;topic models","","0","","11","","","17-20 Nov. 2013","","IEEE","IEEE Conference Publications"
"SocialRank: Social network influence ranking method","Z. Yang; X. Huang; J. Xiu; C. Liu","Sch. Of Software Eng., Beijing Univ. of Posts & Telecommun., Beijing, China","2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems","20131114","2012","02","","591","595","The influence of an individual in the social network is a both subjective and objective matter, which depends on the quality of his web pages and the altitudes of the others. But there is still much that can be said objectively about the relative influence of every individual in the social network. This paper describes SocialRank, a method for measuring the influence of an individual objectively and mechanically using his attributes and the relations between him and the others. We give the compartments of SocialRank to basic attributes and PageRank. We show how accurately SocialRank access relative influence.","2376-5933;23765933","Electronic:978-1-4673-1857-0; POD:978-1-4673-1856-3","10.1109/CCIS.2012.6664243","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664243","Individual influence;Information retrieval;Pagerank;Social networks;Socialrank","Computer science;Cooling;Data mining;Educational institutions;Facebook;Web pages","information retrieval;social networking (online)","PageRank;SocialRank method;Web pages quality;social network influence ranking method;user attributes","","0","1","8","","","Oct. 30 2012-Nov. 1 2012","","IEEE","IEEE Conference Publications"
"Poster: User-centered query expansion model for health disparities research","S. Lee; S. Belkasim","Department of Computer Science, Georgia State University, Atlanta, 30302-3994, USA","2013 IEEE 3rd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)","20131015","2013","","","1","1","Health disparities are gap in health status driven by social inequities. The differences have affected how many people get sick or how often diseases cause death. In order to reduce the health disparities, many research projects have been led by National Institutes of Health. In this paper, we propose a user-centered query expansion model that assists the health disparity researches. The proposed model is tested on recent health disparities research documents.","","Electronic:978-1-4799-0716-8; POD:978-1-4799-0715-1","10.1109/ICCABS.2013.6629211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629211","document similarity;health disparities;information retrieval;query expansion","Computational modeling;Computer science;Diseases;Educational institutions;Search engines;Semantics","diseases;document handling;medical information systems;query processing;user centred design","National Institutes of Health;diseases;health disparity research document;health status;social inequities;user-centered query expansion model","","0","","1","","","12-14 June 2013","","IEEE","IEEE Conference Publications"
"Automatic keyphrase extraction techniques: A review","V. M. H. Lim; S. F. Wong; T. M. Lim","Science & Technology, Sunway University, Subang, Malaysia","2013 IEEE Symposium on Computers & Informatics (ISCI)","20130926","2013","","","196","200","Keyphrases are useful in organizing an overwhelming number of resources while providing ease of access for information retrieval. However, manually assigning keyphrases to a document is very expensive in terms of both human resources and time consumption. Therefore, there is a need for automatic keyphrase extraction. While techniques for automatic keyphrase extraction have been researched for a number of years, the accuracy rate remains low. This paper reviews existing automatic keyphrase extraction techniques with the goal of understanding their strengths and weaknesses. In future improvement of automatic keyphrase extraction technique, these identified strengths should be preserved while the weaknesses should be addressed so that a higher accuracy rate can be achieved. The results show that two important sub-processes should be the focus for further research. The paper also proposes a future study to address this issue.","","Electronic:978-1-4799-0210-1; POD:978-1-4799-0208-8","10.1109/ISCI.2013.6612402","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6612402","Attribute Score;Automatic Keyphrase Extraction;Information Retrieval;Keyphrase Extraction","Accuracy;Computers;Context;Data mining;Filtration;Supervised learning;Training","information retrieval","automatic keyphrase extraction techniques;information retrieval","","1","","14","","","7-9 April 2013","","IEEE","IEEE Conference Publications"
"Generating customized web search result through community driven search engine","B. S. Hantono; G. D. Putra","Dept. of Electr. Eng. & Inf. Technol., Univ. Gadjah Mada, Yogyakarta, Indonesia","2013 International Conference on Information Technology and Electrical Engineering (ICITEE)","20131202","2013","","","127","130","These days, the growth of web has led it to a big source of information. Web search engine plays an important role of searching desired information from this enormous web. However, search engine provides the same result independently to the user while actually each user has different preference. In this paper, we present a novel method of customized web search result generation to provide a better result according to community's preference. We benefit from proxy servers, which are widely used in a community network to reduce bandwidth needs. Proxy servers are, actually, providing the user preference within its access log that contains accessed URLs. Instead of web crawler, we will use this logs, which is always updated as users browse the web through this proxy. This would be the base of our customized web search. As the proxy log only covers URL list, we still need to crawl the information contained in an URL. When the crawling method has completed, document vector is created to make those data to be more machine friendly. Eventually, searching process is carried out by utilizing the vector space model.","","Electronic:978-1-4799-0425-9; POD:978-1-4799-0422-8","10.1109/ICITEED.2013.6676225","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676225","customized web search;information retrieval;proxy log;search engine;tf-idf","","Internet;document handling;information retrieval;search engines","Web crawler;Web search engine;World Wide Web;access log;accessed URL;community driven search engine;community network;crawling method;customized Web search result;document vector;proxy servers;searching process;vector space model","","0","","10","","","7-8 Oct. 2013","","IEEE","IEEE Conference Publications"
"An analytical study of clustered file space properties","I. A. R. Moghrabi","M.I.S. Dept., Gulf Univ. for Sci. & Technol., Hawally, Kuwait","2013 Science and Information Conference","20131114","2013","","","609","612","Response time of an information system can be improved by reducing the number of buckets accessed when retrieving a document set. One approach is to cluster the document base in such a way to ensure greater probability that identifying records will be physically contiguously located. This paper considers a clustering algorithm that controls the file space density through the use of a user-specified threshold value for which we will show how this influences file density and retrieval performance. The method is particularly well-suited to the reorganization of traditional multi-attribute files. Statistical models are constructed to predict the file performance parameters. Our results reveal those models, before and after clustering, are reasonably accurate.","","Electronic:978-0-9893193-0-0; POD:978-1-4799-1272-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661801","Information Retrieval;clustering;large data","Clustering algorithms;Estimation;Hamming distance;Indexes;Information retrieval;Vectors","file organisation;information retrieval;pattern clustering;statistical analysis","clustered file space properties;clustering algorithm;document set retrieval;file density;information system;multiattribute file reorganization;statistical models;user-specified threshold value","","0","","16","","","7-9 Oct. 2013","","IEEE","IEEE Conference Publications"
"Automatic text categorization of marathi documents using clustering technique","S. R. Vispute; M. A. Potey","DYPCOE, Pune, India","2013 15th International Conference on Advanced Computing Technologies (ICACT)","20140116","2013","","","1","5","The purpose of the present work is creating an intelligent system to retrieve desired documents in Marathi language. The system also focuses on providing the personalized documents in Marathi language to the end user based on their interests identified from the browsing history. This paper presents the automatic categorization of Marathi documents and the literature survey of the related work done in automatic categorization of text documents. Several supervised learning techniques are exists for the classification of text documents namely Decision trees, Support Vector machine (SVM), Neural Network, Ada Boost and Naïve Bayes etc. Several clustering techniques are also available for text categorization namely K-means, Suffix Tree Clustering (STC), Semantic Online Hierarchical Clustering (SHOC), Label Induction Grouping Algorithm (LINGO) etc. In the literature survey it is found that vector space model (VSM) gives better result than probabilistic model. This paper presents categorization of the Marathi text documents using Lingo Clustering algorithm based on VSM. The data set consists of 107 Marathi documents of 3 different categories-Tourism, Health Programmes and Maharashtra festivals. The result shows that the performance of the LINGO clustering algorithm is good for categorizing the Marathi text documents. For the Marathi documents overall accuracy of the system is 91.10%.","","CD-ROM:978-1-4673-2816-6; Electronic:978-1-4673-2818-0; POD:978-1-4673-2817-3","10.1109/ICACT.2013.6710543","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710543","Clustering;Information filtering;Information retrieval;Internet search;Text categorization","Classification algorithms;Clustering algorithms;Internet;Matrix decomposition;Support vector machines;Text categorization;Vectors","information retrieval;natural language processing;pattern classification;pattern clustering;text analysis","Lingo clustering algorithm;Maharashtra festivals;Marathi documents automatic categorization;Marathi text documents;VSM;clustering technique;document retrieval;health programmes;intelligent system;text documents automatic categorization;tourism;vector space model","","2","","17","","","21-22 Sept. 2013","","IEEE","IEEE Conference Publications"
"Natutal language processing: Perspective of CIC-IPN","A. Gelbukh","Centra de Investigacion en Computacion, Institute Politecnico Nacional, Mexico City, Mexico","2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","20131021","2013","","","2112","2121","In this paper I outline part of the results of fifteen years of research of the Natural Language Processing (NLP) Laboratory of CIC-IPN. NLP is a set of technologies that in long run will allow for dialogue with robots in our everyday language or for overcoming the language barrier between people. Our group's work is concentrated on the internal tasks of this technology, such as resolving ambiguities and constructing dictionaries.","","Electronic:978-1-4673-6217-7; POD:978-1-4799-1664-1","10.1109/ICACCI.2013.6637507","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637507","information retrieval;natural language processing;sentiment analysis;similarity measures;word sense disambiguation","Conferences;Informatics","natural language processing","CIC-IPN;NLP laboratory;language barrier;natural language processing laboratory;robot dialogue","","0","","109","","","22-25 Aug. 2013","","IEEE","IEEE Conference Publications"
"Towards multi-policy support for IaaS clouds to secure data sharing","Y. Fairweather; D. Shin","Comput. Sci. Dept., New Mexico Inst. of Min. & Technol., Socorro, NM, USA","9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20131212","2013","","","31","39","Infrastructure as a service (IaaS) is a cloud service model that provides storage and computation services for users at a low price. A recent report from Gartner indicates that IaaS will be the fastest growing area among all of the cloud service models in the near future, and thus it is strongly envisioned that multiple companies will use IaaS clouds to share information among them. However, the current access control mechanisms in IaaS platforms do not have the ability to enable flexible data sharing among companies while addressing security problems such as information and privacy leaking. In this paper, we propose two IaaS cloud reference architectures that enforce cloud-level Chinese Wall security (CWS) policy to prevent information leaking among companies. The new architectures are also able to support customized domain level access control policies such as role-based access control (RBAC), privacy-preserving information retrieval, and single sign on (SSO). The reference architectures were implemented using Eucalyptus and its data storage service called Walrus; therefore, our approach can also be applied to commercial clouds like Amazon S3. The result of performance analysis has shown that our architectures are feasible, scalable, and efficient.","","Electronic:978-1-936968-92-3; POD:978-1-4799-2754-8; USB:978-1-936968-91-6","10.4108/icst.collaboratecom.2013.254127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679967","Access Control;Chinese Wall Security Policy;Identity Management;Infrastructure as a service (IaaS);Secure Information Retrieval","Access control;Cloud computing;Companies;Computer architecture;Databases;History","authorisation;cloud computing;data privacy;electronic data interchange","Amazon S3;CWS;Eucalyptus;IaaS cloud reference architectures;RBAC;SSO;Walrus;access control mechanisms;cloud service model;cloud-level Chinese wall security policy;commercial clouds;data storage service;domain level access control policies;flexible data sharing;information leaking;infrastructure as a service;multipolicy support;privacy leaking;privacy-preserving information retrieval;role-based access control;secure data sharing;security problems;single sign on","","2","","28","","","20-23 Oct. 2013","","IEEE","IEEE Conference Publications"
"Normalizing source code vocabulary to support program comprehension and software quality","L. Guerrouj","DGIGL - SOCCER Lab, Ptidej Team, Polytechnique Montr&#x00E9;al, Qu&#x00E9;bec, Canada","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","1385","1388","The literature reports that source code lexicon plays a paramount role in program comprehension, especially when software documentation is scarce, outdated or simply not available. In source code, a significant proportion of vocabulary can be either acronyms and-or abbreviations or concatenation of terms that can not be identified using consistent mechanisms such as naming conventions. It is, therefore, essential to disambiguate concepts conveyed by identifiers to support program comprehension and reap the full benefit of Information Retrieval-based techniques (e.g., feature location and traceability) whose linguistic information (i.e., source code identifiers and comments) used across all software artifacts (e.g., requirements, design, change requests, tests, and source code) must be consistent. To this aim, we propose source code vocabulary normalization approaches that exploit contextual information to align the vocabulary found in the source code with that found in other software artifacts. We were inspired in the choice of context levels by prior works and by our findings. Normalization consists of two tasks: splitting and expansion of source code identifiers. We also investigate the effect of source code vocabulary normalization approaches on software maintenance tasks. Results of our evaluation show that our contextual-aware techniques are accurate and efficient in terms of computation time than state of the art alternatives. In addition, our findings reveal that feature location techniques can benefit from vocabulary normalization when no dynamic information is available.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606723","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606723","Source code linguistic analysis;information retrieval;program comprehension;software quality","Context;Dictionaries;Software maintenance;Software quality;Vocabulary","reverse engineering;software maintenance;software quality;system documentation;ubiquitous computing;vocabulary","context levels;contextual information;contextual-aware techniques;feature location;information retrieval-based techniques;linguistic information;program comprehension;software artifacts;software documentation;software maintenance tasks;software quality;source code comments;source code identifiers expansion;source code identifiers splitting;source code lexicon;source code vocabulary normalization approaches","","1","","26","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"Relevant document retrieval via discrete stochastic optimization","S. H. Ren","Shanghai Int. Studies Univ., Shanghai, China","2013 10th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","20140123","2013","","","73","76","In this paper, a relevant document retrieval method is proposed for document retrieval systems with vector space models (VSM). In recent years, with the size of the database becomes extremely large, there becomes a high demanding of an accurate and fast-time document retrieval algorithm. Based on the maximum similarity criterion, a document retrieval algorithm using the discrete stochastic optimization method is proposed with the user query to retrieve the relevant documents. The proposed algorithm has the self-learning capability for most of the computational effort is spent at the global optimal document and converges fast to the relevant documents in the database. Numerical results demonstrate that the proposed algorithm has a good convergence property and satisfied document retrieval performance in the database.","","CD-ROM:978-1-4799-2444-8; Electronic:978-1-4799-2446-2; POD:978-1-4799-2447-9","10.1109/ICCWAMTIP.2013.6716603","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716603","Information retrieval;discrete stochastic optimization;document retrieval algorithm;vector space model","Convergence;Databases;Genetic algorithms;Information retrieval;Optimization;Stochastic processes;Vectors","document handling;optimisation;query processing;stochastic processes","VSM;computational effort;convergence property;discrete stochastic optimization method;global optimal document;maximum similarity criterion;relevant document retrieval method;self-learning capability;user query;vector space models","","1","","12","","","17-19 Dec. 2013","","IEEE","IEEE Conference Publications"
"A contextual approach towards more accurate duplicate bug report detection","A. Alipour; A. Hindle; E. Stroulia","Department of Computing Science, University of Alberta, Edmonton, Canada","2013 10th Working Conference on Mining Software Repositories (MSR)","20131010","2013","","","183","192","Bug-tracking and issue-tracking systems tend to be populated with bugs, issues, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system being discussed. Many bug reporters lack the skills, vocabulary, knowledge, or time to efficiently search the issue tracker for similar issues. As a result, issue trackers are often full of duplicate issues and bugs, and bug triaging is time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval tools, such as BM25F used by Sun et al. In our work, we extend the state of the art by investigating how contextual information, relying on our prior knowledge of software quality, software architecture, and system-development (LDA) topics, can be exploited to improve bug-deduplication. We demonstrate the effectiveness of our contextual bug-deduplication method on the bug repository of the Android ecosystem. Based on this experience, we conclude that researchers should not ignore the context of software engineering when using IR tools for deduplication.","2160-1852;21601852","Electronic:978-1-4673-2936-1; POD:978-1-4673-2934-7","10.1109/MSR.2013.6624026","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624026","contextual information;deduplication;duplicate bug reports;information retrieval;machine learning;textual similarity;triaging","Accuracy;Androids;Computer bugs;Context;Humanoid robots;Software;Sun","Linux;information retrieval;program debugging;software architecture;software quality","Android ecosystem;IR tools;LDA topics;bug repository;contextual bug-deduplication method;contextual information;duplicate bug report detection;information-retrieval tools;software architecture;software engineering;software quality;system-development","","17","","26","","","18-19 May 2013","","IEEE","IEEE Conference Publications"
"Efficient database pruning for large-scale cover song recognition","J. Osmalskyj; S. Piérard; M. Van Droogenbroeck; J. J. Embrechts","INTELSIG Laboratory, Departement EECS, University of Li&#x00E8;ge, Belgium","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","714","718","This paper focuses on cover song recognition over a large dataset, potentially containing millions of songs. At this time, the problem of cover song recognition is still challenging and only few methods have been proposed on large scale databases. We present an efficient method for quickly extracting a small subset from a large database in which a correspondence to an audio query should be found. We make use of fast rejectors based on independent audio features. Our method mixes independent rejectors together to build composite ones. We evaluate our system with the Million Song Dataset and we present composite rejectors offering a good trade-off between the percentage of pruning and the percentage of loss.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637741","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637741","Chromas;Cover Songs;Million Song Dataset;Music Information Retrieval;Rejectors","Abstracts;Databases","audio signal processing;music;query processing","Million Song dataset;audio query;composite rejectors;cover song recognition;database pruning;independent audio features;music information retrieval","","1","","21","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Automatic Generation of a Qualified Medical Knowledge Graph and Its Usage for Retrieving Patient Cohorts from Electronic Medical Records","T. Goodwin; S. M. Harabagiu","Human Language Technol. Res. Inst., Univ. of Texas at Dallas Dallas, Dallas, TX, USA","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","363","370","An extraordinary amount of clinical information is available within Electronic Medical Records. However, interpreting this knowledge typically demands a significant level of clinical understanding. This can facilitated by access to structured knowledge bases. However, even if vast, biomedical knowledge bases have very limited relational information available. In contrast, clinical text expresses many relations between concepts using an extraordinary amount of variation regarding the author's belief state - whether a medical concept is present, uncertain, or absent. In this paper, we propose a method for automatically constructing a graph of clinically related concepts based on their belief state. For this purpose, we first devise a method for classifying the belief state of certain medical concepts. Second, we designed a technique for constructing a graph of related medical concepts qualified by the physician's belief value. Thirdly, we demonstrate several techniques for inferring the similarity between qualified medical concepts, and present a generalized algorithm for determining the second-order similarity between qualified medical concepts. Finally, we show that incorporating the knowledge encoded from this graph yield competitive results when applied to query expansion for the retrieval of hospital patient cohorts.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.68","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693543","bioinformations;electronic medical records;information retrieval;knowledge graph","Context;Hospitals;Medical diagnostic imaging;Semantics;Unified modeling language","knowledge based systems;medical information systems;query formulation","belief state classification;biomedical knowledge bases;clinical information;clinical text;clinical understanding;electronic medical records;hospital patient cohorts retrieval;medical concepts;physician belief value;qualified medical knowledge graph;query expansion;relational information;second-order similarity;structured knowledge bases","","1","","22","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"An approach for semantic query expansion based on maximum entropy-hidden Markov model","R. Jothilakshmi; N. Shanthi; R. Babisaraswathi","","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","20140130","2013","","","1","5","The ineffectiveness of information retrieval systems is mostly caused by the inaccurate query formed by a few keywords that reflect actual user information need. One well known technique to overcome this limitation is Automatic Query Expansion (AQE), whereby the user's original query is improved by adding new features with a related meaning. It has long been accepted that capturing term associations is a vital part of information retrieval. It is therefore mainly to consider whether many sources of support may be combined to forecast term relations more precisely. This is mainly significant when frustrating to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms. This paper presents a approach to expand the user query using three level domain model such as conceptual level(underlying Domain knowledge), linguistic level(term vocabulary based on Wordnet), stochastic model ME-HMM2 which combines (HMM (Hidden Markov Model and Maximum Entropy(ME) models) stores the mapping between such levels, taking into account the linguistic context of words.","","Electronic:978-1-4799-3926-8; POD:978-1-4799-3927-5","10.1109/ICCCNT.2013.6726755","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726755","Hidden Markov Model;Information Retrieval;Ontology;Query expansion;Viterbi algorithm","Context;Data mining;Hidden Markov models;Information retrieval;Ontologies;Pragmatics;Semantics","hidden Markov models;maximum entropy methods;query processing","AQE;HMM;Wordnet;actual user information need;automatic query expansion;conceptual level;domain knowledge;inaccurate query;information retrieval systems;linguistic context;linguistic level;maximum entropy models;maximum entropy-hidden Markov model;original query;semantic query expansion;semantic relations;stochastic model;term relations forecasting;term vocabulary;three level domain model;user query","","0","","23","","","4-6 July 2013","","IEEE","IEEE Conference Publications"
"Analysis of user comments: An approach for software requirements evolution","L. V. G. Carreño; K. Winbladh","Department of Electrical and Computer Engineering, University of Delaware, Newark, DE USA","2013 35th International Conference on Software Engineering (ICSE)","20130926","2013","","","582","591","User feedback is imperative in improving software quality. In this paper, we explore the rich set of user feedback available for third party mobile applications as a way to extract new/changed requirements for next versions. A potential problem using this data is its volume and the time commitment involved in extracting new/changed requirements. Our goal is to alleviate part of the process through automatic topic extraction. We process user comments to extract the main topics mentioned as well as some sentences representative of those topics. This information can be useful for requirements engineers to revise the requirements for next releases. Our approach relies on adapting information retrieval techniques including topic modeling and evaluating them on different publicly available data sets. Results show that the automatically extracted topics match the manually extracted ones, while also significantly decreasing the manual effort.","0270-5257;02705257","Electronic:978-1-4673-3076-3; POD:978-1-4673-3075-6","10.1109/ICSE.2013.6606604","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606604","Information Retrieval;Requirements;Software Evolution;Topic Modeling;User Comments;User feedback","Adaptation models;Analytical models;Androids;Data mining;Humanoid robots;Manuals;Software","information retrieval;mobile computing;software quality","automatic topic extraction;information retrieval technique;requirement engineering;sentence representation;software quality;software requirement evolution;third party mobile application;topic modeling;user comment analysis;user feedback","","10","","22","","","18-26 May 2013","","IEEE","IEEE Conference Publications"
"MICS: Multimodal image collection summarization by optimal reconstruction subset selection","J. E. Camargo; F. A. González","MindLab Research Group, Bogot&#x00E1;, Colombia, Universidad Nacional de Colombia","2013 8th Computing Colombian Conference (8CCC)","20131021","2013","","","1","6","This paper presents a new method to automatically select a set of representative images from a larger set of retrieved images for a given query. We define an image collection summary as a subset of images from the collection, which are visually and semantically representative. To build such a summary we propose MICS, a method that fuses two modalities, textual and visual, in a common latent space, and use it to find a subset of images from which the collection visual content could be reconstructed. We conducted experiments on a collection of tagged images and demonstrate the ability of our approach to build summaries with representative visual and semantic content. The initial results show that the proposed method is able to build a meaningful summary that can be integrated in an image collection exploration system.","","Electronic:978-1-4799-1056-4; POD:978-1-4799-1055-7","10.1109/ColombianCC.2013.6637539","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637539","image collection summarization;information retrieval;latent factor analysis;machine learning;multimodal clustering","Algorithm design and analysis;Clustering algorithms;Feature extraction;Image reconstruction;Microwave integrated circuits;Semantics;Visualization","image reconstruction;image representation;image retrieval;set theory","MICS;collection visual content reconstruction;image collection exploration system;image retrieval;image subset;latent space;multimodal image collection summarization;optimal reconstruction subset selection;representative image set selection;representative semantic content;representative visual content;tagged image collection;textual modalities;visual modalities","","1","","19","","","21-23 Aug. 2013","","IEEE","IEEE Conference Publications"
"Cover song identification with direct chroma feature extraction from AAC files","T. M. Chang; E. T. Chen; C. B. Hsieh; P. C. Chang","Dept. of Commun. Eng., Nat. Central Univ., Jhongli, Taiwan","2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)","20131114","2013","","","55","56","This paper proposes a low-complexity and effective feature extraction method derived directly from AAC files. Unlike traditional methods that must decode audio files and then compute fast Fourier transform coefficients, the proposed system directly maps the modified discrete cosine transform coefficients into a 12-dimensional chroma feature without fully decoding it. To accelerate the matching time, segmentation is applied to reduce the time dimension in the feature space. In addition, the dynamic programming technique is used to match songs to various tempos. The experimental results show that the proposed system achieves a 62% accuracy rate, which is an improvement over the traditional FFT-based system, and reduces the computational complexity by approximately 35%.","2378-8143;23788143","Electronic:978-1-4799-0892-9; POD:978-1-4799-0891-2","10.1109/GCCE.2013.6664919","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664919","AAC;MDCT;chroma feature;cover song;music information retrieval","Accuracy;Computational complexity;Decoding;Dynamic programming;Feature extraction;Heuristic algorithms;Indexes","audio coding;discrete cosine transforms;dynamic programming;fast Fourier transforms;feature extraction;information retrieval;music;pattern matching","AAC file;FFT-based system;audio file decoding;audio segmentation;audio song matching time;cover song identification;direct chroma feature extraction;discrete cosine transform coefficient;dynamic programming technique;fast Fourier transform coefficient;feature space;music information retrieval","","5","","6","","","1-4 Oct. 2013","","IEEE","IEEE Conference Publications"
"Evaluating automatically estimated chord sequences","J. Pauwels; G. Peeters","STMS IRCAM-CNRS-UPMC, France","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","20131021","2013","","","749","753","In this paper, we perform an in-depth evaluation of a large number of algorithms for chord estimation that have been submitted to the MIREX competitions in 2010, 2011 and 2012. Therefore we first present a rigorous scheme to describe evaluation methods in a sound, unambiguous way that extends previous work specifically to take into account the large variance in chord estimation vocabularies and to perform evaluations on select sets of chords. Then we take a look at the evaluation metrics used so far and propose some alternative ones. Finally, we use these different methods to get a deeper insight into the strengths of each of the competing algorithms and show that the choice of evaluation measure greatly influences the ranking.","1520-6149;15206149","Electronic:978-1-4799-0356-6; POD:978-1-4799-0357-3; USB:978-1-4799-0355-9","10.1109/ICASSP.2013.6637748","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6637748","chord estimation;evaluation procedure;large scale evaluation;music information retrieval","Algorithm design and analysis;Estimation;Hidden Markov models;Limiting;Measurement;Music information retrieval;Vocabulary","information retrieval;music","MIREX competitions;automatically estimated chord sequence evaluation;chord estimation vocabulary;competing algorithms;music information retrieval","","2","","23","","","26-31 May 2013","","IEEE","IEEE Conference Publications"
"Improving Search Query Matching for Electronic TV Program Guide Data Extraction","D. Kiselev; R. Rzepka; K. Araki","Grad. Sch. of Inf. Sci. & Technol., Hokkaido Univ., Sapporo, Japan","2013 IEEE Seventh International Conference on Semantic Computing","20140102","2013","","","146","149","This paper describes a system for searching the Web-based Japanese TV program guide. The system features using morphological parsing and part-of-speech analysis to locate words with nominal and attributive semantic features in the query. Such words are matched mandatorily when searching the TV program guide text, while other words are matched optionally. Moreover, certain words and morphemes are removed from the query as they are considered to have little semantic value. The system checks every query against a stop list of such words and morphemes. Other processing methods, e.g. reversing the search phrase word order and allowing ""zero or more words"" between the search target words, are also utilized. The present paper uses TV guide search examples to demonstrate how the proposed method can improve Japanese TV program data search results. The paper also contains a few ideas about ways the method could be used for other languages.","","Electronic:978-0-7695-5119-7; POD:978-1-4799-1371-8","10.1109/ICSC.2013.34","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693509","EPG;Information Retrieval;Lexical Semantics;Morphological Parsing;NLP;Query Processing","Broadcasting;Educational institutions;Meteorology;Search problems;Semantics;TV","Internet;pattern matching;query processing;television;text analysis","TV program guide text;Web-based Japanese TV program guide;attributive semantic features;electronic TV program guide data extraction;morphological parsing;nominal semantic features;part-of-speech analysis;search query matching;word location;word matching","","1","","12","","","16-18 Sept. 2013","","IEEE","IEEE Conference Publications"
"Improving Feature Location by Enhancing Source Code with Stereotypes","N. Alhindawi; N. Dragan; M. L. Collard; J. I. Maletic","Dept. of Comput. Sci., Kent State Univ., Kent, OH, USA","2013 IEEE International Conference on Software Maintenance","20131202","2013","","","300","309","A novel approach to improve feature location by enhancing the corpus (i.e., source code) with static information is presented. An information retrieval method, namely Latent Semantic Indexing (LSI), is used for feature location. Adding stereotype information to each method/function enhances the corpus. Stereotypes are terms that describe the abstract role of a method, for example get, set, and predicate are well-known method stereotypes. Each method in the system is automatically stereotyped via a static-analysis approach. Experimental comparisons of using LSI for feature location with, and without, stereotype information are conducted on a set of open-source systems. The results show that the added information improves the recall and precision in the context of feature location. Moreover, the use of stereotype information decreases the total effort that a developer would need to expend to locate relevant methods of the feature.","1063-6773;10636773","Electronic:978-0-7695-4981-1; POD:978-1-4673-5218-5","10.1109/ICSM.2013.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676901","feature location;information retrieval;method stereotypes;program comprehension;software maintenance","Context;Feature extraction;Large scale integration;Open source software;Semantics;Standards;Taxonomy","indexing;information retrieval;program diagnostics;public domain software;source coding","LSI;feature location;information retrieval;latent semantic indexing;open-source systems;source code;static-analysis approach","","6","","33","","","22-28 Sept. 2013","","IEEE","IEEE Conference Publications"
"Exploring new features for music classification","R. Foucard; S. Essid; G. Richard; M. Lagrange","Institut Mines-T&#x00E9;l&#x00E9;com - TELECOM ParisTech, CNRS-LTCI 37, rue Dareau 75014 Paris, France","2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)","20131003","2013","","","1","4","Automatic music classification aims at grouping unknown songs in predefined categories such as music genre or induced emotion. To obtain perceptually relevant results, it is needed to design appropriate features that carry important information for semantic inference. In this paper, we explore novel features and evaluate them in a task of music automatic tagging. The proposed features span various aspects of the music: timbre, textual metadata, visual descriptors of cover art, and features characterizing the lyrics of sung music. The merit of these novel features is then evaluated using a classification system based on a boosting algorithm on binary decision trees. Their effectiveness for the task at hand is discussed with reference to the very common Mel Frequency Cepstral Coefficients features. We show that some of these features alone bring useful information, and that the classification system takes great advantage of a description covering such diverse aspects of songs.","2158-5873;21585873","Electronic:978-1-4799-0833-2; POD:978-1-4799-0831-8","10.1109/WIAMIS.2013.6616154","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616154","Autotag-ging;Boosting;Features;Missing features;Music information retrieval","Boosting;Feature extraction;Hidden Markov models;Instruments;Mel frequency cepstral coefficient;Music;Semantics","cepstral analysis;decision trees;identification technology;meta data;signal classification","appropriate features;automatic music classification;binary decision trees;boosting algorithm;cover art;induced emotion;mel frequency cepstral coefficients features;music automatic tagging;music genre;semantic inference;textual metadata;timbre;unknown songs;visual descriptors","","0","","17","","","3-5 July 2013","","IEEE","IEEE Conference Publications"
"CASIA: A fast audio indexing and retrieval: Application of CASIT method to audio documents","L. Guezouli; M. A. Ouddan; H. Essafi; L. Guezouli","LaSTiC UHL-Batna, 1, rue Boukhlouf Med EL Hadi, Batna University, 05000 Batna, Algeria","2013 3rd International Conference on Information Technology and e-Services (ICITeS)","20131010","2013","","","1","6","This paper propose a based CASIT [5] audio retrieval model. CASIT method is developed for our text information retrieval system witch gives good results. We have extended the CASIT method to the video and audio applications. In this paper we present its application in the audio content-based retrieval. The adapted method is called CASIA (in French: CAlcul de la SImilarité Audio). CASIA identify subsequence of audio using basic audio descriptors. Each audio document is pre-processed before the extraction of a set of basic audio descriptors, which characterize the temporal and the spectral information. Therefore an audio signal is represented by a characteristic sequence called in this paper ""sequence fingerprint"".","","Electronic:978-1-4799-0133-3; POD:978-1-4799-0130-2","10.1109/ICITeS.2013.6624069","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624069","Audio Processing;CASIA;Information Retrieval (IR);Sequence Fingerprint","Databases;Fingerprint recognition;Fourier transforms;Interference;Music;Speech;Vectors","audio signal processing;indexing;information retrieval","CASIA;CASIT method;audio content-based retrieval;audio document;audio indexing;audio retrieval model;sequence fingerprint;text information retrieval system","","0","","10","","","24-26 March 2013","","IEEE","IEEE Conference Publications"
