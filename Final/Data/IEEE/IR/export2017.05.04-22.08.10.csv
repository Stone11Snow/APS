"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7866108,7867740,7863182,7863189,7863268,7863211,7863195,7863201,7863184,7863185,7819481,7860039,7860355,7859985,7860065,7860212,7860551,7860022,7861172,7861031,7859990,7858437,7853465,7856757,7801816,7856756,7856753,7857204,7854182,7857226,7854187,7851501,7852322,7849999,7852346,7845088,7847757,7848049,7844724,7847211,7847043,7849104,7846663,7848537,7847030,7847824,7846930,7844355,7847042,7844746,7848459,7847115,7844220,7847925,7846566,7841077,7841019,7841085,7842109,7840320,7840978,7840666,7840982,7840986,7802629,7841015,7840780,7840845,7840980,7841081,7840984,7840910,7840985,7838590,7836732,7837995,7836581,7839613,7839516,7837956,7836738,7838279,7839601,7837912,7836384,7839524,7838231,7838050,7836091,7836039,7833308,7836045,7779108,7830087,7832920,7832908,7829757,7829900,7829770,7832918",2017/05/04 22:08:10
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Identifying and Scheduling Loop Chains Using Directives","I. J. Bertolacci; M. M. Strout; S. Guzik; J. Riley; C. Olschanowsky","Univ. of Arizona, Tucson, AZ, USA","2016 Third Workshop on Accelerator Programming Using Directives (WACCPD)","20170202","2016","","","57","67","Exposing opportunities for parallelization while explicitly managing data locality is the primary challenge to porting and optimizing existing computational science simulation codes to improve performance and accuracy. OpenMP provides many mechanisms for expressing parallelism, but it primarily remains the programmer's responsibility to group computations to improve data locality. The loopchain abstraction, where data access patterns are included with the specification of parallel loops, provides compilers with sufficient information to automate the parallelism versus data locality tradeoff. In this paper, we present a loop chain pragma and an extension to the omp for to enable the specification of loop chains and high-level specifications of schedules on loop chains. We show example usage of the extensions, describe their implementation, and show preliminary performance results for some simple examples.","","Electronic:978-1-5090-6152-5; POD:978-1-5090-6153-2","10.1109/WACCPD.2016.010","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836581","","Electronic mail;Fuses;Grammar;Processor scheduling;Programming;Schedules","abstract data types;application program interfaces;data handling;information retrieval;parallel programming;program compilers;scheduling","OpenMP;compilers;computational science simulation code optimization;data access patterns;data locality management;information;loop chain identification;loop chain pragma;loop chain scheduling;loopchain abstraction;parallel loops;porting","","","","","","","14-14 Nov. 2016","","IEEE","IEEE Conference Publications"
"Grafeno: Semantic graph extraction and operation","A. F. G. Sevilla; A. Fernández-Isabel; A. Díaz","Department of Software Engineering and Artificial Intelligence, Universidad Complutense de Madrid, Madrid, Spain","2016 Eleventh International Conference on Digital Information Management (ICDIM)","20170126","2016","","","133","138","Grafeno is a Natural Language Processing library for doing semantics. It represents semantic information with a graph structure, and is able to automatically extract this representation from the dependency analysis of a text. It aims to encompass the different possible approaches to doing graph semantics by being as modular and flexible as possible. It also provides functionality for operating on the graph and performing different experiments. In this article, we explain its design and use, and show its potential with two use cases.","","Electronic:978-1-5090-2641-8; POD:978-1-5090-2642-5","10.1109/ICDIM.2016.7829770","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829770","Concept Graphs;Information Extraction;Natural Language Processing;Semantics","Data mining;Libraries;Pipelines;Pragmatics;Semantics;Syntactics;Transforms","graph theory;information retrieval;natural language processing;semantic networks;text analysis","Grafeno;graph semantics;graph structure;natural language processing library;semantic graph extraction;semantic information representation;text dependency analysis","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"From Query to Usable Code: An Analysis of Stack Overflow Code Snippets","D. Yang; A. Hussain; C. V. Lopes","Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","391","401","Enriched by natural language texts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries. With the goal of developing automated tools with the Stack Overflow snippets and surrounding text, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When using text search engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets? A total of 3M code snippets are analyzed across four languages: C#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.047","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832918","automatic program generation;code mining","C# languages;Google;Java;Natural languages;Syntactics;Usability;Web search","C# language;Java;authoring languages;natural language processing;query processing;question answering (information retrieval);search engines;source code (software)","C# language;Google search;Java language;JavaScript language;Python language;Stack Overflow code snippets;annotated snippets;automatic program generation;code-centric knowledge base;natural language annotations;natural language queries;natural language questions and answers;source code;text search engines","","","","","","","14-15 May 2016","","IEEE","IEEE Conference Publications"
"An Algorithm for Analyzing the City Residents' Activity Information through Mobile Big Data Mining","Y. Guo; J. Zhang; Y. Zhang","Dept. of Comput. Sci., Nankai Univ., Tianjin, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","2133","2138","The increasing of mobile devices results in the recent mobile big data era. A large number of useful information can be extracted from mobile big data. Extracting the residents' activity information from mobile big data is more and more popular in recent years because of its lower cost and higher accuracy. In this paper, we propose an algorithm to mine some meaningful residents' activity information from massive mobile data using Apache Spark. We first develop a weighted agglomerative hierarchical clustering algorithm to dig out the hot areas of a city and then the pedestrian flow of one hot area is analyzed. Next, we screen out those people who work in one hot area and dig out their residences and the destinations where they go after work and then the distribution situation of their residences and destinations are get based on the weighted agglomerative hierarchical clustering algorithm mentioned above. The results of this research reflect the city residents' activity information more authentic because the data we use is massive and is generated by the real activities of city residents.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0328","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847211","Apache Spark;data mining;mobile big data;the residents' activity information","Base stations;Big data;Clustering algorithms;Data mining;Mobile communication;Mobile handsets;Urban areas","Big Data;data mining;information retrieval;mobile computing;pattern clustering;pedestrians;town and country planning;traffic engineering computing","Apache Spark;city planning;city resident activity information analysis;information extraction;mobile Big Data mining;pedestrian flow;traffic planning;weighted agglomerative hierarchical clustering algorithm","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Statistical Atmospheric Parameter Retrieval Largely Benefits From Spatial–Spectral Image Compression","J. García-Sobrino; J. Serra-Sagristà; V. Laparra; X. Calbet; G. Camps-Valls","Group on Interactive Coding of Images, Universitat Aut&#x00F2;noma de Barcelona, Bellaterra, Spain","IEEE Transactions on Geoscience and Remote Sensing","20170224","2017","55","4","2213","2224","The infrared atmospheric sounding interferometer (IASI) is flying on board of the Metop satellite series, which is part of the EUMETSAT Polar System. Products obtained from IASI data represent a significant improvement in the accuracy and quality of the measurements used for meteorological models. Notably, the IASI collects rich spectral information to derive temperature and moisture profiles, among other relevant trace gases, essential for atmospheric forecasts and for the understanding of weather. Here, we investigate the impact of near-lossless and lossy compression on IASI L1C data when statistical retrieval algorithms are later applied. We search for those compression ratios that yield a positive impact on the accuracy of the statistical retrievals. The compression techniques help reduce certain amount of noise on the original data and, at the same time, incorporate spatial-spectral feature relations in an indirect way without increasing the computational complexity. We observed that compressing images, at relatively low bit rates, improves results in predicting temperature and dew point temperature, and we advocate that some amount of compression prior to model inversion is beneficial. This research can benefit the development of current and upcoming retrieval chains in infrared sounding and hyperspectral sensors.","0196-2892;01962892","","10.1109/TGRS.2016.2639099","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7819481","Infrared atmospheric sounding interferometer (IASI);JPEG 2000;M-CALIC;kernel methods;lossy compression;near-lossless compression;spectral transforms;statistical retrieval","Atmospheric measurements;Atmospheric modeling;Image coding;Satellites;Temperature measurement;Transform coding;Transforms","atmospheric techniques;atmospheric temperature;data compression;hyperspectral imaging;information retrieval;infrared detectors","EUMETSAT polar system;IASI L1C data;IASI data;Metop satellite series;atmospheric forecasts;hyperspectral sensor;infrared atmospheric sounding interferometer;infrared sounding sensor;meteorological model;moisture profile;retrieval chains;spatial-spectral image compression;statistical atmospheric parameter retrievals;statistical retrieval algorithms;temperature profile;trace gases;water vapor atmospheric profile","","","","","","20170116","April 2017","","IEEE","IEEE Journals & Magazines"
"An efficient parallel topic-sensitive expert finding algorithm using spark","Y. M. Yang; C. D. Wang; J. H. Lai","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P. R. China","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3556","3562","Expert finding is an important technique to obtain the user authority ranking in community question answering (CQA) websites. ZhihuRank is a topic-sensitive expert finding algorithm, which is based on both LDA and PageRank. Currently, with the amount of participants and documents increasing rapidly in CQA websites, how to parallel expert finding algorithms for big data analysis has received significant attention. In this paper, we find that the Spark framework is more suitable for paralleling expert finding algorithms than the MapReduce framework, which is a memory-based parallel computing model to support complicated iterative algorithms. As an example, we parallel ZhihuRank using MLlib's LDA and GraphX's PageRank in Spark. Experiments have been conducted on large-scale real data from Zhihu<sup>1</sup> (the most popular CQA website in China). And the experimental results confirmed the effectiveness and scalability of our proposed approach.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841019","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841019","Community Question Answering;Distributed Computing;Expert Finding;LDA;PageRank","Algorithm design and analysis;Computational modeling;Heuristic algorithms;Libraries;Machine learning algorithms;Programming;Sparks","Big Data;Web sites;data analysis;iterative methods;parallel algorithms;question answering (information retrieval);search engines","CQA Web sites;LDA;MapReduce framework;PageRank;Spark framework;ZhihuRank;big data analysis;community question answering Web sites;iterative algorithms;memory-based parallel computing model;parallel topic-sensitive expert finding algorithm;user authority ranking","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A data mining application on moving object data","Y. Yakufu; C. E. Atay","Graduate School of Natural and Applied Sciences, Dokuz Eylul University, Izmir, Turkey","2016 Eleventh International Conference on Digital Information Management (ICDIM)","20170126","2016","","","91","95","With the wide availability of GPS devices in our lives, massive amounts of object movement data have been collected from various moving object targets, such as mobile devices, animals, and vehicles. In the last decade, Moving Object Databases (MOD) have attracted many researchers. Analyzing such data has deep implications in many areas, such as ecological study and traffic control. In this study, we focus on moving object data (moving points) analysis and retrieve valuable information for knowledge discovery. In this research, a moving object data model is implemented in the object-relational database system, additionally some special queries and data mining techniques are performed. Retrieving information directly from unorganized spatial-temporal data is almost impossible. However, not only a vast amount of spatial-temporal data sets organized into MOD data model but also the discovery of valuable knowledge from spatial-temporal data to help decision support processes is possible now owing to this research implementation.","","Electronic:978-1-5090-2641-8; POD:978-1-5090-2642-5","10.1109/ICDIM.2016.7829757","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829757","MOD;moving object data;trajectory data mining","Data mining;Data models;Databases;Knowledge discovery;Object oriented modeling;Spatiotemporal phenomena;Trajectory","Global Positioning System;data analysis;data mining;data models;decision support systems;information retrieval;relational databases;visual databases","GPS devices;MOD;data mining;data model;decision support processes;information retrieval;knowledge discovery;moving object data analysis;moving object databases;object-relational database system;spatialtemporal data;special queries","","","","","","","19-21 Sept. 2016","","IEEE","IEEE Conference Publications"
"The past is never dead. It's not even past. 3D models for the knowledge of cultural heritage","A. Ippolito; C. Bartolomei; M. Attenni","Department of History, Drawing and Restoration of Architecture, Sapienza University of Rome, Rome, Italy","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","8","The research aims to define the operative procedure for constructing digital archives based on three-dimensional models generated by massive and fast acquisition of data regarding objects of architecture. Today digital objects have become a tradition in the cultural production and a business cycle with the objective to disseminate and communicate cultural data. The problem centers on a definition of data necessary for setting up digital archives. These data will be extracted from material obtained from surveys as well as from informative documentation and need to be chosen and structured. Hence the research aims at identifying the kind of information, tangible and intangible, principal and essential of the Architectural object and classifying them into distinctive and characteristic categories which make it possible to cognize the given object.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863182","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863182","Architectural data communication;Archives 1D-2D-3D;Survey;Villa Borghese;metadata;photomodeling","Cultural differences;Data models;Global communication;Instruments;Solid modeling;Three-dimensional displays;Two dimensional displays","architecture;computer graphics;history;information retrieval systems;pattern classification","3D models;architectural object;classification;cultural heritage;digital archives;three-dimensional models","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"Towards fast algorithms for estimating Personalized PageRank using commonly generated random walks","D. Vial; V. Subramanian","University of Michigan, United States","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","852","855","Personalized PageRank (PPR) is a measure of the importance of the nodes in graph G = (V, E) from the perspective of some s E V . PPR has been used in many applications, such as recommending who a user should follow on Twitter. Due to the scale of networks of interest, precomputing PPR requires prohibitive storage, while computing exactly at query time is slow. As such, many PPR estimation algorithms have been proposed. In this extended abstract, we focus on the related PPR search problem, wherein one aims to estimate PPR of T = {v E V : v relevant to query from s}. We begin with a description of PPR and PPR search using Bidirectional-PPR. We provide a path interpretation of this algorithm, which leads us to define a class of similar algorithms. We conclude with two such algorithms intended for use in PPR search.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852322","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852322","","Algorithm design and analysis;Clustering algorithms;Data mining;Markov processes;Partitioning algorithms;Standards;Twitter","information retrieval;search engines;search problems;social networking (online)","PPR;PPR estimation algorithm;Personalized PageRank;Twitter;prohibitive storage;search problem","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Privacy-preserving string search for genome sequences with FHE bootstrapping optimization","Y. Ishimaki; H. Imabayashi; K. Shimizu; H. Yamana","Waseda University, Tokyo Japan","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3989","3991","Privacy-preserving string search is a crucial task for analyzing genomics-driven big data. In this work, we propose a cryptographic protocol that uses Fully Homomorphic Encryption (FHE) to enable a client to search on a genome sequence database without leaking his/her query to the server. Though FHE supports both addition and multiplication over encrypted data, random noise inside ciphertexts grows with every arithmetic operation especially multiplication, which results in incorrect decryption when the noise amount exceeds its threshold called level. There are two approaches to avoid the incorrect decryption: one is setting the sufficient level that assures correct decryption within the limited number of operations, and the other is resetting the noise by the method called bootstrapping. It is important to find an optimal balance between overhead caused by the level and overhead caused by the bootstrapping, since using higher level deteriorates the performance of all the arithmetic operations, while the more number of bootstrappings causes more expensive overhead. In this study, we propose an efficient approach to minimize the number of bootstrappings while reducing the level as much as possible. Our experimental result shows that it runs at most 10 times faster than a naïve approach.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841085","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841085","Bootstrapping;Fully Homomorphic Encryption (FHE);Genome Sequence;PBWT;String Search","Bioinformatics;Databases;Encryption;Genomics;Protocols;Servers","Big Data;cryptography;data analysis;data privacy;genomics;information retrieval;medical information systems;random noise","FHE bootstrapping optimization;arithmetic operation;bootstrappings;ciphertexts;cryptographic protocol;data encryption;fully homomorphic encryption;genome sequence database;genomics-driven Big Data analysis;level;multiplication operation;privacy-preserving string search;random noise","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Extending hybrid Conditional Random Fields approach of Named Entity Recognition for Marathi tweets","M. L. Patawar; M. A. Potey","Dept. of Computer Engineering, D. Y. Patil College of Engineering, Akurdi, Pune, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","5","Named Entity Recognition (NER) is gaining popularity in multiple Information Retrieval applications as it facilitates information extraction. Main goal of NER is to obtain named entities which are usually proper nouns, temporal entities and numerical values. Initial Named Entity Recognizers were designed to deal with formal English text. With increased use of social media, many IR and Natural Language Processing based applications designed to get information from short text like tweets. Formal text based standard NER systems fail to deal with such short text due to limited information and presence of noise. Ample work has been done for NER systems which handle English short text. Relatively Indian languages, especially Marathi, need to build dedicated NER systems to extract named entities from tweets. From multiple approaches of NER, a Conditional Random Fields (CRFs) has shown good accuracy for some Indian languages like Hindi. So here we have proposed a dedicated NER system with CRF based hybrid approach to identify NEs from Marathi tweets. Multiple linguistic features are used in addition to Marathi gazetteers to facilitate the task.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7860039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860039","Gazetteers;Linguistic features;Named Entities;Named Entity Recognition;Parts of Speech tags;Tweets","Feature extraction;Natural language processing;Pragmatics;Social network services;Standards;Training","information retrieval;natural language processing;social networking (online)","CRF;Hindi;IR;Indian languages;Marathi gazetteers;Marathi tweets;NER;formal English text;formal text based standard NER systems;hybrid conditional random fields approach;information retrieval applications;named entity recognition;natural language processing based applications;short text like tweets;social media","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Indexing process insight and evaluation","H. Kaur; V. Gupta","UIET, Panjab University, Chandigarh","2016 International Conference on Inventive Computation Technologies (ICICT)","20170126","2016","3","","1","5","Indexing is an important process in Information Retrieval (IR) systems. It forms the core functionality of the IR process since it is the first step in IR and assists in efficient information retrieval. Indexing reduces the documents to the informative terms contained in them. It provides a mapping from the terms to the respective documents containing them. Once effective index has been built for the collection of documents, retrieval process is simplified. Indexing proceeds at four stages namely content specification, tokenization of documents, processing of document terms, and index building. The index can be stored in the form of different data structures namely direct index, document index, lexicon and inverted index. Index can be built by applying different algorithms or schemes such as single-pass in-memory indexing, blocked-indexing, etc. This paper explains the indexing process with the various data structures and algorithms used for indexing and finally analyses the different indexing approaches with respect to time, memory usage and mean average precision.","","DVD:978-1-5090-1283-1; Electronic:978-1-5090-1285-5; POD:978-1-5090-1286-2","10.1109/INVENTIVE.2016.7830087","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7830087","index;index compression;indexing;stemming;tokenization","Algorithm design and analysis;Data structures;Dictionaries;Encoding;Indexing;Vocabulary","indexing;information retrieval systems","IR system;content specification;data structures;direct index;document index;document terms processing;documents tokenization;index building;indexing approach;indexing process;information retrieval system;informative terms;inverted index;lexicon","","","","","","","26-27 Aug. 2016","","IEEE","IEEE Conference Publications"
"Dynamically visualizing the relationships between Web pages depending on user's search viewpoints","D. Arakawa; H. Nakayama; R. Onuma; H. Kaminaga; Y. Miyadera; S. Nakamura","Department Computer Science and Mathematics, Fukushima University, 1 Kanayagawa, 960-1296, Japan","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","20170216","2016","","","474","475","Opportunities of continuous Web exploration with trial and error have been increased in intellectual creative activities such as research work and PBL. However, it is often difficult to pick up useful pages. In such exploration, information that a user particularly needs and his/her viewpoints changes in accordance with progress of the work and trial and error in it. Therefore, it is important but difficult to understand the relationships between obtained pages and to examine the promising information based on the latest perspectives. We aimed to develop the mechanism for dynamic visualization of the relationships between the pages. This paper mainly describes the outline of our dynamic visualization and methods for extracting the relationships between the pages using focused page of a user as a base point.","","Electronic:978-1-908320-73-5; POD:978-1-5090-4852-6","10.1109/ICITST.2016.7856757","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856757","Dynamic Visualization;Relationships Between Web Pages;Search History Data;Web Search","","Internet;data visualisation;information retrieval","Web exploration;Web page;dynamic visualization mechanism;relationship extraction;user search viewpoint","","","","","","","5-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Optimizing Social Connections for Efficient Information Acquisition","C. Kong; G. Luo; L. Tian; X. Cao","Dept. of Comput. Sci., Georgia State Univ., Atlanta, GA, USA","2016 IEEE Global Communications Conference (GLOBECOM)","20170206","2016","","","1","6","Social networks such as Twitter and Facebook have become important sources for users to acquire information. In those social networks, users obtain information from the posts/reposts of their social connections. To acquire information efficiently, users are motivated to connect to users who offer attractive and timely information. In this paper, we study how to effectively optimize social connections to optimize the efficiency of information Acquisition. We define this as the problem of Social Connection Optimization for efficient Information Acquisition (SCOIA). We present our analysis on the information accuracy and timeliness to measure the efficiency of information acquisition. Based on the analysis, a novel User Set Selection (USS) algorithm is then proposed to efficiently solve the SCOIA problem. Our simulations based on the crawled Twitter dataset show that the proposed algorithm can efficiently identify user connections, leading to high information acquisition accuracy, low spam rate and low information acquisition latency.","","Electronic:978-1-5090-1328-9; POD:978-1-5090-1329-6","10.1109/GLOCOM.2016.7842109","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7842109","","Computer science;Correlation;Electronic mail;Information processing;Optimization;Twitter","information retrieval;social networking (online)","SCOIA;USS;information accuracy;information timeliness;social connection optimization for efficient information acquisition;social networks;user set selection","","","","","","","4-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Combined with augmented reality navigation applications in the library","D. Y. Liu","Department of Applied Geoinformatics, Chia Nan University of Pharmacy & Science, Taiwan","2016 International Conference on Advanced Materials for Science and Engineering (ICAMSE)","20170206","2016","","","441","443","Due to the increasing popularity of mobile devices and network caused by the traditional way of navigation has been gradually combined with the convenience of mobile devices, and the integration of voice, video, wireless transmission and database technology, through real-time Internet transmission to develop a mobile navigation system. Mobile navigation system does not require a lot of written information, they provide a lot of information can be stored and organized, and can be combined with virtual reality and augmented reality technology, when the navigation application can be used as large-scale exhibition. Currently mobile navigation systems are also widely used in museums, galleries, libraries and other exhibitions in historic buildings. Thus, in the case of scientific and technological progress, more diversified ways of learning, and the library is also actively provide opportunities for mobile learning through mobile devices. This article will explore the current situation combined with augmented reality mode library in navigating, and future trends provide mobile learning in the library.","","Electronic:978-1-5090-3869-5; POD:978-1-5090-3870-1","10.1109/ICAMSE.2016.7840320","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840320","Augmented Reality;Mobile navigation","Augmented reality;Computers;Libraries;Mobile communication;Mobile handsets;Navigation","Internet;augmented reality;digital libraries;information retrieval;mobile computing;museums;real-time systems","augmented reality navigation;galleries;historic buildings;libraries;mobile devices;mobile navigation system;museums;real-time Internet transmission;virtual reality","","","","","","","12-13 Nov. 2016","","IEEE","IEEE Conference Publications"
"Forensic Analysis of Email on Android Volatile Memory","L. Chen; Y. Mao","Inst. of Comput. Forensics, Chongqing Univ. of Posts & Telecommun., Chongqing, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","945","951","With the popularity of smart phones and the emergence of the mobile office mode, the traditional email forensics that works for computer has been already unable to satisfy the demands of reality, so forensic work needs to be expanded to a range of mobile devices, such as mobile phone, tablet, etc. In this paper, we will focus on examining if we can discover email-related information in the volatile memory of the mobile phone. Specifically, we choose Android mobile as a research focus, and two Chinese mainstream Android email applications-MailMaster and QQMail are as email client to the experimental test. Finally, we not only sort out the email-related information stored in the volatile memory, but also identify the patterns of the information saved in the memory. Moreover, based on these patterns, we also develop a tool named EmailFinder that can automatically extract the email-related information from memory dump. It can be utilized as a forensic tool on Android phones to assist forensic investigators retrieve email-related evidence from memory dump.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0160","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847043","email client;email forensics;email-related information;patterns;volatile memory","Data mining;Electronic mail;Forensics;Kernel;Mobile communication;Smart phones","digital forensics;digital storage;electronic mail;information retrieval;mobile computing;smart phones","Android volatile memory;Chinese mainstream Android email applications;EmailFinder;MailMaster;QQMail;automatic email-related information extraction;email-related evidence retrieval;forensic analysis;memory dump;mobile devices;mobile office mode;mobile phone;smart phones;tablet","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"SSDP: A Simple Evolutionary Approach for Top-K Discriminative Patterns in High Dimensional Databases","T. Pontes; R. Vimieiro; T. B. Ludermir","Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","361","366","It is a great challenge to companies, governments and researchers to extract knowledge in high dimensional databases. Discriminative Patterns (DPs) is an area of data mining that aims to extract relevant and readable information in databases with target attribute. Among the algorithms developed for search DPs, it has highlighted the use of evolutionary computing. However, the evolutionary approaches typically (1) are not adapted for high dimensional problems and (2) have many nontrivial parameters. This paper presents SSDP (Simple Search Discriminative Patterns), an evolutionary approach to search the top-k DPs adapted to high dimensional databases that use only two easily adjustable external parameters.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.072","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839613","Contrast Sets;Discriminative Patterns;Emerging Patterns;Evolutionary Computing;Microarray;Subgroups Discovery","Companies;Databases;Genetics;Measurement;Search problems;Sociology;Statistics","data mining;information retrieval;very large databases","SSDP;data mining;evolutionary approach;evolutionary computing;high dimensional databases;high dimensional problems;information extraction;knowledge extraction;nontrivial parameters;simple search discriminative patterns;top-k DP;top-k discriminative patterns","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"WEFEST: Word Embedding Feature Extension for Short Text Classification","L. Sang; F. Xie; X. Liu; X. Wu","Sch. of Comput. Sci. & Inf. Eng., Hefei Univ. of Technol., Hefei, China","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","677","683","Short text classification is a crucial task for information retrieval, social medial text categorization, and many other applications. In reality, due to the inherent sparsity and the limited information available in the short texts, learning and classifying short texts is a significant challenge. In this paper, we propose a new framework, WEFEST, which expands short texts using word embedding for classification. WEFEST is rooted on the deep language model, which learns a new word embedding space, by using word correlations, such that semantically related words also have close feature vectors in the new space. By using word embedding features to help expand the short tests, WEFEST can enrich the word density in the short texts for effective learning, by following three major steps. First, each short text in the training dataset is enriched by using pre-trained word feature embedding. Then the semantic similarity between two short texts is calculated by using the statistical frequency information retrieved from the trained model. Finally, we use the nearest neighbor algorithm to achieve short text classification. Experimental results on Chinese news title dataset validate the effectiveness of the proposed method.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0101","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836732","","Computational modeling;Context;Neural networks;Search engines;Semantics;Training;Vocabulary","information retrieval;natural language processing;statistical analysis;text analysis","Chinese news title dataset;WEFEST;deep language model;nearest neighbor algorithm;pre-trained word feature embedding;semantic similarity;statistical frequency information retrieval;word embedding feature extension for short text classification","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Digital archiving for interdisciplinary knowledge transfer in intangible heritage","S. W. Y. Hedren","Library, Student & Academic Services Department, Nanyang Technological University, Singapore","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","4","“Exploring the crossroads of linguistic diversity: language contact in Southeast Asia” is an interdisciplinary project with a disparate team of 11 researchers from linguistics, art, design and media. A range of digital assets in different formats, including publications, films and datasets, were created from the fieldwork and research done. Using this project as a case study, this paper seeks to explore an approach on how digital assets from an interdisciplinary research project can be captured, preserved and (re)presented in a form of a digital archive. It results in a digital archive with dedicated views for each type of digital asset to meet specific viewing needs. It also used a modular design approach to achieve flexibility and meet the knowledge transfer objectives of the research project.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863189","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863189","digital archive;intangible heritage;interdisciplinary research;knowledge transfer","Asia;Films;Knowledge transfer;Layout;Metadata;Pragmatics;Videos","art;information retrieval systems;linguistics","art;design;digital archive;digital assets;intangible heritage;interdisciplinary knowledge transfer;interdisciplinary research project;linguistics;media;modular design","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"Keyphrase extraction methodology from short abstracts of medical documents","E. Amer; K. M. Fouad","Computer Science Dept., Faculty of Computers and Informatics, Banha University, Egypt","2016 8th Cairo International Biomedical Engineering Conference (CIBEC)","20170130","2016","","","23","26","Keyphrases are the keywords that provide the semantic metadata about document that represents the top concepts inside an article. Number of natural language processing (NLP) methodologies; such as text summarization, text mining and information retrieval applications, consider Keyphrase Extraction as a critical step to accomplish its tasks. This paper presents a method to extract keyphrases from abstracts of medical research papers. The proposed method utilizes DBpedia to gain access to terms that may be of interest to the candidate keyphrases extracted from the original document. Experimental results showed that the proposed method outperforms other related methods in terms of higher precision and F-measure values.","","Electronic:978-1-5090-2987-7; POD:978-1-5090-2988-4","10.1109/CIBEC.2016.7836091","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836091","Keyphrase Extraction;medical abstracts","Computer science;Data mining;Encyclopedias;Feature extraction;Natural language processing;Semantics","data mining;electronic health records;feature extraction;information retrieval;meta data;natural language processing;text analysis","DBpedia;F-measure values;information retrieval applications;keyphrase extraction methodology;medical documents;medical research papers;natural language processing;semantic metadata;short abstracts;text mining;text summarization","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Enriched Multimodal Representations of Music Performances: Online Access and Visualization","E. Maestre; P. Papiotis; M. Marchini; Q. Llimona; O. Mayor; A. Pérez; M. M. Wanderley","Universitat Pompeu Fabra","IEEE MultiMedia","20170209","2017","24","1","24","34","The authors provide a first-person outlook on the technical challenges involved in the recording, analysis, archiving, and cloud-based interchange of multimodal string quartet performance data as part of a collaborative research project on ensemble music making. To facilitate the sharing of their own collection of multimodal recordings and extracted descriptors and annotations, they developed a hosting platform through which multimodal data (audio, video, motion capture, and derived signals) can be stored, visualized, annotated, and selectively retrieved via a web interface and a dedicated API. This article offers a twofold contribution: the authors open their collection of enriched multimodal recordings, the Quartet dataset, to the community, and they introduce and enable access to their multimodal data exchange platform and web application, the Repovizz system. This article is part of a special issue on multimedia technologies for enriched music.","1070-986X;1070986X","","10.1109/MMUL.2017.3","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849104","Internet/web technologies;audio;bowed string;data analysis;data exchange;motion capture;multimedia;multimodal data;music performance;remote visualization;string quartet;video;visualization;web access","Collaborateive work;Context awareness;Data visualization;Instruments;Media;Microphones;Multiple signal classification;Music;Recording","Internet;cloud computing;data visualisation;information retrieval;multimedia computing;music","Repovizz system;Web interface;cloud-based multimodal string quartet performance data interchange;descriptor annotation;descriptor extraction;enriched multimodal music performance representations;ensemble music making;multimedia technologies;multimodal data annotation;multimodal data storage;multimodal data visualisation;multimodal recordings;online access;online visualization;quartet dataset;selective data retrieval","","","","","","","Jan.-Mar. 2017","","IEEE","IEEE Journals & Magazines"
"Extracting topic keywords from Sina Weibo text sets","S. Xu; J. Guo; X. Chen","School of Computer Engineering and Science, Shanghai University, 200444, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","668","673","Sina Weibo is one of the most popular microblogging website in China. It has more than 500 million registered users and the daily production of posters is over 100 million, with a market penetration similar to Twitter. Mining the useful information from large volume of fragmented short texts is a fundamental but very challenging research work. This paper proposes a method LET(LDA&Entropy&Tex-trank) to extract topic keywords from Sina Weibo topics text sets. LET considers both topic influence of keywords and topic discrimination of keyword that combines the merits of LDA, Entropy and TextRank. In addition, we design a new standard evaluation method KESS (topic KEywords Sta-ndard Sequence). Based on KESS, we can compute the offset loss scores for the four different keywords extraction methods. Extensive simulations show that LET is a comparatively efficient and effective method to obtain topic words from hot topics of Sina Weibo.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846663","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846663","Entropy;KESS;LDA;LET;Short texts;Sina Weibo;Topic keywords","Entropy;Hidden Markov models;Market research;Mathematical model;Semantics;Standards;Twitter","Web sites;data mining;information retrieval;text analysis","China;KESS;LDA&entropy&tex-trank;LET;Sina Weibo text sets;Twitter;information mining;market penetration;microblogging Website;topic discrimination;topic keyword extraction;topic keywords standard sequence","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"Implementation AES with digital signature for secure web-based electronic archive","T. Hermawan; R. W. Wardhani","Sekolah Tinggi Sandi Negara, National Crypto Institute, 16330 Bogor, Indonesia","2016 8th International Conference on Information Technology and Electrical Engineering (ICITEE)","20170228","2016","","","1","6","This research addresses adding the encryption technic and a digital signature technic to web-based electronic archive in order to prevent cybercrime problem such as robbery, modification and unauthorized access. The Secure Electronic Archive Application intends to combine given services such as confidentiality, integrity, authentication, and non repudiation. Encryption is used to give a confidentiality service with 128 bit AES algorithm. In this research encryption implements to give a confidential service with 128 bit AES algorithm. AES 128 bit algorithm has not been exposed to related key attack. The digital signature is used to provide integrity, authentication, and non repudiation with SHA 256 bit and RSA 1024 bit. The Application will be secured by entering the archive name, symmetric key, private key, and passphrase. Implemented in web programming, Analysis of the application will be explained by a use case diagram and the testing would be web testing. Such as Functional testing, Usability Testing, Interface Testing, Compatibility Testing, Performance Test, and Security Test. Encryption implementation in this research can prevent archive thievery which is shown on implementation and proved on web testing. As the result of Functional Testing, the archive can not be read after it has been encrypted. Therefore, the aplication can give information about archive modification and authentication with showing a notification after the sign has been done.","","Electronic:978-1-5090-4139-8; POD:978-1-5090-4140-4; USB:978-1-5090-4138-1","10.1109/ICITEED.2016.7863268","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863268","digital signature;encryption;secure web-based electronic archive based on web programming","Authentication;Digital signatures;Encryption;Organizations;Testing","Internet;digital signatures;information retrieval systems;program testing","128 bit AES algorithm;Web programming;Web-based electronic archive security;archive name;compatibility testing;cybercrime problem prevention;digital signature;functional testing;interface testing;passphrase;performance test;private key;security test;symmetric key;usability testing","","","","","","","5-6 Oct. 2016","","IEEE","IEEE Conference Publications"
"Algorithm for finding influential user: Based on user's information diffusion region","A. Namtirtha; S. Gupta; A. Dutta; B. Dutta; F. Coenen","Department of IT, NIT Durgapur, India","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2734","2738","To spread emergency information through a social network many people keep track of influential users for faster diffusion process. But due to the enormous growth of social media volume, identifying influential user is quite challenging. Many researchers have focused on user's micro-blogging activities to find influential user. We propose a new algorithm to rank the user on the basis of the user's probable information diffusion region that is information reachability in the network. Our algorithm successfully able to find out top-influential users in case of email and random datasets. The presented comparison study shows that our proposed algorithm outperforms PageRank and Highest Follower algorithms.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848537","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848537","","Blogs;Correlation;Diffusion processes;Electronic mail;Measurement;Network topology;Social network services","information dissemination;information retrieval;social networking (online)","PageRank;email datasets;emergency information;highest follower algorithms;influential user;information reachability;micro-blogging activities;random datasets;social media volume;social network;user information diffusion region","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"Opening up dark digital archives through the use of analytics to identify sensitive content","B. B. Borden; J. R. Baron","Drinker Biddle & Reath LLP, Washington, D.C.","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3224","3229","The Nation's history is going dark: without technological solutions, presidential and federal e-mail and other electronic records accessioned into the US National Archives will remain effectively inaccessible to the public due to sensitive content considerations, including most notably PII, for many decades. Analytics offers the means for achieving earlier public access to digital collections of public records while protecting the privacy of records creators and third-parties.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840978","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840978","PII;analytics;archives;predictive coding;sensitive data;technology assisted review","Electronic mail;Law;Libraries;NIST;Privacy","data privacy;electronic mail;government data processing;information retrieval systems","US National Archives;dark digital archives;e-mail;electronic records;privacy protection;public records digital collection;sensitive content identification","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Data-Oriented M2M Messaging Mechanism for Industrial IoT Applications","Z. Meng; Z. Wu; C. Muvianto; J. Gray","School of Electrical and Electronic Engineering, The University of Manchester, Manchester, U.K.","IEEE Internet of Things Journal","20170216","2017","4","1","236","246","Machine-to-machine (M2M) communication is a key enabling technology for the future industrial Internet of Things applications. It plays an important role in the connectivity and integration of computerized machines, such as sensors, actuators, controllers, and robots. The requirements in flexibility, efficiency, and cross-platform compatibility of the intermodule communication between the connected machines raise challenges for the M2M messaging mechanism toward ubiquitous data access and events notification. This investigation determines the challenges facing the M2M communication of industrial systems and presents a data-oriented M2M messaging mechanism based on ZeroMQ for the ubiquitous data access in rich sensing pervasive industrial applications. To prove the feasibility of the proposed solution, the EU funded PickNPack production line with a reference industrial network architecture is presented, and the communication between a microwave sensor device and the quality assessment and sensing module controller of the PickNPack line is illustrated as a case study. The evaluation is carried out through qualitative analysis and experimental studies, and the results demonstrate the feasibility of the proposed messaging mechanism. Due to the flexibility in dealing with hierarchical system architecture and cross-platform heterogeneity of industrial applications, this messaging mechanism deserves extensive investigations and further evaluations.","2327-4662;23274662","","10.1109/JIOT.2016.2646375","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801816","Collaborative automation;ZeroMQ (ZMQ);industrial Internet of Things (IIoT);machine-to-machine (M2M);messaging mechanism","","Internet of Things;electronic messaging;hierarchical systems;industrial engineering;information retrieval;machine-to-machine communication;microwave devices;sensors","EU funded PickNPack production line;ZeroMQ;computerized machines;cross-platform compatibility;cross-platform heterogeneity;data-oriented M2M messaging mechanism;hierarchical system architecture;industrial Internet of Things applications;industrial IoT applications;intermodule communication;machine-to-machine communication;microwave sensor device;quality assessment controller;quality sensing module controller;reference industrial network architecture;ubiquitous data access;ubiquitous event notification","","","","","","20161229","Feb. 2017","","IEEE","IEEE Journals & Magazines"
"Division and replication for data with public auditing scheme for cloud storage","S. D. Salunkhe; D. Patil","Computer Networks, NMIET, Talegaon Dabhade, Pune, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","5","Data Outsourcing in cloud computing, require high security measures as the data may be tampered by illegal users or attackers. This may affect the performance of cloud, as an increase in retrieval time of data. While applying the security measures it is required to take under consideration the retrieval time of data. In this paper, both the issues are tried to overcome by scattering the data file over the cloud nodes. The cloud nodes store only a fraction of the file which is divided into multiple fragments. The cloud nodes are aligned with certain distance using the concept T-coloring where an attacker is unable to track the next node. To improve the retrieval time the fragments are replicated over the nodes which are remained by placing the fragments. To enhance the security standard the fragmented file is encrypted before placement on cloud node. An auditing scheme is employed which continually monitor the cloud nodes. If any damage occurs the auditing system regenerate the data. This system provide security and auditing of cloud data.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7859985","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859985","Audit;Decryption;Encryption;Fragments","Cloud computing;Encryption;Image color analysis;Servers;Systems architecture","auditing;cloud computing;cryptography;information retrieval;outsourcing;storage management","T-coloring;cloud computing;cloud data auditing;cloud data security;cloud nodes;cloud performance;cloud storage;data division;data file scattering;data outsourcing;data replication;data retrieval time;file fragmentation;public auditing;security standard","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Using the IBM Watson cognitive system in educational contexts","I. Kollia; G. Siolas","Big Data and Business Analytics Center of Competence, IBM, Athens, Greece","2016 IEEE Symposium Series on Computational Intelligence (SSCI)","20170213","2016","","","1","8","In the current paper we describe how Watson Experience Manager (WEM), an industrial Question Answering (QA) tool developed by IBM, has been used in an educational context at the National Technical University of Athens (NTUA). During the postgraduate course on Data Science, three student teams experimented with WEM's QA capabilities on three different topics, namely, Nutrition, Autism and New York sightseeing. We present WEM, the workflow followed by the teams together with qualitative and quantitative experimental evaluations.","","Electronic:978-1-5090-4240-1; POD:978-1-5090-4241-8","10.1109/SSCI.2016.7849999","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7849999","Business Software in Higher Education;Data Science;Deep Question Answering;Watson Experience Manager","Business;Context;Diseases;HTML;Knowledge discovery;Testing;Training","cognitive systems;computer science education;question answering (information retrieval)","Data Science;IBM Watson cognitive system;NTUA;National Technical University of Athens;New York sightseeing;QA tool;WEM;Watson Experience Manager;autism;educational contexts;industrial question answering tool;nutrition","","","","","","","6-9 Dec. 2016","","IEEE","IEEE Conference Publications"
"Mapping the intangible cultural heritage of ethnic communities: Designing an interactive cultural history of Koreatown","K. H. A. Kang","School of Art, Design and Media, Nanyang Technological University, Singapore","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","6","This paper presents the interactive online cultural history “The Seoul of Los Angeles: Contested Identities and Transnationalism in Immigrant Space” (http://seoulofla.com/). Informed by interaction design and urban studies, this project examines and visualizes the sociocultural networks shaping immigrant communities and how local neighborhoods negotiate a sense of place within an increasingly globalized space. Geographer Doreen Massey recognizes space not as a static entity but as the product of interrelations from the immensity of the global to the intimately tiny. These interrelations are part of a story, an interpreted history that changes and develops over time. One could recognize cultural heritage in a similar way - as dynamic and part of a narrative trajectory that is not merely frozen in a romanticized or essentialist past. Much of what constitutes the dynamics of ethnic community formation is intangible as it is largely a lived experience rather than one that is necessarily documented or archived. As such, this project serves as a digital archive and platform for community storytelling that enriches our understanding of the city and the often intangible narratives that create a sense of place. Currently, Los Angeles has the largest population of Koreans in the United States living outside of Korea. Nicknamed the “L.A. district of Seoul City”, most visitors understand Koreatown as an extension of Seoul. But, what most people may not know is that the majority of inhabitants who comprise its residential and working class population are not Korean, but Latino. The everyday space of this community is inhabited by a mix of immigrants coming from Mexico, Central and South America, and other parts of Asia including Bangladesh. These networks of nationalisms converge in the urban space of Koreatown. This contests predominant conceptions of ethnic enclaves being understood as homogenous and - akes us re-imagine what we think we understand about them-they are increasingly becoming polycentric in complex ways. Combining design, documentary and issues in contemporary media studies including global/local relations, ethnic and urban studies, this work uses new media and mapping to create greater awareness of our built environment and the peoples who populate it. Mapping is a dynamic system that changes according to the shifts in culture and community that characterize any geographic place. How can this system be visualized in order to read a space with newly informed imaginations? What kind of urban interfaces could be designed to communicate with the spaces we move through and what overlooked stories could be uncovered in order to enrich our understanding of cities and the intangible cultural histories embedded in them? Such questions are explored in this project.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863211","Koreatown;Los Angeles;cultural history;ethnicity;interactive;mapping;urban studies","Cultural differences;Databases;Global communication;History;Sociology;Statistics;Urban areas","history;information retrieval systems;interactive systems;social sciences","Asia;Bangladesh;Central America;Korea;Koreatown;Los Angeles;Mexico;Seoul City;South America;United States;community storytelling;digital archive;ethnic communities;ethnic enclaves;immigrant communities;intangible cultural heritage;interactive online cultural history;nationalisms;sociocultural networks","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"A Novel Verifiable and Dynamic Fuzzy Keyword Search Scheme over Encrypted Data in Cloud Computing","X. Zhu; Q. Liu; G. Wang","Sch. of Inf. Sci. & Eng., Central South Univ., Changsha, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","845","851","In recent years, cloud computing becomes more and more popular. Users outsource large amount of encrypted documents to the cloud in order to avoid information leakage. Searchable encryption technique is a desirable service to enable users search on encrypted data. In most existing searchable encryption schemes, they only provide exact keyword search. Fuzzy keyword search improves the system usability because it allows users to make spelling errors or format inconsistencies. Besides, verifiable encryption schemes usually consider a semitrusted server and verify the authenticity of the search results. However, the server may be malicious, which may modify/delete some encrypted files or forge erroneous results in order to save its storage space or computation ability. In this paper, we investigate the searchable encryption problem in the presence of a malicious server, the verifiable searchability is needed to provide users the ability to detect the potential misbehavior. We propose a verifiable and dynamic fuzzy keyword search (VDFS) scheme to offer secure fuzzy keyword search, update the outsourced document collection and verify the authenticity of the search result. Our scheme is proved universally composable (UC) security by rigorous security analysis.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0147","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847030","Cloud computing;Dynamic fuzzy search;Malicious server;Searchable encryption;Verifiable search","Cloud computing;Encryption;Indexes;Keyword search;Public key;Servers","cloud computing;cryptography;information retrieval","UC security;VDFS scheme;cloud computing;data encryption;malicious server;searchable encryption problem;security analysis;universally composable security;verifiable and dynamic fuzzy keyword search scheme","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Random surfing on multipartite graphs","A. N. Nikolakopoulos; A. Korba; J. D. Garofalakis","Department of Computer Engineering and Informatics, University of Patras","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","736","745","Equipping an imaginary Random Surfer of the Web with the ability to teleport, was Page et al.'s creative way to justify a mathematical necessity; the Teleportation Matrix. Despite being essential for ensuring the ergodicity of the underlying Markov chain, the standard definition of this matrix treats the nodes of the graph in a simplistic and “leveling” way that can prove counterintuitive - especially for applications of the method on graphs of heterogeneous data. In this work, we focus on such graphs and we propose a novel alternative teleportation model that yields a well-defined ranking vector, while being as easy to handle as the traditional teleportation. We explore the theoretical implications of our model, and we reveal a wealth of nice properties that result to direct computational advantages over PageRank. We conduct a set of experiments using real-world datasets and we verify both the useful computational characteristics of our model and its favorable qualitative performance. Our promising findings suggest there remain more to be explored, and maybe much to be gained, by revisiting the teleportation model; a neglected part of PageRank that is typically taken for granted by the majority of applications in the literature.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840666","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840666","Markov Chains;Near Decomposability;PageRank;Random Surfer Model;k-partite Graphs","Computational modeling;Damping;Markov processes;Sparse matrices;Standards;Teleportation","Markov processes;Web sites;graph theory;information retrieval;matrix algebra","Markov chain;PageRank;heterogeneous data;imaginary random Web surfer;mathematical necessity;multipartite graphs;random surfing;teleportation matrix;teleportation model","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Fast approach for automatic data retrieval using R programming language","T. D. Chung; R. Ibrahim; S. M. Hassan; N. S. Rosli","Department of Electrical and Electronic Engineering, Universiti Teknologi PETRONAS, 32610, Bandar Seri Iskandar, Perak, Malaysia","2016 2nd IEEE International Symposium on Robotics and Manufacturing Automation (ROMA)","20170209","2016","","","1","4","The development of big data analytics in recent years has enabled wide use of analysis tools such as R in various industries including robotics and automation. Its most recent reported application for data analysis is limited to only a small dataset and the processing time was not reported. Thus, this work proposes a fast approach for automatic data retrieval using R, a powerful programming language for statistical and big data analysis. The developed algorithm performs data retrieval using a source file and a map file as inputs and produces a desired output file. Based on the experiment results on a real dataset, significantly low processing time can be achieved using the approach. In addition, the developed algorithm is general and thus can be applied in various similar data retrieval applications such as pooling sub-dataset in manufacturing environment involving the use of robotics and automation.","","Electronic:978-1-5090-0928-2; POD:978-1-5090-0929-9","10.1109/ROMA.2016.7847824","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847824","R;algorithm;automatic;data retrieval","Algorithm design and analysis;Automation;Big data;Computer languages;Indexes;Search problems","Big Data;data analysis;industrial robots;information retrieval;manufacturing systems;production engineering computing;statistical analysis","R programming language;automatic data retrieval;big data analytics;data analysis;manufacturing environment;robotics;source file;statistical analysis;subdataset pooling","","","","","","","25-27 Sept. 2016","","IEEE","IEEE Conference Publications"
"Development and Evaluation of Novel eBook Interface for Scaffolding Thinking Context in the Teaching of Writing","G. D. Chen; C. Y. Wang; C. K. Chang","Dept. of Comput. Sci. & Inf. Eng., Nat. Central Univ., Taoyuan, Taiwan","2016 International Conference on Educational Innovation through Technology (EITT)","20170202","2016","","","174","177","Effective reading strategies, including graphic organizers, question answering, story structure, etc., can help students through reading comprehension. However, those reading strategies are not fully support in both paper books and eBooks. Students, who master those reading strategies, can learn effectively. In contrast, students without effective reading strategies cannot grasp the thinking context and their learning outcomes of writing are not good enough. This study presents a novel eBook interface to show the thinking maps with question answering mechanism on the same page. The thinking maps and question answering mechanism will stimulate students to reflect on reading content, which is others' excellent composition, and help students reach better writing outcomes. This study was conducted by design-based research methodology. After iteratively refining the novel eBook interface implementation, 60 subjects were recruited for system evaluation. The experimental results show that students in the experimental group can significantly acquire more story structure and key vocabulary than students in control group. Also their subjective opinions show that they are satisfied with the thinking maps and question answering mechanism provided by domain experts.","","Electronic:978-1-5090-6138-9; POD:978-1-5090-6139-6","10.1109/EITT.2016.41","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839516","design research method;eBook;reading comprehension;reading strategy;thinking map","Context;Education;Electronic publishing;Reflection;Urban areas;Visualization;Writing","computer aided instruction;electronic publishing;question answering (information retrieval);student experiments;user interfaces","graphic organizers;learning outcomes;novel ebook interface;question answering mechanism;reading comprehension;scaffolding thinking context;story structure;teaching;writing","","","","","","","22-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"Optimizing the Multiclass F-Measure via Biconcave Programming","H. Narasimhan; W. Pan; P. Kar; P. Protopapas; H. G. Ramaswamy","Harvard Univ., Cambridge, MA, USA","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1101","1106","The F-measure and its variants are performance measures of choice for evaluating classification and retrieval tasks in the presence of severe class imbalance. It is thus highly desirable to be able to directly optimize these performance measures on large-scale data. Recent advances have shown that this is possible in the simple binary classification setting. However, scant progress exists in multiclass settings with a large number of classes where, in addition, class-imbalance is much more severe. The lack of progress is especially conspicuous for the macro-averaged F-measure, which is the widely preferred F-measure variant in multiclass settings due to its equal emphasis on rare classes. Known methods of optimization scale poorly for macro F-measure, often requiring run times that are exponential in the number of classes. We develop BEAM-F, the first efficient method for directly optimizing the macro F-measure in multiclass settings. The challenge here is the intractability of optimizing a sum of fractional-linear functions over the space of confusion matrices. We overcome this difficulty by formulating the problem as a biconcave maximization program and solve it using an efficient alternating maximization approach that involves a Frank-Wolfe based iterative solver. Our approach offers guaranteed convergence to a stationary point and experiments show that, for a range synthetic data sets and real-world applications, our method offers superior performance on problems exhibiting large class imbalance.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0143","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837956","Alternating maximization;Biconcave program;Class imbalance;Frank-Wolfe method;Macro F-measure;Multiclass classification","Harmonic analysis;Optimization;Standards;Stochastic processes;Support vector machines;Training;Tuning","concave programming;information retrieval;iterative methods;matrix algebra;pattern classification","BEAM-F;Frank-Wolfe based iterative solver;biconcave maximization program;classification tasks;confusion matrices;fractional-linear functions;macro-averaged F-measure;multiclass F-measure;multiclass settings;retrieval tasks","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Gaze network extraction from bookmarks in accordance with search intentions","T. Amano; R. Onuma; H. Nakayama; H. Kaminaga; Y. Miyadera; S. Nakamura","Department Computer Science and Mathematics, Fukushima University, 1 Kanayagawa, 960-1296, Japan","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","20170216","2016","","","472","473","Opportunities for Web exploration have been remarkably increased. In particular, complicated Web search with a stack of search and acquisitions of Web pages is significant in long-term intellectual activity. In such exploration, it is important to identify the noteworthy pages and their relationships in increasing bookmark in accordance with change of search intentions. However, it is difficult to grasp them since bookmarks tends to increase according to various search intentions and their changes. In this research, we develop a method for extracting the relationships between Web page by several different related elements from bookmarks. We also develop a method for extracting the gaze network based on centrality analysis. This paper mainly describes the framework of our proposed methods.","","Electronic:978-1-908320-73-5; POD:978-1-5090-4852-6","10.1109/ICITST.2016.7856756","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856756","Centrality Analysis;Information Visualization;Noteworthy Page Network;Search History Data;Search Intentions","Visualization;Web pages","Internet;human computer interaction;information retrieval","Web page;bookmarks;centrality analysis;gaze network extraction;search intentions","","","","","","","5-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"Trust Enhancement over Range Search for Encrypted Data","X. Yang; T. T. Lee; J. K. Liu; X. Huang","Fujian Provincial Key Lab. of Network Security & Cryptology, Fujian Normal Univ., Fuzhou, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","66","73","Data outsource in the cloud has become inevitable trend nowadays, which significantly motivates the development of range search. Order-preserving encryption (OPE) as the most efficient scheme for range search has received increasing interest in both the industries and the cryptographic communities. However, the state of arts show that current OPE schemes either lack efficiency or suffer from severe data leakage. Further, none of them implement OPE in the parallel system. Thus, our work aims to address these issues. In this paper, we propose a non-interactive OPE scheme for encrypted parallel database system, which consists of six functions to satisfy the basic operations for data retrieval. In order to enhance the security of non-interactive protocols, one-to-two mapping technique is utilized to hide data distribution as well as data frequency. It can not only protect data from ciphertext-only attack (COA) but also accomplish the higher security level: indistinguishability under a partial ordered ciphertext-only attack (IND-POCA).","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0047","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846930","Non-interactive;Order-preserving encryption;Privacy;Range search;Security","Cloud computing;Database systems;Encryption;Protocols","cloud computing;cryptography;data protection;information retrieval;parallel databases;trusted computing","COA protection;IND-POCA;data distribution;data leakage;data outsource;data protection;data retrieval;encrypted parallel database system;indistinguishability under a partial ordered ciphertext-only attack;noninteractive OPE;noninteractive protocols;one-to-two mapping;order-preserving encryption;range search;security enhancement;trust enhancement","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Linked file system: Towards exploiting linked data technology in file systems","S. R. Mashwani; A. Rauf; S. Khusro; S. Mahfooz","Department of Computer Science, University of Peshawar, Peshawar, Pakistan","2016 International Conference on Open Source Systems & Technologies (ICOSST)","20170202","2016","","","135","141","Most of the file systems today are based on desktop metaphor utilizing hierarchies of folders and sub-folders for information storage and retrieval. A folder is usually assumed to store the semantically related information in the form of files and sub-folders. However, there always exist related files somewhere else in the local file system hierarchy. Users often fail to find these disconnected files or discover the relationship among these. File systems maintain a plain set of metadata items about each file which could be utilized to discover some of the hidden relationships. However, this meta-information is very sparse as most of this metadata is rarely entered by the user. Moreover, files in a file system are mostly private and cannot get the benefit of collaborative tagging. Web of Data, on the other hand also contains information related to the file system objects which could be utilized for linking with information in file systems. The purpose of introducing Linked Data technology was to solve similar issues on the web. This article investigates the exploitation of Linked Data technology in file systems and provides a comprehensive review on the possible use of Linked Data Technology in file systems.","","CD:978-1-5090-5585-2; Electronic:978-1-5090-5586-9; POD:978-1-5090-5587-6","10.1109/ICOSST.2016.7838590","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838590","File Systems;Linked Open Data;Semantic Desktop;Semantic File Systems;Semantic Web","Best practices;File systems;Joining processes;Metadata;Resource description framework;Semantics","Internet;Linked Data;information retrieval systems;meta data","Web of Data;collaborative tagging;desktop metaphor;folder-subfolder hierarchies;information storage and retrieval;linked data technology;linked file system;local file system hierarchy;metadata items","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Enhancing Performance of naïve bayes in text classification by introducing an extra weight using less number of training examples","S. P. Shathi; M. Delowar Hossain; M. Nadim; S. G. R. Riayadh; T. Sultana","Department of Computer Science and Engineering, Hajee Mohammad Danesh Science and Technology University, Bangladesh","2016 International Workshop on Computational Intelligence (IWCI)","20170223","2016","","","142","147","This paper presents an effective and efficient method for classifying text documents in order to deliver feasible information retrieval using naïve bayes algorithm. Today lots of algorithms have earned good score in the field of information retrieval, Naïve Bayes is one of them. In this paper, a Weight Matrix is introduced during training text documents which is combination of term frequency (TF) and inverse class frequency(ICF) and later this weighted term is powered by a significant number and added with the posteriori value during the prediction time of Naïve Bayes (NB) algorithm to establish a better and efficient performance of the classification task. Here the precedence base element TF results an additional weight for each term (word) of the text. On the other hand, ICF gives each common word a low score. Finally the combinational term `Weight Matrix' gives an extra weight and balances weight where necessary. As a result, improve the performance accuracy of the NB classifier. Experimental results show that NB with Weight Matrix rarely demotes accuracy compared to standard Naïve Bayes, instead of enhancing accuracy dramatically.","","Electronic:978-1-5090-5769-6; POD:978-1-5090-5770-2","10.1109/IWCI.2016.7860355","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860355","Feature selection;ICF;Naïve Bayes;TF;Text categorization;Weight Matrix","Computational intelligence;Conferences;Decision support systems;Handheld computers","Bayes methods;information retrieval;matrix algebra;pattern classification;text analysis","ICF;information retrieval;inverse class frequency;naïve Bayes;posteriori value;term frequency;text document classification;weight matrix","","","","","","","12-13 Dec. 2016","","IEEE","IEEE Conference Publications"
"Topic Extraction Method from Millions of Tweets Based on Fast Feature Selection Technique CWC","T. Hashimoto; D. Shepard; T. Kuboyama; K. Shin","Chiba Univ. of Commerce, Chiba, Japan","2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)","20170202","2016","","","724","731","Social media offers a wealth of insight into how significant topics such as the Great East Japan Earthquake, the Arab Spring, and the Boston Bombing affect individuals. The scale of available data, however, can be intimidating: during the Great East Japan Earthquake, over 8 million tweets were sent each day from Japan alone. Conventional word vector-based topic-detection techniques for social media that use Latent Semantic Analysis, Latent Dirichlet Allocation, or graph community detection often cannot scale to such a large volume of data due to their space and time complexity. To alleviate this problem, we have already proposed an efficient method for topic extraction by leveraging our original fast feature selection algorithm, CWC, which vastly reduces the number of features to track. While we begin with word count vectors of authors and words for each time slot (in our case, every 30 minutes), we make clusters from each time slot by a matrix decomposition technique to identify clusters and adapt CWC to extract discriminative words from each cluster. This method makes it possible to detect topics from high dimensional datasets. In this paper, to demonstrate our method's effectiveness, we extract topics from a dataset of over two hundred million tweets sent following the Great East Japan Earthquake and compare them with the result extracted by LDA, the current most popular topic extraction method. With CWC, we can identify topics from this dataset with great speed and accuracy.","","Electronic:978-1-5090-5910-2; POD:978-1-5090-5911-9","10.1109/ICDMW.2016.0107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836738","CWC;Feature Selection;Great East Japan Earthquake;Topic Extraction;Twitter","Data mining;Earthquakes;Electronic mail;Feature extraction;Matrix decomposition;Social network services;Time series analysis","feature selection;information retrieval;social networking (online)","CWC;fast feature selection technique;high dimensional datasets;matrix decomposition technique;social media;topic extraction method;tweets;word vector-based topic-detection techniques","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"e-Reconstruction and e-archiving of iconic architectural heritage: A complete example","H. Esmaeili; H. Thwaites; P. C. Woods","Centre for Research-Creation in Digital Media (CRCDM), Faculty of Arts, Sunway University, Malaysia","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","8","This paper presents a real case of virtual preservation of an architectural heritage right before its disappearance, in absence of protection. This covers many different areas from technical aspects to history of the building and its importance in a way that the archived information can be useful for various target audiences e.g. historians, architects, and many others. Conservation of national heritage in developing countries especially in Southeast Asia is plagued by `Conservation - Development Conflicts'. Buildings that should have been protected as national heritage have been demolished. Digital archiving is an insurance to record the buildings that have architectural or historical merit but are not gazetted as national heritage and are left without protection against future demolition. This paper reports part of a project - Post-Independence Architecture Atlas' by the University of Malaya, in collaboration with Multimedia University, Malaysia. A total of 30 buildings and other structures were fully recorded in the first stage. The Pekeliling Flats also known as `Tunku Abdul Rahman Flats' were constructed during 1960s as the first pilot project for introducing Industrialized Building System to Malaysia aiming at affordable-housing for the low-income group. These flats were the second high-rise residential buildings in Malaysia after the Suleiman Courts. Despite their historical significance, being the first high-rise residential buildings in Kuala Lumpur, the Suleiman Courts were demolished in 1986. There appear to be no records of the Suleiman Courts such as technical data or plans. The situation for the Pekeliling Flats was the same as the Suleiman Courts and no official records of the technical data were found during the investigation. The Pekeliling Flats demolition started in January 2014 and was estimated to be finished by November 2015 (finished earlier). A decision was made to virtually reconstruct these flats to be digitally preserved for the future- Full 3D reconstruction was performed including the full coverage videography (using drones) and photography. The digital restoration can be used by architectural historians, architecture schools, and many others. The 3D model can be printed for exhibition or educational purposes. This paper will discuss the importance of digital archiving by presenting step by step process of e-Reconstruction and e-Archiving of the Pekeliling Flats. The main objective is to demonstrate how these types of information should be archived in order to be beneficial in the future”.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863195","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863195","Architectural Heritage;Digital Archiving;Digital Heritage;Kuala Lumpur;Malaysia;Pekeliling Flats;Virtualization;e-Archiving;e-Reconstruction","Drones;Floors;Photography;Satellites;Three-dimensional displays;Visualization","architecture;buildings (structures);computer graphics;history;information retrieval systems;photography;video recording","3D model;Pekeliling Flats;PostIndependence Architecture Atlas;Southeast Asia;Suleiman Courts;Tunku Abdul Rahman Flats;architectural heritage;developing countries;digital archiving;digital preservation;digital restoration;e-Archiving;e-Reconstruction;full 3D reconstruction;high-rise residential buildings;industrialized building system;national heritage conservation;photography;videography;virtual preservation","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"SHTM: A neocortex-inspired algorithm for one-shot text generation","Yuwei Wang; Yi Zeng; Bo Xu","Institute of Automation, Chinese Academy of Sciences, Beijing, China","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000898","000903","Text generation is a typical nature language processing task, and is the basis of machine translation and question answering. Deep learning techniques can get good performance on this task under the condition that huge number of parameters and mass of data are available for training. However, human beings do not learn in this way. People combine knowledge learned before and something new with only few samples. This process is called one-shot learning. In this paper, we propose a neocortex based computational model, Semantic Hierarchical Temporal Memory model (SHTM), for one-shot text generation. The model is refined from Hierarchical Temporal Memory model. LSTM is used for comparative study. Results on three public datasets show that SHTM performs much better than LSTM on the measures of mean precision and BLEU score. In addition, we utilize SHTM model to do question answering in the fashion of text generation and verifying its superiority.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844355","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844355","","Biological system modeling;Brain modeling;Computational modeling;Neurons;Pattern recognition;Predictive models;Semantics","language translation;learning (artificial intelligence);natural language processing;question answering (information retrieval);text analysis","SHTM;deep learning;machine translation;nature language processing;neocortex based computational model;one shot learning;one shot text generation;question answering;semantic hierarchical temporal memory model","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Understanding computational web archives research methods using research objects","E. Maemura; C. Becker; I. Milligan","Digital Curation Institute, University of Toronto, Toronto, Canada","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3250","3259","Use of computational methods for exploration and analysis of web archives sources is emerging in new disciplines such as digital humanities. This raises urgent questions about how such research projects process web archival material using computational methods to construct their findings. This paper aims to enable web archives scholars to document their practices systematically to improve the transparency of their methods. We adopt the Research Object framework to characterize three case studies that use computational methods to analyze web archives within digital history research. We then discuss how the framework can support the characterization of research methods and serve as a basis for discussions of methods and issues such as reuse and provenance. The results suggest that the framework provides an effective conceptual perspective to describe and analyze the computational methods used in web archive research on a high level and make transparent the choices made in the process. The documentation of the research process contributes to a better understanding of the findings and their provenance, and the possible reuse of data, methods, and workflows.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840982","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840982","computational archival science;computational methods;digital curation;research objects;web archives","Big data;Computational modeling;Context;Cultural differences;Documentation;History;Libraries","Internet;information retrieval","Web archival material;Web archives sources analysis;Web archives sources exploration;computational Web archives research methods;computational methods;digital history research;digital humanities;research object framework;research objects;research projects","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Breaking down the invisible wall to enrich archival science and practice","K. Thibodeau","National Archives and Records Administration (retired), Washington, DC, US","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3277","3282","This article reviews the state of archival science where basic concepts have been subject to a long stream of criticisms without satisfactory resolution of the issues identified. It establishes a ground for progress by articulating criteria for evaluating archival concepts and proposes a path forward by enriching archival science with concepts and methods from systemic functional linguistics and graph theory. Finally, it demonstrates how borrowing from these fields would satisfy the proposed criteria.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840986","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840986","Archival Science;graph theory;systemic functional linguistics","Big data;Context;Electronic mail;Graph theory;History;Pragmatics;Standards","graph theory;information retrieval;records management","archival concepts;archival science;functional linguistics;graph theory;invisible wall","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Accessing and distributing large volumes of NetCDF data","R. Devarakonda; Y. Wei; M. Thornton","Climate Change Science Institute, Oak Ridge National Laboratory, 1 Bethel Valley Rd, Oak Ridge, TN USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3966","3967","In this paper, we will discuss how NASA's Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC) is distributing large volumes of `structured' data using Daily Surface Weather Data and a corresponding Climatological Summaries Dataset (Daymet) as an example.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841077","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841077","Daymet;ORNL DAAC;Single Pixel Tool;THREDDS","Big data;Data mining;Data visualization;Distributed databases;Meteorology;North America;Servers","data structures;geophysics computing;information dissemination;information retrieval;information retrieval systems","Climatological Summaries Dataset;Daily Surface Weather Data;Daymet;NASA Oak Ridge National Laboratory Distributed Active Archive Center;NetCDF data;ORNL DAAC;data access;structured data distribution","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Performance study of the index structures in audited environment","M. Kvet; M. Vajsova","University of Zilina, Faculty of Management Science and Informatics, Slovakia","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","20170216","2016","","","459","464","Development in information systems has brought the need for universal access to data stored in computer systems using database approach. It should ensure quality, reliability, performance with emphasis on rising data amount. The first part of the paper deals with the index structure definition, index access methods, which delimit access type. However, another significant factor is the data security. The second part of the paper deals with the audit as one element of complex activities. Standard database auditing does not influence defined index, but can generate too much data regardless the query type. Fine-grained auditing is new opportunity based on attribute granularity. Experiment section highlights limitations of index access methods, when adding new audit policy. As we can see, new audit characteristic definition can significantly degrade performance due to adding new conditions to the query consequencing sequential table data processing.","","Electronic:978-1-908320-73-5; POD:978-1-5090-4852-6","10.1109/ICITST.2016.7856753","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856753","FGA;data indexing;database audit;database performance;query policy;tablespace","Analytical models;Indexes;Reliability;Remuneration;Security","database management systems;indexing;information retrieval;reliability","computer systems;data access;database approach;database auditing;index structures;information systems;quality;reliability;sequential table data processing","","","","","","","5-7 Dec. 2016","","IEEE","IEEE Conference Publications"
"An efficient method for keyword set enhancement for crawling using genetic algorithms","C. Singh; T. Choudhury; H. Yadav; U. S. Verma","Dept. of CSE, Dronacharya College of Engineering Gurgaon, India","2016 IEEE 1st International Conference on Power Electronics, Intelligent Control and Energy Systems (ICPEICES)","20170216","2016","","","1","4","This paper proposes an efficient method for keyword set enhancement. As we know that the information is rapidly growing in the world, it is very difficult for search engine to retrieve the required information correctly in a proper way. The web crawler is a main component of the search engine and their optimization would have a big force for the improvement of the searching efficiency. The active nature and huge size of the information of the web need of continuous documentation and updating of the data which is web based retrieval system. Keyword optimization gives support to the information system so as to find efficient information from day to day changing web environment. The outcomes indicate that the approach could make the keyword collections with higher quality than that of the traditional techniques.","","Electronic:978-1-4673-8587-9; POD:978-1-4673-8588-6","10.1109/ICPEICES.2016.7853465","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7853465","Crawling;Genetic Algorithm;Keyword","Biological cells;Engines","Internet;genetic algorithms;information retrieval;search engines","Web based retrieval system;crawling;genetic algorithm;information retrieval;keyword optimization;keyword set enhancement;search engine","","","","","","","4-6 July 2016","","IEEE","IEEE Conference Publications"
"UML models change impact analysis using a text similarity technique","D. Kchaou; N. Bouassida; H. Ben-Abdallah","FSEGS, University of Sfax, Tunisia","IET Software","20170209","2017","11","1","27","37","Given the inevitable software evolution, change impact analysis (CIA) is a vital activity in the software development life cycle. Existing CIA methods either focus on one model produced during one development phase or ignore the semantic dependencies among the various models produced throughout the development phases. The herein proposed CIA method overcomes this limit by exploiting the structural and semantic dependencies within and inter-UML diagrams. It uses a graph technique to model the structural dependencies and an information retrieval (IR) technique to handle the semantic traceability between the use case documentation and the sequence diagrams. To identify the most appropriate IR technique to the CIA context, the authors present a quantitative experimentation of term frequency-inverse document frequency and latent semantic indexing (LSI), two widely used IR techniques. In addition, to evaluate the overall performance of their method, they present an evaluation performed on changes in the open source system JHotDraw 7.4.1 compared with its 7.5.1 version and changes performed on a real existing application. Using LSI, their method achieves an average precision of 84% and a recall of 91% in the requirements CIA and management.","1751-8806;17518806","","10.1049/iet-sen.2015.0113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847757","","","Unified Modeling Language;document handling;graph theory;indexing;information retrieval;public domain software;software maintenance","CIA method;IR technique;LSI;UML models;change impact analysis;development phase;graph technique;information retrieval technique;inter UML diagrams;latent semantic indexing;open source system JHotDraw 7.4.1;requirement change;semantic dependencies;semantic traceability;sequence diagrams;software development life cycle;software evolution;structural dependencies;term frequency-inverse document frequency;text similarity technique;use case documentation;within UML diagrams","","","","","","","2 2017","","IET","IET Journals & Magazines"
"An Evaluation for Different Pricing Mechanisms under the Sponsored Search with Various Bidding Processes","C. K. Tsung; H. Ho; S. Lee","Dept. of Comput. Sci. & Inf. Eng., Nat. Chin-Yi Univ. of Technol., Tahchung, Taiwan","2016 International Computer Symposium (ICS)","20170220","2016","","","28","32","In this paper, we build a sponsored search auction platform to simulate and analysis the auction result for auctioneers. We propose to use geometrically decreasing sequence to capture the click behavior of internet users. In our simulations, the results show that theoretical properties of the charging mechanisms are captured, and the proposed platform reflects the charging mechanism's property. Therefore, the auctioneer can use the proposed platform to simulate the auction and estimate the result in expectations.","","Electronic:978-1-5090-3438-3; POD:978-1-5090-3439-0","10.1109/ICS.2016.0015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7858437","Generalized Second Price;Sponsored Search Auction;Vindictive Bidding","Advertising;Electronic mail;Google;Internet;Mathematical model;Mechanical factors;Pricing","Internet;advertising data processing;consumer behaviour;electronic commerce;information retrieval;pricing;tendering","Internet users click behavior;advertising service;bidding processes;charging mechanisms;pricing mechanisms;sponsored search auction platform","","","","","","","15-17 Dec. 2016","","IEEE","IEEE Conference Publications"
"Evaluating the importance of Web comments through metrics extraction and opinion mining","R. Santos; J. P. Vieira; J. Barbosa; C. Sá; E. Moura; R. Moura; R. Sousa","Computer Science Department, Federal University of Piaui, Teresina, Piaui, Brazil","2016 35th International Conference of the Chilean Computer Science Society (SCCC)","20170130","2016","","","1","11","The evolution of e-commerce and Online Social Networks made significant g rowth of t he W eb a nd as consequence, available information increase quite every day, making the task of analyzing the reviews manually almost impossible for the decision-making process. Due to the amount of information, the creation of automatic methods of knowledge extraction and data mining has become necessary. This paper presents a Web application prototype where from a review are returned the feeling (positive, negative or neutral), its features and other analysis metrics using Natural Language Processing and Sentiment Analysis in order to define t he m ost important comments to be taken into consideration in the decision-making process. Experiments show efficacy i n t he p recision o f reviews with negative polarity and recall of reviews with positive polarity in 84.93% and 94.33% respectively and the most important comments were found in a measure considered satisfactory of 50% in F-Measure in both positive and neutral polarities.","","Electronic:978-1-5090-3339-3; POD:978-1-5090-3340-9","10.1109/SCCC.2016.7836039","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836039","Artificial N eural Network;Fuzzy Systems;Metrics;Opinion Mining;Reviews","Blogs;Data mining;Decision making;Electronic mail;Measurement;Nanoelectromechanical systems;Support vector machines","Internet;data mining;decision making;information retrieval;sentiment analysis;social networking (online)","Web comment;decision-making process;metrics extraction;natural language processing;online social networks;opinion mining;sentiment analysis","","","","","","","10-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"GuiTones-I: An audio-visual database of monophonic guitar tones","A. Aggarwal; R. Kumar; T. Sahay; M. Chandra","Department of Mechanical Engineering, Birla Institute of Technology Mesra, Ranchi, Jharkhand 835215, India","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","497","500","Automatic music transcription (AMT) is considered one of the most complex problems in music information retrieval (MIR). Many attempts of transcribing polyphonic music have been made in the past but monophonic tones, that build melodious music pieces, still remain largely untouched. The foremost approach is preparation of a database consisting of isolated and continuous monophonic sounds in order to proceed with the recognition and transcription of monophonic music pieces. In this paper one such audio-visual database of monophonic guitar tones for multi-modal signal processing has been introduced and evaluated for a test case of both online and offline recognition and transcription. The database comprises of recorded audio samples of first nine frets for all six strings and images of different ways in which the fretboard was held while playing these frets. A total of over 10,000 audio-visual samples have been recorded by 40 amateur and professional guitarists. The prepared database will be made available for free download under a CC BY-NC-SA 4.0 license and in DVDs at a nominal cost covering shipping and handling charges.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848049","Audio-visual database;automatic guitar transcription;guitar monophonic sounds;multi-modal signal processing","Data acquisition;Data models;Databases;Instruments;Lighting;Music;Signal processing","audio databases;audio signal processing;information retrieval;music;visual databases","AMT;CC BY-NC-SA 4.0 license;DVD;GuiTones-I;MIR;audio samples;audio-visual database;automatic music transcription;handling charges;melodious music pieces;monophonic guitar tones;monophonic music pieces;monophonic tones;multimodal signal processing;music information retrieval;offline recognition;polyphonic music;professional guitarists;shipping charges","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"A method for ontology-based user profile adaptation in personalized document retrieval systems","B. Maleszka","Faculty of Computer Science and Management, Wroclaw University of Science and Technology, Wybrzeze Wyspianskiego 27, 50-370, Poland","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","003187","003192","Information overload is one of the most important problems in context of personalized document retrieval systems. In this paper we propose to use ontology-based user profile. Ontological structures are appropriate to represent relations between concepts in user profile. We present a method for determining user profile based on his current activities. Results obtained in experimental evaluation are promising.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844724","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844724","","Adaptation models;Conferences;Context;Correlation;Cybernetics;Ontologies;Recommender systems","information retrieval;ontologies (artificial intelligence)","information overload;ontological structures;ontology-based user profile adaptation;personalized document retrieval systems","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Fast Nearest Neighbor Search with Transformed Residual Quantization","J. Yuan; X. Liu","Vipshop (US) Inc., San Jose, CA, USA","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","971","976","Product quantization (PQ) and residual quantization (RQ) have been successfully used to solve fast nearest neighbor search problems thanks to their exponentially reduced complexities of both storage and computation with respect to the codebook size, Recent efforts have been focused on employing optimization strategies and seeking more effective models. Based on the observation that randomness typically increases in subsequent residual spaces, we propose a new strategy, called, transformed RQ (TRQ), that jointly learns a local transformation per residual cluster with an ultimate goal to further reduce overall quantization errors. Additionally we propose a hybrid approximate nearest search method based on the proposed TRQ and PQ. We show that our methods achieve significantly better accuracy on nearest neighbor search than both the original and the optimized PQ on several benchmark datasets.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0175","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838279","Residual quantization;image retrieval;nearest neighbor search;product quantization","Complexity theory;Computational modeling;Distortion;Indexing;Nearest neighbor searches;Quantization (signal);Transforms","information retrieval;search problems;vector quantisation","PQ;TRQ;fast nearest neighbor search;optimization strategies;product quantization;reduced complexities;residual quantization;transformed RQ;transformed residual quantization","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"A Pay-As-You-Go Methodology for Ontology-Based Data Access","J. F. Sequeda; D. P. Miranker","Capsenta","IEEE Internet Computing","20170301","2017","21","2","92","96","A successfully repeated use case for Semantic Web technologies is Ontology-Based Data Access for data integration. In this approach, an ontology serves as a uniform conceptual federating model, which is accessible to both IT developers and business users. Here, two challenges for developing an OBDA system are considered: ontology and mapping engineering, along with a pay-as-you-go methodology that addresses these challenges and enables agility.","1089-7801;10897801","","10.1109/MIC.2017.46","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867740","Internet/Web technologies;OBDA;Ontology-Based Data Access;Semantic Web;linked data;ontologies","Data integration;Databases;Knowledge discovery;OWL;Ontologies;Semantics;Terminology","business data processing;data integration;information retrieval;ontologies (artificial intelligence);semantic Web","data integration;mapping engineering;ontology based data access;pay-as-you-go methodology;semantic Web technologies","","","","","","","Mar.-Apr. 2017","","IEEE","IEEE Journals & Magazines"
"Fast Reused Function Retrieval Method Based on Simhash and Inverted Index","Y. Qiao; X. Yun; Y. Zhang","Inst. of Comput. Technol., Beijing, China","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","937","944","It is a common phenomenon to reuse code from open source code or personal previous work in software/malware development. In addition, compilers often insert many functions when compiling. Therefore, to fast identify these reused functions in binary executables and trace their origins is helpful for reverse engineering, software copyright protection, malware detection and correlation and so on. Much research in recent years has focused on code similarity identification, whereas only a few researchers have addressed the problem of reused function retrieval. In this paper, we proposed a method for fast retrieving reused function in a large corpus of code based on simhash and inverted index. First of all, we constructed a code database including massive binary executables, their functions, the code blocks of functions and the simhash value of code blocks and the inverted index of these elements. Then, for a function to be retrieved, we split it into several code blocks according to the jump instructions and jump addresses, and calculated simhash value for every code block. Similar code blocks could be fast retrieved from the code database based on simhash. Consequently, we could easily retrieve those possibly similar functions using inverted index, and further locate them in binary executables. The experimental evaluation shows that our method achieves high accuracy rate and recall rate, and has a fast speed.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0159","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847042","forensics;retrieval method;reused function;simhash","Binary codes;Indexes;Malware;Reverse engineering;Search engines;Semantics","copyright;information retrieval;invasive software;public domain software;reverse engineering;software reusability","Simhash;code database;code similarity identification;fast reused function retrieval method;inverted index;malware detection;open source code;reuse code;reused function retrieval;reverse engineering;several code blocks;simhash value;software copyright protection;software-malware development","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"A flexible architecture for selection and visualization of information in emergency situations","B. S. Nascimento; A. S. Vivacqua; M. R. S. Borges","Graduate Program on Informatics (PPGI), Universidade Federal do Rio de Janeiro (UFRJ), Brazil","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","003317","003322","Emergency command and control centers (CC) are integrated facilities to assist and handle crisis situations. In these CCs, operators suffer both with information shortage and overload. This paper focuses on the information overload problem. Operators often do not have adequate access to information that may be relevant in the decision-making process. In many CC centers, information is stored without filtering or refinement for later use, and its visualization is pre-defined at design time, offering no possibility of customization. To address the problems created by this lack of support, we propose an architecture to enable fast and easy selection from multiple information sources, and definition of appropriate visualizations. Following this architecture, we developed the “Emergency Dashboard”, a system that provides selection of a desired set of data and presents it, using configurable dashboards. We also provide collaborative mechanisms, given that, at times, it is not possible for a single user to handle such large datasets alone.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844746","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844746","collaboration;dashboard;emergency management;information visualization","Collaboration;Computer architecture;Conferences;Context;Data visualization;Decision making;Visualization","data visualisation;decision making;emergency services;groupware;information retrieval;software architecture","collaborative mechanisms;crisis situations;decision making;emergency command and control centers;emergency dashboard;flexible architecture;information access;information selection;information visualization","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Archiving new media arts in interactive digital space","Y. P. Lim; K. C. Tan; P. S. Lim","Faculty of Creative Multimedia, Multimedia University, Malaysia","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","3","New media arts are an innovative genre of artworks in which artistic expression is explored and developed by the artist. This genre of new media arts is created in a vast number of ways. Currently, digitized collection systems are commonly used by galleries and museums for traditional art [1]. The need for documentation and archiving complex new media artworks is now increasingly crucial for future preservation and representation. This study aims to establish criteria of aesthetic media representation for media archived in an interactive digital space and explores a generic solution for an interactive digital space. Based on the Interactive Art Taxonomy [2], visitors to the digital space can create, interact, and experience the new media art. This taxonomy demonstrates that archiving of the content can take place for documentation and future preservation in the strategic context for the creation. This digital archiving process proposed will be a useful model to be adopted or adapted by universities, museums, and other institutions in terms of archiving media arts practices.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863201","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863201","archiving;digital space;immersive environment;interactive;new media arts;style","Art;Databases;Documentation;Media;Multimedia communication;Space exploration;Taxonomy","art;information retrieval systems;interactive systems;multimedia computing;museums","digitized collection systems;future preservation;future representation;galleries;interactive art taxonomy;interactive digital space;museums;new media art archiving;traditional art","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"LifeRescue: A web based application for emergency responders during fire emergency response","V. Nunavath; A. Prinz","Department of Information and Communication Technology, University of Agder, CIEM Research Group, Jon Lilletuns Vei 9, 4879, Grimstad, Norway","2016 3rd International Conference on Information and Communication Technologies for Disaster Management (ICT-DM)","20170216","2016","","","1","8","In order to respond to any kind of building fire emergencies, first-responders have to use lot of time to get access to the emergency data such as location of the victims who are still inside the building, location of the hazardous material, location of the resources and location of the exits in order to perform search and rescue. However, search is possibly one of the most dangerous activities on the fire ground. Sometimes the visibility is zero and the environment is really hot. Because of the limited operating time in the building, the key to successful search is how quickly emergency responders can get access to the emergency related information in order to save victims and the property. In this paper, we present a developed web portal called LifeRescue, which enables the emergency responders (both on-scene and remote) to get access to the emergency information during search and rescue operation. This application facilitates awareness of the information such as number of victims who are still inside the building and their location to the emergency responders who are at command center as well as at on-scene. Our system also provides the information related to the location of the fire and also its development through sensor data.","","Electronic:978-1-5090-5234-9; POD:978-1-5090-5235-6","10.1109/ICT-DM.2016.7857204","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857204","Emergency webportal;Fire Emergency response;Information availability;Information sharing;University Building","Decision support systems","emergency management;emergency services;fires;information retrieval","LifeRescue;Web based application;Web portal;emergency data access;emergency information access;emergency responders;fire emergency response;search and rescue operation","","","","","","","13-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Utility Change Point Detection in Online Social Media: A Revealed Preference Framework","A. Aprem; V. Krishnamurthy","Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada","IEEE Transactions on Signal Processing","20170206","2017","65","7","1869","1880","This paper deals with change detection of utility maximization behavior given a dataset. In online social media, such changes occur due to the effect of marketing, advertising, or changes in ground truth. First, we use the revealed preference framework to detect the unknown time point (change point) at which the utility function changed. We derive necessary and sufficient conditions for detecting the change point. Second, in the presence of noisy measurements, we propose a method to detect the change point and construct a decision test. Also, an optimization criteria is provided to recover the linear perturbation coefficients. Finally, to reduce the computational cost, a dimensionality reduction algorithm using Johnson-Lindenstrauss transform is presented. The results developed are illustrated on two real datasets: Yahoo! Tech Buzz dataset and Youstatanalyzer dataset. By using the results developed in the paper, several useful insights can be gleaned from these datasets. First, the changes in ground truth affecting the utility of the agent can be detected by utility maximization behavior in online search. Second, the recovered utility functions satisfy the single crossing property indicating strategic substitute behavior in online search. Finally, due to the large number of videos in YouTube, the utility maximization behavior was verified through the dimensionality reduction algorithm.","1053-587X;1053587X","","10.1109/TSP.2016.2646667","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7802629","Social media;YouTube;big-data;change point detection;dimensionality reduction;nonparametric detection;revealed preference;strategic substitution behavior;utility maximization","Microeconomics;Probes;Signal processing;Signal processing algorithms;YouTube","information retrieval;optimisation;social networking (online)","Johnson-Lindenstrauss transform;Yahoo! Tech Buzz dataset;YouTube;Youstatanalyzer dataset;dimensionality reduction algorithm;linear perturbation coefficients;online social media;optimization criteria;revealed preference framework;utility change point detection;utility maximization behavior","","","","","","20161230","April1, 1 2017","","IEEE","IEEE Journals & Magazines"
"Secure and reliable transmission of chemical molecule structure using QR code and secret sharing","K. R. Chaudhari; S. Patil","Department of Computer Engineering, Pimpri-Chinchwad College of Engineering, Pune-44, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","5","In chemical informatics and computational chemistry various methods and criteria's have been developed for representation of chemical structures in a computer readable format. A QR code representation of molecular structures is explored that permits the user to read and input molecular structures into computer systems in a fully automated fashion. The SMILES format is chosen where the input via QR code is fast and error free due to the 2D barcodes used which employ error correction and fully automatic. When a QR code contains important data or private information, the risk of exposure of data becomes a problem. Encryption of data may cause a single point failure. This paper proposes an idea which provides the security to chemical structure using secret sharing. The QR code of chemical structure is used as an input for secret sharing algorithm. Shares are generated of the QR code and distributed among number of participants and later on using threshold (t, n) secret sharing QR code is reconstructed back. The experimental results show that the proposed scheme is accurate as retrieved chemical data and chemical molecule structures from any t shares are same as original ones. As single share is not revealing anything about the secret data the scheme is secure. The scheme is reliable as out of n any t shares are enough to reconstruct the secret. Also the limitation of single point failure is overcome using (t, n) secret sharing scheme. So the proposed scheme is accurate, secure and reliable.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7860065","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860065","Barcodes;Chemical Molecule Strucutre;QR Codes;SMILES format;Secret Sharing","Chemicals;Periodic structures;Reliability;Tutorials","QR codes;chemistry computing;cryptography;data privacy;error correction;information retrieval;reliability","2D barcodes;QR code representation;SMILES format;chemical data retrieval;chemical informatics;chemical molecule structure;chemical structure representation;computational chemistry;data encryption;error correction;private information;secret reconstruction;secret sharing algorithm;secure transmission;single point failure;transmission reliability","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Data level conflicts resolution for multi-sources heterogeneous databases","A. Mirza; I. Siddiqi","Department of Computer Science, Bahria University, Islamabad, Pakistan","2016 Sixth International Conference on Innovative Computing Technology (INTECH)","20170209","2016","","","36","40","Integration of data from multiple, heterogeneous databases is a commonly encountered scenario in information retrieval systems where the user is to be provided with a unified view of information. These data sources may be from different vendors, may comprise different schemas and, could be physically at different locations. A number of techniques have been developed to merge schemas of multiple databases and address the resulting conflicts. While schema integration and resolution of schema conflicts has been an active area of research, relatively lesser attention is paid to the issue of data level conflicts. These conflicts may arise while merging the actual data that is retrieved from multiple databases. A dedicated mechanism needs to be implemented to identify and resolve the conflicts encountered during data integration which makes the subject of our study. This paper identifies few major and frequently occurring data level conflicts which are likely to arise while integrating data. We propose a framework that identifies and resolves these conflicts. The framework is implemented as a software system which we call DHResol. The system evaluated to merge two different databases successfully identified and resolved the discussed conflicts.","","Electronic:978-1-5090-2000-3; POD:978-1-5090-2001-0","10.1109/INTECH.2016.7845088","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845088","Data Fusion;Data Integration;Data Level Conflicts;Data Merging;Database;Database Integration","Data integration;Databases;Erbium;Merging;Null value;Servers;XML","database management systems;information retrieval systems","DHResol;data integration;data level conflict issue;data level conflict resolution;information retrieval systems;multisource heterogeneous databases","","","","","","","24-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"A high-throughput multi-match priority encoder for data retrieval on 65-nm SOTB CMOS process","X. T. Nguyen; H. T. Nguyen; C. K. Pham","Department of Engineering Science, University of Electro-Communications, Chofu, Tokyo 182-8585, Japan","2016 IEEE Region 10 Conference (TENCON)","20170209","2016","","","2392","2395","In this paper, a high-throughput multi-match priority encoder (MPE) for data retrieval is implemented in a low-power 65-nm SOTB CMOS process. This approach employs an 8-bit priority encoder (PE) as a basement and utilizes a new design architecture to construct a 2,048-bit MPE (MPE2K). The experimental results on an FPGA proved that the operating frequency of MPE2K is 1.42 times higher than that of similar design, while the resource utilization is 4.39 times lower. Moreover, when being applied in a 100-MHz data analytics system, MPE2K achieves the minimum throughput of 99.8 Mbps. The post-simulation results on SOTB process indicate that MPE2K can operate at 312 MHz and consumes 9.56 mW at 1.0 V. Additionally, at 0.4 V, the leakage current in idle mode is reduced approximately 203.8 times due to the usage of reverse body bias voltage.","","Electronic:978-1-5090-2597-8; POD:978-1-5090-2598-5; USB:978-1-5090-2596-1","10.1109/TENCON.2016.7848459","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848459","","Data analysis;Field programmable gate arrays;Hardware;Indexes;Simulation;Throughput;Transistors","CMOS digital integrated circuits;data analysis;electronic engineering computing;field programmable gate arrays;information retrieval","FPGA;MPE2K;data analytics system;data retrieval;design architecture;high-throughput multimatch priority encoder;low-power SOTB CMOS process;reverse body bias voltage;silicon-on-thin BOX;size 65 nm","","","","","","","22-25 Nov. 2016","","IEEE","IEEE Conference Publications"
"A Novel Hash-Based File Clustering Scheme for Efficient Distributing, Storing, and Retrieving of Large Scale Health Records","T. D. Dang; D. Hoang; P. Nanda","Sch. of Comput. & Commun., Univ. of Technol. Sydney, Sydney, NSW, Australia","2016 IEEE Trustcom/BigDataSE/ISPA","20170209","2016","","","1485","1491","Cloud computing has been adopted as an efficient computing infrastructure model for provisioning resources and providing services to users. Several distributed resource models such as Hadoop and parallel databases have been deployed in healthcare-related services to manage electronic health records (EHR). However, these models are inefficient for managing a large number of small files and hence they are not widely deployed in Healthcare Information Systems. This paper proposed a novel Hash-Based File Clustering Scheme (HBFC) to distribute, store and retrieve EHR efficiently in cloud environments. The HBFC possesses two distinctive features: it utilizes hashing to distribute files into clusters in a control way and it utilizes P2P structures for data management. HBFC scheme is demonstrated to be effective in handling big health data that comprises of a large number of small files in various formats. It allows users to retrieve and access data records efficiently. The initial implementation results demonstrate that the proposed scheme outperforms original P2P system in term of data lookup latency.","","Electronic:978-1-5090-3205-1; POD:978-1-5090-3206-8","10.1109/TrustCom.2016.0232","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847115","P2P systems;cloud based P2P systems;clustering files;file distributions;hashing scheme","Cloud computing;Computational modeling;Data mining;Distributed databases;Engines;Medical services;Peer-to-peer computing","cloud computing;electronic health records;information retrieval;pattern clustering;peer-to-peer computing;storage management","EHR;HBFC;P2P structures;big health data handling;cloud computing;cloud environments;computing infrastructure model;data management;data records access;data records retrieval;distributed resource models;electronic health records;files distribution;hash-based file clustering;hashing;healthcare information systems;healthcare-related services;large scale health records distribution;large scale health records retrieval;large scale health records storage;resources provisioning","","","","","","","23-26 Aug. 2016","","IEEE","IEEE Conference Publications"
"Real-time voice adaptation with abstract normalization and sound-indexed based search","M. A. Midtlyng; Y. Sato","Graduate School of Computer and Information Sciences, Hosei University, Tokyo, Japan","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","20170209","2016","","","000060","000065","This paper proposes a two-step based real-time voice adaptation system in the field of speech processing. Step one combines recording and pre-processing to construct a voice profile. Secondly is the real-time raw input of the voice's adaptation to a target voice. The fact that individual voices' structure are habitually varying, we suggest a method for converting into a comparable format. The new method is called abstract normalization which cuts the voice data into smaller sounds and generate an abstracted, simplified version of the data using a level of abstraction along with parameter fitting. The normalized data is used to generate a sound-index which is a sequence hash that represents the data in a simpler fashion. The indices are used to compare different sounds/voices for adaptation. This effectively transforms the speech-related challenges into a search problem rather than a biometric one. To assess the approach, voice profile data are compared against each other as a method to verify the sound-index. Ultimately, a real-time voice input using alternating levels of abstraction is run against a Norwegian voice profile. The degree of adaptation success is measured in percentage, and experimental results show that while accuracy is not yet excellent, the concept was validated.","","Electronic:978-1-5090-1897-0; POD:978-1-5090-1898-7; USB:978-1-5090-1819-2","10.1109/SMC.2016.7844220","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844220","Voice adaptation;parameter fitting;search algorithm;speech processing;voice profile","Artificial intelligence;Microphones;Real-time systems;Search problems;Speech;Speech processing;Training data","file organisation;indexing;information retrieval;search problems;speech processing","Norwegian voice profile;abstract normalization;adaptation success;comparable format;parameter fitting;real-time voice adaptation;real-time voice input;search problem;sequence hash;sound-index;sound-indexed based search;speech processing;speech-related challenges;two-step based real-time voice adaptation system;voice profile data","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"A content-aware hybrid architecture for answering questions from open-domain texts","M. M. Hoque; P. Quaresma","Department of Computer Science and Engineering, Ahsanullah University of Science and Technology","2016 19th International Conference on Computer and Information Technology (ICCIT)","20170223","2016","","","293","298","The current work blends the different paradigms of Question Answering systems and presents a content-aware hybrid architecture for an open-domain factoid questions. It combines a knowledge-based, information extraction-based and a web-based approach in a pipelined architecture to construct an answer to a question keeping the context and discourse of the question in view. The proposed semantic-aware hybrid architecture was compared with other QA systems designed over standard benchmark data. The work has shown enough potential in terms of accuracy and time domain complexity and can be used effectively as a semantic understanding-based QA system.","","Electronic:978-1-5090-4090-2; POD:978-1-5090-4091-9","10.1109/ICCITECHN.2016.7860212","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860212","entity detection;hybrid architecture;knowledge-based system;pipelined architecture;question answering system;semantic core;semantic parsing;text-knowledge","Computer architecture;Computers;Context;Information technology;Knowledge based systems;Knowledge discovery;Semantics","Internet;computational complexity;question answering (information retrieval);text analysis","Web-based approach;content-aware hybrid architecture;information extraction-based approach;knowledge-based approach;open-domain factoid questions;open-domain texts;question answering;semantic understanding-based QA system;time domain complexity","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Build watson: An overview of DeepQA for the Jeopardy! Challenge","D. Ferrucci","IBM Research, Hawthorne, NY, USA","2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)","20170213","2010","","","1","1","Computer systems that can directly and accurately answer peoples' questions over a broad domain of human knowledge have been envisioned by scientists and writers since the advent of computers themselves. Open domain question answering holds tremendous promise for facilitating informed decision making over vast volumes of natural language content. Applications in business intelligence, healthcare, customer support, enterprise knowledge management, social computing, science and government would all benefit from deep language processing. The DeepQA project (www.ibm.com/deepqa) is aimed at illustrating how the advancement and integration of Natural Language Processing (NLP), Information Retrieval (IR), Machine Learning (ML), massively parallel computation and Knowledge Representation and Reasoning (KR&R) can greatly advance open-domain automatic Question Answering. An exciting proof-point in this challenge is to develop a computer system that can successfully compete against top human players at the Jeopardy! quiz show (www.jeopardy.com). Attaining champion-level performance Jeopardy! requires a computer to rapidly answer rich open-domain questions, and to predict its own performance on any given category/question. The system must deliver high degrees of precision and confidence over a very broad range of knowledge and natural language content and with a 3-second response time. To do this DeepQA generates, evidences and evaluates many competing hypotheses. A key to success is automatically learning and combining accurate confidences across an array of complex algorithms and over different dimensions of evidence. Accurate confidences are needed to know when to “buzz in” against your competitors and how much to bet. Critical for winning at Jeopardy!, High precision and accurate confidence computations are just as critical for providing real value in business settings where helping users focus on the right content sooner and with gre- ter confidence can make all the difference. The need for speed and high precision demands a massively parallel compute platform capable of generating, evaluating and combing 1000's of hypotheses and their associated evidence. In this talk I will introduce the audience to the Jeopardy! Challenge and describe our technical approach and our progress on this grand-challenge problem.","","Electronic:978-1-4503-0178-7; POD:978-1-5090-5032-1","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851501","Algorithms;Design;Experimentation;Human Factors;Performance","Cognition;Computers;Decision making;Knowledge discovery;Knowledge representation;Natural language processing","information retrieval;knowledge management;knowledge representation;learning (artificial intelligence);natural language processing","DeepQA;IR;Information Retrieval;Jeopardy;KR&R;ML;NLP;business intelligence;business settings;decision making;deep language processing;enterprise knowledge management;human knowledge;knowledge representation-and-reasoning;machine learning;natural language processing;open-domain automatic question answering;social computing","","","","","","","11-15 Sept. 2010","","IEEE","IEEE Conference Publications"
"The technical hashtag in Twitter data: A hadoop experience","I. Moise","ETH Zurich, Switzerland","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3519","3528","The continuously growing wealth of data has radically changed the data science landscape. At the same time, Big Data tools have known important progress in terms of optimising performance and scalability. However, applying them into practical deployment settings is still a challenging task that is highly dependent on the particularities of the data. In this paper, we present our experiences with implementing a Big Data analytics pipeline with the purpose of extracting value from Twitter data. We acquire and process nearly 60 million tweets that capture the recent outbreaks of the Ebola and Zika viruses. Our processing pipeline first extracts useful information from tweets and then applies a topic modelling technique, provided by Mahout, a Hadoop-based machine learning library. We further extend our Twitter analysis with the study of temporal evolution of daily sentiment toward an important topic, as expressed through the social platform. We highlight at each level, the technical challenges originating from the specific nature of Twitter data and the lessons drawn from our work.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841015","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841015","Big Data;Hadoop;Mahout;Twitter data analysis;optimisation;performance;sentiment analysis;topic modelling","Big data;Data acquisition;Data analysis;Data mining;Data models;Pipelines;Twitter","Big Data;information retrieval;learning (artificial intelligence);parallel processing;sentiment analysis;social networking (online)","Big Data analytics pipeline;Big Data tools;Ebola viruses;Hadoop;Hadoop-based machine learning library;Mahout library;Twitter analysis;Twitter data;Zika viruses;daily sentiment evolution;data science;information extraction;performance optimisation;scalability optimisation;social platform;technical hashtag;temporal evolution;topic modelling","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Exposing wiktionary translations with performance in mind","C. Cargile; G. Santhanakrishnan; A. Olmsted","Dept. Computer Science, College of Charleston, Charleston, SC","2016 International Conference on Information Society (i-Society)","20170216","2016","","","85","87","In this paper, we explore some of the challenges to extracting Wiktionary data so it can be used for machine translation. We provide a use case of considerations made in the development of a web service that transforms raw data in a wiktionary xml dump file into a web-service that provides language translation. Our case study focuses on the performance of this service in comparison to a service using remote calls to the Wiktionary API.","","Electronic:978-1-9083-2061-2; POD:978-1-5090-2545-9; USB:978-1-9083-2062-9","10.1109/i-Society.2016.7854182","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7854182","Wiktionary;WordNet;lexical knowledge base;machine readable dictionary;machine translation;web service","Databases;Electronic publishing;Encyclopedias;Semantics;Web services","Web services;Web sites;XML;application program interfaces;information retrieval;language translation","Web service development;Wiktionary API;Wiktionary XML dump file;Wiktionary data extraction;Wiktionary translations;language translation;machine translation;service performance","","","","","","","10-13 Oct. 2016","","IEEE","IEEE Conference Publications"
"Investigating How ""Good"" Characteristics' Presence Are Related with Questions' Performance: An Empirical Study on a Programming Community","C. Souza; J. Remígio; F. Aragão; E. Costa; J. Fechine","Dept. of Syst. Anal. & Dev., Fed. Inst. of Educ., Monteiro, Brazil","2016 5th Brazilian Conference on Intelligent Systems (BRACIS)","20170202","2016","","","289","294","Social query is the practice of sharing questions through collaborative environments. In order to receive help, askers usually broadcast their requests to the entire community. However, the prerequisite to receive help is to have the problem noticed by someone who is able and available to answer. Previous works have identified a correlation between the characteristics of the questions and the outcome of receiving or not an answer. These findings suggest that there are some characteristics that are more likely to attract the helpers' attention. Our proposal is to analyze the CQA history to identify how these so-called ""good"" characteristics affect the performance of shared questions. Our results suggest that: (1) answered questions present more of these ""good"" characteristics than unanswered ones, (2) the more ""good"" characteristics are present in a question, the more people it attracts, (3) the more people are attracted by a question, the faster it will be answered and more responses will be received, (4) answered questions attract more people than unanswered ones, (5) difficulty does not play a role in attracting people.","","Electronic:978-1-5090-3566-3; POD:978-1-5090-3567-0","10.1109/BRACIS.2016.060","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839601","Community Question Answering Sites;Question Attractiviness;Question Quality;Question Redesign;Social Query","Correlation;Facebook;History;Message systems;Programming;Time factors","query processing;question answering (information retrieval);social networking (online)","CQA history;answered questions;collaborative environments;programming community;question characteristics;questions performance;social query","","","","","","","9-12 Oct. 2016","","IEEE","IEEE Conference Publications"
"Social media in emergencies: How useful can they be","D. Knuth; H. Szymczak; P. Kuecuekbalaban; S. Schmidt","Department of Health and Prevention, University Greifswald, Greifswald, Germany","2016 3rd International Conference on Information and Communication Technologies for Disaster Management (ICT-DM)","20170216","2016","","","1","7","The widespread use and integration of social media into the daily lives of many citizens has also resulted in their widespread use during emergency situation. Due to the possible reach of social media, their real-time character and multimedia integration they can be, to some extent, practical emergency communication tools. However, drawbacks such as unconfirmed or unreliable information, privacy issues and possible technical problems limit their usefulness. To get a more differentiated view on the usefulness of social media, the perceived usefulness of various social media channels (Facebook, YouTube and Twitter) as well as Apps in different phases of emergencies was investigated. Furthermore, besides different social media and different phases, two kinds of usage were investigated, namely information exchange and behavior instructions. A total sample of 1147 participants from eight European countries participated in this study. As a method, a vignette approach was chosen to systematically vary the three factors of social media, phase of emergency and usage. The implications are that rather than treating social media channels in general, it is important to consider them separately and to consider the different phases of the emergency cycle (i.e. preparedness, response and recovery). All in all, the results illustrate that social media offer great potentials in all phases of an emergency with a specific focus on before and after. They can be used to provide people with emergency relevant information and even support the appropriate behavior with respect to the emergency.","","Electronic:978-1-5090-5234-9; POD:978-1-5090-5235-6","10.1109/ICT-DM.2016.7857226","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7857226","Emergency;Social media;crisis communication;usefulness","Decision support systems","data privacy;emergency management;information retrieval;social networking (online)","emergency communication tools;emergency relevant information;emergency situations;information exchange;privacy issues;social media","","","","","","","13-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"HISC/R: An Efficient Hypersparse-Matrix Storage Format for Scalable Graph Processing","R. Kirchgessner; G. D. L. Torre; A. D. George; V. Gleyzer","Dept. of Electr. & Comput. Eng., Univ. of Florida, Gainesville, FL, USA","2016 6th Workshop on Irregular Applications: Architecture and Algorithms (IA3)","20170130","2016","","","70","73","The need to analyze increasingly larger graph datasets has driven the exploration of new methods and unique system architectures for graph processing. One such method moves away from the typical edge- and vertex-centric approaches and describes graph algorithms using linear-algebra operations, bringing the added benefits of predictable data-access patterns and ease of implementation. The performance of this approach is limited by the sparse nature of graph adjacency matrices, which leads to inefficient use of memory bandwidth, and reduced scalability in distributed systems. In order to maximize the scalability and performance of these linear-algebra systems, we require new sparse-matrix storage formats capable of maximizing memory throughput and minimizing latency, while maintaining low storage overhead. In this paper, we present an overview of a novel sparse-matrix storage format called Hashed-Index Sparse-Column/Row (HISC/R) which guarantees constant-time row or column access complexity at low storage overhead, while also supporting online non-zero element insertions and deletions. We evaluate the performance of HISC/R using randomly generated Kronecker graphs, demonstrating a 19% reduction in memory footprint, and 40% reduction in memory reads, for sparse matrix/matrix multiplication compared to competing formats.","","Electronic:978-1-5090-3867-1; POD:978-1-5090-3868-8","10.1109/IA3.2016.018","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833308","","Complexity theory;Indexes;Memory management;Prediction algorithms;Scalability;Sparse matrices","computational complexity;graph theory;information retrieval;linear algebra;matrix multiplication;minimisation;sparse matrices;storage management","HISC-R;Kronecker graphs;constant-time column access complexity;constant-time row access complexity;distributed systems;edge-centric approaches;efficient hypersparse-matrix storage format;graph adjacency matrices;hashed-index sparse-column-row;linear-algebra operations;linear-algebra system performance maximization;linear-algebra system scalability maximization;memory bandwidth;memory footprint;online nonzero element deletions;online nonzero element insertions;predictable data-access patterns;scalable graph processing;sparse matrix-matrix multiplication;sparse-matrix storage format;vertex-centric approaches","","","","","","","13-13 Nov. 2016","","IEEE","IEEE Conference Publications"
"A data access prediction unit for multimedia applications","T. Alawneh; A. Elhossini","Technical University of Berlin Embedded Systems Architectures (AE), Einsteinufer 17, D-10587, Berlin, Germany","2016 28th International Conference on Microelectronics (ICM)","20170209","2016","","","125","128","A large number of algorithms, which work on multimedia data, such as images and videos, perform data processing over rectangular regions of pixels. If this distinctive data access and other data accesses are exploited properly, it can yield significant application performance improvements. For the purpose of achieving efficient exploitation for these data accesses, their information should be monitored and detected at runtime. For this purpose, we propose a data access prediction unit for multimedia applications. Our simulation results reveal that the accuracy of predicting the data access information achieved by the proposed unit is, on average, 87.4%, for the evaluated workloads, when the proposed unit utilizes a history table with 64 entries.","","Electronic:978-1-5090-5721-4; POD:978-1-5090-5722-1","10.1109/ICM.2016.7847925","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847925","Data access;Prediction;Sequential;Stride;Two dimensional","History;Mathematical model;Multimedia communication;Prediction algorithms;Prefetching;Streaming media;Two dimensional displays","field programmable gate arrays;information retrieval;multimedia communication;video signal processing","data access information;data access prediction unit;data processing;multimedia applications;multimedia data","","","","","","","17-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automated assessment of multi-step answers for mathematical word problems","J. C. S. Kadupitiya; S. Ranathunga; G. Dias","Department of Computer Science and Engineering, University of Moratuwa, Sri Lanka","2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer)","20170126","2016","","","66","71","We present a system to automatically grade the mathematical word questions. The questions that we currently consider are at the level of GCE (General Certificate of Education) Ordinary Level (O/L) Mathematics paper standard in Sri Lanka. The solutions to these questions are open-ended multi step answers. The system uses a regular expression based information retrieval approach to validate the expressions in the answers. The implemented system properly evaluates student answers using a marking rubric and awards full/partial marks. We have tested the performance of the system using 500 answer scripts for five different questions from 50 students. The grades given by the system are compared against the manual grading marks and only one answer was graded wrongly. Therefore, the accuracy of the system is 99.8%.","","CD:978-1-5090-6076-4; Electronic:978-1-5090-6078-8; POD:978-1-5090-6079-5; Paper:978-1-5090-6077-1","10.1109/ICTER.2016.7829900","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829900","Automatic Grading;Computer Aided Assessment;Mathematical word problem;Multi-Step Answers","Computers;Manuals;Mathematical model;Standards;Training data;XML","computer aided instruction;information retrieval;mathematics computing","GCE;Sri Lanka;automated assessment;general certificate of education;mathematical word problems;multistep answers;open-ended multistep answers;ordinary level mathematics paper standard;student answer evaluation","","","","","","","1-3 Sept. 2016","","IEEE","IEEE Conference Publications"
"An edge-set based large scale graph processing system","L. Zhou; Y. Xia; H. Zang; J. Xu; M. Xia","The Ohio State University, Columbus, OH 43210, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","1664","1669","Next generation analytics will be all about graphs, though performance has been a fundamental challenge for large scale graph processing. In this paper, we present an industrial graph processing engine for exploring various large scale linked data, which exhibits superior performance due to the several innovations. This engine organizes a graph as a set of edge-sets, compatible with the traditional edge-centric sharding for graphs, but becomes more amenable for large scale processing. Each time only a portion of the sets are needed for computation and the data access patterns can be highly predictable for prefetch for many graph computing algorithms. Due to the sparsity of large scale graph structure, this engine differentiates logical edge-sets from the edge-sets physically stored on the disk, where multiple logical edge-sets can be organized into a same physical edge-set to increase the data locality. Besides, in contrast to existing solution, the data structures utilized for the physical edge-sets can vary from one to another. Such heterogeneous edge-set representation explores the best graph processing performance according to local data access patterns. We conduct experiments on a representative set of property graphs on multiple platforms, where the proposed system outperform the baseline systems consistently.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840780","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840780","edge-set;graph;parallel;prefetch","Algorithm design and analysis;Analytical models;Big data;Complexity theory;Engines;Parallel processing;Prefetching","Big Data;data structures;graph theory;information retrieval;storage management","data structures;edge-set based large scale graph processing system;graph computing algorithms;heterogeneous edge-set representation;local data access patterns","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Event Grounding from Multimodal Social Network Fusion","H. Cho; J. Yeo; S. W. Hwang","Pohang Univ. of Sci. & Technol., Pohang, South Korea","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","835","840","This paper studies the problem of extracting real world event information from social media streams. Although existing work focuses on event signals of bursty mentions extracted from a single-source of textual streams, these signals are likely to be noisy due to ambiguous occurrences of individual mentions. To extract accurate event signals, we propose a framework capable of ""grounding"" mentions to unique event using multiple social networks with complementary strength. We show that our framework jointly using multiple sources outperforms state-of-the-arts using publicly available datasets.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0099","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837912","event detection;event grounding;multimodality;social network","Event detection;Flickr;Grounding;Lattices;Metadata;Twitter","information retrieval;social networking (online)","complementary strength;event grounding;information extraction;multimodal social network fusion","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Crowdsensing and analyzing micro-event tweets for public transportation insights","T. Hoang; P. H. Cher; P. K. Prasetyo; E. P. Lim","School of Information Systems, Singapore Management University, Singapore","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2157","2166","Efficient and commuter friendly public transportation system is a critical part of a thriving and sustainable city. As cities experience fast growing resident population, their public transportation systems will have to cope with more demands for improvements. In this paper, we propose a crowdsensing and analysis framework to gather and analyze realtime commuter feedback from Twitter. We perform a series of text mining tasks identifying those feedback comments capturing bus related micro-events; extracting relevant entities; and, predicting event and sentiment labels. We conduct a series of experiments involving more than 14K labeled tweets. The experiments show that incorporating domain knowledge or domain specific labeled data into text analysis methods improves the accuracies of the above tasks. We further apply the tasks on nearly 200M public tweets from Singapore over a six month period to show that interesting insights about bus services and bus events can be derived in a scalable manner.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840845","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840845","classification;crowdsensing;information extraction;micro-events analysing;sentiment analysis;transportation","Data mining;Dictionaries;Public transportation;Roads;Twitter;Urban areas","information retrieval;social networking (online);text analysis;traffic engineering computing","Singapore;Twitter;bus events;bus services;commuter friendly public transportation system;crowdsensing;domain knowledge;domain specific labeled data;event label prediction;fast growing resident population;microevent tweets analysis;public transportation insights;realtime commuter feedback analysis;realtime commuter feedback gathering;relevant entities extraction;sentiment label prediction;text analysis methods;text mining tasks","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Automating e-Surveys","Z. Davis; C. Brill; H. Siddiqui; A. Olmsted","Department of Computer Science, College of Charleston, Charleston, SC, USA","2016 International Conference on Information Society (i-Society)","20170216","2016","","","103","104","In this paper we investigate the problem of manual survey completion for the purposes of market validation. This study resides in the application domain of business market validation as a motivating example. The amount of data which can be output for market validation can be drastically increased through the process of automation. Through the use of automatic survey completion, we seek to decrease the amount of time required for each survey and therefore increase the efficiency of market data collection. Each survey completed with automation increases the efficiency and results in more market data collection. We outline a chrome application which opens a survey website and answers designated questions. We next document the improvements provided by the automation. Then we overview the results and discuss the available improvements to survey automation process.","","Electronic:978-1-9083-2061-2; POD:978-1-5090-2545-9; USB:978-1-9083-2062-9","10.1109/i-Society.2016.7854187","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7854187","e-Surveys;market validation","Automation;Companies;Computer science;Data collection;Manuals;Pressing","Web sites;marketing data processing;question answering (information retrieval)","Chrome application;automatic survey completion;business market validation;e-survey automation;market data collection;question answering;survey Web site;survey automation process","","","","","","","10-13 Oct. 2016","","IEEE","IEEE Conference Publications"
"Domain-Specific Cross-Language Relevant Question Retrieval","B. Xu; Z. Xing; X. Xia; D. Lo; Q. Wang; S. Li","Coll. of Comput. Sci. & Technol., Zhejiang Univ., Hangzhou, China","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","413","424","In software development process, developers often seek solutions to the technical problems they encounter by searching relevant questions on Q&A sites. When developers fail to find solutions on Q&A sites in their native language (e.g., Chinese), they could translate their query and search on the Q&A sites in another language (e.g., English). However, developers who are non-native English speakers often are not comfortable to ask or search questions in English, as they do not know the proper translation of the Chinese technical words into the English technical words. Furthermore, the process of manually formulating cross-language queries and determining the weight of query words is a tedious and time-consuming process. For the purpose of helping Chinese developers take advantage of the rich knowledge base of the English version of Stack Overflow and simplify the retrieval process, we propose an automated crosslanguage relevant question retrieval (CLRQR) system to retrieve relevant English questions on Stack Overflow for a given Chinese question. Our CLRQR system first extracts essential information (both Chinese and English) from the title and description of the input Chinese question, then performs domain-specific translation of the essential Chinese information into English, and formulates a query with highest-scored English words for retrieving relevant questions in a repository of 684,599 Java questions in English from Stack Overflow. To evaluate the performance of our proposed approach, we also propose four online retrieval approaches as baselines. We randomly select 80 Java questions in SegmentFault and V2EX (two Chinese Q&A websites for computer programming) as the query Chinese questions. Each approach returns top-10 most relevant questions for a given Chinese question. We invite 5 users to evaluate the relevance of the retrieved English questions. The experiment results show that CLRQR system outperforms the four baseline approaches, and the statistical tes- s show the improvements are significant.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.049","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832920","Cross-Language Question Retrieval;Domain-Specific Translation","Algorithm design and analysis;Computers;Data mining;Frequency-domain analysis;Java;Software;Vocabulary","natural language processing;query processing;question answering (information retrieval);software engineering;word processing","CLRQR system;Chinese developers;Chinese technical words;English questions;English technical words;SegmentFault;V2EX;automated cross language relevant question retrieval;cross-language queries;domain-specific cross-language relevant question retrieval;domain-specific translation;essential Chinese information;native language;non-native English speakers;query Chinese questions;query words;software development process;stack overflow;statistical tests;time-consuming process","","","","","","","14-15 May 2016","","IEEE","IEEE Conference Publications"
"A Secure IoT-Based Healthcare System With Body Sensor Networks","K. H. Yeh","National Dong Hwa University, Hualien, Taiwan","IEEE Access","20170127","2016","4","","10288","10299","The ever-increasing advancement in communication technologies of modern smart objects brings with it a newera of application development for Internet of Things (IoT)-based networks. In particular, owing to the contactless-ness nature and efficiency of the data retrieval of mobile smart objects, such as wearable equipment or tailored bio-sensors, several innovative types of healthcare systems with body sensor networks (BSN) have been proposed. In this paper, we introduce a secure IoT-based healthcare system, which operates through the BSN architecture. To simultaneously achieve system efficiency and robustness of transmission within public IoT-based communication networks, we utilize robust crypto-primitives to construct two communication mechanisms for ensuring transmission confidentiality and providing entity authentication among smart objects, the local processing unit and the backend BSN server. Moreover, we realize the implementation of the proposed healthcare system with the Raspberry PI platform to demonstrate the practicability and feasibility of the presented mechanisms.","2169-3536;21693536","","10.1109/ACCESS.2016.2638038","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779108","Authentication;body sensor networks;internet of things (IoT);security","Authentication;Body sensor networks;Internet of things;Network security;Smart devices","Internet of Things;body sensor networks;health care;information retrieval;message authentication","Raspberry PI platform;application development;backend BSN server;body sensor networks;data retrieval;entity authentication;local processing unit;mobile smart objects;public Internet of Things-based communication networks;robust cryptoprimitives;secure IoT-based healthcare system;tailored biosensors;transmission confidentiality;wearable equipment","","","","","","20161209","2016","","IEEE","IEEE Journals & Magazines"
"Prototyping collaborative (co-)archiving practices: From archival appraisal to co-archival facilitation","E. M. Nilsson","School of Arts and Communication (K3), Faculty of Culture and Society, Malm&#x00F6; University, Malm&#x00F6;, Sweden","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","4","This paper presents a series of prototyped collaborative (co-)archiving practices developed within the interdisciplinary research project Living Archives. The aim is to explore co-archiving practices for involving underrepresented voices in contributing to our archives, and to create conditions for accessing intangible heritage resources beyond traditional methods. The methodological approach is design research, and participatory design. Six co-archiving practices are presented, designed to invite the user groups to collect, store and share their memories and cultural heritages. We argue that the co-archiving practices prototyped assume an inclusive and a democratic approach. They allow for the involvement of many senses when accessing and generating archive material in an open, but still highly structured way. Applying such co-archiving approach could potentially result in more representative archives, and support archivists interested in going from a focus on archival appraisal to co-archival facilitation.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863184","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863184","activist archivist;co-archiving;design research;intangible cultural heritage;participatory design;prototyping;underrepresented communities","Collaboration;Cultural differences;Games;Global communication;History;Prototypes;Soil","design;history;information retrieval systems","archival appraisal;coarchival facilitation;collaborative coarchiving practice prototyping;cultural heritages;design research;intangible heritage resource accessing;participatory design","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"Optimizing PGAS Overhead in a Multi-locale Chapel Implementation of CoMD","R. Haque; D. Richards","","2016 PGAS Applications Workshop (PAW)","20170202","2016","","","25","32","Chapel supports distributed computing with an underlying PGAS memory address space. While it provides abstractions for writing simple and elegant distributed code, the type system currently lacks a notion of locality i.e. a description of an object's access behavior in relation to its actual location. This often necessitates programmer intervention to avoid redundant non-local data access. Moreover, due to insufficient locality information the compiler ends up using “wide” pointers-that can point to non-local data-for objects referenced in an otherwise completely local manner, adding to the runtime overhead.In this work we describe CoMD-Chapel, our distributed Chapel implementation of the CoMD benchmark. We demonstrate that optimizing data access through replication and localization is crucial for achieving performance comparable to the reference implementation. We discuss limitations of existing scope-based locality optimizations and argue instead for a more general (and robust) type-based approach. Lastly, we also evaluate code performance and scaling characteristics. The fully optimized version of CoMD-Chapel can perform to within 62%-87% of the reference implementation.","","Electronic:978-1-5090-5214-1; POD:978-1-5090-5215-8","10.1109/PAW.2016.009","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836384","Chapel;CoMD;Distributed computing;Locality;PGAS;Parallel programming","Arrays;Computational modeling;Electronics packaging;Force;Optimization;Reactive power;Runtime","distributed processing;information retrieval;parallel programming;program compilers;source code (software);storage management","CoMD-chapel;PGAS memory address space;compiler;distributed code;distributed computing;multilocale chapel implementation;nonlocal data access","","","","","","","14-14 Nov. 2016","","IEEE","IEEE Conference Publications"
"Development and evaluation of a software repository reusable aspects","G. Vidal; S. Casas","GISP - Instituto de Tecnolog&#x00ED;a Aplicada, Universidad Nacional de la Patagonia Austral, R&#x00ED;o Gallegos, Argentina","2016 35th International Conference of the Chilean Computer Science Society (SCCC)","20170130","2016","","","1","9","Software repositories provide access to various assets; however, it is not presently possible to access reusable aspects on these sites. The repositories and the publication and retrieval methods were designed and developed for software components. The presence of non-functional requirements in software repositories has been demonstrated, but these haven't been implemented for aspects-oriented languages. This paper presents an extension of the Unified Framework for Software Components specification to support aspects. The extended framework enables definition of a specification scheme based on the methods of Faceted Schemes. This specification scheme represents the basis for the development of a tool style “proof of concept”. Metrics were analyzed for the developed tool; these allowed us to evaluate the publication and retrieval of aspects, which showed optimal retrieval results.","","Electronic:978-1-5090-3339-3; POD:978-1-5090-3340-9","10.1109/SCCC.2016.7836045","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7836045","aspects;publication and recovery;reuse;software repositories","Color;Containers;Debugging;Measurement;Monitoring;Software;XML","aspect-oriented programming;formal specification;information retrieval;software metrics;software reusability","aspects-oriented languages;faceted schemes;nonfunctional requirements;optimal retrieval results;proof of concept tool style;publication methods;retrieval methods;software component specification;software metrics;software repository reusable aspect development;software repository reusable aspect evaluation;unified framework","","","","","","","10-14 Oct. 2016","","IEEE","IEEE Conference Publications"
"Replicated convolutional codes: A design framework for repair-efficient distributed storage codes","B. Zhu; X. Li; H. Li; K. W. Shum","School of Electronic and Computer Engineering, Peking University, Shenzhen, China","2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)","20170213","2016","","","1018","1024","Erasure-coded distributed storage systems can offer reliable storage services in a cost-effective manner. However, when disk failures occur in such systems, it is desirable to recreate the lost data with the help of surviving nodes to preserve the data redundancy. A key requirement during the recover process is to minimize the repair locality and computational complexity. We propose a simple framework for constructing storage codes that yield small repair locality and low repair complexity. The basic idea behind this framework is to take multiple instances of convolutional (tail-biting) codes, and then carefully arrange the coded symbols on storage nodes. The resultant codes enjoy the desirable repair-by-transfer property, and perform efficient repair by simple XOR operations. Moreover, we also evaluate the proposed codes atop an HDFS cluster testbed and compare the empirical performance with state-of-the-art repair-efficient storage codes.","","Electronic:978-1-5090-4550-1; POD:978-1-5090-4551-8","10.1109/ALLERTON.2016.7852346","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852346","Distributed storage;convolutional codes;locally repairable codes;regenerating codes;repair-by-transfer","Bandwidth;Convolutional codes;Distributed databases;Encoding;Maintenance engineering;Redundancy;Systematics","computational complexity;convolutional codes;data handling;distributed memory systems;error correction codes;information retrieval","HDFS cluster testbed;XOR operations;computational complexity;data redundancy;disk failures;erasure-coded distributed storage systems;recover process;repair complexity;repair locality;repair-by-transfer property;repair-efficient distributed storage codes;replicated convolutional codes;storage nodes;storage services;tail-biting codes","","","","","","","27-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"Dynamic Proofs of Retrievability with improved worst case overhead","Xiaoqi Yu; Nairen Cao; Jun Zhang; Siu-Ming Yiu","The University of Hong Kong, China","2016 IEEE Conference on Communications and Network Security (CNS)","20170223","2016","","","576","580","Proofs of Retrievability (POR) is a scheme to build a verifiable storage on a remote server that a client can access the data randomly, and periodically execute an efficient Audit protocol to ensure that the data is intact. In dynamic POR, the difficulties are to maintain the latest version of the data while achieving efficient Update and Audit. In this paper, we propose PDPOR that achieves the best worse-case overhead O(logN) for both Write and Audit protocols together with the best worse-case bandwidth. We also prove the security of DPOR using simpler techniques.","","Electronic:978-1-5090-3065-1; POD:978-1-5090-3066-8","10.1109/CNS.2016.7860551","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860551","","Bandwidth;Binary trees;Conferences;Databases;Protocols;Security;Servers","file servers;information retrieval;protocols;storage management;trusted computing","PDPOR;audit protocol;dynamic proofs of retrievability;improved worst case overhead;remote server;worse-case bandwidth;worse-case overhead","","","","","","","17-19 Oct. 2016","","IEEE","IEEE Conference Publications"
"The Application of Information Processing Technology in Serving Chinese News Teachers","S. Zhu; D. Guo; J. Song","Coll. of Inf. Sci. & Technol., Beijing Normal Univ., Beijing, China","2016 International Conference on Educational Innovation through Technology (EITT)","20170202","2016","","","213","217","Chinese news is an important course in the international Chinese teaching, which is mainly to help learners master the common words and expressions in Chinese news, and achieve the ability to read Chinese news. In view of the difficulties and problems existing in the teaching preparation of Chinese news teachers, Combined with the information processing technology the paper puts forward the system serving teachers of Chinese news, which can improve the present teaching preparation process. After obtaining large-scale news text, filtering, classifying, tracking hot events, determining difficulty levels and other processing, the system provides news text to teachers as teaching materials, at the same time provides assistant tools at several different levels including vocabulary teaching, chunk teaching, sentence pattern teaching and discourse teaching, and helps teachers to quickly complete teaching preparation.","","Electronic:978-1-5090-6138-9; POD:978-1-5090-6139-6","10.1109/EITT.2016.49","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839524","Chinese news;Information processing;News text acquisition;Teacher serving system","Databases;Dynamic scheduling;Education;Information processing;Libraries;Vocabulary;Web pages","computer aided instruction;information resources;information retrieval;natural language processing;teaching","Chinese expressions;Chinese news teachers;Chinese words;assistant tools;chunk teaching;discourse teaching;information processing technology;international Chinese teaching;news text acquisition;sentence pattern teaching;teaching preparation process;vocabulary teaching","","","","","","","22-24 Sept. 2016","","IEEE","IEEE Conference Publications"
"Mind the explanatory gap: Quality from quantity","J. Bunn","Department of Information Studies, University College London, London, UK","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3240","3244","This paper makes a contribution to the development of computational archival science by thinking about and linking computational and archival thinking. It suggests that archival thinking and the archival problem space encompasses questions about the nature of consciousness, highlighting how these questions seem to be apparent within the fundamental archival principles of respect des fonds, provenance and original order. It seeks to shift attention back to provenance, not just in the sense of where a given object has come from, but also in the sense of the grounds on which it becomes an object and it suggests that the application of computational methods and tools for archival purposes and the integration of computational with archival thinking may offer a way to maintain awareness of this more philosophical dimension in practice.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840980","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840980","consciousness;ontology;provenance","Big data;Computers;Context;Information processing;Organizations;Substrates","cognition;information retrieval systems","archival principles;archival problem space;archival thinking;cognitive process;computational archival science;computational thinking;consciousness","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"“What makes a pro eating disorder hashtag”: Using hashtags to identify pro eating disorder tumblr posts and Twitter users","L. He; J. Luo","Institute of Data Science, University of Rochester, Rochester, NY 14627","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3977","3979","Eating disorders are detrimental to emotional and physical health conditions. The proliferation of content promoting eating disorders (pro-ED) on social media is a concerning trend that has received attention in recent years. On popular social network websites such as Tumblr and Twitter, there exist online pro-ED communities that focus on having eating disorder as a lifestyle choice as opposed to psychiatric illness. In this paper, we used hashtags to develop two well-trained classifiers: one identifies pro-ED Tumbler posts, and the other identifies pro-ED Twitter users. We also compared pro-ED hashtags across these two networks.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7841081","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7841081","","Conferences;Data science;Decision trees;Market research;Tagging;Twitter","information retrieval;pattern classification;social networking (online)","pro eating disorder hashtag;pro-ED Tumbler posts;pro-ED Twitter users;social media;well-trained classifiers","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"K-NN data classification technique using semantic search on encrypted relational data base","N. Uttarwar; M. A. Pradhan","Dept. of Computer Engineering, AISSMS, COE, Pune, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","6","Information Mining has wide applications in various sector, like bank sector, medical field and various research centers. Cloud computing has most used in now a days. Request is one of the usually used assignments as a piece of data mining applications. Security is the main part of the cloud computing and it has difficult to handle. Two fields now a day has most useful. As far back as decade, because of various assurance issues and various theoretical issues have been proposed under differing security models. In this paper, we focus classification over encoded data. In particular, we propose a k-NN classifier over encoded data in the cloud. In the existing system privacy preserving data mining problem cannot solve. And multi-party computation not handle properly. The proposed system guarantees that security of customer's information request. And work on DMED (Data mining over encrypted data) model.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7860022","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7860022","DMED (Data Mining over encrypted data);Data Mining;K-NN;Security","Cloud computing;Databases;Encryption;Organizations;Servers","cloud computing;cryptography;data mining;information retrieval;pattern classification;relational databases","DMED;K-NN data classification;cloud computing;data mining over encrypted data;information mining;relational database encryption;semantic search","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Low-Rank Sparse Feature Selection for Patient Similarity Learning","M. Zhan; S. Cao; B. Qian; S. Chang; J. Wei","Xi'an Jiaotong Univ., Xi'an, China","2016 IEEE 16th International Conference on Data Mining (ICDM)","20170202","2016","","","1335","1340","Comparing and identifying similar patients is a fundamental task in medical domains - an efficient technique can, for example, help doctors to track patient cohorts, compare the effectiveness of treatments, or predict medical outcomes. The goal of patient similarity learning is to derive a clinically meaningful measure to evaluate the similarity amongst patients represented by their key clinical indicators. However, it is challenging to learn such similarity, as medical data are usually high dimensional, heterogeneous, and complex. In addition, a desirable patient similarity is dependent on particular clinical settings, which implies supervised learning scheme is more useful in medical domains. To address these, in this paper we present a novel similarity learning approach formulated as the generalized Mahalanobis similarity function with pairwise constraints. Considering there always exists some features non-discriminative and contains redundant information, we encode a low-rank structure to our similarity function to perform feature selection. We evaluate the proposed model on both UCI benchmarks and a real clinical dataset for several medical tasks, including patient retrieval, classification, and cohort discovery. The results show that our similarity model significantly outperforms many state-of-the-art baselines, and is effective at removing noisy or redundant features.","","Electronic:978-1-5090-5473-2; POD:978-1-5090-5474-9","10.1109/ICDM.2016.0182","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7837995","","Algorithm design and analysis;Electronic mail;Linear programming;Measurement;Medical diagnostic imaging;Medical services;Sparse matrices","feature selection;information retrieval;learning (artificial intelligence);medical computing;patient treatment;pattern classification","UCI benchmarks;classification;clinical dataset;clinical indicators;clinical settings;cohort discovery;complex high dimensional heterogeneous medical data;feature selection;generalized Mahalanobis similarity function;low-rank sparse feature selection;low-rank structure;medical domains;pairwise constraints;patient retrieval;patient similarity learning;similarity model;supervised learning","","","","","","","12-15 Dec. 2016","","IEEE","IEEE Conference Publications"
"Computational provenance: DataONE and implications for cultural heritage institutions","R. J. Sandusky","University Library, University of Illinois at Chicago, Chicago, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3266","3271","Provenance data is a type of metadata that computer scientists argue can support trustworthy and reliable replication of scientific results. From its origins in scientific workflow systems and database theory, and with concurrent interest from the ecological informatics community, a standard data model (PROV) and extensions for DataONE (ProvONE) have led to initial implementations in several tools commonly used by scientists (R, MATLAB) and in a global federation of scientific data repositories (DataONE). DataONE's support for ingest, storage, indexing and retrieval of provenance data is presented. Implications for libraries are identified. A research agenda for exploring the applicability of the PROV model to cultural heritage institutions - archives and museums - and their digital asset management systems is presented.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840984","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840984","Provenance;archives;computational provenance;libraries;museums","Biological system modeling;Context;Cultural differences;Libraries;Mathematical model;Metadata","database indexing;history;information retrieval;meta data;museums","DataONE;DataONE extensions;Matlab tool;PROV model;ProvONE;R tool;archives;computational provenance data;cultural heritage institutions;digital asset management systems;ecological informatics community;global federation;metadata;museums;provenance data indexing;provenance data ingestion;provenance data retrieval;provenance data storage;scientific data repositories;scientific workflow systems;standard data model","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"An event extraction method by semantic role analysis","Z. Shun-rui; X. Yu-qing; Z. Xin-jian; Y. Hui; Z. Xiao-wen","The First Engineers Scientific Research Institute, Wuxi 214035, China","2016 International Conference on Audio, Language and Image Processing (ICALIP)","20170209","2016","","","591","595","The paper does study on event extraction from news on the internet by the method of semantic role analysis. We annotate the sentence in the news from argument annotator, extract the argument structure of the head verb, convert the arguments to specific semantic roles of the verb, and then match the semantic roles to the event elements. This paper puts forward and studies on how to use VerbNet and SemLink resources to match the verb's arguments and event elements. The experiment was carried out on the 1000 news corpus crawled from the web, and the result shows that the F value is up to 70.6% and has certain application value.","","CD:978-1-5090-0652-6; Electronic:978-1-5090-0654-0; POD:978-1-5090-0655-7","10.1109/ICALIP.2016.7846566","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7846566","Argument structure;Event elements;Event extraction;Semantic role","Context;Data mining;Feature extraction;Libraries;Semantics;Training","information resources;information retrieval;natural language processing;text analysis","Internet;SemLink resources;VerbNet resources;Web crawling;argument annotator;argument structure extraction;event extraction method;news corpus;semantic role analysis;semantic role matching;sentence annotation","","","","","","","11-12 July 2016","","IEEE","IEEE Conference Publications"
"Finding Experts in Community Question Answering Based on Topic-Sensitive Link Analysis","J. Yang; S. Peng; L. Wang; B. Wu","Beijing Key Lab. of Intell. Telecommun. Software & Multimedia, Beijing Univ. of Posts & Telecommun., Beijing, China","2016 IEEE First International Conference on Data Science in Cyberspace (DSC)","20170302","2016","","","54","60","Community question answering(CQA) websites such as Quora and StackOverflow provide a new way of asking and answering questions which are not well served by general web search engines. With the huge volume and ever-increasing number of users and questions, effective strategies of ranking experts for different questions need to be proposed. In this paper, we first make some analysis on the network structure of the CQA website. Based on these works, we further propose an expert finding method NEWHITS, which considers the topical similarity of the users and can well adapts to the feature of the CQA. Then, we apply the NEWHITS algorithm to user authority ranking. The comparison experiments with StackOverflow data are conducted and the experimental results demonstrate that the method we proposed performs better than traditional link analysis methods in the user authority ranking.","","Electronic:978-1-5090-1192-6; POD:978-1-5090-1193-3","10.1109/DSC.2016.35","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866108","CQA;Expert Finding;Link Analysis;NEWHITS;Topical Similarity","Algorithm design and analysis;Analytical models;Electronic mail;Multimedia communication;Social network services;Software;Telecommunications","Internet;question answering (information retrieval);search engines","CQA website;NEWHITS;Quora;Stack Overflow;Web search engines;community question answering;topic-sensitive link analysis;user authority ranking","","","","","","","13-16 June 2016","","IEEE","IEEE Conference Publications"
"The integration of information technologies in the knowledge based organizations","C. Pirnau; M. Titu; L. Rosca","Faculty of Information Technology, Lumina Univ. of South-East Europe, Bucharest, Romania","2016 8th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)","20170223","2016","","","1","6","Relations between the users' expertise, complexity of tasks to be solved by knowledge-based organizations and mission of smart information systems, is a multi-dimensional structure that should provide the necessary conceptual framework of sustainable integrated smart development. The users of smart information systems face a variety of problems, such as: system access, data recovery, identifying and retrieving a specific document and its contents, and difficulties associated with understanding and interpreting of the used system on stages and methods following to be used, respectively problems on understanding the meaning of information provided. Using smart information systems and smart database systems are an area developing very dynamically in computer science, which involves finding solutions to real life problems, involving the application of smart technologies in order to obtain efficient results.","","DVD:978-1-5090-2044-7; Electronic:978-1-5090-2047-8; POD:978-1-5090-2048-5","10.1109/ECAI.2016.7861172","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7861172","Big Data;KTP Programme;Knowledge;Modelling and Simulation","Companies;Data models;Databases;Knowledge based systems;Object oriented modeling","document handling;information retrieval;information systems;information technology;knowledge based systems;organisational aspects","data recovery;document identification;document retrieval;information technologies;knowledge based organizations;smart information systems;sustainable integrated smart development;system access;task complexity;users expertise","","","","","","","June 30 2016-July 2 2016","","IEEE","IEEE Conference Publications"
"Between documentation and sample: Creating a digital cultural heritage archive of gravesites, tombs and tombstones","O. Streiter; J. X. Morris","Nat. Univ. of Kaohsiung, Depart. of West. Languages and Literature, Nat. Chengchi University, Asia-Pacific SpatioTemporal Institute, Centre d&#x00E9;tudes fran&#x00E7;ais sur la Chine Contemporaine, Kaohsiung, Taipei & Taipei, Taiwan, R.O.C.","2016 22nd International Conference on Virtual System & Multimedia (VSMM)","20170228","2016","","","1","9","Our research aims at the development of a large-scale digital archive on funerary and epigraphic practices with a focus on Taiwan and Penghu. Designed to cater to a great variety of research questions in the Humanities and Social Sciences, the archive has to respond equally to the requirements of qualitative and quantitative analyses. In this paper, we present an interpretation on what this might mean and how the archive has been constructed to achieve this aim. Starting from definitions of documentation and sample, as qualities of survey data that facilitate a qualitative and quantitative analysis respectively, we identify conflicts and congruence as well as possible crossovers in these two notions. Then, using the nine year history of our survey on tombstones and our first steps in surveying land god shrines, we show how different survey data have been merged into one archive and what the benefits of this hybrid approach are. We also reflect on the restrictions that arise from this hybrid design, especially for the interpretation of quantitative data. We conclude that such a hybrid archive is superior to any of the approaches in isolation, particularly in projects related to the digitization of endangered cultural heritage.","","Electronic:978-1-4673-8993-8; POD:978-1-4673-8994-5","10.1109/VSMM.2016.7863185","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7863185","Cultural heritage;Penghu;Taiwan;census;digital archiving;documentation;epigraphies;sample;survey","Cultural differences;Documentation;History;Sociology;Statistical analysis;Temperature","history;information retrieval systems","Penghu;Taiwan;digital cultural heritage archive;epigraphic practices;funerary practices;gravesites;land god shrines;tombstones","","","","","","","17-21 Oct. 2016","","IEEE","IEEE Conference Publications"
"A bigraph representation model and directional search mechanism for debates","D. T. N. Nguyen; Y. Kiyoki","Graduate School of Media and Governance Keio University, Japan","2016 International Electronics Symposium (IES)","20170223","2016","","","357","362","The beauty of thinking resides not only in finding answers but also in challenging already existing ones and creating intelligible networks of thoughts and ideas of many different perspectives. Understanding that the essence of critical thinking presents in the flows of questions and answers and aiming at a tool of augmenting it, we propose a new knowledge representation model based on bipartite graphs and a set of functions aided to explore those graphs. A bipartite graph has two disjoint vertex sets that are question and answer sets. Every edge connects a question to an answer holds an evidence for the answer and every edge connects an answer to a question holds an argument raised from the answer. The directional search mechanism is particularly designed to reuse reasoning flows in the debate graphs by a projecting function between the question and answer sets. We also introduce a framework placing the model and the search mechanism altogether in collaborative applications. Many philosophical debates are collected to demonstrate the advantages of the bigraph model besides its simplicity. The analysis and experiments on optimizing semantic search show a sufficient performance for a real-time application.","","CD:978-1-5090-1638-9; Electronic:978-1-5090-1640-2; POD:978-1-5090-1641-9","10.1109/ELECSYM.2016.7861031","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7861031","collaborative system;critical thinking augmentation;directional search;graph-based data model","Bipartite graph;Collaboration;Data models;Databases;Knowledge representation;Media;Semantics","graph theory;knowledge representation;question answering (information retrieval);search problems","bigraph representation model;bipartite graphs;critical thinking;debate graphs;directional search mechanism;disjoint vertex sets;knowledge representation model;semantic search","","","","","","","29-30 Sept. 2016","","IEEE","IEEE Conference Publications"
"IBM PAIRS curated big data service for accelerated geospatial data analytics and discovery","S. Lu; X. Shao; M. Freitag; L. J. Klein; J. Renwick; F. J. Marianno; C. Albrecht; H. F. Hamann","IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, USA","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","2672","2675","IBM's Physical Analytics Integrated Data Repository and Services (PAIRS) is a geospatial Big Data service. PAIRS contains a massive amount of curated geospatial (or more precisely spatio-temporal) data from a large number of public and private data resources, and also supports user contributed data layers. PAIRS offers an easy-to-use platform for both rapid assembly and retrieval of geospatial datasets or performing complex analytics, lowering time-to-discovery significantly by reducing the data curation and management burden. In this paper, we review recent progress with PAIRS and showcase a few exemplary analytical applications which the authors are able to build with relative ease leveraging this technology.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840910","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840910","GIS;Hadoop & HBase for geospatial data;big data analytics;data management systems;machine learning","Agriculture;Geospatial analysis;Meteorology;Remote sensing;Satellites;Spatial resolution","Big Data;data analysis;information retrieval","IBM PAIRS curated big data service;IBM Physical Analytics Integrated Data Repository and Services;accelerated geospatial data analytics;data curation;data discovery;data management;geospatial big data service;geospatial dataset retrieval;private data resources;public data resources;spatio-temporal data;time-to-discovery reduction;user contributed data layer","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Appraising digital archives with Archivematica","M. Shallcross","Bentley Historical Library, University of Michigan, Ann Arbor, MI (U.S.A.)","2016 IEEE International Conference on Big Data (Big Data)","20170206","2016","","","3272","3276","The Bentley Historical Library, funded by a generous grant from the Andrew W. Mellon Foundation, has developed a new Appraisal and Arrangement tab in the Archivematica digital preservation system as part of its “ArchivesSpace-Archivematica-DSpace Workflow Integration” project. This new functionality permits users to conduct large-scale appraisal of digital archives as part of a largely automated workflow that extends from the point of acquisition through deposit in a preservation repository.","","Electronic:978-1-4673-9005-7; POD:978-1-4673-9006-4","10.1109/BigData.2016.7840985","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7840985","appraisal;digital archives;digital preservation;open source software","Appraisal;Big data;Digital preservation;Libraries;Metadata;Open source software;Security","digital preservation;information retrieval systems","Archivematica digital preservation system;ArchivesSpace-Archivematica-DSpace Workflow Integration project;Bentley Historical Library;digital archives;large-scale appraisal;preservation repository","","","","","","","5-8 Dec. 2016","","IEEE","IEEE Conference Publications"
"Locating Bugs without Looking Back","T. Dilshener; M. Wermelinger; Y. Yu","Comput. & Commun. Dept., Open Univ., Milton Keynes, UK","2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)","20170126","2016","","","286","290","Bug localisation is a core program comprehension task in software maintenance: given the observation of a bug, where is it located in the source code files? Information retrieval (IR) approaches see a bug report as the query, and the source code files as the documents to be retrieved, ranked by relevance. Such approaches have the advantage of not requiring expensive static or dynamic analysis of the code. However, most of state-of-the-art IR approaches rely on project history, in particular previously fixed bugs and previous versions of the source code. We present a novel approach that directly scores each current file against the given report, thus not requiring past code and reports. The scoring is based on heuristics identified through manual inspection of a small set of bug reports. We compare our approach to five others, using their own five metrics on their own six open source projects. Out of 30 performance indicators, we improve 28. For example, on average we find one or more affected files in the top 10 ranked files for 77% of the bug reports. These results show the applicability of our approach to software projects without history.","","Electronic:978-1-4503-4186-8; POD:978-1-5090-2242-7","10.1109/MSR.2016.037","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832908","Software maintenance;bug localisation;bug reports;empirical study;key positions;lexical search;stack trace","Computer bugs;Data mining;History;Java;Libraries;Semantics;Software","information retrieval;program debugging;public domain software;software maintenance","IR approaches;bug localisation;core program comprehension task;information retrieval approaches;open source projects;software maintenance;software projects;source code files","","","","","","","14-15 May 2016","","IEEE","IEEE Conference Publications"
"Improving data storage security in cloud environment using public auditing and threshold cryptography scheme","R. Suryawanshi; S. Shelke","Sinhagad Academy of Engineering, India","2016 International Conference on Computing Communication Control and automation (ICCUBEA)","20170223","2016","","","1","6","Cloud Computing is a framework where services are provided to the clients on pay per use basis. Many of the organizations uses cloud servers for outsourcing their sensitive data. Along with all the advantages of cloud computing there comes a threat of security of sensitive outsourced data. Cloud users cannot rely only on cloud service provider for the security reasons of the outsourced data. On that account there is need of Third Party Authenticator who will authenticate the cloud data on behalf of data owners. This paper proposes two schemes in order to ensure the security of the data stored on cloud. First one is Public Auditing in which homomorphic linear authenticator with random masking is used for auditing process. The second scheme is of Threshold Cryptography in which Capability List is used to ensure the data access control. Our first scheme ensures that TPA will not gain any knowledge about the sensitive data during auditing process. Our second scheme guarantee that malicious users could not misuse the data stored on clouds.","","Electronic:978-1-5090-3291-4; POD:978-1-5090-3292-1","10.1109/ICCUBEA.2016.7859990","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859990","Cloud Computing;Cloud Storage;Data Integrity;Third party auditor;Threshold Cryptography","Access control;Cloud computing;Cryptography;Data privacy;Protocols;Servers","auditing;cloud computing;cryptography;information retrieval;message authentication;outsourcing","auditing process;cloud computing;cloud environment;data access control;data storage security;data stored security;public auditing;random masking;sensitive data outsourcing;third party authenticator;threshold cryptography","","","","","","","12-13 Aug. 2016","","IEEE","IEEE Conference Publications"
"Bee Colony Based Worker Reliability Estimation Algorithm in Microtask Crowdsourcing","A. Moayedikia; K. L. Ong; Y. L. Boo; W. Yeoh","Dept. of Inf. Syst. & Bus. Analytics, Deakin Univ., Geelong, VIC, Australia","2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)","20170202","2016","","","713","717","Estimation of worker reliability on microtask crowdsourcing platforms has gained attention from many researchers. On microtask platforms no worker is fully reliable for a task and it is likely that some workers are spammers, in the sense that they provide a random answer to collect the financial reward. Existence of spammers is harmful as they increase the cost of microtasking and will negatively affect the answer aggregation process. Hence, to discriminate spammers and non-spammers one needs to measure worker reliability to predict how likely that a worker put an effort in solving a task. In this paper we introduce a new reliability estimation algorithm works based on bee colony algorithm called REBECO. This algorithm relies on Gaussian process model to estimate reliability of workers dynamically. With bees that go in search of pollen, some are more successful than the others. This maps well to our problem, where some workers (i.e., bees) are more successful than other workers for a given task thus, giving rise to a reliability measure. Answer aggregation with respect to worker reliability rates has been considered as a suitable replacement for conventional majority voting. We compared REBECO with majority voting using two real world datasets. The results indicate that REBECO is able to outperform MV significantly.","","Electronic:978-1-5090-6167-9; POD:978-1-5090-6168-6","10.1109/ICMLA.2016.0127","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838231","Bee colony;Crowdsourcing;Gaussian process model;Worker reliability estimation","Australia;Business;Crowdsourcing;Estimation;Gaussian processes;Heuristic algorithms;Reliability","Gaussian processes;evolutionary computation;information retrieval;personnel;production engineering computing;reliability","Gaussian process model;REBECO algorithm;bee colony based worker reliability estimation algorithm;financial reward;microtask crowdsourcing platform","","","","","","","18-20 Dec. 2016","","IEEE","IEEE Conference Publications"
"Linking Multimedia to Microblogs for Metadata Extraction","P. Gaspar; J. Simko","Fac. of Inf. & Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia","2016 Third European Network Intelligence Conference (ENIC)","20170202","2016","","","90","97","Social media contain vast quantities of content that could be mined for useful information. One type of such information are metadata for TV shows and other multimedia. Nowadays, broadcasting companies, TV channels and fans keep their pages fed with posts and comments on the TV shows being broadcasted. These microblogs potentially contain a lot of additional information on TV shows, which can complement existing structured knowledge bases (e.g. IMDB, Wikipedia). However, microblogs are usually not explicitly connected to multimedia entities in knowledge repositories. This work focuses on finding these mappings to enable future acquisition of metadata from microblogs. Our method is based on applying several natural language processing methods to microblog posts to identify TV shows in named entities, hashtags and external links. We evaluate our method by using an annotated dataset of posts from Facebook feeds. For the optimal setting we scored the precision 0.92.","","Electronic:978-1-5090-3455-0; POD:978-1-5090-3456-7","10.1109/ENIC.2016.021","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838050","interlinking metadata;mutlimedia metadata;social networks","Feature extraction;Knowledge based systems;Metadata;Schedules;TV;Tagging;Twitter","data acquisition;data mining;information retrieval;knowledge based systems;meta data;multimedia computing;natural language processing;social networking (online)","Facebook feeds;IMDB;TV channels;TV shows;Wikipedia;annotated posts dataset;broadcasting companies;content mining;metadata acquisition;metadata extraction;microblog posts;multimedia entities;natural language processing methods;social media;structured knowledge bases","","","","","","","5-7 Sept. 2016","","IEEE","IEEE Conference Publications"
