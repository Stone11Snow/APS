"http://ieeexplore.ieee.org/search/searchresult.jsp?ar=7021731,7019393,7020620,7019261,7019162,7020621,7014768,7016906,7014748,7016903,7017715,7017213,7016922,7014568,7014583,7017209,7017099,7015961,7016651,7017395,7016887,7014591,7016783,7014750,7012871,7012963,7011615,6997020,7011325,7011328,7011304,7009567,7008680,6999206,6999239,7007663,7009192,6999327,7009715,7009322,7009727,7007181,7007893,6999197,7009576,7005150,6645370,7005195,7001520,7001460,7000766,7001352,7004412,7001869,7004453,7004283,7001528,7004247,7004498,7002391,7004312,7004273,7000177,7000168,7000931,7004444,7004391,7004528,7000096,6999694,6999065,6998874,6996107,6997363,6996649,6996196,6996216,6996586,6994986,6997670,6996128,6996194,6998393,6997369,6996200,6996148,6998468,6823714,6992834,6993377,6993859,6992956,6993414,6991400,6987557,6991082,6990394,6991402,6991404,6991418",2017/05/04 22:16:17
"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","MeSH Terms",Article Citation Count,Patent Citation Count,"Reference Count","Copyright Year","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"Scaled Entity Search: A method for media historiography and response to critiques of big humanities data research","E. Hoyt; K. Hughes; D. Long; A. Tran; K. Ponto","Department of Communication Arts, University of Wisconsin-Madison, Madison, WI, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","51","59","Search has been unfairly maligned within digital humanities big data research. While many digital tools lack a wide audience due to the uncertainty of researchers regarding their operation and/or skepticism towards their utility, search offers functions already familiar and potentially transparent to a range of users. To adapt search to the scale of Big Data, we offer Scaled Entity Search (SES). Designed as an interpretive method to accompany an under-construction application that allows users to search hundreds or thousands of entities across a corpus simultaneously, SES balances critical reflection on the entities, corpus, and digital with an appreciation of how all of these factors interact to shape both our results and our future questions. Using examples from film and broadcasting history, we demonstrate the process and value of SES as performed over a corpus of 1.3 million pages of media industry documents.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004453","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004453","big data critiques;film;historiography;radio;search","Big data;Data mining;Educational institutions;History;Indexes;Libraries;Media","Big Data;humanities;information retrieval","big humanities data research;digital humanity;media historiography;scaled entity search","","0","","38","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Bayes topic prediction model for focused crawling of vertical search engine","Weihong Zhang; Yong Chen","National Engineering Research Center for S&T Resources Sharing Services, School of Computer Science and Engineering, Beihang University, Beijing, China","2014 IEEE Computers, Communications and IT Applications Conference","20150122","2014","","","294","299","Vertical search is an important topic in the design of search engines as it offers more abundant and more precise results on specific domain compared with large-scale search engines, like Google and Baidu. Prior to this paper, most vertical search engines were built using manually selected and edited materials, which was time and money consuming. In this paper, we propose a new information resource discovery model and build a crawler in the vertical search engine, which can selectively fetch webpages relevant to a pre-defined topic. The model includes three aspects. First, webpages are transformed into term vectors. TF-TUF , short for Term Frequency-Topic Unbalanced Factor , is proposed as the weighting schema in vector space model. In the schema,we put more weight on terms whose frequencies differ a lot among topics, which will contribute more in the topic prediction we believe. Second, we use Bayes method to predict the topics of the webpages, where topic labeled text is used for training in advance. The specific method about using Bayes to predict the topic is illustrated in the algorithm section. Third, we create a focused crawler using the topic prediction result. The prediction result is used not only to filter the irrelevant webpages but also to direct the crawler to the areas, which are most possible to be topic relevant. The whole three aspects work together to reach the goal of discovering the topic relevant materials on the web efficiently, in building a vertical search engine. Our experiment shows that the average prediction accuracy of our proposed model can reach more than 85%. For application, we also used the proposed model to build ""Search Engine for S&T"" (http://nstr.com.cn/search), a vertical search engine in science field.","","Electronic:978-1-4799-4811-6; POD:978-1-4799-4810-9","10.1109/ComComAp.2014.7017213","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017213","Focused Crawler;Naïve Bayes;Term Frequency;Topic Unbalanced Factor;Vertical Search Engine","Accuracy;Crawlers;Information filtering;Predictive models;Search engines;Training;Vectors","information retrieval;learning (artificial intelligence);search engines","Baidu;Bayes topic prediction model;Google;TF-TUF factor;Web page fetching;focused crawling;information resource discovery model;term frequency-topic unbalanced factor;term vectors;topic labeled text;vector space model;vertical search engine","","0","","17","","","20-22 Oct. 2014","","IEEE","IEEE Conference Publications"
"Scheduling data access in Smart Grid networks utilizing context information","M. Findrik; J. Groenbaek; R. L. Olsen","Forschungszentrum Telekommunikation Wien - FTW, Vienna, Austria","2014 IEEE International Conference on Smart Grid Communications (SmartGridComm)","20150115","2014","","","302","307","Current electrical grid is facing increased penetration of intermittent energy resources, in particular wind and solar energy. Fast variability of the power supply due to renewable energy resources can be balanced out using different energy storage systems or shifting the loads. Efficiently managing this fast flexibility requires two-way data exchange between a controller and sensors/meters via communication networks. In this paper we investigated scheduling of data collection utilizing meta-data from sensors that are describing dynamics of information. We show the applicability of this approach for a constraint communication networks of the smart grid and compared three general data access mechanisms, namely, push, pull and event-based.","","Electronic:978-1-4799-4934-2; POD:978-1-4799-4933-5; USB:978-1-4799-4935-9","10.1109/SmartGridComm.2014.7007663","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007663","","Analytical models;Data models;Delays;Markov processes;Quality of service;Sensors;Smart grids","energy storage;information retrieval;meta data;power engineering computing;power supply quality;power system management;smart power grids;solar power;wind power","constraint communication network;context information;data access scheduling;data collection scheduling;energy storage system;intermittent renewable energy resource penetration;load shifting;metadata;power supply variation;smart grid network;solar energy;two-way data exchange;wind energy","","1","","14","","","3-6 Nov. 2014","","IEEE","IEEE Conference Publications"
"Natural human-robot interaction for elderly and disabled healthcare application","Q. Zhao; D. Tu; S. Xu; H. Shao; Q. Meng","Shanghai Key Laboratory of Intelligent Manufacturing and Robotics, Shanghai, P. R. China","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20150115","2014","","","39","44","Natural human-robot interaction plays an important role in effective nursing services system provided by service robots for the elderly and disabled people. This paper proposed a multimodal “human-robot integration” collaboration system, and set up a shared collaboration interface between human and service robot. Consequently, Users and service robots can naturally communicate and retrieve information from the collaborative interface with multimodality(e.g. head gesture, eye gaze) in an interactive dialogue approach. By this way, making the service robots fully understand human's intention, so they can collaborate and complete tasks well. Furthermore, some experiments were conducted, and the results suggest that it is effective to identify user's intention in light of the advantage of different modalities, the shared collaboration interface can provide more information both from human and robots to improve the naturalness of human-machine collaboration. The proposed methods can provide a new way for exploiting human-service robots cooperation.","","Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8","10.1109/BIBM.2014.6999239","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999239","Healthcare System;Human-Robot Interaction;Multimodality;Service Robot","Collaboration;Head;Medical services;Mobile robots;Sensors;Service robots","biomedical optical imaging;gaze tracking;geriatrics;gesture recognition;handicapped aids;health care;human-robot interaction;information retrieval;medical image processing;medical robotics;patient care","disabled healthcare application;effective nursing service system;elderly healthcare application;eye gaze;head gesture;human-machine collaboration naturalness;human-service robots cooperation;information retrieval;interactive dialogue approach;multimodal human-robot integration collaboration system;natural human-robot interaction;shared collaboration interface;user intention identification;user-service robot communication","","0","","9","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Automatic textual aggregation approach of scientific articles in OLAP context","B. Mustapha; L. Sabine; O. Youcef","LIM Laboratory, University of Laghouat, Algeria","2014 10th International Conference on Innovations in Information Technology (IIT)","20141218","2014","","","30","35","In the last decade, Online Analytical Processing (OLAP) has taken an increasingly important role in Business Intelligence. Approaches, solutions and tools have been provided for both databases and data warehouses, which focus mainly on numerical data. These solutions are not suitable for textual data. Because of the fast growing of this type of data, there is a need for new approaches that take into account the textual content of data. In the context of Text OLAP (OLAP on text or documents), the measure can be textual and need a suitable aggregation function for OLAP operations such as roll-up. We present in this paper a new aggregation function for textual data. Our approach is based on the affinity between keywords and uses the search of cycles in a graph to find the aggregated keywords. We also present performances and a comparison with three other methods. The experimental study shows good results for our approach.","","Electronic:978-1-4799-7212-8; POD:978-1-4799-7213-5; USB:978-1-4799-7211-1","10.1109/INNOVATIONS.2014.6987557","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987557","OLAP;aggregation function;graph;textual data","Benchmark testing;Clustering algorithms;Complexity theory;Context;Data models;Pragmatics;Runtime","competitive intelligence;data mining;graph theory;information retrieval;text analysis","OLAP context;aggregated keywords;aggregation function;automatic textual aggregation approach;business intelligence;data warehouses;databases;graph cycles;online analytical processing;roll-up;scientific articles;text OLAP","","0","","21","","","9-11 Nov. 2014","","IEEE","IEEE Conference Publications"
"The Adaptive Projection Forest: Using adjustable exclusion and parallelism in metric space indexes","L. P. Thompson; W. Xu; D. P. Miranker","University of Texas at Austin","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","615","620","This paper introduces an indexing method for searching diverse data types that is easily parallelizable for use with large data sets. This method, the Adaptive Projection Forest (APF) is a partition-based metric-space indexing method, which provides generic retrieval solutions for data sets for which similarity is defined by a metric-distance function. The APF is uniquely suited to alleviate problems typically encountered in metric-space indexing because it adaptively incorporates exclusion, a method that removes data near a partition boundary and creates multiple trees for use in parallel computing. The use of exclusion allows the index to be more effective when data falls near partition boundaries, where traditional pruning is not always possible. The APF's use of exclusion also allows it to have greater success in parallel environments, meaning that the APF algorithm can be more effectively used on large data sets with diverse data types. In the APF index, the proportion of excluded data is adjusted dynamically at each index node by locally determining the dimension, k, of the projection of the metric space onto the real numbers. The algorithm, which provides asymptotic algorithmic guarantees for nearest neighbor search, is presented along with a parallel implementation of the APF. Across a suite of real-world and synthetic benchmarks the APF demonstrates favorable empirical results, measured in number of calculations, when compared with the emVP, MVP, and SA indexes. Experiments also reveal that number of calculations can be minimized when a critical parameter, the width of the exclusion region, is set much smaller than the value suggested by asymptotic algorithmic analysis.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004283","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004283","","Algorithm design and analysis;Indexing;Partitioning algorithms;Program processors;Throughput;Vegetation","indexing;information retrieval;parallel processing;trees (mathematics);very large databases","APF algorithm;APF index;MVP index;SA index;adaptive projection forest;asymptotic algorithmic analysis;asymptotic algorithmic guarantees;diverse data types;emVP index;exclusion region;generic retrieval solution;large data set;metric space indexes;metric-distance function;multiple trees;nearest neighbor search;parallel computing;parallel environment;partition boundary;partition-based metric-space indexing method;pruning","","0","","18","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Disaster mitigation support system using Twitter and GIS","T. Funayama; Y. Yamamoto; M. Tomita; O. Uchida; Y. Kajita","Graduate School of Science Tokai University Hiratsuka, Japan","2014 Twelfth International Conference on ICT and Knowledge Engineering","20150108","2014","","","18","23","We can easily get the Tweets from the Twitter and open data about population etc. Therefore we are constructing a system to extract disaster information and analyze Tweet in Twitter by using API (Application Programming Interface), Web service changing geographical information and R etc. If we want to know where is occurring an earthquake, a flood and etc, we could want to get those information which are seismic intensity, weather and traffic conditions. According this study was decided to get those information by using API. And a system which get and show those information are constructed.","2157-0981;21570981","Electronic:978-1-4799-8027-7; POD:978-1-4799-8028-4","10.1109/ICTKE.2014.7001528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001528","API;Disaster mitigation;GIS;Google map;Morphological Analysis;R;Twitter","Data visualization;Educational institutions;Google;Image color analysis;Sociology;Statistics;Twitter","Web services;application program interfaces;emergency management;geographic information systems;information analysis;information retrieval;social networking (online)","API;GIS;Twitter;Web service;application programming interface;disaster information extraction;disaster mitigation support system;geographic information system;seismic intensity;traffic condition;tweet analysis;weather condition","","1","","6","","","18-21 Nov. 2014","","IEEE","IEEE Conference Publications"
"Automatic evaluation of search engines: Using webpages' content, web graph link structure and websites' popularity","R. Badie; M. Azimzadeh; M. S. Zahedi; S. Samuri","IT Faculty, Iran Telecommunication Research Center, Tehran, Iran","Telecommunications (IST), 2014 7th International Symposium on","20150108","2014","","","556","562","Measurement of the information retrieval effectiveness of web search engines is an important problem because it can determine which of the available search engines can provide better results to its users. But because of the need of human assessors and their judgments this Measurement is expensive and time consuming. So for solving this problem we introduced an automatic method that is applicable quickly and provides a ranking on the web search engines that were under evaluation without the need of having relevance judgments. In the experiments that were conducted we compared our method with the one proposed by Can et al. and the results acquired from human assessors' evaluations. The comparisons showed that the introduced method provides rankings that are consistent with the rankings resulted from human assessors' evaluations and are better than the ones achieved by the Can et al.'s method. Also it is shown that the consistencies observed are statistically significant, so the proposed method can be used for evaluation of web search engines in real environments.","","CD-ROM:978-1-4799-5358-5; Electronic:978-1-4799-5359-2; POD:978-1-4799-5360-8","10.1109/ISTEL.2014.7000766","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000766","Information retreival;Performance evaluation;Relevance Judgment;Web search engines ranking","Engines;Google;Internet;Search engines;Web pages;Web search","Web sites;information retrieval;search engines","Web graph link structure;Webpage content;Website popularity;human assessor evaluations;information retrieval effectiveness measurement;search engine automatic evaluation","","1","","18","","","9-11 Sept. 2014","","IEEE","IEEE Conference Publications"
"Evolving interest based user groups using PSO algorithm","S. Ganesan; A. I. U. Sivaneri; S. Selvaraju","Dept. of IST, CEG, Anna University, Chennai, India","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","6","Any Web site may have the continuous improvement based on the getting information of the users' needs. There is a step to achieve it by the collection of users' search data and analysis of those data. The swarm intelligence technique of Particle Swarm Optimization(PSO) is applied for evolving similar user groups. PSO is important to identify the web users' travels with the same interests. The data set comprises of web log files obtained by collecting the user logs during a six month period. The PSO algorithm is attempted for user categorization. In web search, the users are grouped into different categories based on their similar travels. The grouping performance of the PSO technique is compared with the techniques of DBSCAN and Kmeans.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996196","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996196","Clustering;ODP taxonomy;PSO algorithm;Web Usage Mining","Clustering algorithms;Crawlers;IP networks;Optimization;Particle swarm optimization;Taxonomy","Internet;data analysis;data mining;information retrieval;particle swarm optimisation","DBSCAN comparison;K-means comparison;PSO algorithm;Web log files;Web search;Web site continuous improvement;Web user interests;Web user travel identification;data analysis;evolving interest based user group;particle swarm optimization;swarm intelligence technique;user categorization;user log collection;user search data collection","","0","","31","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Virtualized dynamic URL assignment web crawling model","W. R. Bhaginath; S. Shingade; M. Shirole","Department of CE and IT, V.J.T.I, Matunga, Mumbai, India","2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014)","20150119","2014","","","1","7","Web search engines are software systems that help to retrieve the information from the net by accepting the input in the form of query and providing the result as files, pages, images or information. These search engines heavily rely on the web crawlers that interact with millions of the web pages given a seed URL or a list of seed URLs. However, these crawlers demand a large amount of computing resources. The efficiency of web search engines depends upon the performance of the crawling processes. Despite the continuous improvement in the crawling processes still there is a need of improvement towards more efficient and low cost crawler. Most of the crawlers existing today have a centralized coordinator that brings the disadvantage of single point failure. Taking into consideration the shortfalls of the existing crawlers, this paper proposes an architecture of a distributed web crawler. The architecture addresses two issues of the existing web crawlers: the first is to create a low cost web crawler using the concept of virtualization of cloud computing. The second issue is a balanced load distribution based on dynamic assignment of the URLs. The first issue is solved using mutli-core machines where each multi-core processor is divided into number of virtual machines (VM) that can perform different crawling task in parallel. Second issue is addressed using a clustering algorithm that assigns requests to the machines as per the availability of the clusters thereby realizing the balance among components according to their real-time condition. This paper discusses a distributed architecture and details of the implementation of the proposed algorithm.","2347-9337;23479337","Electronic:978-1-4799-6393-5; POD:978-1-4799-6394-2","10.1109/ICAETR.2014.7012963","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012963","Clustering algorithm;Crawler;Dynamic assignment;K-means clustering;Seeds;Virtualization","Computational modeling;Crawlers;HTML;Hardware;Pipeline processing;Software;Uniform resource locators","Web sites;cloud computing;information retrieval;online front-ends;search engines","Web crawling model;Web pages;Web search engine;balanced load distribution;centralized coordinator;cloud computing;clustering algorithm;distributed Web crawler;distributed architecture;dynamic assignment;low cost Web crawler;multicore processor;mutlicore machines;software system;virtual machine;virtualized dynamic URL assignment","","0","","12","","","1-2 Aug. 2014","","IEEE","IEEE Conference Publications"
"XML-based fully automatic assessment system for IT skills","A. Chorana; A. Lakhdari; H. Cherroun; S. O. Naoui","Computer science and mathematics department, Faculty of Science, Amar TELIDJI University of Laghouat, Po Box 37G, Ghardaia road, Laghouat 03000-Algeria","2014 International Conference on Web and Open Access to Learning (ICWOAL)","20150115","2014","","","1","6","The need for IT skills assessment and certification is indisputable for both persons and institutions. Thus, the recourse to automatic assessment systems able to answer the strong demand of such assessment or certification is inevitable. In this paper, we propose a new approach to automatically assess IT skills in authentic context. This approach relies on the potential of XML format and its related efficient technologies which allow the extraction of information from XML documents. We divide our assessment approach into two steps. In the first, we transform both Students documents and answers model to an intermediate representation using XML format for which some preprocessing is done. In the second step, we extract from the teacher's correct document the required skills as paths of the tree representation of the XML document. In order to assign a mark we measure similarities between these paths and those of the student document. We have conducted an experimental study to validate our approach. Thus, we have developped a tool based on the proposed approach to automatically assess Word Processing skills using Microsoft Office Word program. The evaluation of our tool is performed on a real exam with one hundred students. The results show the accuracy and the suitability of this solution for assessing IT skills.","","Electronic:978-1-4799-5739-2; POD:978-1-4799-5740-8","10.1109/ICWOAL.2014.7009192","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009192","","Data mining;Educational institutions;Electronic mail;Europe;Software;Text processing;XML","XML;computer science education;document handling;information retrieval;tree data structures;word processing","IT skill assessment;Microsoft Office Word program;Word Processing skills;XML documents;XML format;XML-based fully automatic assessment system;certification;information extraction;student document;teacher correct document;tree representation","","0","","28","","","25-27 Nov. 2014","","IEEE","IEEE Conference Publications"
"Automatic Question Generation system","P. Pabitha; M. Mohana; S. Suganthi; B. Sivanandhini","Dept of Computer Technology, MIT, Anna University, Chennai, India","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","5","The process of automating the question generation consists of many tasks. Selecting the target content (what to ask), question type (who, why, how) and actual question generation are the major issue of Automatic Question Generation. Certain definitions retrieved is available in Wikipedia either directly or is the outcome of executing set of sub queries for each key phrase categories The problem in the existing system is that some of the definition sentences which are taken out from Wikipedia were implicit. The proposed system overcomes the problems by using Supervised Learning Approach, Naïve Bayes method. It also extends its work to use Summarization, Noun Filtering and Question Generation in the aim of generating semantically correct questions.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996216","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996216","Automatic Question Generation;Key phrases;Naïve Bayes;Noun Filtering;Stemming;Summarization;Supervised Machine Learning","Data mining;Information filters;Information services;Information technology;Market research","Bayes methods;Web sites;information filtering;learning (artificial intelligence);query processing;question answering (information retrieval);text analysis","Wikipedia;automatic question generation system;definition sentences;key phrase categories;naïve Bayes method;noun filtering;question type;subqueries;summarization;supervised learning approach","","0","","16","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Attribute-Based Keyword Search and Data Access Control in Cloud","J. Li; L. Zhang","Shanghai Key Lab. of Trustworthy Comput., Software Eng. Inst. East China Normal Univ., Shanghai, China","2014 Tenth International Conference on Computational Intelligence and Security","20150122","2014","","","382","386","As more and more data is outsourced to cloud which is assumed to be a semi-trusted server, it is necessary to encrypt the sensitive data stored in the cloud. However, it brings a series of problems, such as: How to search over the encrypted data efficiently and securely? How should a data owner grant search capabilities to the data users? To solve these problems, we propose two attribute-based keyword search and data access control schemes based on public-key searchable encryption and attribute based encryption. Our solutions allow a data owner to control the access policy and grant the search policy to any data user who wants to retrieve the encrypted data efficiently.","","CD-ROM:978-1-4799-7433-7; Electronic:978-1-4799-7434-4; POD:978-1-4799-7435-1","10.1109/CIS.2014.113","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016922","Cloud storage;access control;attribute-based encryption;public-key searchable encryption","Access control;Cloud computing;Encryption;Indexes;Keyword search","authorisation;cloud computing;information retrieval;public key cryptography;storage management","attribute based encryption;attribute-based keyword search scheme;cloud storage;data access policy control scheme;public-key searchable encryption;search policy control;semi trusted server;sensitive data encryption","","3","","9","","","15-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"Multilevel partitioning of large unstructured grids","O. O. Akande; P. J. Rhodes","Intel Corporation, 5000 W Chandler Blvd, Chandler, AZ 85226","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","317","322","Scientific datasets today are often far too large to fit into a single machine's memory or even a single disk. Partitioning multidimensional arrays across several machines or disks has become increasingly necessary. However, relatively little work has been done for unstructured grids composed of a collection of simplicial cells. Our previous work investigated partitioning unstructured grids at the disk level and its effect on overall system performance. In this paper, we build upon prior work by investigating the effect of an in-core partitioning performed on top of the existing disk level partitioning. The granularity of in-core partitioning has varying effect on the overall system performance. Based on our test results, we propose a formula for choosing an effective partitioning for large unstructured grids to facilitate fast data retrieval. We also examine the performance benefits of declustering unstructured grids across several disks. Given this declustered dataset, we describe and explore a parallel data retrieval method that takes advantage of prior knowledge of a user access pattern. Our test results demonstrate very significant performance gains.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004247","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004247","","Geometry;Lattices;Loading;Prefetching;Shape;System performance;Topology","grid computing;information retrieval;storage management","declustered dataset;disk level partitioning;granularity;in-core partitioning;large unstructured grids;multidimensional array;multilevel partitioning;parallel data retrieval method;scientific dataset;simplicial cell;single disk;single machine memory;user access pattern","","0","","34","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Automatic mapping rules and OWL ontology extraction for the OBDA Ontop","F. Khazalah; Z. Malik; A. Rezgui","Wayne State University, USA","10th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20150122","2014","","","225","232","Extracting Ontop mapping rules and OWL ontology manually from a relational schema is a tedious task. We present an automatic approach for extracting Ontop mappings and OWL ontology from an existing database schema. The end users can access the underlying data source through SPARQL queries. A SPARQL query is written according to the extracted ontology and the end user does not need to know about the underlying data source and its schema. The proposed approach takes into consideration the different relationships between entities of the database schema. Instead of extracting a flat ontology that is an exact copy of the database schema, it extracts a rich ontology. The extracted ontology can also be used as an intermediate between a domain ontology and the underlying database schema. The experiment results indicate that the extracted mappings and ontology are accurate. i.e., end users can query all data (using SPARQL) from the underlying database source in the same way as if they have written SQL queries.","","Electronic:978-1-63190-043-3; POD:978-1-4799-5328-8","10.4108/icst.collaboratecom.2014.257493","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014568","","Data mining;Engines;OWL;Ontologies;Relational databases;Resource description framework","SQL;information retrieval;knowledge representation languages;ontologies (artificial intelligence);relational databases","OBDA Ontop;OWL ontology;Ontop mapping rules;SPARQL query;automatic mapping rules;ontology-based data access;underlying database source","","0","","20","","","22-25 Oct. 2014","","IEEE","IEEE Conference Publications"
"Implementation of a distributed web community crawler","S. Park; Y. Lee","Chungnam National University, Daejeon, Republic of Korea","The 16th Asia-Pacific Network Operations and Management Symposium","20141229","2014","","","1","6","A web community is an important space for online users to exchange information, ideas and thoughts. Due to collective intelligence of the web communities, marketing and advertisement activities have been highly focused on these sites. While articles in the web communities are open to the public, they cannot be easily collected and analyzed, because they are written in natural languages and their formats are diverse. Though many web crawlers are avaialble, they are not good at gathering web documents. First, the URLs of web articles are frequently changed and redundant, which will make the crawling job difficult. Second, the amount of articles is significantly large that the crawler should be designed in a scalable manner. Therefore, we propose a distributed web crawler optimized for collecting articles from popular communities. From the experiemnts we showed that our implementation achieves high throughput compared with the open-source crawler, Nutch.","","Electronic:978-4-88552-288-8","10.1109/APNOMS.2014.6996586","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996586","Distributed web crawler;community;web forum","Communities;Crawlers;Linux;Throughput;Uniform resource locators;Web pages","Internet;information retrieval;public domain software","Nutch open-source crawler;Web document gathering;advertisement activity;collective intelligence;distributed Web community crawler;marketing activity","","1","","20","","","17-19 Sept. 2014","","IEEE","IEEE Conference Publications"
"An Identification Algorithm for Rigid Athlete Numbers in Sports Video","S. Zhang; Z. Li","Inf. Eng. Sch., Commun. Univ. of China, Beijing, China","2014 Fourth International Conference on Instrumentation and Measurement, Computer, Communication and Control","20141229","2014","","","36","39","Recently, sports video analysis for retrieval purpose has become a hot research area. As important clues to sports video content analysis, detection and recognition of the number of athletes is a direct method to identify athletes. In this paper, we propose a method of detect rigid number of athletes. The number of athletes, as an example, we explored methods of detection and identification of the rigid deformed characters. First, we adopted a method that combines edge features and color features to locate the athlete number cloth. Then we used perspective transformation for correction of deformed area to identify the rigid deformation characters. Finally, for the candidate regional characters after adjustment, we employed the method of character template matching for identification. Experimental results on typical sports images show the potential of our algorithm.","","Electronic:978-1-4799-6575-5; POD:978-1-4799-6576-2","10.1109/IMCCC.2014.16","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6994986","edge detection;number identification;perspective transformation;rigid number","Algorithm design and analysis;Feature extraction;Games;Image color analysis;Image edge detection;Pattern recognition","image colour analysis;information retrieval;sport;video retrieval","athletes identification;color features;edge features;identification algorithm;retrieval purpose;rigid athlete numbers;rigid deformed characters;sports images;sports video content analysis","","0","","11","","","18-20 Sept. 2014","","IEEE","IEEE Conference Publications"
"The Bot will serve you now: Automating access to archival materials","J. A. Westgard","Digital Systems and Stewardship Division, University of Maryland Libraries, College Park, MD, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","73","74","This poster describes a field study that was undertaken in spring 2014 at the US National Archives as part of the MLS in Curation and Management of Digital Assets program at the University of Maryland's College of Information Studies. The primary focus of the field study was the augmentation of open-source software for the batch loading of image files and metadata into the repository associated with Wikipedia, specifically to improve the handling of multi-page documents. A secondary focus was the importation into Wikidata of existing, freely available authority records, records that can facilitate the disambiguation of organizational names and the automatic linking of Wikipedia articles.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004498","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004498","Autonomous agents;image processing;open source software;semantic web;social computing","Educational institutions;Electronic publishing;Encyclopedias;Internet;Libraries;Open source software","Web sites;document handling;information retrieval;meta data;multi-agent systems;public domain software","Wikidata;Wikipedia articles;archival materials;automatic linking;autonomous agents;batch loading;image files;metadata;multipage document handling;open-source software;organizational names;repository","","0","","7","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Comparative study on evaluative measures of search engines","J. N. Singh; S. K. Dwivedi","Department of Computer science, Babasaheb Bhimrao Ambedkar University, Lucknow, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","20150122","2014","","","1","6","The main concepts of different evaluation forum are to promote the evaluation of information retrieval systems by providing large test collections of structured documents. There are various forum that are working and providing training data in the evaluation of search engines. In the better evaluation of search engines, various important evaluative measures have been used, they are popular statistical measures and automatic approaches which includes Vector space model(VSM), Okapi similarity measure and Cover density ranking(CDR) that have previously been used to evaluate the search engines performance. In this paper, our work intends to discuss the main aspects of various forums and an evaluative measures used in performance evaluation of search engines and provides a comprehensive comparison for statistical methods and automatic methods.","","Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8","10.1109/ICRITO.2014.7014748","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014748","Pricision;Recall;Search Engines;Similarity Measure;Vector Space model","Density measurement;Extraterrestrial measurements;Search engines;Statistical analysis;Vectors;Web sites","document handling;information retrieval;search engines;statistical analysis","CDR;VSM;automatic approaches;automatic methods;cover density ranking;evaluation forum;information retrieval systems;okapi similarity measure;search engine evaluative measures;statistical measures;statistical methods;structured documents;training data;vector space model","","0","","14","","","8-10 Oct. 2014","","IEEE","IEEE Conference Publications"
"Making your programming questions be answered quickly: A content oriented study to technical Q&A forum","Y. Wang","Department of Informatics, University of California, Irvine, 92617, USA","10th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20150122","2014","","","368","377","Online programming forums enable programming knowledge sharing across organizational boundaries. Understanding how questions are asked and answered in forums will not only help developer to access the knowledge they need fast but bring important design implications. We report a study of Q&A process on MSDN's visual C# general forum. This study is content oriented instead of conventional social factor analysis to Communities of Q&A. We identified eight topic categories through two-round card sorting.We also explored various content feature's influence to Q&A process. A qualitative analysis was performed to identify different life-cycle patterns of questions. These findings highlight the role of content features, and the interaction effects between them. Based on these findings, we make a set of suggestions to information seekers on how to make their questions be answered faster, and derive implications for technical forums design and operation. To verify our findings, we also conducted a small replication to a Java technical forum and compared the results.","","Electronic:978-1-63190-043-3; POD:978-1-4799-5328-8","10.4108/icst.collaboratecom.2014.257384","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014583","","Communities;Java;Media;Message systems;Programming;Software;Sorting","C++ language;object-oriented programming;question answering (information retrieval)","Java technical forum;Visual C++ general forum;content oriented analysis;online programming forums;programming knowledge sharing;programming questions;qualitative analysis;social factor analysis;technical question-and-answer forum;two-round card sorting","","1","","34","","","22-25 Oct. 2014","","IEEE","IEEE Conference Publications"
"Creating quranic question taxonomy","P. Saeedi; S. Heidari; M. Farhoodi","Information Technology Faculty, Iran Telecommunication Research Center, Tehran, Iran","2014 22nd Iranian Conference on Electrical Engineering (ICEE)","20150105","2014","","","1070","1074","The knowledge of answer type is a valuable information in question answering(QA) systems which can be applied in searching for the answers. In this paper, we have prepared a question taxonomy which cover answer types in Quranic domain. It has 83 fine-grained and 30 coarse-grained semantic classes which are in a hierarchical structure. These semantic classes are extracted from about 6000 Quranic questions which are gathered from credible websites. This taxonomy is evaluated using the Quranic ontology which is currently developed in Quranjooy<sup>1</sup> project at ITRC<sup>2</sup>. Quranic taxonomy has been applied in question classification and answer extraction of Quranjooy. it can significantly improve the answer extraction performance.","2164-7054;21647054","DVD:978-1-4799-4410-1; Electronic:978-1-4799-4409-5; POD:978-1-4799-4408-8","10.1109/IranianCEE.2014.6999694","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999694","","Data mining;Knowledge discovery;Ontologies;Semantics;Speech;Taxonomy;Uniform resource locators","Web sites;ontologies (artificial intelligence);question answering (information retrieval)","ITRC;QA;Quranic ontology;Quranic question taxonomy;Quranic questions;Quranjooy project;answer extraction performance;coarse-grained semantic classes;credible websites;fine-grained semantic classes;question answering systems","","2","","12","","","20-22 May 2014","","IEEE","IEEE Conference Publications"
"A novel traffic information estimation method based on mobile network signaling","L. C. Kao; Z. Tsai","Telecommunication Laboratory of Chunghwa Telecom, Taoyuan, Taiwan, ROC","The 16th Asia-Pacific Network Operations and Management Symposium","20141229","2014","","","1","6","Two commonly used methods for traffic information rely on Vehicle Detector (VD) and Global Positioning System-Based Vehicle Probe (GVP); however, they have some confinements, such as high cost for construction and maintenance, and limited coverage. For the sake of overcoming dilemmas happened on VD and GVP, Cellular-Based Vehicle Probe (CVP) comes into being. However, the current applications for CVP mainly focus on arteries or freeways, where traffic information for longer distance is derived from two Inter-Visitor Location Register Location Area Update (Inter-VLR LAU) events with various Location Area Code (LAC) borders, and the one for shorter distance is from two consecutive handover events. The perplexity of available CVP techniques comes about is that there are no two Inter-VLR LAU events with various LAC borders and few handover events in scenic spots. In order to expand the applications for CVP to scenic spots, a cost-effective and flexible method utilizing mobile network signalling called Enhanced CVP (ECVP) is proposed. The key concept for ECVP is that we adopt Inter-VLR LAU events at the origin and all kinds of communication events at the destination to retrieve traffic information. The inaccuracy of ECVP consists in the uncertainty of event occurred time at the destination. Therefore, with a view to acquiring more accurate traffic information, three novel Reinforced CVP (RCVP) algorithms, inclusive of Fixed r percent samples CVP (F-RCVP), Dynamic r percent samples (D-RCVP), and Dynamic r percent samples with Discarding former samples (DD-RCVP), are presented. Numerical results show that F-RCVP is suitable for scenic spots that the LAC border only contain samples resulting from cars. By contrast, if the samples consist of both cars and motorcycles, it is recommended that D-RCVP and DD-RCVP are introduced.","","Electronic:978-4-88552-288-8","10.1109/APNOMS.2014.6996107","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996107","CVP;GVP;LAU;VD;handover;traffic information","Handover;Heuristic algorithms;Mobile communication;Mobile computing;Roads;Traffic control","Global Positioning System;cellular radio;information retrieval;mobility management (mobile radio);telecommunication signalling;traffic information systems","D- RCVP;DD-RCVP;ECVP;F-RCVP;GVP;LAC;RCVP algorithm;VD;cellular-based vehicle probe;dynamic percent sample;enhanced CVP;fixed percent sample CVP;global positioning system based vehicle probe;handover;interVLR LAU;intervisitor location register location area update;location area code;mobile network signaling;percent sample with discarding former sample;reinforced CVP;traffic information estimation method;traffic information retrieval;vehicle detector","","0","","15","","","17-19 Sept. 2014","","IEEE","IEEE Conference Publications"
"Different interventions for the treatment of irritable bowel syndrome-adjusted indirect comparison","Q. Li; W. t. Liu; J. j. Xu; J. h. Xie; X. b. Yang","The Second Affiliated Hospital of Guangzhou University of Chinese Medicine, Guangzhou, China","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20150115","2014","","","67","71","Background Many therapies are commonly used to help treat irritable bowel syndrome (IBS), including pharmacological and non-pharmacological approaches. However, there is a lack of direct evidence to help the clinicians make a decision. Objectives The aim of this review is to determine whether one of the approaches is more benefit than any of the others for the treatment of IBS through adjusted indirect comparison. Methods We searched the Cochrane Library (from inception to March 2014) using the keyword “irritable bowel syndrome” for systematic reviews. According to the pre-specified selection criteria, qualified trials were selected from the identified reviews. We calculated pooled random- or fixed-effects estimates according to the type of treatment for the proportions of treatment response. Adjusted indirect comparison was used for the pooled RRs of any two types of treatments with placebo as the common control. The primary outcome was improvement of patients global assessment. The second outcome included the improvement of IBS-symptom score and improvement of symptoms of abdominal pain. Results Nine systematic reviews were identified including 203 trials. Ultimately, 62 studies (N= 11,326) fulfilled inclusion criteria. For primary outcome global assessment, herbal medicine was associated with greater effects than western active medicine (RR, 1.34 [95%CI, 1.03-1.75]). For secondary outcome, the adjusted indirect comparison showed that there was no statistically significant difference between acupuncture and western active medicine in symptom severity. Herbal medicine may possibly be more effective than western active medicine in reducing abdominal pain, although the wide confidence intervals preclude any definite conclusions, with a RR of 1.18 [95%CI, 0.65-2.15]. Conclusions Herbal medicine may be more effective than western active medicine for the treatment of irritable bowel syndrome, especially in relieving the global symptoms, which may help t- e clinicians to make a decision in routine practice.","","Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8","10.1109/BIBM.2014.6999327","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999327","data mining;indirect comparison;intervention;irritable bowel syndrome","Antidepressants;Medical diagnostic imaging;Pain;Psychology;Systematics","data mining;decision making;information retrieval;medical computing;medical disorders;patient treatment;reviews;statistical analysis;text analysis","Cochrane Library search;IBS intervention;IBS-symptom score improvement;RR pooling;abdominal pain reduction;abdominal pain symptom improvement;acupuncture effect;adjusted indirect comparison;clinician decision making;confidence interval;fixed effect estimation;herbal medicine effect;inclusion criteria;irritable bowel syndrome treatment;keyword search;nonpharmacological therapy;patient global assessment;pharmacological treatment;placebo;primary patient outcome;qualified trial selection;random effect estimation;secondary patient outcome;selection criteria;statistical analysis;symptom severity;systematic review;treatment response;treatment type;western active medicine effect","","0","","14","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"An Enhanced TAC/AA Competition Model Designed for Sponsored Search Market","B. Li; X. Fang; R. Sun; X. Luo; W. h. Chang","Dept. of Comput., Hong Kong Polytech. Univ., Kowloon, China","2014 International Conference on IT Convergence and Security (ICITCS)","20150126","2014","","","1","4","This article describes an improved competition model named NightMare Agent for Trading Agent Competition Ad Auction which simulate the real Sponsored Search Market. The experiment result shows that the proposed model had satisfactory performance, especially fitted with the high capacity situation. Furthermore, this article also evaluates the results with other classic models, and discusses the future works.","","Electronic:978-1-4799-6541-0; POD:978-1-4799-6542-7","10.1109/ICITCS.2014.7021731","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021731","","Advertising;Algorithm design and analysis;Educational institutions;Games;Internet;Search engines;Servers","advertising data processing;electronic commerce;information retrieval;multi-agent systems;search engines","TAC-AA competition model;nightmare agent;sponsored search market;trading agent competition ad auction","","0","","5","","","28-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Web service sub-chain recommendation leveraging graph searching","ZeHui Cheng; Beibei Yao; Xiaolei Wang; ZhangBing Zhou","China University of Geosciences (Beijing), China","2014 IEEE Computers, Communications and IT Applications Conference","20150122","2014","","","271","275","Nowadays an increasing number of Open Geospatial Consortium Web services (OWSs) is built and being available on the Web for the accessibility and processing of geospatial information. Typically, the specific requirement specified by a certain user can be satisfied by a chain of OWSs. In this setting, retrieving and recommending sub-chains of possible service invocations is critical. Leveraging the semantic similarity between the name and text description of parameters and operations, the degree that represents the invocation possibility between operations in OWSs is calculated. Thereafter, the service network model is constructed that captures the possible invocations between operations. Given a user's requirement represented in terms of the initial and ending operations, possible sub-chains of operations are retrieved, ranked and recommended. Based on which the user can select the most appropriate sub-chain with respect to her specific requirement.","","Electronic:978-1-4799-4811-6; POD:978-1-4799-4810-9","10.1109/ComComAp.2014.7017209","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017209","OGC Web Services;Service Network Model;Sub-Chain Recommendation","Computational modeling;Educational institutions;Geology;Geospatial analysis;Semantics;Vectors;Web services","Web services;geographic information systems;information retrieval;recommender systems","OWS;geospatial information;open geospatial consortium Web services;service network model;sub-chain recommendation;text description","","0","","14","","","20-22 Oct. 2014","","IEEE","IEEE Conference Publications"
"OntoBAeval: Ontology Based Automatic Evaluation of Free-Text Response","P. Thanawala; J. Pareek; M. Shah","Dept. of Comput. Sci., Ahmedabad Univ., Ahmedabad, India","2014 IEEE Sixth International Conference on Technology for Education","20150115","2014","","","189","190","Semantic Web facilitated e-learning, where Computer-Assisted Assessment-CAA has vitally contributed for effective use of computers to evaluate Open-ended questions for free-text response. Domain Ontology as an aid to CAA, plays a very pivotal role by preserving domain-specific conceptual knowledge that not only endorse the semantic capability of Information Retrieval as Question Answering, but is also used to evaluate the response. OntoBAeval is Ontology based tool for automatic evaluation of free-text short responses submitted by users in Learning Management Systems-LMS based discussion forums or Community-based Question Answer (Q & A) forums. In this paper we propose architecture based on simple Natural Language Processing-NLP techniques, Word net and hand-coded logic to make sense of user Questions and submitted responses using the Semantic Web Ontology Language-OWL. The experimental results show that the proposed approach, which is tested on Computer Science subject Operating Systems, can be effectively used to evaluate the response by providing score that a learner uses to satisfy the learner's objective.","","Electronic:978-1-4799-6489-5; POD:978-1-4799-6490-1","10.1109/T4E.2014.6","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009567","Computer Assisted Assessment;Natural Language Processing;Ontology;Semantic Web","Computer architecture;Computers;Educational institutions;OWL;Ontologies","knowledge representation languages;learning management systems;natural language processing;ontologies (artificial intelligence);question answering (information retrieval);semantic Web","CAA;LMS based discussion forums;NLP techniques;OWL;OntoBAeval;Wordnet;automatic free-text short response evaluation;community-based question answer forums;computer science;computer-assisted assessment;domain-specific conceptual knowledge;e-learning;hand-coded logic;information retrieval;learning management systems;natural language processing;ontology based tool;open-ended questions;operating systems;question answering;semantic Web ontology language;semantic capability","","0","","10","","","18-21 Dec. 2014","","IEEE","IEEE Conference Publications"
"A hybrid approach for question classification in Persian automatic question answering systems","E. Sherkat; M. Farhoodi","ICT Research Institute (ITRC) Tehran, Iran","2014 4th International Conference on Computer and Knowledge Engineering (ICCKE)","20141222","2014","","","279","284","Question classification plays a major role in automatic question answering systems. The performance of a question answering system depends directly to the performance of its question classification section. A question classifier associates a label or category to each question which represents semantic class of its answer. There exist different approaches such as rule-based, machine learning and hybrid approaches for solving this problem. In this paper we have introduced a novel hybrid question classification approach for Persian closed-domain question answering systems. The proposed approach is used practically in an online automatic question answering system. The experimental results show the usefulness of combining rule-based and machine learning question classification approaches for highly inflectional languages such as Persian. We got the satisfactory results according to high number of question classes.","","Electronic:978-1-4799-5487-2; POD:978-1-4799-5488-9","10.1109/ICCKE.2014.6993377","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993377","Automatic Question Answering;Machine Learning;Persian Language;Question Classification;Rule-based","Detectors;Feature extraction;Kernel;Knowledge discovery;Magnetic heads;Support vector machines;Taxonomy","knowledge based systems;learning (artificial intelligence);natural language processing;pattern classification;question answering (information retrieval)","Persian automatic question answering systems;Persian closed-domain question answering systems;hybrid approach;inflectional languages;machine learning question classification approach;online automatic question answering system;question classifier;rule-based approach","","3","","15","","","29-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Improving automatic bug assignment using time-metadata in term-weighting","R. Shokripour; J. Anvik; Z. M. Kasirun; S. Zamani","University of Malaya, Malaysia","IET Software","20141222","2014","8","6","269","278","Assigning newly reported bugs to project developers is a time-consuming and tedious task for triagers using the traditional manual bug triage process. Previous efforts for creating automatic bug assignment systems use machine learning and information-retrieval techniques. These approaches commonly use tf-idf, a statistical computation technique for weighting terms based on term frequency. However, tf-idf does not consider the metadata, such as the time frame at which a term was used, when calculating the weight of the terms. This study proposes an alternate term-weighting technique to improve the accuracy of automatic bug assignment approaches that use a term-weighting technique. This technique includes the use of metadata in addition to the statistical computation to calculate the term weights. Moreover, it restricts the set of terms used to only nouns. It was found that when using only nouns and the proposed term-weighting technique, the accuracy of an automatic bug assignment approach improves from 12 to 49% over tf-idf for three open-source projects.","1751-8806;17518806","","10.1049/iet-sen.2013.0150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992834","","","information retrieval;learning (artificial intelligence);meta data;program debugging;software fault tolerance;statistical analysis","automatic bug assignment improvement;information-retrieval technique;machine learning technique;manual bug triage process;statistical computation technique;term frequency;term-weighting technique;time-metadata technique","","0","","","","","12 2014","","IET","IET Journals & Magazines"
"A weighted lexicon-based generative model for opinion retrieval","X. W. Liao; H. Chen; J. J. Wei; Z. Y. Yu; G. L. Chen","College of Mathematics and Computer Science, Fuzhou University, 350116, China","2014 International Conference on Machine Learning and Cybernetics","20150115","2014","2","","821","826","In recent years, opinion retrieval attracted a growing research interest as online users' opinions become more and more valuable for market survey, political polls, etc. The goal of opinion retrieval is to find relevant and opinionate documents according to a user's query. Compared with previous lexicon-based generative model for opinion retrieval considering that the sentiment words are equal for a query, which cannot reflect different sentiment words' relevant opinion strength, we propose a graph-based approach by using HITS model to capture the sentiment words' relevant opinion strength. Then the weights are incorporated into the weighted lexicon-based generative model for opinion retrieval. Experimental results on two datasets show the effectiveness of the proposed generative model. Compared with the baseline approach, improvements of 4% and 11% have been obtained on two real datasets.","2160-133X;2160133X","CD-ROM:978-1-4799-4217-6; Electronic:978-1-4799-4215-2; POD:978-1-4799-4214-5","10.1109/ICMLC.2014.7009715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009715","Generative model;HITS model;Lexicon weighting;Opinion retrieval;Sentiment words","Abstracts;Analytical models;Computational modeling;Twitter","data mining;graph theory;information retrieval","HITS model;graph-based approach;hyperlink-induced topic search;opinion retrieval;sentiment words;weighted lexicon-based generative model","","0","","25","","","13-16 July 2014","","IEEE","IEEE Conference Publications"
"Remote phone controller: An application to control smart phone","M. Chowdhury; A. Imteaj; K. H. Patwary; S. Chowdhury","Dept. of Computer Science & Engineering, Chittagong University of Engineering & Technology Chittagong, Bangladesh","2014 9th International Forum on Strategic Technology (IFOST)","20141218","2014","","","105","108","In the recent epoch smartphone has become increasingly popular & its acceptance is inflating at an astonishing rate. At the present time people use their smartphone to accumulate important or private files & also to effectuate almost all the official & business works. So losing phone can be caused unbearable sufferings; such as private or official files can be wielded by others or the possibility of mislaying all the indispensable data. To truncate such stresses, we propounded an application named `Remote Phone Controller'. The idea is based on controlling a smart phone by another device which can be either a normal phone or a smart phone. The application has the functionality to help the user retrieving important files, contact numbers, call & message log, tracking location, informing replaced SIM number, deleting folder, recording voice or snapping photo automatically & send it back etc. Through this application, the user will attain the proficiency of controlling his smart phone predominantly. Hence, “Remote Phone Controller” will succour the users to regain the lost data & lessen their stresses.","","Electronic:978-1-4799-6062-0; POD:978-1-4799-6063-7","10.1109/IFOST.2014.6991082","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991082","Android Developer Tool;SMS;Security;Smart phone","Databases;Electronic mail;Home appliances;Logic design;Postal services;Smart phones","information retrieval;smart phones","call log;epoch smart phone;file retrieval;message log;remote phone controller;tracking location","","0","","14","","","21-23 Oct. 2014","","IEEE","IEEE Conference Publications"
"The identification of Chinese named entity in the field of medicine based on Bootstrapping method","L. Long; J. Yan; L. Fang; P. Li; X. Liu","College of Electronic Information and Control Engineering, Beijing University of Technology, Beijing, China","2014 International Conference on Multisensor Fusion and Information Integration for Intelligent Systems (MFI)","20141229","2014","","","1","6","In the process of medicine information extraction, there are many Named Entity (NE) need to be recognized But currently the research on identification of NE in the field of medicine, such as physician, hospital, disease, medicine NE etc. is rarely. So in this paper we present an new approach for Named Entity Recognition (NER) in the field of medicine based on Bootstrapping method This method primarily consists of two key steps. The first step is Bootstrapping training procedure. Introduction of Bootstrapping, a self-expanding technology, make this method get rid of human supervision. The idea is to start from a initial feature set extracted from contextual information of a little annotated corpus that corresponding to the concept for the target NE type, and successively form a complete feature set of NE. The other is NER procedure. The feature set of NE adopted from Bootstrapping training procedure is expressed as class feature vector (CFV). Contextual information of candidate NE is expressed as example feature vector (EFV). The degree of similarity of CFV and EFV determines whether the candidate is a NE. We carried out several experiments using corpus from the internet for physician and disease NER The F-measure of physician and disease NER on the test data is 0.926 and 0.969 respectively. The result above show that the Bootstrapping method has the key advantage of NER.","","Electronic:978-1-4799-6732-2; POD:978-1-4799-6733-9","10.1109/MFI.2014.6997670","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997670","Bootstrapping method;Named Entity Recognition;similarity;the feature set","Context;Diseases;Feature extraction;Hidden Markov models;Training;Vectors","feature extraction;information retrieval;learning (artificial intelligence);medical computing;medical information systems","CFV;Chinese named entity identification;EFV;F-measure;NER;bootstrapping training procedure;class feature vector;example feature vector;feature set extraction;medicine information extraction;named entity recognition","","0","","23","","","28-29 Sept. 2014","","IEEE","IEEE Conference Publications"
"Automated query analysis techniques for semantics based question answering system","S. Prabhumoye; P. Rai; L. S. Sandhu; Priya L; Sowmya Kamath S","Department of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore, India","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","6","Search engines have always played an important role in helping web users to rapidly find information on the Web. However, their function is limited to returning a list of query relevant documents with reasonably good precision, but huge recall. The task of actually processing the returned documents to get the required information is the responsibility of the user. In recent years, Question-Answer systems are gaining popularity and have garnered much research interest in view of the proposed Semantic Web and future availability of fully structured data. The advantage of QA systems is that users have the luxury of asking queries in natural language and also get a precise answer instead of just displaying a list of links to documents that may or may not be relevant. This paper presents a question answer search engine prototype that uses natural language processing, natural language generation, question classification and query logs to find a precise answer to the submitted query. This is ongoing work and we focus on the methodology of query analysis in this paper. We describe our strategy of automatic query analysis by classifying it into nine categories and understanding the meaning of the query. We also discuss in detail how each of the question categories are automatically processed and how the proposed system determines the key word or key phrase to be searched.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996128","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996128","natural language understanding;query processing;question answer systems;text analysis","Databases;Information technology;Knowledge discovery;Market research;Natural languages;Search engines;Semantics","Internet;natural language processing;query processing;question answering (information retrieval);search engines;semantic Web","QA systems;Web users;automated query analysis techniques;automatic query analysis;natural language generation;natural language processing;query logs;query relevant documents;question answer search engine prototype;question classification;question-answer systems;semantic Web;semantics based question answering system","","0","","22","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Design and implementation of competent web crawler and indexer using web services","Santhosh Kumar D K; M. Kamath","Department of Computer Science & Engineering, NMAM Institute of Technology, Nitte, Karnataka, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","20150126","2014","","","1672","1677","Today the internet has become a part of human beings life. To get the information what the user is requesting is the job of search engine which indeed takes the help of web crawler. Designing and developing a competent web crawler is a challenging task. This paper proposes Web crawler and Indexer. The WebCrawler consist of crawler services and indexer services and realized as web services. The crawler and indexer services communicate using XML, SOAP and WSDL. The web pages are fetched and parsed for retrieving all the hyperlinks by the crawler service, and then the same process is continued recursively using the Breadth-First strategy. The result of crawler service is downloaded and given as an input to the indexer services by passing the message using web services. Then the indexer service parses the HTML pages, removes stop words, stemming of keywords are carried out as pre-processing steps. Finally the result is stored in the form of inverted index.","","CD-ROM:978-1-4799-3913-8; Electronic:978-1-4799-3914-5; POD:978-1-4799-3915-2","10.1109/ICACCCT.2014.7019393","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019393","Breadth first Strategy;Tokenization;hyperlink;stemming;stop-words","Crawlers;HTML;Search engines;Simple object access protocol;Uniform resource locators;Web pages","Web services;XML;indexing;information retrieval;search engines","HTML pages;Internet;SOAP;WSDL;Web crawler;Web indexer;Web services;XML;breadth-first strategy;hyperlink retrieval;inverted index;search engine","","0","","16","","","8-10 May 2014","","IEEE","IEEE Conference Publications"
"Tweet Segmentation and Its Application to Named Entity Recognition","C. Li; A. Sun; J. Weng; Q. He","State Key Lab of Software Engineering, School of Computer, Wuhan University, P.R., China","IEEE Transactions on Knowledge and Data Engineering","20141224","2015","27","2","558","570","Twitter has attracted millions of users to share and disseminate most up-to-date information, resulting in large volumes of data produced everyday. However, many applications in Information Retrieval (IR) and Natural Language Processing (NLP) suffer severely from the noisy and short nature of tweets. In this paper, we propose a novel framework for tweet segmentation in a batch mode, called HybridSeg. By splitting tweets into meaningful segments, the semantic or context information is well preserved and easily extracted by the downstream applications. HybridSeg finds the optimal segmentation of a tweet by maximizing the sum of the stickiness scores of its candidate segments. The stickiness score considers the probability of a segment being a phrase in English (i.e., global context) and the probability of a segment being a phrase within the batch of tweets (i.e., local context). For the latter, we propose and evaluate two models to derive local context by considering the linguistic features and term-dependency in a batch of tweets, respectively. HybridSeg is also designed to iteratively learn from confident segments as pseudo feedback. Experiments on two tweet data sets show that tweet segmentation quality is significantly improved by learning both global and local contexts compared with using global context alone. Through analysis and comparison, we show that local linguistic features are more reliable for learning local context compared with term-dependency. As an application, we show that high accuracy is achieved in named entity recognition by applying segment-based part-of-speech (POS) tagging.","1041-4347;10414347","","10.1109/TKDE.2014.2327042","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823714","Twitter stream;Wikipedia;linguistic processing;named entity recognition;tweet segmentation","Context;Electronic publishing;Encyclopedias;Internet;Pragmatics;Semantics","information retrieval;natural language processing;social networking (online)","HybridSeg framework;IR;NLP;POS tagging;context information;context learning;information retrieval;linguistic features;named entity recognition;natural language processing;segment-based part-of-speech tagging;semantic information;stickiness score;tweet segmentation","","11","","37","","20140530","FEBRUARY 1 2015","","IEEE","IEEE Journals & Magazines"
"A combined random noise perturbation approach for multi level privacy preservation in data mining","S. Chidambaram; K. G. Srinivasagan","NEC, India","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","6","Now a Days huge volume of personal and sensitive data is collected and retrieved by various enterprises like social networking system, health networks, financial organizations and retailers. There are three main entities such as data owner; the database service provider and the client are mainly involved in this type of outsourced based data model. So that is more essential for the privacy preservation of the owner. Privacy preservation is a main challenging area in data mining. In that, Data based privacy perturbation technique is the standard model which performs the data transformation process before publishing the data. This paper proposes Additive Multiplicative Perturbation Privacy Preserving Data Mining (AM-PPDM) which is suitable for multiple trust level. In that, the random noise perturbation is applied to individual values before the data are published. This hybrid approach improves the privacy guarantee value during the reconstruction process. In AM-PPDM, the generated random Gaussian noise multiplied with the original data to produce different perturbed copies at various trust levels. By implementing this approach, the diversity attack is completely avoided during the reconstruction process.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996194","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996194","Diversity attack;Gaussian noise;Privacy preservation;Random perturbation;multilevel trust","Additives;Covariance matrices;Data privacy;Gaussian noise;Joints;Privacy","Gaussian noise;data mining;data privacy;health care;information retrieval;retailing;social networking (online);stock markets","AM-PPDM;additive multiplicative perturbation privacy preserving data mining;combined random noise perturbation;data retrieval;data transformation;financial organizations;health networks;multilevel privacy preservation;random Gaussian noise;retailers;social networking system","","0","","18","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Linked open data representation of historical heritage of Bangladesh","S. Chakraborty; M. H. H. Rahman; M. H. Seddiqui","Dept. of Computer Science & Engineering, University of Chittagong, Chittagong - 4331, Bangladesh","Computer and Information Technology (ICCIT), 2013 16th International Conference on","20141229","2014","","","242","248","The historical heritage is a vast domain that generates distributed and heterogeneous data repositories in the existing World Wide Web (WWW). The integration of these heterogeneous data repositories and retrieving data from WWW using traditional search engine is a challenging task. In this regard, our research is to demonstrate the feasibility of semantic web technology for creating machine understandable description of entities associated with the creation and maintenance of historical heritage of Bangladesh. Moreover, our proposed approach uses the essence of semantic knowledge base composed of real historical heritage data that are interlinked with data from the Geo-Bangladesh, a machine understandable Linked Open Data (LOD) repository of geographic related data to the administrative structure of Bangladesh and retrieve specific information using semantic search tools like SPARQL. We perform a number of experiments with SPARQL to retrieve and inference specific information that shows usefulness of our proposed Linked Open Data on historical heritage of Bangladesh.","","Electronic:978-1-4799-3497-3; POD:978-1-4799-3498-0","10.1109/ICCITechn.2014.6997363","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997363","","Educational institutions;Knowledge based systems;Ontologies;Resource description framework;Search engines;Semantics;Vocabulary","data handling;history;information retrieval;semantic Web","Bangladesh;Geo-Bangladesh;LOD repository;SPARQL tool;WWW;World Wide Web;heterogeneous data repository;historical heritage;information retrieval;linked open data representation;search engine;semantic Web technology;semantic knowledge base","","0","","25","","","8-10 March 2014","","IEEE","IEEE Conference Publications"
"An efficient time-stamping solution for long-term digital archiving","M. Vigil; C. Weinert; D. Demirel; J. Buchmann","Technische Universita&#x00A8;t Darmstadt, Germany","2014 IEEE 33rd International Performance Computing and Communications Conference (IPCCC)","20150122","2014","","","1","8","Long-term archiving of digital data is necessary to meet many legal requirements. For example, hospitals in many countries must keep health records of patients for decades. Archiving usually relies on digital signatures and time-stamps to prove the security properties of archived data, such as integrity and proof of existence. Moreover, archived data often needs to be updated, e.g. a new prescription is added to a patient's record, but without compromising integrity and proof of existence. To date, a solution that guarantees integrity and proof of existence indefinitely and allows for updates on archived data is Content Integrity Service (CIS). In this paper, we introduce an improved version of CIS named Content Integrity Service with Skip Lists (CISS) that changes the time-stamping process of CIS by using two different types of time-stamps together with skip lists. We demonstrate that CISS outperforms CIS by analyzing the algorithms and running experiments in realistic scenarios.","1097-2641;10972641","Electronic:978-1-4799-7575-4; POD:978-1-4799-7576-1; USB:978-1-4799-7846-5","10.1109/PCCC.2014.7017099","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017099","archiving;efficiency;existence;integrity;long-term;performance;time-stamp","Aging;Computers;History;Public key;TV","data integrity;information retrieval systems;records management","CIS;CISS;archived data integrity;archived data updates;content integrity service with skip lists;long-term digital archiving;time-stamping solution","","0","","14","","","5-7 Dec. 2014","","IEEE","IEEE Conference Publications"
"Research on database design of smart grid based on multi-agent","X. Kong; J. Sun","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","2014 International Conference on Power System Technology","20141222","2014","","","1554","1558","With the development and application of smart grid which is characterized by informatization and digitization, one of critical studies is to process, store, analyze and access to the data in the database that consists of massive and real-time information of power grid, which has such characteristics as large-scale, real-time and multi-type. So a kind of database design approach for bulk power grid based on multi-agent approach is presented in this paper. Firstly, the characteristics and requirements of massive and real-time database in smart grid are analyzed, and corresponding realization strategies are put forward. Then the database design of smart grid based on multi-agent approach is given. Finally, a test case is used as to verify the feasibility and effectiveness of proposed method.","","Electronic:978-1-4799-5032-4; POD:978-1-4799-5033-1","10.1109/POWERCON.2014.6993859","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993859","database design;mass information;multi-agent;real-time data;smart grid","Data processing;Distributed databases;Real-time systems;Smart grids","information retrieval;multi-agent systems;power engineering computing;smart power grids","database design;digitization;mass information;multiagent approach;power grid;smart grid","","0","","11","","","20-22 Oct. 2014","","IEEE","IEEE Conference Publications"
"A Brief Survey of PageRank Algorithms","F. Chung","Department of Computer Science and Engineering, University of California, San Diego, CA","IEEE Transactions on Network Science and Engineering","20141230","2014","1","1","38","42","We examine several PageRank approximation algorithms. Quantitative analyses are provided to illustrate the extraordinary effectiveness of the PageRank computation.","2327-4697;23274697","","10.1109/TNSE.2014.2380315","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998874","PageRank;approximation algorithms;random walks","Algorithm design and analysis;Approximation algorithms;Approximation methods;Graph theory;Ranking (statistics);Search methods;Web search","approximation theory;information retrieval","PageRank approximation algorithm;PageRank computation;quantitative analysis","","1","","34","","20141225","Jan.-June 1 2014","","IEEE","IEEE Journals & Magazines"
"A forensics tool of Foxmail client","L. Xie; G. Chen","College of Mathematics and Computer Science, Fuzhou University Fuzhou, Fujian, China","The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)","20150115","2014","","","400","405","Computer forensics is an important part of the computer crime investigation. So a Forensics system aimed to Foxmail data analysis and investigation is proposed in this paper. The system can index the email data based on Lucene technology and the MMSeg participle algorithm, and provides the full text retrieval of the email content function. Subsequently, a visual communication graph would be generated according to the retrieval results. Based on the communication graph, an algorithm on graphic structure analysis is proposed, and by means of centrality and minimum traffic filtering method, multilevel analysis is carried out on the graph, which can detect the possible suspicious personnel and their relationships and provide strong support for the computer crime investigation.","","CD-ROM:978-1-4799-5457-5; Electronic:978-1-4799-5458-2; POD:978-1-4799-5459-9","10.1109/ICSAI.2014.7009322","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009322","Lucene;Network analysis;centrality;email forensics","Algorithm design and analysis;Electronic mail;Filtering;Forensics;Indexes;Personnel;Postal services","computer crime;data analysis;digital forensics;electronic mail;graph theory;information retrieval;text analysis","Foxmail client;Lucene technology;MMSeg participle algorithm;computer crime investigation;computer forensics;email content function;email data;forensics system;forensics tool;foxmail data analysis;graphic structure analysis;multilevel analysis;suspicious personnel;text retrieval;traffic filtering method;visual communication graph","","1","","22","","","15-17 Nov. 2014","","IEEE","IEEE Conference Publications"
"A maintenance algorithm of FDS based mathematical expression index","S. Q. Yang; X. D. Tian","Faculty of Mathematics and Computer Science, Hebei University, Baoding 071002, China","2014 International Conference on Machine Learning and Cybernetics","20150115","2014","2","","888","892","Mathematical expressions have complex two dimensions structure which could not be processed properly by traditional full-text retrieval method. It is necessary for us to research and develop special retrieval theory and technology including the indexing and matching methods of mathematical expressions which fully considers the characteristics of them. In this paper, a maintenance algorithm is designed for the mathematical expression index based on FDS which we given previously. Firstly, the existing achievements about mathematical expression recognition and retrieval are introduced and discussed. Then, the FDS is defined based on formula structural tree and the corresponding mathematical expression index is constructed. Finally, the index maintenance algorithms including the searching, inserting, and deleting operations of index items corresponding to the FDS based index are designed. The experimental result shows that the proposed method is helpful for the mathematical resources management and retrieval.","2160-133X;2160133X","CD-ROM:978-1-4799-4217-6; Electronic:978-1-4799-4215-2; POD:978-1-4799-4214-5","10.1109/ICMLC.2014.7009727","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009727","FDS;Index item;Index maintenance algorithm;Mathematical expression index","Abstracts;Indexes;Information management;Maintenance engineering","indexing;information retrieval;mathematics computing","FDS based mathematical expression index;formula structural tree;index maintenance algorithms;mathematical expression recognition;mathematical expression retrieval;mathematical resource management","","0","","17","","","13-16 July 2014","","IEEE","IEEE Conference Publications"
"Melody Extraction for Vocal Polyphonic Music Based on Bayesian Framework","L. Song; M. Li; Y. Yan","Key Lab. of Speech Acoust. & Content Understanding, Inst. of Acoust., Beijing, China","2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20141229","2014","","","570","573","In order to automatically extract the main melody contours from polyphonic music especially vocal melody songs, we present an effective approach based on a Bayesian framework. According to various information from the music signals, we use a pitch evolution model describing how pitch contour changes and an acoustic model representing the acoustic characteristics when the pitch is a hypothesized one, and obtain the optimal melody contour utilizing a Viterbi algorithm. The experimental results on the RWC popular music database indicate that the overall accuracy achieves 73.4%.","","Electronic:978-1-4799-5390-5; POD:978-1-4799-5391-2","10.1109/IIH-MSP.2014.147","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998393","Bayesian framework;Viterbi algorithm;melody extraction;polyphonic music","Harmonic analysis;Instruments;Multiple signal classification;Music;Speech;Speech processing","Bayes methods;acoustic signal processing;audio databases;feature extraction;information retrieval;music","Bayesian framework;RWC popular music database;Viterbi algorithm;acoustic model;automatic main melody contour extraction;music signals;optimal melody contour;pitch contour;pitch evolution model;vocal melody songs;vocal polyphonic music","","0","","16","","","27-29 Aug. 2014","","IEEE","IEEE Conference Publications"
"ImNER Indonesian medical named entity recognition","W. Suwarningsih; I. Supriana; A. Purwarianti","School of Electrical Engineering and Informatics, Institut Teknologi Bandung Bandung, Indonesia","2014 2nd International Conference on Technology, Informatics, Management, Engineering & Environment","20150119","2014","","","184","188","We propose a medical named entity recognition for medical question answering system with Indonesian language. The aim is to provide a good medical named entity grammar by only using the available language resource. Our strategy here is to build the features most often used for the recognition and classification of medical named entities. We organize them along two different axes: word-level and list features, document and corpus features. For the reason we built our own features to Indonesian medical named entities and used it as the feature of the available with SVM Software. By using 3000 sentences, the highest accuracy score achieved is about 90%.","","CD-ROM:978-1-4799-4806-2; Electronic:978-1-4799-4805-5; POD:978-1-4799-4804-8","10.1109/TIME-E.2014.7011615","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011615","Document and corpus features;Medical named entity;SVM engine;Word-level features","Accuracy;Conferences;Knowledge discovery;Medical diagnostic imaging;Support vector machines;Taxonomy;Training","grammars;medical information systems;natural language processing;question answering (information retrieval);support vector machines","ImNER;Indonesian language;Indonesian medical named entity recognition;SVM software;corpus features;document features;list feature;medical named entity grammar;medical question answering system;word-level","","0","","15","","","19-21 Aug. 2014","","IEEE","IEEE Conference Publications"
"Real time anomaly detection in wide area monitoring of smart grids","J. Wu; J. Xiong; P. Shil; Y. Shi","IBM Thomas J. Watson Research Center Yorktown Heights, NY, 10598","2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","20150108","2014","","","197","204","The real time anomaly detection in wide area monitoring of smart grids is critical to enhance the reliability of power systems. However, capturing the features of anomalous interruption and then detecting them at real time is difficult for large-scale smart grids, because the measurement data volume and complexity increases drastically with the exponential growth of data from the immense intelligent monitoring devices to be rolled out and the need for fast information retrieval from those mass data. Most of existing anomaly detection methods fail to handle it well. This paper proposes a spatial-temporal correlation based anomalous behavior model to capture the characteristics of anomaly such as transmission line outages in smart grid. Inspired by Ledoit-Wolf Shrinkage (LWS) method, we develop the real time anomaly detection (ReTAD) algorithm to overcome the issue of gigantic measurement data volume. The proposed algorithm is not only suitable for large number of power systems with high dimensional measurement data, but at the same time is also low computational complexity to apply for real time detection. Using 14-, 30, and 2383-bus systems, our experimental study demonstrates that our proposed ReTAD algorithm successfully detects the anomalous events at real time.","1092-3152;10923152","Electronic:978-1-4799-6278-5; POD:978-1-4799-6279-2; USB:978-1-4799-6277-8","10.1109/ICCAD.2014.7001352","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001352","","Analytical models;Correlation;Covariance matrices;Real-time systems;Time measurement;Transmission line matrix methods;Transmission line measurements","computational complexity;information retrieval;power system faults;power system measurement;power system reliability;smart power grids","LWS method;Ledoit-Wolf Shrinkage method;ReTAD algorithm;anomalous interruption detection;computational complexity;data measurement;gigantic data measurement;immense intelligent monitoring device;information retrieval;large-scale smart grids wide area monitoring;power system reliability;real time anomaly detection algorithm;spatial-temporal correlation based anomalous behavior model","","0","","28","","","2-6 Nov. 2014","","IEEE","IEEE Conference Publications"
"Robustness threshold methodology for multicriteria based ranking using imprecise data","B. Rizzon; S. Galichet; V. Clivillé","LISTIC, Universit&#x00E9; de Savoie, Annecy, France","2014 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM)","20150115","2014","","","1","8","It is well established that making decisions from defined data according to various criteria requires the use of MultiCriteria Decision Aiding or Analysis (MCDA) methods. However the necessary input data for these approaches are often ill-known especially when the data are a priori estimated. The common MCDA approaches consider these data as singular/scalar values. This paper deals with the consideration of more realistic, values by studying the impact of imprecision on a classical “precise” ranking established with ACUTA, a method based on additive utilities. We propose a generic approach to establish the concordance of pairwise relations of preference despite interval-based imprecision by complementing ACUTA with a computation of Kendall's index of concordance and of a threshold for maintaining this concordance. The methodology is applied to an industrial case subjected to Sustainable Development problems.","","Electronic:978-1-4799-4467-5; POD:978-1-4799-4466-8","10.1109/MCDM.2014.7007181","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007181","","Additives;Computational modeling;Correlation;Data models;Electronic mail;Robustness;Sustainable development","data analysis;decision making;information retrieval","Kendall concordance index;MCDA method;imprecise data;multicriteria based ranking;multicriteria decision aiding method;multicriteria decision analysis method;pairwise preference relations;robustness threshold methodology;sustainable development","","0","","29","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Extracting Dependencies from Software Changes: An Industry Experience Report","T. Wetzlmaier; C. Klammer; R. Ramler","Software Analytics & Evolution, Software Competence Center Hagenberg GmbH, Hagenberg, Austria","2014 Joint Conference of the International Workshop on Software Measurement and the International Conference on Software Process and Product Measurement","20150105","2014","","","163","168","Retrieving and analyzing information from software repositories and detecting dependencies are important tasks supporting software evolution. Dependency information is used for change impact analysis, defect prediction as well as cohesion and coupling measurement. In this paper we report our experience from extracting dependency information from the change history of a commercial software system. We analyzed the software system's evolution of about six years, from the start of development to the transition to product releases and maintenance. Analyzing the co-evolution of software artifacts allows detecting logical dependencies between system parts implemented with heterogeneous technologies as well as between different types of development artifacts such as source code, data models or documentation. However, the quality of the extracted dependencies relies on established development practices and conformance to a defined change process. In this paper we indicate resulting limitations and recommend further processing and filtering steps to prepare the dependency data for subsequent analysis and measurement activities.","","Electronic:978-1-4799-4174-2; POD:978-1-4799-7874-8","10.1109/IWSM.Mensura.2014.12","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000096","change history;dependency analysis;logical coupling;mining software repositories","Couplings;Data mining;History;Java;Servers;Software systems","information analysis;information retrieval;software maintenance","change impact analysis;cohesion measurement;commercial software system;coupling measurement;defect prediction;dependency detection;dependency extraction;filtering step;information analysis;information retrieval;processing step;software artifact coevolution;software change;software repositories","","0","","19","","","6-8 Oct. 2014","","IEEE","IEEE Conference Publications"
"Necessity of customer inputs for online group shopping using Support Vector Machines","P. Desai; G. R. Kulkarni","Computer Science and Engineering-JJTU, Jhunjhunu, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","20150122","2014","","","1","6","Online group shopping is feasible if the online store comprises useful products. The product needs to attract customer interest. The customer will have varied choices to choose from the available products. For feasibility of Online group shopping by customers the products have to be properly classified. The site offers the customer to form a group based on his/her interest. The site gives the best deal on a product, the threshold or the limit to form a group is proposed by the site. The basic problem with Online stores is that they offer discounts to customers without knowing the customer interest on a particular product. The gap of Online group shopping is rectified by taking feedback from the customer when the customer visits the site. Discounts are offered to the customer based on customer interest. In this paper the focus is on group of people interested in a particular product from among the ones displayed on the site and predict the potential buyer based on feedback taken. The paper uses Support Vector Machine for generating results. Paper further compares SVM with C4.5 algorithm. The comparative analysis of true positive and true negative is 98% and 98% respectively for SVM after testing, C4.5 giving 76% for true positive and 98% for true negative. It's known that SVM works well for linear classification. SVM is not about “yes/no” classification, but it is trained to give numerical values. The paper deals with proper classification of data or products. The paper further proves that good classification leads to better retrieval of information. The question is, are the customers able to specify the product being purchased? If yes a feedback is taken. Second question being once the customers specify the product is the product information specified with fast response. The number of feedbacks are used for generating a threshold. Based on threshold values discounts are offered to customers leading to better business. The paper shows SVMs performs - etter than C4.5 even in the presence of some noise.","","Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8","10.1109/ICRITO.2014.7014768","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014768","C4.5;SVM;supervised learnin;testing;threshold;training","Business;Noise;Support vector machines;Testing;Text categorization;Training;Training data","Internet;customer services;information retrieval;pattern classification;retail data processing;support vector machines","C4.5 algorithm;SVM;comparative analysis;customer input;customer interest;data classification;information retrieval;online group shopping;online store;product information;support vector machine;true negative;true positive","","0","","18","","","8-10 Oct. 2014","","IEEE","IEEE Conference Publications"
"Opportunities and challenges of the Internet of Things for healthcare: Systems engineering perspective","F. Fernandez; G. C. Pallis","Department TFB-ATSI FI-ETSIINF, Polytechical University of Madrid (UPM), Campus de Montegancedo, 28660 Madrid, Spain","2014 4th International Conference on Wireless Mobile Communication and Healthcare - Transforming Healthcare Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH)","20150122","2014","","","263","266","In the incoming world of Internet of Things (IoT) for healthcare, different distributed devices will gather, analyze and communicate real time medical information to open, private or hybrid clouds, making it possible to collect, store and analyze big data streams in several new forms, and activate context dependent alarms. This innovative data acquisition paradigm allows continuous and ubiquitous medical data access from any connected device over the Internet, and a novel health application ecosystem emerges. In these complex ecosystems could be insufficient to discuss only classical requirements regarding hardware issues and software support of individual elements. In the involved multidisciplinary development area, with intricate vertical and horizontal markets, it is essential a close collaboration between the corresponding stakeholders: endusers, application domain experts, hardware designers, software developers, market specialists, road mapping strategists and even the collaboration of visionaries to implement successful healthcare ecosystems. In this paper we describe some crucial systems engineering viewpoints to analyse the corresponding complex decision space. We complement the general examination of the IoT space by commenting some particular examples and specific details, which correspond to remarkable options of the involved dimensions.","","Electronic:978-1-63190-014-3; POD:978-1-4799-5024-9; USB:978-1-63190-013-6","10.1109/MOBIHEALTH.2014.7015961","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015961","Internet of Things (IoT);big data analytics;cloud computing;eHealth;healthcare;mHealth;medical body area networks;systems engineering","Biomedical monitoring;Ecosystems;Medical services;Monitoring;Sensors;Systems engineering and theory;Wireless communication","Big Data;Internet of Things;biomedical equipment;cloud computing;data acquisition;data analysis;data privacy;electronic data interchange;health care;information retrieval;information storage;medical information systems;real-time systems;systems engineering;telemedicine","application domain expert;big data stream analysis;big data stream collection;big data stream storage;complex decision space;context dependent alarm activation;continuous medical data access;enduser;general IoT space examination;hardware designer;hardware requirement;health application ecosystem;healthcare Internet of Things;healthcare IoT challenge;healthcare IoT opportunities;healthcare ecosystem;horizontal market;hybrid cloud;innovative data acquisition paradigm;market specialist;multidisciplinary development area;open cloud;private cloud;real time medical information acquisition;real time medical information analysis;real time medical information communication;road mapping strategist;software developer;software support requirement;stakeholder collaboration;systems engineering;ubiquitous medical data access;vertical market;visionary collaboration","","0","","8","","","3-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Bangla news classification using naive Bayes classifier","A. N. Chy; M. H. Seddiqui; S. Das","Dept. of Computer Science & Engineering, University of Chittagong, Chittagong - 4331, Bangladesh","Computer and Information Technology (ICCIT), 2013 16th International Conference on","20141229","2014","","","366","371","Web is gigantic and being constantly update. Bangla news in web are rapidly grown in the era of information age where each news site has its own different layout and categorization for grouping news. These heterogeneity of layout and categorization can not always satisfy individual user's need. Removing these heterogeneity and classifying the news articles according to user preference is a formidable task. In this paper, we propose an approach that provides a user to find out news articles which are related to a specific classification. We use our own developed web crawler to extract useful text from HTML pages of news article contents to construct a Full-Text-RSS. Each news article contents is tokenized with a modified light-weight Bangla Stemmer. In order to achieve better classification result, we remove the less significant words i.e. stop - word from the document. We apply the naive Bayes classifier for classification of Bangla news article contents based on news code of IPTC. Our experimental result shows the effectiveness of our classification system.","","Electronic:978-1-4799-3497-3; POD:978-1-4799-3498-0","10.1109/ICCITechn.2014.6997369","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997369","","Computers;Dictionaries;Information technology;Layout;Taxonomy;Training;Vectors","Bayes methods;Internet;Web sites;information retrieval;pattern classification;text analysis","Bangla Stemmer;Bangla news article content classification;Full-Text-RSS;HTML pages;IPTC news code;Web crawler;naive Bayes classifier;news grouping;news site;useful text extraction;user preference","","1","","29","","","8-10 March 2014","","IEEE","IEEE Conference Publications"
"App-based system diagnosis using mobile information systems","J. Ziegler; L. Urbas; H. Lenz; H. Richter","Technische Universit&#x00E4;t Dresden, Chair of Process Control Systems Engineering","Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)","20150112","2014","","","1","7","This paper presents a study of the potential of integrated services for app-based system diagnosis using mobile information systems. First, a task and context analysis is provided for the application of integrated system diagnosis in the context of plant asset management (PAM). Second, exemplary scenarios are presented which demonstrate IT services for system analysis based on self-organizing maps, system diagnosis with a graph-based approach and support for therapeutic measures on defective assets. Third, a mobile information system is presented that allows users to list, describe, filter, and select assets, to browse through available diagnoses, and to follow step-by-step instructions for recommended therapeutic measures. The mobile information system provides a graphical representation, description, and selection of assets by using a P&I diagram as well as a chart visualization of process values. The system further provides access to both static and transient data sources such as engineering data of the plant, process data of the technical process and the results of complex analysis services. This mobile information system demonstrates the suitability of the proposed app-based approach. Finally, the results of a focus group workshop are summarized, showing the potential of mobile information systems in an industrial setting, barriers that still remain.","1946-0740;19460740","Electronic:978-1-4799-4845-1; POD:978-1-4799-4844-4","10.1109/ETFA.2014.7005150","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005150","","Context;Industries;Information systems;Maintenance engineering;Medical treatment;Mobile communication;Monitoring","asset management;industrial plants;information retrieval;information systems;manufacturing data processing;mobile computing;self-organising feature maps;task analysis","IT services;P and I diagram;app-based system diagnosis;asset selection;chart visualization;complex analysis services;context analysis;defective assets;graph-based approach;graphical description;graphical representation;industrial setting;integrated services;mobile information systems;plant asset management;recommended therapeutic measures;self-organizing maps;static data sources;system analysis;system diagnosis;task analysis;transient data sources","","2","","13","","","16-19 Sept. 2014","","IEEE","IEEE Conference Publications"
"Effects of perceived task complexity on the information types needed, information sources and channels used by lawyers","N. Adewale; Y. Mansor","Department of Library and Information Science KICT, International Islamic University Malaysia Kuala Lumpur, Malaysia","The 5th International Conference on Information and Communication Technology for The Muslim World (ICT4M)","20150126","2014","","","1","6","Tasks and its features, especially the complexity dimension has been identified as a major drive for information seeking. This study explores the effects of the perceived task complexity and uncertainty on the information types needed and the information sources used by commercial lawyers in Nigeria. A survey method was adopted and data was gathered with the use of questionnaires. The analysis and hypotheses testing was done using the smartPLS 2.0 M3 software. Findings indicate direct positive relationships among the constructs, except for the relationship between perceived task complexity and the information sources consulted by lawyers.","","Electronic:978-1-4799-6242-6; POD:978-1-4799-6243-3","10.1109/ICT4M.2014.7020620","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020620","Perceived task complexity;information needs;information seeking;information sources","Decision support systems","computational complexity;information retrieval;law administration","Nigeria;commercial lawyers;complexity dimension;information seeking;information sources;information types;perceived task complexity;smartPLS 2.0 M3 software","","0","","40","","","17-18 Nov. 2014","","IEEE","IEEE Conference Publications"
"Map++: A crowd-sensing system for automatic map semantics identification","H. Aly; A. Basalamah; M. Youssef","Dept. of Computer and Sys. Eng., Alexandria University, Egypt","2014 Eleventh Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)","20141218","2014","","","546","554","Digital maps have become a part of our daily life with a number of commercial and free map services. These services have still a huge potential for enhancement with rich semantic information to support a large class of mapping applications. In this paper, we present Map++, a system that leverages standard cell-phone sensors in a crowdsensing approach to automatically enrich digital maps with different road semantics like tunnels, bumps, bridges, footbridges, crosswalks, road capacity, among others. Our analysis shows that cell-phones sensors with humans in vehicles or walking get affected by the different road features, which can be mined to extend the features of both free and commercial mapping services. We present the design and implementation of Map++ and evaluate it in a large city. Our evaluation shows that we can detect the different semantics accurately with at most 3% false positive rate and 6% false negative rate for both vehicle and pedestrian-based features. Moreover, we show that Map++ has a small energy footprint on the cell-phones, highlighting its promise as a ubiquitous digital maps enriching service.","2155-5486;21555486","Electronic:978-1-4799-4657-0; POD:978-1-4799-4656-3; USB:978-1-4799-4658-7","10.1109/SAHCN.2014.6990394","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990394","","Acceleration;Feature extraction;Legged locomotion;Reactive power;Roads;Semantics;Sensors","cartography;information retrieval systems","Map++ system;commercial map services;crowd-sensing system;crowdsensing approach;free map services;map semantics identification;pedestrian-based features;road features;road semantics;ubiquitous digital maps enriching service;vehicle features","","2","","39","","","June 30 2014-July 3 2014","","IEEE","IEEE Conference Publications"
"Novel frequent sequential patterns based probabilistic model for effective classification of web documents","H. Haleem; P. K. Sharma; M. M. Sufyan Beg","Department of Computer Engineering Jamia Millia Islamia New Delhi, India","2014 International Conference on Computer and Communication Technology (ICCCT)","20150108","2014","","","361","371","Web page classification has been one of essential tasks in web information retrieval such as delivering content specific search results, focused crawling and maintaining web-directory projects like DMOZ, etc. This paper presents a novel probabilistic web page classification scheme that utilizes the occurrences of frequent sequential patterns to determine the class of the document. As being suggested by many previous works in the field of text mining, patterns possess more relevant information about the document than individual words. This paper is an attempt to successfully make use of this hypothesis for classification of web documents. After testing this novel approach on RCV1 dataset, we were able to obtain classify the test documents with 88% accuracy.","","Electronic:978-1-4799-6758-2; POD:978-1-4799-6759-9","10.1109/ICCCT.2014.7001520","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001520","","Abstracts;Accuracy;Probabilistic logic;Testing;Text mining;Web pages","Internet;classification;data mining;information retrieval;probability","RCV1 dataset;Web document classification;Web information retrieval;Web page classification;frequent sequential pattern;probabilistic model;text mining","","1","","32","","","26-28 Sept. 2014","","IEEE","IEEE Conference Publications"
"An intuitive user interface for motion retrieval on a mobile multi-touch device","N. Tammachat; N. Pantuwong","Faculty of Information Technology King Mongkut's Institute of Technology Ladkrabang Bangkok, Thailand 10520","2014 6th International Conference on Information Technology and Electrical Engineering (ICITEE)","20150115","2014","","","1","4","This paper proposes a technique that allows novice users to retrieve motions using a mobile multi-touch device. A user first inputs a motion that he wants to obtain by manipulating a part of a character skeleton with multiple fingers. The proposed system automatically retrieves a couple of candidate motions from a motion database and visualizes them to the user. He selects one of them and manipulates a different part of the skeleton. Then the system generates a new motion by synthesizing multiple motion data retrieved through different parts of skeleton manipulations. By manipulating skeletons and retrieving motions iteratively, the user can obtain his desired motion in an efficient and easy manner. A pilot study has been conducted using the proposed system to clarify problems to be solved and functions to be improved.","","CD-ROM:978-1-4799-5301-1; Electronic:978-1-4799-5303-5; POD:978-1-4799-5304-2","10.1109/ICITEED.2014.7007893","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007893","","Animation;Bones;Databases;Information technology;Three-dimensional displays;User interfaces","information retrieval;solid modelling;user interfaces","3D models;intuitive user interface;iterative motion;mobile multitouch device;motion database;motion retrieval;skeleton manipulations","","0","","10","","","7-8 Oct. 2014","","IEEE","IEEE Conference Publications"
"Spatial access method for urban geospatial database management: An efficient approach of 3D vector data clustering technique","S. Azri; U. Ujang; A. A. Rahman; F. Anton; D. Mioc","3D GIS Research Group, Dept. of Geoinformation, Faculty of Geoinformation and Real Estate, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia","Ninth International Conference on Digital Information Management (ICDIM 2014)","20141218","2014","","","92","97","In the last few years, 3D urban data and its information are rapidly increased due to the growth of urban area and urbanization phenomenon. These datasets are then maintain and manage in 3D spatial database system. However, performance deterioration is likely to happen due to the massiveness of 3D datasets. As a solution, 3D spatial index structure is used as a booster to increase the performance of data retrieval. In commercial database, commonly and widely used index structure for 3D spatial database is 3D R-Tree. This is due to its simplicity and promising method in handling spatial data. However, 3D R-Tree produces serious overlapping among nodes. The overlapping factor is important for an efficient 3D R-Tree to avoid replicated data entry in a different node. Thus, an efficient and reliable method is required to reduce the overlapping nodes in 3D R-Tree nodes. In this paper, we proposed a 3D geospatial data clustering to be used in the construction of 3D R-Tree and respectively could reduce the overlapping among nodes. The proposed method is tested on 3D urban dataset for the application of urban infill development. By using several cases of data updating operations such as building infill, building demolition and building modification, the proposed method indicates that the percentage of overlapping coverage among nodes is reduced compared with other existing approaches.","","Electronic:978-1-4799-5421-6; POD:978-1-4799-5422-3","10.1109/ICDIM.2014.6991400","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991400","3D Spatial Index;3D Urban Data Management;Information Management;Spatial Access Method;Urban Data Clustering","Buildings;Geometry;Geospatial analysis;Indexes;Spatial databases;Three-dimensional displays;Urban areas","data handling;information retrieval;pattern clustering;visual databases","3D R-Tree;3D geospatial data clustering;3D spatial database system;3D spatial index structure;3D urban dataset;3D vector data clustering technique;building demolition;building infill;building modification;data retrieval;spatial access method;spatial data handling;urban geospatial database management;urban infill development","","0","","12","","","Sept. 29 2014-Oct. 1 2014","","IEEE","IEEE Conference Publications"
"Tibetan-Chinese cross language named entity extraction based on comparable corpus and naturally annotated resources","Y. Sun; W. Guo; X. Zhao","School of Information Engineering, Minzu University of China, 100081, Beijing, China","2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","20150115","2014","","","288","295","Tibetan-Chinese named entity extraction can effectively improve the performance of Tibetan-Chinese cross language question answering system, information retrieval, machine translation and other researches. In the condition of no practical Tibetan named entity recognition system and Tibetan-Chinese translation model, this paper proposes a method to extract Tibetan-Chinese entities based on comparable corpus and naturally annotated resources from webs. The main work of this paper is in the following: (1) Tibetan-Chinese comparable corpus construction. (2) Combining sentence length, word matching and boundary term features, using multi-feature fusion algorithm to obtain parallel sentences from comparable corpus. (3) Tibetan-Chinese entity mapping based on the maximum word continuous intersection model of parallel sentence. Finally, the experimental results show that our approach can effectively find Tibetan-Chinese cross language named entity.","","Electronic:978-1-4799-4518-4; POD:978-1-4799-4517-7","10.1109/CIDM.2014.7008680","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008680","Tibetan-Chinese named entity;comparable corpus;maximum word continuous intersection model;parallel sentence","Educational institutions;Electronic publishing;Encyclopedias;Feature extraction;Internet;Lead","Internet;language translation;question answering (information retrieval)","Tibetan-Chinese comparable corpus construction;Tibetan-Chinese cross language;Tibetan-Chinese named entity extraction;Tibetan-Chinese translation model;boundary term features;information retrieval;machine translation;maximum word continuous intersection model;multifeature fusion algorithm;naturally annotated resources;parallel sentences;question answering system;word matching","","0","","21","","","9-12 Dec. 2014","","IEEE","IEEE Conference Publications"
"Determining parameters for efficient retrieval in index structures for hybrid data spaces","C. Kropf","Institute of Information Systems Hof University D-95028 Hof","Ninth International Conference on Digital Information Management (ICDIM 2014)","20141218","2014","","","98","103","Different kinds of access methods supporting boolean retrieval in hybrid data spaces exist. We inspect a class of these index structures using a categorization of low and high frequently occurring keywords. This access method uses a basic R*-Tree augmented with bitlists for the representation of a set of terms. Two limits are given for these access methods in realistic environments: the length of the bitlist B Length and the limit separating the set of low and high frequently occurring terms H Limit. This paper presents a theoretical analysis of the setup of H Limit as well as an empirical analysis of the bitlist length for two different corpora in a typical database environment. The final target of this paper is the determination of the free parameters to provide efficient retrieval of data in realistic application domains.","","Electronic:978-1-4799-5421-6; POD:978-1-4799-5422-3","10.1109/ICDIM.2014.6991402","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991402","","Electronic publishing;Encyclopedias;Indexes;Internet;Time complexity","information retrieval;tree data structures","Boolean retrieval;R*-Tree;access method;bitlist length;hybrid data spaces;index structure","","0","","15","","","Sept. 29 2014-Oct. 1 2014","","IEEE","IEEE Conference Publications"
"The Internet of Things for healthcare monitoring: Security review and proposed solution","A. Rghioui; A. L'aarje; F. Elouaai; M. Bouhorma","Department of Computer Science, Abdelmalek Essaadi University, LIST Laboratory, FST of Tangier","2014 Third IEEE International Colloquium in Information Science and Technology (CIST)","20150122","2014","","","384","389","The development of the Internet of Things will greatly facilitate the process of patient's diagnosis and monitoring, with small IP-based wireless sensors implemented on the patient's body, his physiological parameters, such as blood pressure and heart rate, can be monitored remotely and continuously. This scenario must absolutely respect the confidentiality and privacy of patient's medical information, only caregivers and authorized persons should possess the right to access these information. The security must be ensured throughout the healthcare application scenario. This paper presents a study of the potential security problems in this application, and proposes a security model as a solution. Our model is based on symmetric cryptography with a proposed key management system and network nodes authentication mechanism.","2327-185X;2327185X","Electronic:978-1-4799-5979-2; POD:978-1-4799-5980-8","10.1109/CIST.2014.7016651","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016651","Healthcare monitoring;Internet of Things;IoT;Network Security;WBAN;WPAN;WSN","Decision support systems;Internet of Things;Medical services;Monitoring;Security;Wireless communication;Wireless sensor networks","IP networks;Internet of Things;authorisation;computer network security;cryptography;data privacy;health care;information retrieval;message authentication;patient diagnosis;patient monitoring;telemedicine;wireless sensor networks","IP-based wireless sensors;Internet of Things;authorized persons;blood pressure;caregivers;healthcare application scenario;healthcare monitoring;heart rate;information access;key management system;network nodes authentication mechanism;patient body;patient diagnosis;patient medical information confidentiality;patient medical information privacy;patient monitoring;physiological parameters;remote monitoring;security model;security problems;security review;symmetric cryptography","","2","","10","","","20-22 Oct. 2014","","IEEE","IEEE Conference Publications"
"A big data management system for energy consumption prediction models","W. Lee; B. W. On; I. Lee; J. Choi","Dept of Computer Science Kyonggi University","Ninth International Conference on Digital Information Management (ICDIM 2014)","20141218","2014","","","156","161","In this work, we develop a prototype about a big data management system for storing, indexing, and searching for huge-scale energy usage data. Rather than existing, commercial relational databases such as Oracle and IBM-DB2, this system is able to provide us with high availability and performance at low cost. It is also able to manage unstructured data and store big data in distributed environment. In addition, using data access APIs, target data is quickly retrieved from our proposed system. To utilize our prototype system, we also propose an energy consumption prediction model based on penalized linear regression-based map/reduce algorithms. Then, we exploit discriminate features with respect to time stamp. Finally, given a time stamp (e.g., 2014-01-05 12:01:08), our proposed learning model will give us a predicted value about the energy usage (e.g., 90 watt) at that time. According to our experimental results obtained from about 7.5 million records, each of which consists of an energy usage and time stamp during three months in 2014, it turns out that our prediction model can predict real values that are very close to actual energy usage at that time, and is about 1.72 times faster than in a single machine.","","Electronic:978-1-4799-5421-6; POD:978-1-4799-5422-3","10.1109/ICDIM.2014.6991404","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991404","","Big data;Database systems;Distributed databases;Electricity;Predictive models;Servers","Big Data;application program interfaces;energy consumption;information retrieval;learning (artificial intelligence);power aware computing;regression analysis;relational databases","IBM-DB2;Oracle;big data management system;commercial relational databases;data access API;distributed environment;energy consumption prediction models;energy usage;huge-scale energy usage data;learning model;penalized linear regression-based mapreduce algorithms;time stamp","","1","","14","","","Sept. 29 2014-Oct. 1 2014","","IEEE","IEEE Conference Publications"
"Wireless Electrocardiogram monitoring using mobile network communication","K. Lerdwuttiaugoon; P. Naiyanetr","Department of Biomedical Engineering, Faculty of Engineering, Mahidol University, Nakhon Pathom, 73170 Thailand","The 7th 2014 Biomedical Engineering International Conference","20150122","2014","","","1","4","Wireless monitoring system is the effective and convenient tools in medical field nowadays. It is widely applied to many physiological parameters such as Electrocardiogram (ECG), Electroencephalogram (EEG), etc. This paper proposes a low cost wireless 1-lead ECG monitoring system that transmits the information to the database server via the mobile network. Our designing system consists of the ECG monitoring circuit, microcontroller circuit and Android mobile phone. The microcontroller interfaces to Android mobile phone via Bluetooth protocol, and the information is transmitted to the database server via FTP protocol. The information was displayed and accessed by the web-service system via the internet protocol. The heart rate accuracy was tested by ECG simulator at different beats per minute. The maximum error and standard deviation are 0.83% and 2.35 respectively.","","Electronic:978-1-4799-6801-5; POD:978-1-4799-6802-2","10.1109/BMEiCON.2014.7017395","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017395","Android application;Electrocardiogram;Mobile network;Wireless monitoring","Biomedical monitoring;Data mining;Electrocardiography;Microcontrollers;Monitoring;Testing;Wires","Bluetooth;Web services;bioelectric potentials;biomedical communication;biomedical electronics;electrocardiography;electroencephalography;information retrieval systems;medical information systems;medical signal processing;microcontrollers;neurophysiology;smart phones","Android mobile phone;Bluetooth protocol;ECG;ECG monitoring circuit;EEG;FTP protocol;database server;designing system;electrocardiogram;electroencephalogram;heart rate accuracy;internet protocol;microcontroller circuit;microcontroller interfaces;mobile network communication;physiological parameters;standard deviation;web-service system;wireless 1-lead ECG monitoring system;wireless electrocardiogram monitoring system","","0","","7","","","26-28 Nov. 2014","","IEEE","IEEE Conference Publications"
"Design, conceptual analysis and implementation of coordinated multipurpose robotic system with smart sensors","I. J. Rohit; M. Dev Anand; D. Jackson","Department of Aeronautical Engineering, Noorul Islam Centre for Higher Education, Kumaracoil - 629 180, Thuckalay, Kanyakumari District, Tamilnadu State, India","2014 International Conference on Control, Instrumentation, Communication and Computational Technologies (ICCICCT)","20141222","2014","","","195","201","This paper attempts to the overview of design, conceptual analysis and implementation of coordinated multipurpose robot system with smart sensors. The aim of this paper is to show how to design and integrate various sensors in a single module and create a data retrieval system which can be used as a black box and an investigation device.","","CD-ROM:978-1-4799-4193-3; Electronic:978-1-4799-4190-2; POD:978-1-4799-4189-6","10.1109/ICCICCT.2014.6992956","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992956","Coordinated Multipurpose;Design;Implementation;Integration;Investigation Device;Smart Sensors","Electric shock;Fires;Gas detectors;Robot sensing systems;Temperature sensors","design engineering;information retrieval systems;intelligent sensors;mobile robots;remotely operated vehicles","black box;conceptual analysis;coordinated multipurpose robotic system;data retrieval system;design;investigation device;smart sensors","","0","","25","","","10-11 July 2014","","IEEE","IEEE Conference Publications"
"Certificateless public key encryption with keyword search","P. Yanguo; C. Jiangtao; P. Changgen; Y. Zuobin","Sch. of Comput. Sci. & Technol., Xidian Univ., Xi'an, China","China Communications","20150107","2014","11","11","100","113","Public Key Encryption with Keyword Search (PEKS), an indispensable part of searchable encryption, is stock-in-trade for both protecting data and providing operability of encrypted data. So far most of PEKS schemes have been established on Identity-Based Cryptography (IBC) with key escrow problem inherently. Such problem severely restricts the promotion of IBC-based Public Key Infrastructure including PEKS component. Hence, Certifcateless Public Key Cryptography (CLPKC) is efficient to remove such problem. CLPKC is introduced into PEKS, and a general model of Certifcateless PEKS (CLPEKS) is formalized. In addition, a practical CLPEKS scheme is constructed with security and efficiency analyses. The proposal is secure channel free, and semantically secure against adaptive chosen keyword attack and keyword guessing attack. To illustrate the superiority, massive experiments are conducted on Enron Email dataset which is famous in information retrieval field. Compared with existed constructions, CLPEKS improves the efficiency in theory and removes the key escrow problem.","1673-5447;16735447","","10.1109/CC.2014.7004528","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004528","certificateless public key encryption;keyword search;provable secure","Computer security;Electronic mail;Encryption;Public key cryptography;Receivers;Search engines;Servers","data protection;information retrieval;public key cryptography","CLPKC;Enron Email dataset;IBC-based public key infrastructure;PEKS schemes;certificateless PEKS;certificateless public key cryptography;certificateless public key encryption;data protection;identity-based cryptography;information retrieval field;key escrow problem;keyword guessing attack;public key encryption with keyword search;searchable encryption;stock-in-trade","","0","","","","","Nov. 2014","","IEEE","IEEE Journals & Magazines"
"Extraction of feature vectors for analysis of musical instruments","M. Joshi; S. Nadgir","Department of Electronics & Telecommunication, Cummins College of Engineering, Pune, India","2014 International Conference on Advances in Electronics Computers and Communications","20150108","2014","","","1","6","Music data analysis and retrieval has become a vital and challenging research field due to the increasing need to manage large amount of musical data present on multimedia. Music analysis deals with extracting various features, which describe the music signal, and use those features to process the music samples for various applications such as indexing, retrieval, automatic classification, etc. This paper presents analysis performed on Indian musical instrumental signal to extract different features in temporal, spectral, cepstral & wavelet domain. Analysis is performed on a large database consisting of around 700 sample files of five musical instruments specifically Indian musical instruments namely flute, dholaki, harmonium, santoor & sanai, belonging to woodwind, percussion, keyboard, string & brass category respectively. Feature vectors in the four domains are extracted for all files in the database. These feature vectors are further analyzed to observe their usefulness in discrimination amongst the instruments.","","Electronic:978-1-4799-5496-4; POD:978-1-4799-5497-1","10.1109/ICAECC.2014.7002391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002391","Mel Frequency Cepstral Coefficient (MFCC);Wavelet Transform (WT);feature extraction","Databases;Feature extraction;Instruments;Mel frequency cepstral coefficient;Multiple signal classification;Music","cepstral analysis;data analysis;feature extraction;information retrieval;music;signal processing;vectors","Indian musical instrumental signal;cepstral analysis;feature vector extraction;music data analysis;music retrieval;spectral analysis;temporal analysis;wavelet analysis","","0","","15","","","10-11 Oct. 2014","","IEEE","IEEE Conference Publications"
"N-gram Index Structure Study for Semantic Based Mathematical Formula","Y. Xu; W. Su; M. Cheng; Z. Qu; H. Li","Coll. of Inf. Sci. & Eng., Lanzhou Univ., Lanzhou, China","2014 Tenth International Conference on Computational Intelligence and Security","20150122","2014","","","293","298","Recently mathematical formula retrieval has become a hot and difficult problem in the field of information science research. The paper presents an N-grams division method of mathematical formula, determines the granularities of division by experiments, and proposes calculating method of a sub-formula weight based on the complexity of formula, the length of N-grams and depth. In addition, this paper considers the impact that operators made on weights of sub-formulas and gives a method of calculating the weight that sub-formula shares in the whole formula. Experiments show that the methods of N-grams division and index construction have great help for sub-formula matching and weighting computation. The methods can also improve the recall and precision of sub-formulas. The quick and feasible semantic based method can greatly enhance the semantic search capabilities of mathematical search.","","CD-ROM:978-1-4799-7433-7; Electronic:978-1-4799-7434-4; POD:978-1-4799-7435-1","10.1109/CIS.2014.174","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016903","Formula search;Math Search;MathML;N-grams division;Search engine;Sub-formula weight","Complexity theory;Educational institutions;Indexing;Mathematical model;Search engines;Semantics","information retrieval;mathematics computing;pattern matching;search engines","N-gram index structure study;N-grams division method;formula complexity;index construction;information science research;semantic based mathematical formula retrieval;semantic search capabilities;subformula matching;subformula weight;weighting computation","","0","","19","","","15-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"Topic Detection in Twitter Based on Label Propagation Model","D. Huang; D. Mu","Sch. of Autom., Northwest Polytech. Univ., Xi'an, China","2014 13th International Symposium on Distributed Computing and Applications to Business, Engineering and Science","20150105","2014","","","97","101","Many kinds of huge amount of tweets about real-world events are generated everyday in Twitter. However, the disorganization messages required to be classified by topics and events are one of challenges to get knowledge effectively. To solve the problem, we propose a novel method that combines the cluster algorithm with label propagation algorithm to detect topics in twitter. First, we use canopy cluster algorithm to cluster tweets, canopy cluster algorithm could divides a tweet into different clusters, and the tweet which only belongs to one cluster will be labeled. Second, the mechanism of label propagation is used to label the tweets that in the overlapping of different clusters. In order to evaluate our algorithm, we use two baseline algorithms, LDA (Latent Dirichlet Allocation) and Single-Pass cluster algorithm. We apply three algorithms on tweet dataset with three topics and some noisy data, and experiment results show our method outperforms other algorithms on precision and recall rate.","","CD-ROM:978-1-4799-4170-4; Electronic:978-1-4799-4169-8; POD:978-1-4799-4168-1","10.1109/DCABES.2014.23","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999065","cluster algorithm;label propagation model;topic detection;twitter","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Computational modeling;Time complexity;Twitter;Vectors","information retrieval;pattern clustering;social networking (online)","LDA algorithm;Twitter;canopy cluster algorithm;disorganization messages;label propagation model;latent Dirichlet allocation;precision rate;recall rate;single-pass cluster algorithm;topic detection;tweet dataset","","0","","17","","","24-27 Nov. 2014","","IEEE","IEEE Conference Publications"
"Harnessing the semantic analysis of tag using Semantic Based Lesk Algorithm","Shankar M.; R. Senthilkumar","Department of Information Technology, Anna University, Chennai, Tamilnadu, India","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","5","In the field of Data retrieval, accessing web resources is frequent task. This domain is shifting radically from the amplified data growth to the way in which it is structured and retrieved across web. This explosive growth of data is the result of billions of people using the Internet and mobile devices for commerce, entertainment, social interactions and as well as the Internet of things that constantly share machine-generated data. Even with lot of research, the task of analyzing this data to extract its business values with precision still remains as a trivial issue. To address this issue, the paper presents a novel Semantic Based Lesk Algorithm (SBLA), which traces the meaning of user defined tags and categorizes the web data by means of Support Vector Machine (SVM) classifier. On comparing with existing methods, the proposed method performs well in extraction of admissible data with the better accuracy and precision as discussed in result analysis.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996200","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996200","Semantic Based Lesk Algorithm;Social tagging system;Support vector machine;Web resources","Information technology;Market research;Navigation;Semantics;Support vector machines;Tagging;Web pages","data analysis;information analysis;information retrieval;pattern classification;support vector machines","SBLA algorithm;SVM classifier;Web resource;data analysis;data extraction;data retrieval;semantic based lesk algorithm;support vector machine;tag semantic analysis","","0","","16","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Domain-Specific Product Named Entity Recognition from Chinese Microblog","X. Yang; H. Huang; X. Xin; Q. Liu; X. Wei","Sch. of Comput. Sci. & Technol., Beijing Inst. of Technol., Beijing, China","2014 Tenth International Conference on Computational Intelligence and Security","20150122","2014","","","218","222","In this paper, we presented an effective method to recognize product named entity from Chinese micro blog. Due to the short, noisy and informal text, it is much more difficult to recognize Pro-NEs from micro blog than traditional media, such as news articles. In order to work out the challenges, we carried out a statistical analysis on the appearance of Pro-NEs in Chinese micro blog, According to the structure of Pro-NEs, we integrated some features and introduced external knowledge to the conditional random fields model. Experiments on an artificial annotated micro blog corpus show that our approach can improve the performance of the product named entity recognition from microblog.","","CD-ROM:978-1-4799-7433-7; Electronic:978-1-4799-7434-4; POD:978-1-4799-7435-1","10.1109/CIS.2014.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016887","CRFs;Chinese;microblog;product named entity recognition","Cellular phones;Companies;Computational linguistics;Context;Hidden Markov models;Media;Text recognition","information retrieval;social networking (online);statistical analysis","Chinese microblog;Pro-NE;artificial annotated micro blog corpus;conditional random field model;domain-specific product named entity recognition;external knowledge;statistical analysis","","0","","15","","","15-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"OWL, RDF, RDFS inference derivation using Jena semantic framework & pellet reasoner","Javed Ahmad Khan; S. Kumar","Dept. of Computer Science & Engineering, Ambedkar Institute of Advanced Communication, Technologies & Research, GGSIP University, Delhi, India","2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014)","20150119","2014","","","1","8","Semantic web is an extension of World Wide Web. In Semantic web each resources and services is annotated with the help of web ontology language (OWL). Resource description framework (RDF) represents the web information into its properties and values. Resource description framework schema (RDFS) represents information resources into classes and properties are defined with the help of these classes. In this paper we create university ontology with the help of protégé 4.3 alpha tool. We have proposed information retrieval framework that produce results with better precision, recall and information overkill. With the help of Jena semantic framework and pellet reasoner tools we analyze that OWL inference provide more relevant information compared to None, RDF and RDFS. We have developed five university domain ontologies but presented the result only for two ontologies. We have validated that our proposed framework provide more relevant information for these two ontologies.","2347-9337;23479337","Electronic:978-1-4799-6393-5; POD:978-1-4799-6394-2","10.1109/ICAETR.2014.7012871","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012871","Jena semantic framework;OWL;Pellet reasoner;Protégé 4.3 alpha tool;RDF;RDFS;Semantic Web","Conferences;Educational institutions;OWL;Ontologies;Resource description framework;XML","inference mechanisms;information retrieval;knowledge representation languages;ontologies (artificial intelligence);semantic Web","Jena semantic framework;OWL inference;Pellet reasoner;RDFS inference derivation;Web ontology language;World Wide Web;information retrieval;protégé 4.3 alpha tool;resource description framework;semantic Web;university ontology","","1","","12","","","1-2 Aug. 2014","","IEEE","IEEE Conference Publications"
"Investigating the accuracy of the openFDA API using the FDA Adverse Event Reporting System (FAERS)","J. Shin","8 Path Solutions LLC","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","48","53","The US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) is a database that contains information on adverse event and medication error reports submitted to the FDA. Each quarter the FDA releases the data to the public, but accessing the data requires researchers to download, import, and consolidate data for every quarter starting from 2004. In an effort to provide easier access to this, the FDA launched the openFDA initiative in June 2014, which gives the public API access to information about adverse events reports. Although the API enables easier access to the FAERS data, the quality of the API design and the features of the data set will determine the reliability of the information retrieved. Thus, errors in the API can result in inaccurate and unreliable data analysis. Furthermore, the number of adverse events reports retrieved by the API for a particular drug can differ from the FAERS data files due to the openFDA harmonization process and the existence of multiple entries and variations for any given drug name in the FAERS data files. Since there are no universal rule that can be used to identify errors or potential issues, we propose evaluating the openFDA API by searching for a particular drug (brand name), Yaz, and the generic name, Drospirenone Ethinyl Estradiol, and comparing the results against the FAERS data files. Our results show that in the case of Yaz, the openFDA API and the drug harmonization process is inaccurate and inconsistent.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004412","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004412","FAERS;FDA;FDA Adverse Events Reporting System;adverse events;openFDA","Accuracy;Big data;Databases;Drugs;IEEE Potentials;US Government agencies","application program interfaces;data analysis;database management systems;drugs;information retrieval","API access;API design quality;FAERS data files;FDA adverse event reporting system;US Food and Drug Administration;data access;data analysis;drug name;information database;information retrieval;medication error reports;openFDA API;openFDA harmonization process;openFDA initiative","","1","","9","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Recall estimation for rare topic retrieval from large corpuses","P. Bommannavar; A. Kolcz; A. Rajaraman","Twitter, Inc.","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","825","834","The problem of finding documents pertaining to a particular topic finds application in a variety of scenarios. Indeed, the demand for topically pertinent documents has led to myriad companies offering services to find and deliver them (perhaps along with sentiment analysis or clustering) to customers for any topics of interest. The methodologies used to uncover relevant documents range from manually curated keyword filters to trained classification models. Any serious topical analysis requires a sound understanding of key metrics behind the retrieval process, two of the most important being precision and recall. While precision can be easily and inexpensively measured by sampling from classified documents and utilizing (paid) human computation to mark incorrectly classified instances, it is not as straightforward to use the same approach for measuring recall. With most topics occurring relatively sparsely, an unbiased sampling approach becomes prohibitively expensive. In this paper, we introduce a recall measurement procedure requiring only relatively few human judgements. The technique makes use of pairs of sufficiently independent classifiers and the paper provides a detailed discussion of how such classifier pairs can be constructed in practice, with a focus on social media classifiers. We report the performance of the proposed method with simple keyword filters as well as with classifiers of progressive levels of complexity and show that under reasonable conditions, recall can be estimated to within 0.10 absolute error and 15% relative error, and often closer with a reduction of cost by a factor of as much as 1000x as compared with unbiased sampling.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004312","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004312","Twitter;classifier evaluation;human evaluation;mechanical turk;recall estimation;social media","Companies;Estimation;Joints;Labeling;Measurement;Media;Twitter","document handling;information retrieval;pattern classification;pattern clustering","classification models;classified documents;clustering;keyword filters;pertinent documents;rare topic retrieval;recall estimation;recall measurement procedure;sentiment analysis;social media classifiers;topical analysis;unbiased sampling","","0","","16","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Mining meaningful topics from massive biomedical literature","P. Zhu; J. Shen; D. Sun; K. Xu","State Key Lab of Software Development Environment, Beihang University, Beijing, China","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20150115","2014","","","438","443","There is huge amount of biomedical and biological literature online or in digital libraries. Moreover, new research papers are published with an exponential growth in recent years. So it is pressing and challenging to mine meaningful topics from massive biomedical literature. The mined topics are helpful to researchers for literature exploration and topic discovery. However, latent topics inferred by traditional topic models are not always coherent and meaningful. In this work, we propose a new methodology to mine meaningful biomedical topics with a combination of several off-the-shelf text mining techniques such as part-of-speech tagging, base noun phrase chunking, K-means clustering and latent Dirichlet allocation, which endow our methodology with scalability and implementation simplicity. We conduct comprehensive experiments on a dataset collected from PubMed. The experimental results demonstrate that our method significantly outperforms a baseline method. We also perform a qualitative analysis and present meaningful biomedical topics and multi-word expressions.","","Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8","10.1109/BIBM.2014.6999197","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999197","","Arteries;Biological system modeling;Biomedical measurement;Cancer;Diseases;Semantics;Tagging","biomedical engineering;data mining;digital libraries;information retrieval systems;information services;medical computing;pattern clustering;text analysis","K-means clustering;PubMed dataset;base noun phrase chunking;baseline method;digital library;latent Dirichlet allocation;latent topic inference;literature exploration;massive biomedical literature;meaningful biomedical topic mining;multi-word expression;off-the-shelf text mining techniques;online biological literature;online biomedical literature;part-of-speech tagging;qualitative analysis;research paper;topic discovery;traditional topic model","","0","","23","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"Personalized web page recommender system using integrated usage and content knowledge","M. Venu Gopalachari; P. Sammulal","Department of CSE, Chaitanya Bharathi Institute of Technology, Gandipet, Hyderabad, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","20150126","2014","","","1066","1071","Now a day, intelligent recommender systems on the web intends to recommend web pages for individual users by discovering useful knowledge from Web usage data and web content data. Knowledge representation for the web contents and integrating with web usage knowledge are the challenging issues to make Web page recommendations effective. This paper presents an effective method to integrate the domain knowledge and web usage knowledge of a website through semantics. Perhaps, a new model is framed to construct a semantic hierarchy of the web log data and the domain contents, which represents the integrated usage knowledge and domain knowledge. This model has two phases: first one is to generate domain knowledge represented with ontology for the website; second one is to generate mappings between web pages and domain terms in the ontology based on the usage of an individual. However this semantically enhanced knowledge representation uses a recommendation strategy to recommend web pages dynamically. The recommendation results have been compared with the results obtained from an advanced existing Web Usage Mining method. Finally, explores the effectiveness of the proposed approach than the existing Web Usage Mining by the analysis of the experiments within the scope of web page recommendations.","","CD-ROM:978-1-4799-3913-8; Electronic:978-1-4799-3914-5; POD:978-1-4799-3915-2","10.1109/ICACCCT.2014.7019261","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019261","Semantic data;Web page recommendations;Web personalization;Web usage mining","Analytical models;Knowledge engineering;Navigation;OWL;Uniform resource locators;Venus;Web pages","Web sites;data mining;information retrieval;ontologies (artificial intelligence);recommender systems","Web content data;Web log data;Web site;Web usage knowledge;Web usage mining method;intelligent recommender system;knowledge representation;ontology;personalized Web page recommender system;semantic hierarchy","","2","","23","","","8-10 May 2014","","IEEE","IEEE Conference Publications"
"SCPS: A Social-Aware Distributed Cyber-Physical Human-Centric Search Engine","H. Shen; J. Liu; K. Chen; J. Liu; S. Moyer","Department of Electrical and Computer Engineering, Clemson University, Clemson","IEEE Transactions on Computers","20150112","2015","64","2","518","532","Advances in ubiquitous sensing, computing and wireless communication technologies are leading to the development of cyber-physical systems (CPS), which promise to revolutionize the way we interact with the physical world. CPS applications, such as healthcare monitoring, may involve many users and objects scattered over a wide area. One critical function of CPS is object search in the physical world through the cyber sphere that enables interaction between the cyber and physical spheres. Some of the previously proposed physical object search engines use RFID tracking, and others collect the information of object locations into a hierarchical centralized server. The difficulty of widely deploying RFID devices, the centralized search, and the need for periodical location information collection prevent CPS from achieving higher scalability and efficiency. To deal with this problem, we propose a Social-aware distributed Cyber-Physical human-centric Search engine (SCPS) that leverages the social network formed by wireless device users for object search. Without requiring periodical location information collection, SCPS locates objects held by users based on the routine user movement pattern. Moreover, using a social-aware Bayesian network, it can accurately predict the users' locations even at the occurrence of exceptional (i.e., non-routine) events (e.g., raining) that break user movement pattern. Thus, SCPS is more advantageous than all previous social network based works which assume that user behaviors always follow a certain pattern. Further, SCPS conducts the search in a fully distributed manner by relying on a distributed hash table (DHT) structure. As a result, SCPS achieves high scalability, efficiency and location accuracy. Extensive real-trace driven simulation results show the superior performance of SCPS compared to other representative search methods including a hierarchical centralized method, a decentralized method, and two social network based methods. - he results also show the effectiveness of different components of SCPS.","0018-9340;00189340","","10.1109/TC.2013.211","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645370","Cyber-physical systems (CPS);distributed hash table (DHT);distributed systems;objection location;scalability","Bayes methods;Indexes;Manganese;Predictive models;Routing;Search problems;Social network services","Bayes methods;distributed processing;information retrieval;search engines;social networking (online);storage management","DHT structure;SCPS;centralized search;computing technologies;distributed hash table structure;healthcare monitoring;object locations;object search;periodical location information collection;physical object search engines;real-trace driven simulation;routine user movement pattern;social network;social-aware Bayesian network;social-aware distributed cyber-physical human-centric search engine;ubiquitous sensing technologies;wireless communication technologies","","6","","50","","20131023","Feb. 2015","","IEEE","IEEE Journals & Magazines"
"Intelligent Web search optimization with reference to mutation operator of Genetic and Cultural Algorithms framework","N. Kaur; J. S. Budwal","School of IT, Apeejay Inst. of Management Technical Campus, Jalandhar-144023, Punjab, INDIA","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","20150126","2014","","","619","623","Web search engines typically respond to user keyword queries by retrieving relevant URL's from their own databases. Web search optimization is an attempt to discover useful information from web based repository. Based on Cultural Algorithms (CA), the web search process is supported by the constraints described in the belief space, and the optimization process is supported by Genetic Algorithms (GA) in the population space. This paper introduces the dual inheritance approach which include genetic and cultural algorithm to retrieve the web based information from the repository. Results suggest that the belief space is an important contributor to the problem solving process for both systems when the number of constraints on the problem becomes large enough.","","CD-ROM:978-1-4799-3913-8; Electronic:978-1-4799-3914-5; POD:978-1-4799-3915-2","10.1109/ICACCCT.2014.7019162","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019162","Belief Space;Cultural Algorithm;Genetic Algorithm;Population Space;Web Search","Cultural differences;Irrigation;Optimization;Redundancy;Sociology;Statistics;Web search","Internet;genetic algorithms;information retrieval;problem solving","URL;Web search process;cultural algorithms;dual inheritance approach;genetic algorithms;information retrieve;intelligent web search optimization;problem solving process","","0","","16","","","8-10 May 2014","","IEEE","IEEE Conference Publications"
"A distributed polygon retrieval algorithm using MapReduce","Q. Guo; B. Palanisamy; H. A. Karimi","Geoinformatics Laboratory, School of Information Sciences, University of Pittsburgh, USA","10th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing","20150122","2014","","","435","436","The proliferation of data acquisition devices like 3D laser scanners had led to the burst of large-scale spatial terrain data which imposes many challenges to spatial data analysis and computation. With the advent of several emerging collaborative cloud technologies, a natural and cost-effective approach to managing such large-scale data is to store and share such datasets in a publicly hosted cloud service and process the data within the cloud itself using modern distributed computing paradigms such as MapReduce. For several key spatial data analysis and computation problems, polygon retrieval is a fundamental operation which is often computed under real-time constraints. However, existing sequential algorithms fail to meet this demand effectively given that terrain data in recent years have witnessed an unprecedented growth in both volume and rate. In this work, we develop a MapReduce-based parallel polygon retrieval algorithm which aims at minimizing the IO and CPU loads of the map and reduce tasks during spatial data processing. The results of the preliminary experiments on a Hadoop cluster demonstrate that the proposed techniques are scalable and lead to more than 35% reduction in execution time of the polygon retrieval operation over existing distributed algorithms.","","Electronic:978-1-63190-043-3; POD:978-1-4799-5328-8","","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014591","","Arrays;Earth;Elevators;Random access memory;Three-dimensional displays;Tin","cloud computing;data acquisition;data analysis;information retrieval","Hadoop cluster;MapReduce-based parallel polygon retrieval algorithm;collaborative cloud technologies;data acquisition devices;distributed computing paradigms;distributed polygon retrieval algorithm;large-scale spatial terrain data;publicly hosted cloud service;spatial data analysis;spatial data processing","","0","","9","","","22-25 Oct. 2014","","IEEE","IEEE Conference Publications"
"An Optimized Full-Text Retrieval System Based on Lucene in Oracle Database","X. Shi; Z. Wang","Sch. of Comput. Sci. & Technol., Donghua Univ., Shanghai, China","2014 Enterprise Systems Conference","20150119","2014","","","61","65","This paper introduces an optimized full-text search system through combining Lucene text search engine and Oracle database together. This system has a good performance on indexing and retrieving for range of values and date. Lucene also provides the ability for paging query. The optimization work is mainly on index creation process and retrieval process. The experimental results show that proposed solution has a better efficiency in the index creation process, which requires less time and ensures the same recall ratio and precision of retrieval results.","","Electronic:978-1-4799-5554-1; POD:978-1-4799-5555-8","10.1109/ES.2014.73","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997020","Lucene ; Oracle ;Full-text search; Numeric index; Paging query; Index Tuning","Educational institutions;Indexing;Optimization;Search engines;Search problems","full-text databases;indexing;information retrieval;optimisation;search engines","Lucene text search engine;Oracle database;full-text retrieval system;full-text search system;index creation process;optimisation method;paging query","","0","","9","","","2-3 Aug. 2014","","IEEE","IEEE Conference Publications"
"Towards Policy Retrieval for Provenance Based Access Control Model","J. Pei; X. Ye","Sch. of Software, Tsinghua Univ., Beijing, China","2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications","20150119","2014","","","769","776","Provenance Based Access Control (PBAC) is a new access control mechanism wherein the access control decisions are made based on a set of assertions about provenance traces. Manually designing a variety of provenance based security policies is not trivial work for big data applications with large amount of provenance entity types and complex provenance dependencies. Policy retrieval can reduce such manual labor by automatically ""learning"" policies from previous provenance traces. In this paper, we look into the composition of PBAC rules to determine the relevant knowledge that should be mined from provenance traces for policy retrieval. We propose a baseline retrieval approach which composes the mined knowledge into candidate rules and verifies them by feeding them into a decision-tree classifier as candidate classification features. We show the feasibility and limitations of the baseline approach with experimenting and thereby present suggestions about the future work for PBAC policy retrieval research.","2324-898X;2324898X","Electronic:978-1-4799-6513-7; POD:978-1-4799-6514-4","10.1109/TrustCom.2014.101","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011325","Access Control;Big Data;Data Provenance;Policy Retrieval;Provenance Based Access Control","Access control;Big data;Data mining;Decision trees;Manuals;Training","Big Data;authorisation;information retrieval","PBAC policy retrieval research;PBAC rules;access control mechanism;baseline retrieval;big data applications;candidate classification features;candidate rules;decision tree classifier;provenance based access control model;provenance based security policies;provenance entity types","","0","","17","","","24-26 Sept. 2014","","IEEE","IEEE Conference Publications"
"An efficient algorithm for density-balanced partitioning in distributed pagerank","S. Sangamuang; P. Boonma; J. Natwichai","Data Engineering and Network Technology Laboratory Department of Computer Engineering Chiang Mai University Chiang Mai 50200, Thailand","Ninth International Conference on Digital Information Management (ICDIM 2014)","20141218","2014","","","118","123","Google's PageRank is the most notable approach for web search ranking. In general, web pages are represented by web-link graph; a web-page is represented by a node, and a link between two pages is represented by an edge. In particular, it is not efficient to perform PageRank of a large web-link graph in a single computer. Distributed systems, such as P2P, are viable choices to address such limitation. In P2P-based PageRank, each computational peer contains a partial web-link graph, i.e., a sub-graph of the global web-link graph, and its PageRank is computed locally. The convergence time of a PageRank calculation is affected by the web-link graph density, i.e., the ratio of the number of edges to the number of nodes, such that if a web-link graph has high density, it will take longer time to converge. As the execution time to compute the P2P-based web ranking is influenced by the execution time of the slowest peer to compute the local ranking, the density-balanced local web-link graph partitioning can be highly desirable. This paper addresses a density-balanced partitioning problem and proposes an efficient algorithm for the problem. The experiment results show that the proposed algorithm can effectively partition graph into density-balanced sub with an acceptable cost.","","Electronic:978-1-4799-5421-6; POD:978-1-4799-5422-3","10.1109/ICDIM.2014.6991418","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6991418","","Approximation algorithms;Binary trees;Computers;Particle separators;Partitioning algorithms;Peer-to-peer computing;Web pages","Web sites;information retrieval;peer-to-peer computing;search engines","Google PageRank;P2P-based PageRank;PageRank calculation;Web pages;Web search ranking;density-balanced local Web-link graph partitioning;density-balanced partitioning problem;distributed pagerank;distributed systems;large Web-link graph","","0","","12","","","Sept. 29 2014-Oct. 1 2014","","IEEE","IEEE Conference Publications"
"Visitors localization in cultural heritages for experience enhancement","R. Giuliano; M. Marzovillo; F. Mazzenga; M. Vari","Department of Innovative Technologies and Processes, Guglielmo Marconi University, Via Plinio 44, 00193 Rome, Italy","2014 Euro Med Telco Conference (EMTC)","20141229","2014","","","1","6","Dedicated apps for smartphone or tablet can enrich the experience of visitors in cultural heritage environment. Visitors could automatically receive multimedia information such as audio, hypertext photos and video supported by accurate indoor localization in the visited places. In this paper we propose an effective architecture for a low cost system providing indoor localization information. The system consists of RFID (Radio Frequency Identification) tags deployed in the area to support indoor localization, the visitor equipment (e.g. the smartphone) provided by the developed app, a small portable RFID reader and a remote management centre able to retrieve the contextual information. A testbed has been setup to provide the effectiveness of the proposed system. Results show that the system requirements are respected enabling the visitor to enhance his/her experience.","","CD-ROM:978-8-8872-3721-4; Electronic:978-8-8872-3720-7; POD:978-1-4799-5596-1","10.1109/EMTC.2014.6996649","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996649","","Accelerometers;Cultural differences;Multimedia communication;Radiofrequency identification;Streaming media;Switches;XML","graphical user interfaces;history;human factors;information retrieval;radiofrequency identification;smart phones","RFID tags;audios;contextual information retrieval;cultural heritage environment;hypertext photos;indoor localization information;low-cost system;multimedia information;portable RFID reader;radio frequency identification tags;remote management centre;smart phone applications;tablet applications;videos;visitor equipment;visitor experience enhancement;visitor localization","","1","","24","","","12-15 Nov. 2014","","IEEE","IEEE Conference Publications"
"Personal learning environment framework for a self-help Maqam-based search system","R. Othman; M. F. Noordin; T. M. Tengku Sembok; A. M. Z. M. Kheder; E. N. Mior Ibrahim; S. H. Kazi; S. R. Abdul Karim","Kulliyyah of ICT, International Islamic University Malaysia","The 5th International Conference on Information and Communication Technology for The Muslim World (ICT4M)","20150126","2014","","","1","6","In this era of technology and ICT learners and tutors are moving towards self-learning and self-set goals. Personal Learning Environments (PLE) are systems that enable learners to manage their own learning process for attaining a specific goal. PLEs interconnect and integrate both formal and informal learning with self-regulated learning in tertiary education. Most of the PLEs are adopted at universities for learners to collaborate and share their ideas and content using different tools, communities and services. In this paper, we have created the architecture for a Personal Learning Environment specifically for the Self-help Maqam-based Search System built upon the seven maqams outlined in Al-Ghazali's Minhajul Abidin. This self-help system assesses a person's current state (maqam) of spirituality with an opportunity to learn ways of improving one's spiritual state as defined in Minhajul Abidin. Thus, we developed a pedagogical framework by combining self-regulated learning and metacognitive knowledge first, so that the framework could be used to map the Personal Learning Environment (PLE) into the Self-help Maqam-based Search System mentioned above.","","Electronic:978-1-4799-6242-6; POD:978-1-4799-6243-3","10.1109/ICT4M.2014.7020621","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020621","Imam Al-Ghazali (RA);Minhajul Abidin;metacognitive knowledge;personal learning environment;self-help systems;self-regulated learning","Communities;Educational institutions;Presses;Process control;Prototypes;Social network services","computer aided instruction;further education;information retrieval","ICT;Minhajul Abidin;PLE;formal learning;informal learning;learners;metacognitive knowledge;personal learning environment framework;self-help Maqam-based search system;self-learning goals;self-regulated learning;self-set goals;spiritual state;tertiary education;tutors","","0","","22","","","17-18 Nov. 2014","","IEEE","IEEE Conference Publications"
"A diabetes diagnostic domain ontology for CBR system from the conceptual model of SNOMED CT","S. El-Sappagh; S. El-Masri; M. Elmogy; A. M. Riad","Faculty of Computers & Information, Minia University, Egypt","2014 International Conference on Engineering and Technology (ICET)","20150122","2014","","","1","7","Domain ontology or background knowledge facilitates the implementation of knowledge intensive case based reasoning systems. Based on the conceptual model of SNOMED CT, this research will propose a methodology for building ontology for diabetes concepts based on most recent diabetes clinical guidelines. All concepts, descriptions and relationships related to diabetes mellitus diagnosis including laboratory tests will be collected from SNOMED CT standard terminology. The resulting subset will be converted into OWL ontology. The resulting ontology can be used in case based reasoning system for semantic retrieval and other tasks.","","Electronic:978-1-4799-5807-8; POD:978-1-4799-5808-5","10.1109/ICEngTechnol.2014.7016783","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016783","Case based reasoning;SNOMED CT;clinical decision support system;diabetes mellitus diagnosis and knowledge management;ontology","Biochemistry;Diabetes;Diseases;Medical diagnostic imaging;Ontologies;Prognostics and health management;Terminology","case-based reasoning;diseases;electronic health records;information retrieval;medical diagnostic computing;ontologies (artificial intelligence);patient diagnosis","CBR system;OWL ontology;SNOMED CT standard terminology;diabetes diagnostic domain ontology;diabetes mellitus diagnosis;knowledge intensive case-based reasoning systems;semantic retrieval","","5","","40","","","19-20 April 2014","","IEEE","IEEE Conference Publications"
"Entity resolution using inferred relationships and behavior","J. Mugan; R. Chari; L. Hitt; E. McDermid; M. Sowell; Y. Qu; T. Coffman","21CT, Inc., Austin, Texas, USA","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","555","560","We present a method for entity resolution that infers relationships between observed identities and uses those relationships to aid in mapping identities to underlying entities. We also introduce the idea of using graphlets for entity resolution. Graphlets are collections of small graphs that can be used to characterize the “role” of a node in a graph. The idea is that graphlets can provide a richer set of features to characterize identities. We validate our method on standard author datasets, and we further evaluate our method using data collected from Twitter. We find that inferred relationships and graphlets are useful for entity resolution.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004273","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004273","","Biological cells;Facebook;Genetic algorithms;Optimization;Orbits;Twitter;Vectors","data mining;graphs;information retrieval;social networking (online)","Twitter;entity resolution;graphlets;inferred relationships;small graphs;standard author datasets","","1","","13","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Automatic news extraction system for Indian online news papers","Y. W. Wanjari; V. D. Mohod; D. B. Gaikwad; S. N. Deshmukh","Dept. of CS & IT, Dr. Babasaheb Ambedkar Marathwada University, Aurangabad(M.H.), INDIA","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","20150122","2014","","","1","6","Now a day's Web technology is getting an emergence importance in day to day life! Everyone is familiar with surfing the Web, uploading personal or important data on the Web, sharing data with friends on social communities. Indian online news Web papers are producing more data every day on the Web. There are various technologies & researches which are focusing on the extraction of relevant information from large web data storage. But still there is requirement of availability of automatic annotation of this extracted information into a systematic way so to be processed further for various purposes. This paper provides an effective approach for the Indian online newspapers which extract contents from news web databases. First, we browse Web pages as per the input URL given by user. Next, we generate a DOM tree of the news Web page data. And at last, we not only identify and extract valuable news from the Indian news web pages but also remove noisy data. Moreover, in this paper we proposed the novel approach for extract data from online Indian newspapers written in the many popular languages such as Marathi, Hindi, Tamil, Gujarati, Kannada, Oriya, Telugu, Punjabi, etc. Experimental results can be analysed much easily on this domain. This proposed system is very first attempt in an India for news extraction from online web pages available in various Indian language.","","Electronic:978-1-4799-6896-1; POD:978-1-4799-6897-8","10.1109/ICRITO.2014.7014750","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014750","DOM tree generation;Data extraction;Tag pattern generation;Wrapper","Browsers;Data mining;Databases;HTML;Manuals;Web pages","Internet;electronic publishing;hypermedia markup languages;information retrieval;natural language processing","DOM tree;Gujarati language;HTML;Hindi language;Indian language;Indian online news Web papers;Kannada language;Marathi language;Oriya language;Punjabi language;Tamil language;Telugu language;URL;Web pages browsing;Web surfing;Web technology;automatic annotation;automatic news extraction system;contents extraction;data extraction;data sharing;document object model;information extraction;large Web data storage;news Web databases;noisy data removal;personal data uploading;social communities","","0","","21","","","8-10 Oct. 2014","","IEEE","IEEE Conference Publications"
"Gathering Alumni Information from a Web Social Network","G. R. Gonçalves; A. A. Ferreira; G. T. d. Assis; A. I. Tavares","Dept. de Comput., Univ. Fed. de Ouro Preto, Ouro Preto, Brazil","2014 9th Latin American Web Congress","20150108","2014","","","100","108","An undergraduate program must prepare its students for the major needs of the labor market. One of the main ways to identify what are the demands to be met is creating a manner to manage information of its alumni. This consists of gathering data from program's alumni and finding out what are their main areas of employment on the labor market or which are their main fields of research in the academy. Usually, this data is obtained through available forms on the Web or forwarded by mail or email, however, these methods, in addition to being laborious, do not present good feedback from the alumni. Thus, this work proposes a novel method to help teaching staffs of undergraduate programs to gather information on the desired population of alumni, semi-automatically, on the Web. Overall, by using a few alumni pages as an initial set of sample pages, the proposed method was capable of gathering information concerning a number of alumni twice as bigger than adopted conventional methods.","","CD-ROM:978-1-4799-6952-4; Electronic:978-1-4799-6953-1; POD:978-1-4799-6954-8","10.1109/LAWeb.2014.17","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000177","alumni information management;focused crawling;linkedin;search engine;web social network","Data mining;Educational institutions;LinkedIn;Sociology;Statistics","Internet;information management;information retrieval;search engines;social networking (online)","Web social network;alumni information gathering;alumni pages;data gathering;employment;information management;labor market;teaching staffs;undergraduate program","","1","","23","","","22-24 Oct. 2014","","IEEE","IEEE Conference Publications"
"The Automatic Classification Research to Medical Imaging Disease Based on Naïve Nayesion Algorithm","N. Luoyong; H. Jiaming; H. Hongbo; H. Bishi; X. Zhe","Hangzhou 1st Hosp., Hangzhou, China","2014 Tenth International Conference on Computational Intelligence and Security","20150122","2014","","","308","311","This paper uses k-means algorithm to cluster medical imaging disease, and applies Naïve Bayesian classifier to realize the automatic classification of diseases, then achieves hierarchical retrieval of medical imaging examination report information by using. Net technology. It improves the retrieval efficiency and provides convenience for doctors to carry out clinical research.","","CD-ROM:978-1-4799-7433-7; Electronic:978-1-4799-7434-4; POD:978-1-4799-7435-1","10.1109/CIS.2014.71","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016906","Naïve Bayesian;automatic classification;hierarchical retrieval;k-means algorithm","Bayes methods;Biomedical imaging;Classification algorithms;Clustering algorithms;Diseases","Bayes methods;biomedical imaging;classification;diseases;information retrieval;medical information systems;pattern clustering","-means algorithm;.Net technology;Naïve Nayesion algorithm;automatic disease classification research;hierarchical medical imaging examination report information retrieval;medical imaging disease cluster;retrieval efficiency improvement","","0","","5","","","15-16 Nov. 2014","","IEEE","IEEE Conference Publications"
"Weighted Category Matching Algorithm in Sensor Cloud for rapid retrieval","S. Ramachandran; S. Shakena Grace; S. Sarjun Beevi","Department of Information Technology, Anna University, Chennai, Tamil Nadu, INDIA","2014 International Conference on Recent Trends in Information Technology","20141229","2014","","","1","5","Sensor-cloud is a relatively new interdisciplinary domain that combines the fields of Wireless Sensor Networks (WSN) and Cloud Computing. The major issue needs to be addressed in Sensor-Cloud is storage and retrieval of data. Data security is preserved by encrypting the sensor data before storing it into the cloud. On analysis of existing methods, there are two different issues, First issue; Encrypted Document matching is performed based on their plaintext keywords which is not suitable for sensor data and Second issue, subscribers are categorized based on their predicates. So, the storage space required is more. To overcome these issues, the paper proposes a new indexing structure which is suitable for sensor data and a new algorithm called Weighted Category Matching Algorithm (WCMA) for quick retrieval. On comparing with existing methods, the proposed WCMA algorithm achieves a better document retrieval and results are discussed in implementation details.","","DVD:978-1-4799-4990-8; Electronic:978-1-4799-4989-2; POD:978-1-4799-7868-7","10.1109/ICRTIT.2014.6996148","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996148","Cloud storage;Ranking;WSN;event matching;publisher/subscriber","Cloud computing;Cryptography;Indexes;Keyword search;Servers;Subscriptions;Wireless sensor networks","cloud computing;cryptography;information retrieval;sensor fusion;string matching","WCMA;WSN;cloud computing;data security;data storage;document retrieval;encrypted document matching;sensor cloud;weighted category matching algorithm;wireless sensor networks","","0","","14","","","10-12 April 2014","","IEEE","IEEE Conference Publications"
"Sorting Topic Specific Web Pages Based on Ontology Knowledge","Q. Song; J. Liu; M. Ni; L. Chen; J. Shen","Coll. of Inf. Eng., Shanghai Maritime Univ., Shanghai, China","2014 Tenth International Conference on Intelligent Information Hiding and Multimedia Signal Processing","20141229","2014","","","880","883","Due to the independence of the domain knowledge, general search engine has ""theme drift"" problem in the search of domain knowledge. Topic specific search engine offers a faster, and more accurate web resources retrieval service, and a good web page sorting algorithm can improve user experience. In this paper, we proposed a new sorting algorithm for topic specific search engine based on ontology knowledge, by using the hierarchy relationship of ontology knowledge to judge the importance of the words in web pages, and sorted web pages with the obtained cumulated score. To verify the effectiveness of the proposed algorithm, we set up an evaluation system and compared it with traditional sorting algorithm based on TF-IDF. The experimental results show that our algorithm is more accurate and have better user experience.","","Electronic:978-1-4799-5390-5; POD:978-1-4799-5391-2","10.1109/IIH-MSP.2014.222","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998468","Topic specific search engine;ontology;topic relevance;web page sorting algorithm","Algorithm design and analysis;Classification algorithms;Clustering algorithms;Ontologies;Search engines;Sorting;Web pages","Web sites;information retrieval;ontologies (artificial intelligence);sorting","TF-IDF;Web resource retrieval service;hierarchy relationship;ontology knowledge;theme drift problem;topic specific Web page sorting algorithm;topic specific search engine;user experience","","1","","7","","","27-29 Aug. 2014","","IEEE","IEEE Conference Publications"
"MMSE: A generalized coherence measure for identifying linear patterns","S. Chen; J. Liu; T. Zeng","School of Computer, Wuhan University, Wuhan 430072, China","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","20150115","2014","","","489","492","Biclustering is very useful in bioinformatics, information retrieval, electoral data analysis, dimension reduction, and so on. It is usually formulated as an optimization problem of searching maximal subsets of rows and columns satisfying some coherence criteria. The found submatrices are called as biclusters. There are several quantitative coherence measurements for linear patterns proposed. However, they are either lack the capability of properly evaluating all subtypes of linear patterns, or sensitive to the noise. In this paper, we propose a coherence measurement for the general linear patterns, the minimal mean squared error (MMSE). By using MMSE, the biclustering algorithms are expected to identify all types of linear patterns, including shifting (additive), scaling (multiplicative), and the general linear (the mixed form of shifting and scaling) ones, if they are presented in the data. Our comparative experimental results have highlighted that MMSE can actually help to identify significant general linear biclusters in artificial and real application data.","","Electronic:978-1-4799-5669-2; POD:978-1-4799-5670-8","10.1109/BIBM.2014.6999206","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999206","biclustering;coherence measurement;gene expression;linear pattern","Bioinformatics;Biomedical measurement;Coherence;Gene expression;Measurement uncertainty;Vectors","bioinformatics;data analysis;genetics;genomics;information retrieval;mean square error methods;optimisation;pattern clustering;search problems","MMSE;artificial application data;biclustering algorithms;bioinformatics;coherence criteria;columns;dimension reduction;electoral data analysis;general linear biclusters;general linear patterns;generalized coherence measure;information retrieval;linear pattern identification;linear pattern subtypes;minimal mean squared error;optimization problem;quantitative coherence measurements;real application data;rows;searching maximal subsets;submatrices","","0","","23","","","2-5 Nov. 2014","","IEEE","IEEE Conference Publications"
"A Rendering-Based Method for Selecting the Main Data Region in Web Pages","L. N. L. Figueiredo; A. A. Ferreira; G. T. d. Assis","Dept. de Comput., Univ. Fed. de Ouro Preto, Ouro Preto, Brazil","2014 9th Latin American Web Congress","20150108","2014","","","24","32","Extracting data from web pages is an important task for several applications, such as comparison shopping and data mining. Much of that data is provided by search result pages, in which each result, called search result record, represents a record from a database. One of the most important steps for extracting such records is identifying, among different data regions from a page, one that contains the records to be extracted. An incorrect identification of this region may lead to an incorrect extraction of the search result records. In this paper, we propose a simple but efficient method that generates path expression to select the main data region from a given page, based on the rendering area information of its elements. The generated path expression may be used by wrappers for extracting the search result records and its data units, reducing its complexity and increasing its accuracy. Experimental results using web pages from several domains show that the method is highly effective.","","CD-ROM:978-1-4799-6952-4; Electronic:978-1-4799-6953-1; POD:978-1-4799-6954-8","10.1109/LAWeb.2014.14","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000168","main data region;path expression;rendering information;visual information;wrapper","Accuracy;Browsers;Data mining;HTML;Rendering (computer graphics);Visualization;Web pages","Internet;information retrieval;rendering (computer graphics)","Web pages;comparison shopping;data extraction;data mining;data region;data units;path expression;rendering area information;rendering-based method;search result record","","0","","25","","","22-24 Oct. 2014","","IEEE","IEEE Conference Publications"
"Mobile application for guiding tourist activities: tourist assistant - TAIS","A. Smirnov; A. Kashevnik; N. Shilov; N. Teslya; A. Shabaev","SPIIRAS, St.Petersburg, Russia","Proceedings of 16th Conference of Open Innovations Association FRUCT","20150108","2014","","","95","100","The paper presents category classification of mobile travel applications accessible at the moment for tourists in application stores for most popular mobile operation systems (Android and iOS). The most interesting category is ""Travel Guides"" that combines ""Information Resources"" and ""Location-Based Services"" category. Authors propose application ""Tourist assistant - TAIS"" that is related to ""Travel Guides"" category and recommends the tourist attractions around. Information about attractions is extracted from different internet sources.","2305-7254;23057254","Electronic:978-5-7577-0489-0; POD:978-1-4799-6226-6","10.1109/FRUCT.2014.7000931","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000931","","Context;Databases;Estimation;Information services;Internet;Mobile communication;Smart phones","Android (operating system);Internet;classification;iOS (operating system);information retrieval;mobile computing;travel industry","Android;Internet sources;TAIS;application stores;attraction information extraction;category classification;iOS;information resources;location-based services;mobile application;mobile operation systems;mobile travel applications;tourist activities;tourist assistant;tourist attractions;travel guides","","5","","20","","","27-31 Oct. 2014","","IEEE","IEEE Conference Publications"
"SpatialPDP: A personalized differentially private mechanism for range counting queries over spatial databases","N. Niknami; M. Abadi; F. Deldar","Faculty of Electrical and Computer Engineering, Tarbiat Modares University Tehran, Iran","2014 4th International Conference on Computer and Knowledge Engineering (ICCKE)","20141222","2014","","","709","715","Spatial databases are rapidly growing due to the large amount of geometric data obtained from geographic information systems, geomarketing, traffic control, and so on. Range counting queries are among the most common queries over spatial databases. They allow us to describe a region in a geometric space and then retrieve some statistics about geometric objects falling within it. Quadtree-based spatial indices are usually used by spatial databases to speed up range counting queries. Privacy protection is a major concern when answering these queries. The reason is that an adversary observing changes in query answers could induce the presence or absence of a particular geometric object in a spatial database. Differential privacy addresses this problem by guaranteeing that the presence or absence of a geometric object has little effect on the query answers. However, the existing differentially private algorithms for spatial databases ignore the fact that different subregions of a geometric space may require different amounts of privacy protection. This causes that the same privacy budget is considered for different subregions, resulting in a significant increase in error measure for subregions with low privacy protection requirements or a major reduction in privacy measure for subregions with high privacy protection requirements. In this paper, we address these shortcomings by presenting SpatialPDP, a personalized differentially private mechanism for range counting queries over spatial databases. It uses a so-called personalized geometric budgeting strategy to allocate different privacy budgets to subregions with different privacy protection requirements. Our experimental results show that SpatialPDP can achieve a reasonable trade-off between error measure and differential privacy, in accordance with the privacy requirements of different subregions.","","Electronic:978-1-4799-5487-2; POD:978-1-4799-5488-9","10.1109/ICCKE.2014.6993414","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993414","differential privacy;personalized geometric budgeting;personalized privacy;range counting query;spatial database","Data privacy;Measurement uncertainty;Noise;Noise measurement;Privacy;Spatial databases","data privacy;quadtrees;question answering (information retrieval);visual databases","SpatialPDP;differential privacy;error measure;geographic information system;geomarketing;geometric data;geometric objects;personalized differentially private mechanism;personalized geometric budgeting strategy;privacy budget;privacy protection requirement;private algorithms;quadtree-based spatial indices;query answers;range counting query;spatial databases;traffic control","","0","","14","","","29-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Using software architectures to retrieve interaction information in eLearning environments","J. C. Benito; F. J. García-Peñalvo; R. Therón; C. Maderuelo; J. S. Pérez-Blanco; H. Zazo; A. Martín-Suárez","GRIAL Research Group, Department of Computers and Automatics, University of Salamanca, Spain","2014 International Symposium on Computers in Education (SIIE)","20150122","2014","","","117","120","This research paper presents a software architecture based on services and deployed in a cloud environment that retrieves, analyzes and presents information collected from a closed eLearning environment like the Virtual Worlds. This software architecture is able to gather the user interaction in the digital platform, organize the interaction data and to perform measurements, estimates and basic analysis of the data, in order to give information to the managers and teachers about usage indicators and the resolution degree of the goals that students need to achieve in these scenarios and learning systems. To test this idea, the paper describe the application of the analytics layer of this software architecture on a real case, so it is possible to understand how can help this kind of architectures in the detection of the achievement of the learning goals or in the knowledge discover about users usage within the learning environment.","","Electronic:978-1-4799-4428-6; POD:978-1-4799-4427-9","10.1109/SIIE.2014.7017715","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017715","Second Life;Software architecture;Virtual Worlds;cloud;eLearning;usage analysis;usage indicators","Computer architecture;Educational institutions;Electronic learning;Laboratories;Second Life;Software architecture;Software measurement","cloud computing;computer aided instruction;data analysis;human computer interaction;information analysis;information retrieval;software architecture","Second Life;Virtual Worlds;cloud environment;data analysis;data estimation;data measurement;digital platform;eLearning environments;interaction information retrieval;learning systems;resolution degree;software architectures;usage analysis;usage indicators;user interaction","","0","","12","","","12-14 Nov. 2014","","IEEE","IEEE Conference Publications"
"Diversification of web search results using post-retrieval clustering","S. Kumar; S. K. Jain; R. M. Sharma","Department of Computer Engineering National Institute of Technology Kurukshetra-136119","2014 International Conference on Computer and Communication Technology (ICCCT)","20150108","2014","","","1","6","Diversification of results in web search engines is a very attractive area for researchers now a days. Information retrieval techniques mainly focus on the relevance of the documents retrieved but these techniques often fail to satisfy each user. In this work, we present a coverage based diversification using post retrieval clustering. We model clusters corresponding to the query based on the features of the web pages such as web pages of similar features are to be in one cluster and web pages from dissimilar features are to be in different clusters. A query can retrieve relevant and diverse result set if all the results cover as many clusters as possible.","","Electronic:978-1-4799-6758-2; POD:978-1-4799-6759-9","10.1109/ICCCT.2014.7001460","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001460","Clustering;Cosine Similarity;Relevance;Result diversification","Algorithm design and analysis;Clustering algorithms;Feature extraction;Search engines;Vectors;Web pages;Web search","information retrieval;search engines","Web pages;Web search diversification;Web search engine;coverage based diversification;information retrieval;post-retrieval clustering","","0","","20","","","26-28 Sept. 2014","","IEEE","IEEE Conference Publications"
"Semantic repository for case-based reasoning in CBM services","P. Aarnio; I. Seilonen; M. Friman","Aalto University, ELEC, Espoo, Finland","Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)","20150112","2014","","","1","8","Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.","1946-0740;19460740","Electronic:978-1-4799-4845-1; POD:978-1-4799-4844-4","10.1109/ETFA.2014.7005195","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005195","case-based reasoning;condition-based maintenance;information integration;ontology;semantic repository;sparql","Cognition;Knowledge based systems;Maintenance engineering;Ontologies;Resource description framework;Semantics;Valves","case-based reasoning;inference mechanisms;information retrieval;ontologies (artificial intelligence);semantic Web","CBM Services;CBM decision process;CBR;SPARQL queries;case-based reasoning;condition-based maintenance;data access;industrial control valves;inference engine supporting ontology;semantic Web technologies;semantic repository","","0","","36","","","16-19 Sept. 2014","","IEEE","IEEE Conference Publications"
"Temporal bipartite projection and link prediction for online social networks","T. Wu; S. H. Yu; W. Liao; C. S. Chang","Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan, R.O.C.","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","52","59","In user-item networks, the link prediction problem has received considerable attentions and has many applications (e.g., recommender systems, ranking item popularity) in recent years. Many previous works commonly fail to utilize the dynamic nature of the networks. This paper focuses on dealing with the temporal information and proposes an algorithm to cope with the link prediction problem on bipartite networks. We describe a temporal bipartite projection method that yields a projected item graph, called the temporal projection graph (TPG). Based on the TPG, we propose a scoring function called STEP (Score for TEmporal Prediction) for each user-item pair. STEP leverages the historical behaviors of individual users and the social aggregated behaviors learned from the TPG for the link prediction problem. Furthermore, we use TPG and PageRank to rank the popularity of items. To validate our algorithms, we perform various experiments by using the DBLP author-conference dataset, the Flickr dataset and the Delicious dataset. We show that our results of the link prediction problem for new links are substantially better than other temporal link prediction algorithms. We also find the item rankings generated by our approach match very well with that existed in the real world.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004444","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004444","PageRank;bipartite network;bipartite network projection;link prediction","Computational complexity;Computational modeling;Educational institutions;History;Social network services;Training","graph theory;information retrieval;social networking (online)","DBLP author-conference dataset;Delicious dataset;Flickr dataset;PageRank;STEP;TPG;bipartite network;link prediction;online social network;projected item graph;score for temporal prediction;scoring function;temporal bipartite projection;temporal projection graph;user-item network","","0","","19","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Semantic Similarity Based Approach for Automatic Evaluation of Free Text Answers Using Link Grammar","U. K. Chakraborty; R. Gurung; S. Roy","Dept. of Comput. Sci. & Eng., Sikkim Manipal Inst. of Technol., Manipal, India","2014 IEEE Sixth International Conference on Technology for Education","20150115","2014","","","218","221","Most approaches towards automatic evaluation of free text answers are keyword centric. Though keywords essentially reflect and represent the primary concept coverage of an answer, they are incomplete without the associated texts. The words occurring before and after the keywords bring out the true meaning. The work presented in this paper proposes a semantic similarity based approach for evaluation of free text answers where both keywords and the associated text contribute to the score. A link grammar based approach is used to extract the keywords from the answer and through the process of identifying and extracting the relational expressions for a keyword the model automatically evaluates a learners' free text response SA to a given question Q with respect to a model answer MA. The score of the automated system have a high co-relation with human evaluator score as can be found from the reported results.","","Electronic:978-1-4799-6489-5; POD:978-1-4799-6490-1","10.1109/T4E.2014.57","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009576","evaluation;keyword;learners' response;link grammar;relational expression;semantic similarity","Computers;Couplings;Grammar;Hardware;Operating systems;Semantics;Speech","context-free grammars;natural language processing;question answering (information retrieval)","automatic evaluation;free text answers;free text response;human evaluator score;link grammar-based approach;semantic similarity","","0","","3","","","18-21 Dec. 2014","","IEEE","IEEE Conference Publications"
"Knowledge representation for lung cancer patients' prognosis","L. Minelli; M. C. d'Ornellas; A. T. Winck","Laborat&#x00F3;rio de Computa&#x00E7;&#x00E3;o Aplicada (LaCA) Centro de Tecnologia (CT) Universidade Federal de Santa Maria (UFSM) 97105-900 Santa Maria-RS, Brasil","2014 IEEE 16th International Conference on e-Health Networking, Applications and Services (Healthcom)","20150108","2014","","","358","363","The gradual increase of cancer cases worldwide has been posing a need on the use of computing resources to accurately retrieve the information recorded in databases. One can highlight the retrieved information importance from a specialist in order to better evaluate pathological response and predict the cancer patient prognosis. This paper presents a way to represent knowledge of cancer registries with emphasis on prognosis. It makes use data mining techniques to find patterns in data stored for patient's lifetime in similar situations. The work is focused on the generation of association rules to find patterns on these registries in order to measure the patient prognosis and drive healthcare experts conclusions. A validation against international oncology organizations and health publications was also made to ensure data and work reliability.","","Electronic:978-1-4799-6644-8; POD:978-1-4799-6645-5; USB:978-1-4799-6643-1","10.1109/HealthCom.2014.7001869","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7001869","Data mining;Health Informatics;Knowledge Representation;Lung Cancer Prognosis","Association rules;Cancer;Indexes;Lungs;Organizations;Prognostics and health management","cancer;data mining;health care;information retrieval;knowledge representation;lung;medical information systems","association rule generation;data mining;health publications;healthcare;information retrieval;international oncology organizations;knowledge representation;lung cancer patient prognosis;pathological response evaluation","","0","","11","","","15-18 Oct. 2014","","IEEE","IEEE Conference Publications"
"A fast and memory-efficient algorithm for learning and retrieval of phenotypic dynamics in multivariate cohort time series","S. Nemati; M. M. Ghassemi","Harvard School of Engineering and Applied Sciences, 33 Oxford Street, Cambridge, MA 02138, USA. Correspondence","2014 IEEE International Conference on Big Data (Big Data)","20150108","2014","","","41","44","Robust navigation and mining of physiologic time series databases often requires finding similar temporal patterns of physiological responses. Detection of these complex physiological patterns not only enables demarcation of important clinical events but can also elucidate hidden dynamical structures that may be suggestive of disease processes. Some specific examples where this physiological signal search may be useful include real-time detection of cardiac arrhythmias, sleep staging or detection of seizure onset. In all these cases, being able to identify a cohort of patients who exhibit similar physiological dynamics could be useful in prognosis and informing treatment strategies. However, pattern recognition for physiological time series is complicated by changes between operating regimes and measurement artifacts. Here we briefly describe an approach we have developed for distributed identification of dynamical patterns in physiological time series using a switching linear dynamical system (SLDS). We present a fast and memory-efficient algorithm for learning and retrieval of phenotypic dynamics in large clinical time series databases. Through simulation we show that the proposed algorithm is at least an order of magnitude faster that the state of the art, and provide encouraging preliminary results based on real recordings of vital sign time series from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-II) database.","","Electronic:978-1-4799-5666-1; POD:978-1-4799-5667-8","10.1109/BigData.2014.7004391","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004391","","Biomedical monitoring;Covariance matrices;Heuristic algorithms;Inference algorithms;Superluminescent diodes;Switches;Time series analysis","bioinformatics;data mining;information retrieval;learning (artificial intelligence);physiology;time series","MIMIC-II database;SLDS;data mining;machine learning;medical informatics;multiparameter intelligent monitoring in intensive care;multivariate cohort time series;phenotypic dynamics retrieval;physiological pattern detection;physiological time series;switching linear dynamical system;vital sign time series","","2","","9","","","27-30 Oct. 2014","","IEEE","IEEE Conference Publications"
"Modeling of Data Provenance on Space and Time Based on OPM","K. Jiehua; X. Guoyan","Comput. & Inf. Coll., HoHai Univ., Nanjing, China","2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications","20150119","2014","","","790","794","During its life cycle, data has to go through different stages, from generation, storage, query, various processing, to deletion or archiving. Meanwhile, all these evolutions can be recorded by data provenance, which can be used for data deduction and credibility verification. Starting from the status of data application and processing, current problems exist in data management have been raised in this paper. Thus, in order to solve the problems, provenance information on space and time was discussed. Then, based on Open Provenance Model (Referred to as OPM), analysis and research were conducted, a more complete description model has been given, which contains time and space provenance information. With this model, it will be more convenient for data query and credibility verification in the following data management.","2324-898X;2324898X","Electronic:978-1-4799-6513-7; POD:978-1-4799-6514-4","10.1109/TrustCom.2014.104","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011328","Annotation Model;Data Provenance;Spatial;Time","Analytical models;Cognition;Computational modeling;Data models;Process control;Semantics;Water conservation","data handling;information retrieval systems;query processing","OPM;archiving;credibility verification;data application;data deduction;data management;data provenance;data query;description model;modeling;open provenance model;space provenance information","","0","","7","","","24-26 Sept. 2014","","IEEE","IEEE Conference Publications"
"A Proposal for a Reference Architecture for Long-Term Archiving, Preservation, and Retrieval of Big Data","P. Viana; L. Sato","Lab. of Archit. & High Performance Comput. (LAHPC), Univ. de Sao Paulo, Sao Paulo, Brazil","2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications","20150119","2014","","","622","629","The volume of data stored in corporate data centers has been growing at a rate of 35% to 50% per year [1]. The exponential growth in data volume leads to some challenges from the technical, operational and financial perspectives. Along with this increase in the data volume the demand for preservation (or retention) of such data has also increased due to government regulations. The convergence of these two trends (growth of data volume and increased demand for preservation) implies that storage systems must support the preservation of data for very long periods of time. Several studies address the archiving, preservation and retrieval of structured data. To the best of our knowledge, there couldn't be found reference architectures specifically focused on the archiving, preservation and retrieval of both unstructured and structured data. Our research goal is to propose a reference architecture for the long term archiving, preservation and retrieval of Big Data.","2324-898X;2324898X","Electronic:978-1-4799-6513-7; POD:978-1-4799-6514-4","10.1109/TrustCom.2014.80","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011304","E-Discovery;archiving;big data;data preservation;reference architecture;retrieval","Conferences;Privacy;Security","Big Data;computer centres;information retrieval;storage management","corporate data centers;data storage systems;data volume;government regulations;long-term Big Data archiving;long-term Big Data preservation;long-term Big Data retrieval;reference architecture;structured data;structured data archiving;structured data preservation","","0","","39","","","24-26 Sept. 2014","","IEEE","IEEE Conference Publications"
